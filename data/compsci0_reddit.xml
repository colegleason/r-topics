<searchresult><compsci /><document><title>Algorithm Development / Pre-coding Advice</title><url>http://www.reddit.com/r/compsci/comments/195375/algorithm_development_precoding_advice/</url><snippet>Hi everyone. I come to you today asking for some algorithm development or pre-coding advice. I'm a second semester sophmore studying Computer Science at a local state college. 

I've always considered myself to be an above average programmer (in comparison to most of the other sophmores studying Comp Sci at my college). One of my current computer science courses is "Algorithms / Data Structures". The big change to this from my previous courses, is that we used to be given similar code examples for the programming assignments and then asked to create and complete the assignment. In this class, the professor tells us what the program should accomplish and we have to develop the algorithm for the problem and implement it in the program. We are no longer being taught any code or programming. In fact, we are able to use whatever programming language we choose to complete the assignments.

My real question is do you have any advice for conceptually setting up an algorithm and program? Tips or ways of approaching that work for you? To be honest, I have in the past thought about the programming assignment beforehand, and then pretty much jumped right into creating the code. I don't mean to say that I haven't had to develop algorithms...but these seem to be a lot more complex. 

Hope I didn't make that confusing. Any suggestions or advice would be greatly appreciated. Thanks guys!

Edit: An example of one of our programming assignments (that I already finished after much trouble) is followed...



"A circular array-based implementation of a queue was discussed in the class. In this implementation a variable count was used to count number of items present in the queue and the condition for both queue-empty and queue-full was FRONT IS ONE SLOT AHEAD OF BACK. A faster implementation declares "MAX + 1" locations for the array items, but use only MAX of them for queue items. One array location is sacrificed and makes FRONT the index of the location before the front of the queue. The conditions for queue-full and queue-empty are "Front==(Back+1)%(max+1) ---- Queue-full"
"Front==Back ------ Queue-empty"
Now, no COUNT variable is require anymore. Write a program implementing this circular array-based approach, which is more efficient and faster."  Developing/creating/designing algorithms can always be a daunting task. My first bit of advice would be to get to know the problem you are addressing, and attempt to break it down into its atomic steps. Keep working it through on paper/whiteboard and anywhere your steps have any ambiguity, break it down further. Once you have a good understanding, and are comfortable with the problem, you can then address designing the algorithm. You have a number of factors to keep in mind. Do you want to make an algorithm that is fast but memory hungry, one that is slower but memory efficient or a trade off of the two? Once you've designed the algorithm in pseudocode, flow charts or state transition diagrams (which ever you prefer, or better yet a mix of them to get the best representation of your algorithm), try running it though by hand with different data. Does it actually work, and do what you expect. Try it with easy data and try it with data you expect to break it. The important thing to determine is, does it do what you expect it to do, in the way you expect it to do?

You shouldn't even touch a programming language when designing an algorithm. Don't even consider them. Your designing an algorithm that should be able to be picked by John Smith and implemented in his language of choice.

The one thing I can not stress enough is don't design an algorithm in code. It will lead to headaches as your fighting the programming language and making constant alterations leading to messy code. Programming is an implementation tool, not a design tool.

Hope some of this helps.  This is spot on. It's vital to do as much as possible before starting to code. I was the same way when I hit my algorithms class -- just give me the task, and I'll start churning out code. I learned, though, that even if you're a solid programmer you still don't think in code as well as you think in English (or whatever your native language may be). By divorcing "what do I want to do" from "how do I make it happen", fixing problems in your code becomes much simpler. No longer are you trying to figure out if bugs in your code are **either** due to your design being wrong **or** if there a problem in your implementation -- you've already spent plenty of time making sure the design is correct before even opening an editor, so it **must** be a problem with implementation. It's hard to notice when you're doing both at the same time, but implementation bugs are much easier to spot than design bugs, so you end up saving yourself a lot of time and heartache. This is spot on. It's vital to do as much as possible before starting to code. I was the same way when I hit my algorithms class -- just give me the task, and I'll start churning out code. I learned, though, that even if you're a solid programmer you still don't think in code as well as you think in English (or whatever your native language may be). By divorcing "what do I want to do" from "how do I make it happen", fixing problems in your code becomes much simpler. No longer are you trying to figure out if bugs in your code are **either** due to your design being wrong **or** if there a problem in your implementation -- you've already spent plenty of time making sure the design is correct before even opening an editor, so it **must** be a problem with implementation. It's hard to notice when you're doing both at the same time, but implementation bugs are much easier to spot than design bugs, so you end up saving yourself a lot of time and heartache. Developing/creating/designing algorithms can always be a daunting task. My first bit of advice would be to get to know the problem you are addressing, and attempt to break it down into its atomic steps. Keep working it through on paper/whiteboard and anywhere your steps have any ambiguity, break it down further. Once you have a good understanding, and are comfortable with the problem, you can then address designing the algorithm. You have a number of factors to keep in mind. Do you want to make an algorithm that is fast but memory hungry, one that is slower but memory efficient or a trade off of the two? Once you've designed the algorithm in pseudocode, flow charts or state transition diagrams (which ever you prefer, or better yet a mix of them to get the best representation of your algorithm), try running it though by hand with different data. Does it actually work, and do what you expect. Try it with easy data and try it with data you expect to break it. The important thing to determine is, does it do what you expect it to do, in the way you expect it to do?

You shouldn't even touch a programming language when designing an algorithm. Don't even consider them. Your designing an algorithm that should be able to be picked by John Smith and implemented in his language of choice.

The one thing I can not stress enough is don't design an algorithm in code. It will lead to headaches as your fighting the programming language and making constant alterations leading to messy code. Programming is an implementation tool, not a design tool.

Hope some of this helps.        A useful method that I've just been taught in my Algorithms/Data Structures module is State Diagrams.

http://en.wikipedia.org/wiki/State_diagram

If you can translate the problem into steps and work out how the algorithm gets to that state, it makes it a lot easier - if diagrams help you. Much obliged. A useful method that I've just been taught in my Algorithms/Data Structures module is State Diagrams.

http://en.wikipedia.org/wiki/State_diagram

If you can translate the problem into steps and work out how the algorithm gets to that state, it makes it a lot easier - if diagrams help you.     </snippet></document><document><title>"River" detection in text</title><url>http://dsp.stackexchange.com/questions/374/river-detection-in-text</url><snippet>  &#8230;and now for a tool that intentionally produces rivers without altering the text. Next step: typographical watermarks.  Every time I feel pretty smart, somebody links to a stack exchange i've never been to, with a topic i don't know, and makes me look like an idiot.  </snippet></document><document><title>The Fastest and Shortest Algorithm for All Well-Defined Problems</title><url>http://www.hutter1.net/ai/pfastprg.htm</url><snippet>  </snippet></document><document><title>Programmers, software engineers, compsci students, what are you working on right now? </title><url>http://www.reddit.com/r/compsci/comments/192d3s/programmers_software_engineers_compsci_students/</url><snippet>Are you working? Programming for fun or programming for school? I'm writing some simple classes in C++ for a 2nd year class right now.    Right now, I'm going skiing.  Yay.  

For school, I'm working on some machine learning nonsense for program synthesis. Honestly, I would much rather work on something elegant and theoretical instead, but c'est la vie. 

For myself, I'm working on a modified tree edit distance algorithm for a "semantic version control" system. It's a slightly more restricted notion of edit distance that fits well with the rest of the system.  I've also been working on my own programming language, but that's on hiatus and has some fundamental problems. 

Finally, I'm trying to learn some category theory and universal algebra because that's the coolest area of CS/math I've found, by far. If only more people shared that view at my university :(.   Programming for a class, building a compiler for a subset of the C language. Programming the code for the compiler in Java. My partner and I are working on generating code right now.
 Cool, I am too writing a compiler for class in Java, also at the code generation stage. 

The fun part is they didn't give us the specs for the IR language, so we need to learn it by trial and error. FUN!    Just turned in an assignment about an hour ago&#8212;they gave us freshman year's DNA sequence alignment homework again, except this time in parallel. After a bunch of correct but far-slower-than-sequential attempts, I've solved this problem a grand total of 9 different ways this week, and, fun as it was, I'm pretty thrilled to be done with it.  I'm trying to solve one of my favorite board games for the two-player case. Turns out there are a lot of possible play-scenarios, though less than the number of particles in the universe, which I worried about for a while.  I may have to rent server time. Which board game? Its called Agricola. That's a great game :) I've actually spent a fair amount of time thinking about AI for it before. I believe my final thoughts were to go close to alpha-beta pruning, and write heuristics (also a little difficult for the game), that evaluate game states and slice off branches that are going poorly. This helps reduce the number of nodes as many moves are clearly sub-optimal and can be removed the from search tree. It wouldn't *solve* the game though, which you indicated was your goal. Not to mention more than 2 players... Good luck :)  I'm writing Java code for the army by day, and making a game engine with C and Haskell by night. Which parts are in Haskell? Any particular reason for that choice of language? I'm writing Java code for the army by day, and making a game engine with C and Haskell by night.  Making a program (in VB.NET [I know...shut up]) that mimics some of the functionality that we lost when my company transitioned from one ERP to another. It's a pretty simple program that involves creating and printing documents that help transportation employees that do dispatch work to effectively organize and manage their region's freight and owner operators. The company that created our ERP charges positively insane per hour rates when it comes to introducing new features, so it's on me.

Sorry for being vague; have a NDA to comply with. There is nothing wrong with VB.NET if it can do what you need. That is true and I agree completely. I sometimes forget that Reddit, by and large, doesn't participate in the whole programming-language-superiority battles that tend to happen on some other forums. That is true and I agree completely. I sometimes forget that Reddit, by and large, doesn't participate in the whole programming-language-superiority battles that tend to happen on some other forums. Making a program (in VB.NET [I know...shut up]) that mimics some of the functionality that we lost when my company transitioned from one ERP to another. It's a pretty simple program that involves creating and printing documents that help transportation employees that do dispatch work to effectively organize and manage their region's freight and owner operators. The company that created our ERP charges positively insane per hour rates when it comes to introducing new features, so it's on me.

Sorry for being vague; have a NDA to comply with.  I made a stoplight to monitor our builds out of Ruby, a Raspberry PI, and a cheap relay module. It includes a server daemon that supports tcp and UNIX sockets, a CLI interface, and a web interface built with Sinatra. I probably went a bit overboard. Where did you source the stoplight from? I've been thinking of doing the same thing.    Trying to figure out ways to optimise the rasterisation of large bitmaps onto a HTML5 canvas so that it doesn't take forever on slower Android tablets. I wish the default browser was Chrome and not this underperformant Webkit :/ Chrome uses WebKit. Trying to figure out ways to optimise the rasterisation of large bitmaps onto a HTML5 canvas so that it doesn't take forever on slower Android tablets. I wish the default browser was Chrome and not this underperformant Webkit :/  Job is mostly Java, nothing special, although the last week has been fighting with our build system and rpmbuild, which I now hate.

For school I've only got the occasional assignment from my cryptography class, nothing much there, it's mostly theory.  I've also got my senior project which is writing the code for a cheap HUD for a car that connects to the OBDII port on your car via bluetooth and is run off a raspberry pi.  That's kinda fun.

For personal stuff I'm just working through Software Testing, Theoretical Computer Science, and Parallel Computing on udacity right now, and have the AI course lined up next, while trying to simultaneously learn Haskell and C Cool, you sound as busy as me ha ha   Working on a Android project for the school, another Android application for shiggles, and a Java compiler for a class.  Finished off a prototype for a big data migration at work today. Numbers look good so I'll do the rest of the code base next week.

This weekend I'll be putting together a prototype for a high performance serialization framework. Moving things to an HDFS?

Im working on better accessing large weblog data. Finished off a prototype for a big data migration at work today. Numbers look good so I'll do the rest of the code base next week.

This weekend I'll be putting together a prototype for a high performance serialization framework. what company do you work for?
           Building a Java framework to support a suite of Selenium tests to be run on a Selenium Grid multithreaded (allowing for a single test to be run in all browsers we support simultaneously). Additionally, integration with Jenkins so that the test suite runs nightly. And finally, setting it up so that the QA guys will be able to use the firefox IDE to generate the test and, using a custom template, export it into JUnit that will sit nicely alongside the already existing tests.       Just finished a client's website (Php and WolfCMS)

Soccer match prediction is a private one.

(both not my 9-5) Curious about the soccer match prediction. How does that work? I assume some number crunching of historic data?  There are several ways about it.
Some assume simple rules such as if home_team has won last 3, then home team wins.
One of my approaches is to use historical data to verify these heuristics.

Neural networks are another approach, relying heavily on historical data.
One very good approach broadly classified the match first (Easy win team1, easy win team2 , hard contest), and then used another network based on that.

I've read an article on baseball prediction based on vectors, this might also be interesting.

All in all, if you want to learn about statistics and machine learning, sports prediction is a huge field for experimentation.

TL;DR 
I am not even interested in soccer

PS: I apologize for the lack of links, on my phone right now.     So many cool projects. I'm just about to write some C to create a few random patterns  - I'm learning OpenGL. Mostly just for fun.          android development for HCI 2 I really dislike android dev     I'm an electrical engineer trying to thresh out my programming skills. Currently, I'm working on a life simulator program that simulates the lives of pre-tribal humans as they evolve over a map. Currently, simple behaviours (eat, sleep, fuck) are coded, but I'm working on implementing genetics and tribalism in future versions.

My end goal is to have a program that simulates the organisms from pre-tribal to a point where they evolve to work together to a common end.

Python, by the way. Beyond that, I'm probably coding in C for the AVR family of microcontrollers.  2nd year CS major (almost... switching from Electrical &amp;amp; Computer Engineering, but I'm not technically a CS major until mid semester once midterm grades come through).

Algorithms homework: build a thesaurus type tool according to the specs (SML + parallel sequences/sets/tables library). I already did the graph theory parts (generating shortest paths), so the rest should be fairly straightforward. Hoping to finish that tonight, so I can start working on...

Systems homework: "cache lab" - code a cache simulator (in C), write then optimize some code using your simulator to reduce cache misses.

My fun projects: earlier today, pushed a few memory leak fixing commits to a cool collage-type program I wrote over winter break, and re-added the kd-tree functionality (as opposed to linear nearest-neighbor search). Seemed to dramatically improve runtime, but the results aren't AS good (though still pretty cool)...

I've been writing a little 2d graphics/physics thing in OCaml with SDL bindings, but I'm not so sure I want to continue that. I really like OCaml, but I don't have a clear idea of where this little app would be heading. Perhaps I'll think of another cool OCaml project over spring break.

I've been considering diving into Kivy to learn more about app programming, but I also should learn C++ for a (likely) internship this summer (although I'm sure I'd still be fairly useful as a C programmer, I figure I'd be MORE useful as a C++ programmer too). Question: what made you switch from ECEN to CS? I'm a computer engineering student wondering what difference it would make to switch to CS. So, a few things...

First of all, I didn't have any programming courses in high school whatsoever. Like, my school didn't even offer any. But it did offer a digital electronics course, which I took. I liked that enough to apply for ECE. By that point, I had pretty much forgotten my love of programming on my calculator, and I actually went to some summer camps for computer science back in middle school.

So, I took some CS and some ECE courses, as all ECE majors are required to do. I found that I really really liked the CS courses (I got to write a video game in Python, then I got to write a VM in C and some other crazy shit in SML :D), and I was pretty ambivalent on the ECE courses. What really made me decide to switch was when I took a math course for ECE and had a terrible time in it... Continuous math just doesn't work well for me. I first thought I could double major, but that seemed pretty challenging to do. I ended up choosing to switch so I could avoid taking Physics E&amp;amp;M, Signals, and Analog (but I did get to take a Digital Logic class where we learned how processors work from the ground up... that was actually really cool).

It's also useful to note that here at CMU, the computer science department is just a notch above... pretty much any other department (aside from stuff like art and drama which are orthogonal). But that translates to the professors and TAs being really dedicated and motivated, which is why I had a great experience in my two semesters of CS courses (I didn't take any first semester), and made me decide to switch.  I'm working on a universal parser that can get, decode, and display data from (almost) any website that has an xml/json feed.  Can you elaborate on your end-goal? Video meta-search engine               **For my job**: Programming in Java (nothing crazy, migration to java 7, spring framework, rabbit-mq) and some Python Scripting 


**At home**: having a lot of fun doing misc. stuff at home, today I'll teach some friends how to code in python and pygame (I haven't read anything about pygame, but it doesn't sound difficult :p). This last two weeks I was programming simple programs in python. Also, udacity's parallel progamming class


**College**: I should be doing a final project for my computers architecture class, but the teacher didn't approve it yet.. (I proposed like 3 different projects and asked for advice on which to pick, one is a 64-bit mini-kernel with FS, the other some hashing functions in C, asm and maybe cuda, and the last one I tought was about an arbitrary arithmetic library in C). Probably I will start this week with one of them. I should be studying for my finals that are this week and the next one &amp;gt;.&amp;lt; You have finals this week? What area of the world to you attend college? You have finals this week? What area of the world to you attend college?    I'm a senior in high school and am trying to write a small 2D RPG in Java for the fun of it. I started teaching myself Java a while back in small bit, but I'm sure I've developed tons of bad habits by now. I'm taking an AP CS course online which they use to teach Java but that is seriously much more of a joke than I thought. I can't wait to go to college!                                 Currently disassembling C code and deciphering the assembly with gdb by stepping through each instruction. Getting input strings that'll defuse this 'bomb' I was given for my Machine Organization class. It's eh, I'd rather just be coding.                                 </snippet></document><document><title>Rasberry .PI with iPython as shell</title><url>http://www.pythononwheels.org/post/blog</url><snippet> </snippet></document><document><title>Methods of Proof &#8212; Contrapositive</title><url>http://jeremykun.com/2013/02/22/methods-of-proof-contrapositive/</url><snippet /></document><document><title>Looking for a good MSc in Sofware Engineering / Informatics program</title><url>http://www.reddit.com/r/compsci/comments/193qk2/looking_for_a_good_msc_in_sofware_engineering/</url><snippet>Hi reddit, 

looking for some help from the community. I have a BSc in Business Computing and have worked for a while after graduating in a software company doing mainly web development (GWT, jQuery, JavaScript, etc). As it happens with all of us i guess, i got tired of doing the same stuff over and over again and would like to do a master's degree in Software Engineering or Informatics (don't think I will be admitted or be able to pull through a full-blown Computer Science program). 

Could maybe some of you suggest worthwhile schools to consider? Right now I am applying to UK universities (King's and Imperial Colleges), as the program of study there looks pretty attractive and it's only 1 year duration. However whenever I look at any other schools the material they teach seems somewhat out-of-date: no teaching of modern languages (scala or python), no mobile development modules, and all the old stuff like methodologies and c++. 

Maybe you are currently at one of the schools where the curriculum is more or less invigorating? i leave in Europe now, but would be willing to relocate (if i find a scholarship to finance the studies outside). 

Many thanks   </snippet></document><document><title>IPoXP: Internet Protocol over Xylophone Players</title><url>http://www.stuartgeiger.com/papers/ipoxp-archive.pdf</url><snippet>  No video demonstration?  OK, I couldn't find it in my skimming of the article:  Once person A hits a note, how does the receiving computer or person B decode the note?  Is there a microphone doing the detection, or does person B have to recognize the note and replay the message back in to his xylophone which the arduino's vibration sensor detects? &amp;gt; The Arduino is also connected to a series of
&amp;gt; LED lights on the local xylophone keys, and a series of
&amp;gt; piezo vibration sensors on the remote xylophone's keys.
&amp;gt; When the Arduino receives a character from the computer
&amp;gt; &#8211; which is one byte or eight bits in length &#8211; it decodes the
&amp;gt; eight 1s and 0s into musical notes, and then flashes the LED
&amp;gt; corresponding to the musical note (Figure 4). When the
&amp;gt; Arduino senses that a key has been hit on the remote
&amp;gt; xylophone, it encodes the musical notes into ASCII
&amp;gt; characters, which it sends to the local computer.

When you're playing the xylophone, it's almost like you're hitting keys on the other computer's keyboard.

Node A displays LEDs above the xylophone which is connected to Node B's piezos.

Each xylophone-operator "device" is half-duplex. </snippet></document><document><title>Nazi Tackles The Steiner Tree Problem</title><url>http://corner.mimuw.edu.pl/?p=354</url><snippet>     </snippet></document><document><title>Loading the memory of a OISC with terms of the Collatz Function </title><url>http://www.reddit.com/r/compsci/comments/190czh/loading_the_memory_of_a_oisc_with_terms_of_the/</url><snippet>I decided to simulate an OISC [1] today, where each instruction is a decrement mod N and goto, where N is the number of addressable locations in memory. 

I loaded the memory up with the first N terms of the Collatz function [2]. Every N cycles I printed the state of the memory to a row in this image, I thought the results were interesting and wanted to share.

http://imgur.com/gOhRsjw [edited to point to imgur]


[1] http://en.wikipedia.org/wiki/One_instruction_set_computer
[2] http://en.wikipedia.org/wiki/Collatz_conjecture  Really cool idea and the result looks interesting, but could you explain the image a bit more? In one row, there is a pixel for each byte or word and the intensity is the value of it, so black is 0 and white is 255 (or whatever if its 1 pixel per word..)? </snippet></document><document><title>TCMalloc : Thread-Caching Malloc</title><url>http://goog-perftools.sourceforge.net/doc/tcmalloc.html</url><snippet /></document><document><title>Can someone explain cache page coloring to me?</title><url>http://www.reddit.com/r/compsci/comments/18yz65/can_someone_explain_cache_page_coloring_to_me/</url><snippet>From what I've read, it sounds like cache page coloring acts like n-way set associativity using some portion of the memory address as an index, managed from software (the OS?) but I don't understand how. Can someone explain it, or point me to and explanation (bonus points if it includes pictures!)?

Thanks in advance.  It's on [Wikipedia](http://en.wikipedia.org/wiki/Cache_coloring).

You already know that each set in the n-way set associative cache can hold n cache lines. Cache coloring assigns memory a "color" based on what set of the cache the memory maps into. Memory accesses to n memory addresses that are in different cache lines with the same color fill the cache set. New memory accesses to uncached pages of the same color will cause cache evictions.

In page coloring, an entire virtual memory page is given a color, which generally forms a set of cache sets that have the same color, but the idea is the same.

I have a pretty good image of this saved somewhere. Is this explanation clear, or should I try and find it? Thanks for the description!  I did google this and read the Wikipedia page on cache coloring.  Your addition helps clear it up a bit.  If it wouldn't be too much trouble, the image would probably be super helpful to me.

I think I am visualizing the organization wrong.  If our cache has m sets of size n, don't we get fewer collisions if we spread nearby memory lines out over multiple sets rather than packing them into the same set?  If n is 4 and m is 256, I can put 4 sequential reads into a single color, whereas I could read 4X256 lines into cache without coloring.

Where am I going wrong?  http://lmgtfy.com/?q=cache+coloring&amp;amp;l=1</snippet></document><document><title>Best Papers vs. Top Cited Papers</title><url>http://arnetminer.org/conferencebestpapers</url><snippet>  I think it's worth mentioning that some papers are selected as best paper because they essentially close a whole subfield of research by providing the optimal solution. Such papers are typically less cited because people don't work on that problem any more.

A case in point: [ICFP](http://www.icfpconference.org/) has a [most influential paper award](http://www.sigplan.org/Awards/Conferences/ICFP/Description) and [in 2011](http://www.sigplan.org/Awards/Conferences/ICFP/Main) it was given to a paper with relatively few citations, precisely because it closed a field of research. Now, a most influential paper award is slightly different from a best paper award but I think my main point still carries over.   This should be hosted as a static page. The server is *so* slow and is sending partial response. It took forever for me for the page to load and when it was finally loaded the layout suddenly was all fucked up. I don't know what technology is behind it but clearly it doesn't handle a small charge very well (imagine if this has been from /r/programming or HN or Slashdot, those three are way bigger than /r/compsci in terms of people following them).  Would be nice if it included some computer architecture conferences too. </snippet></document><document><title>Interesting Writings by Leslie Lamport</title><url>http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html</url><snippet>   </snippet></document><document><title>Research Questions</title><url>http://www.reddit.com/r/compsci/comments/18xexq/research_questions/</url><snippet>Hello, I am a junior Computer Science major and I will be starting research soon, the issue is choosing a topic that interests me. I am very interested in dynamic programming, optimization algorithms, graph algorithms, and grammars. The issue is that I'm not entirely sure how to start a research project. I have a list of interests, thats great, but now what? How do I choose something to apply these to? How do I choose a topic that I'm capable of making advances in that hasn't already been worked on by others? How did you guys decide on the specifications of your undergraduate research, any advice is appreciated. Thanks!


I appreciate all the wonderful advice, I will talk with a few of my professors about the research that they did during their graduate work after I read up on them a bit more.  Much of the successful undergrad research around here comes with folks first attaching to research projects that are ongoing, to learn the ways people think and talk about doing research in a given area and to get a feel for what kinds of things the folks around them think might be interesting to do next.  

Then for the second project, they take on more of a defining and leadership role.  Having people around you who are interested in similar things can be useful for getting help and energy; people who are interested in roughly the same things can give you a lot of useful guidance as you work toward specific projects and topics.

Then the trick becomes thinking about which project to start with, and that's easier: you look around the lab webpages of folks in CS and related departments to see projects and people that sound interesting, talking with friends and/or grad students who are doing research, and making appointments to talk to profs who are doing research that sounds interesting to you and that you can imagine contributing towards. Not all profs are able or willing to work with undergrads, but an email from someone who has done a little homework and expresses serious interest in research will often catch people's attention.

It's not the only way to do it, but I've seen a lot of undergrads get experience and publications that way here; I've been a prof for 6.5 years now and co-authored with 
somewhere around 30 undergrads (often in teams), and several other profs also have real track records of doing fun, interesting research with undergrads. Much of the successful undergrad research around here comes with folks first attaching to research projects that are ongoing, to learn the ways people think and talk about doing research in a given area and to get a feel for what kinds of things the folks around them think might be interesting to do next.  

Then for the second project, they take on more of a defining and leadership role.  Having people around you who are interested in similar things can be useful for getting help and energy; people who are interested in roughly the same things can give you a lot of useful guidance as you work toward specific projects and topics.

Then the trick becomes thinking about which project to start with, and that's easier: you look around the lab webpages of folks in CS and related departments to see projects and people that sound interesting, talking with friends and/or grad students who are doing research, and making appointments to talk to profs who are doing research that sounds interesting to you and that you can imagine contributing towards. Not all profs are able or willing to work with undergrads, but an email from someone who has done a little homework and expresses serious interest in research will often catch people's attention.

It's not the only way to do it, but I've seen a lot of undergrads get experience and publications that way here; I've been a prof for 6.5 years now and co-authored with 
somewhere around 30 undergrads (often in teams), and several other profs also have real track records of doing fun, interesting research with undergrads.  The answers so far are: "Attach yourself to someone who is doing research." That's the right answer.

Alongside that, you should start reading about what you're interested. From what you've said it sounds like you could enjoy Real-Time Systems, Distributed Systems, or Bioalgorithms.

Because I'm a distributed systems junkie here's a list of some seminal distributed system papers from Lamport: 

http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html I agree that you should be reading about what you're interested. Although I would suggest that OP start looking at recent conferences that are in the field of their interest. ACM and USENIX seem to have the bulk of the important CS conferences. Agreed. As an undergrad I thought it would be prudent for OP to get solid on background information, but I suppose that doesn't help him decide his research interests.

Once he does decide what he wants to do, however, knowing the background information, history, and important milestones is really helpful.  </snippet></document><document><title>Undergrad Research Ideas (specifically crypto)</title><url>http://www.reddit.com/r/compsci/comments/18wss6/undergrad_research_ideas_specifically_crypto/</url><snippet>My liberal arts college offers the opportunity to participate in summer research with a professor, and I'm hoping to do so. However, I'm a freshman and will have taken only one CS class, so I need to make it clear that I have the knowledge, interest, and initiative necessary. I recently spoke with a professor, and he agreed to consider different ideas I presented.

I am familiar with a wide range of technologies, but I am still a beginner in all of them. The area that I have delved most deeply into is cryptography, and I'm familiar with hash functions, the basic workings of RSA, the use of hashes for password storage, different types of hash functions, their potential weaknesses, nonces, proof of work, etc. I also understand the very basics of algorithmic analysis and can write in Python and Java. I did a little Haskell over last summer, and while I don't remember many specifics it was very eye-opening and fun. More than anything else, I've found the Bitcoin network extremely interesting, and I understand a good deal of its protocol, how it works, and its potential significance. I would also be very open to anything involving the internet; I know less about its workings but I find it very intriguing.

Anyway, I'm obviously going to be working on this myself, but I'd love some suggestions about specific research topics that would be interesting and challenging, particularly those involving crypto and potentially Bitcoin.  I have a few months before the program actually starts, so I could learn some necessary skills before the research actually began. I'm open to most ideas, and I'll be brainstorming myself (and posting my ideas here for comments and criticism). I would like to be able to get back to him ASAP, so I'll be moving quickly.

Thanks so much for reading, let me know any thoughts you have.   Are you more interested in cryptographic primitives themselves (e.g., AES, RSA) or in building secure systems that assume secure primitives (e.g., TLS, BitCoin)?  The process of designing and analyzing secure primitives is pretty hard and generally requires a lot of complex math.  Building systems on top of existing primitives is a bit easier to get into (but still has its own gotchas of course).

Are there constraints for what makes a suitable "research topic"? Thanks for the reply. I would definitely prefer to focus on secure systems assuming secure primitives, considering that I have no background in number theory. I'm definitely planning on reading up on some mathematical cryptography theory over the next few months, though.

Anything that I could spend two months investigating, theorizing about, and test and that could be published about afterward is a valid research topic. The Bitcoin wikipedia page mentions [Namecoin DNS](http://en.wikipedia.org/wiki/Bitcoin#Applications), which sounds like an interesting variant.  Since you're interested in Bitcoin and proof-of-work systems, perhaps try to brainstorm some more ways to apply the same system to other problems?

Sorry, maybe that's too generic of a suggestion to be useful. :/ Thanks for the reply. I would definitely prefer to focus on secure systems assuming secure primitives, considering that I have no background in number theory. I'm definitely planning on reading up on some mathematical cryptography theory over the next few months, though.

Anything that I could spend two months investigating, theorizing about, and test and that could be published about afterward is a valid research topic.  Man, cryptography is not my area, but I know (and share this reddit account with) some people who do research it. In the meantime, if I were you, I'd do what I've always done when going deeper into a field that I know the basics of: find a conference, journal or publication specialized in that field; find the publication archive; read lots of stuff from years past.

The reasoning behind this is that you're starting just now, and becoming knowledgeable of top-of-the-hill research from 10, 15 years ago might be good for you, and even give you ideas into things to look after. Bitcoin hasn't been around that long, so I reckon you could start from the very literal beginning of the existing documented research...  Look into different homomorphic encryption schemes. How do they compare in efficiency of executing the homomorphic functions that are permitted? What limitations exist for these functions? This is a hot-topic nowadays with Cloud computing being the next big thing.   Hey!
I'm a math undergrad student and I've been looking into cryptography as well (and anything around Haskell or Lisp would be a bonus). Most of my programming experience comes from C (and more recently Mathematica and Matlab) and I really enjoy when I can use math to do some CS. I'm not sure what your research works during summertime but if you want to discuss about cryptography before then with someone who's background is mostly math, I think it could be quite fun! I was thinking maybe implement some kind of encrypted chat system would be a pretty interesting project. 

Feel free to PM me if you want to talk!</snippet></document><document><title>Question on Granularity of Transactions in Intel's Haswell TSX</title><url>http://www.reddit.com/r/compsci/comments/18u60y/question_on_granularity_of_transactions_in_intels/</url><snippet>With the introduction of [transactional memory in Haswell](http://software.intel.com/en-us/blogs/2012/02/07/transactional-synchronization-in-haswell), I am curious what the granularity of memory touched by a transaction is seen as.

From my understanding, the hardware support Intel is adding simply adds more state per cache line representing which thread made the modification to the line, and therefore if two transactions modify the same line but not the same bytes within that line, one of the transactions will fail. While this saves implementation costs of adding more state per byte for each line, it seems to also introduce false transaction failures.

Is this accurate? Does anyone have any idea if they have done anything more to increase the granularity of memory read/modified by transactions?

In general I would love to see some implementation details, though I know Intel is unlikely to release them.   Not sure about the technical details, but that doesn't sound so bad to me. Cache lines are usually about 64 bytes on modern x86 CPUs. The odds of false positives should be fairly small, and remember that a failed transaction can simply be retried until it succeeds.

Intel won't tell you the specifics of their implementation, but their manuals provide quite thorough details as to how specific instructions behave. The questions you asked should be answered when the updated manuals come out. &amp;gt;The odds of false positives should be fairly small

Well it would be application dependent. What if the application is working on a char array, where each transaction updates an element of the array? There would be 64 "logically" separate entities per 64 byte line, and multiple transactions could often hit on the same cache line and cause a lot of false transaction failures.

Perhaps this is a simplistic example that illustrates the worst case, but I'm sure you get the point. It would not surprise me that they decided this was very acceptable to avoid the extra overhead in tracking individual bytes. &amp;gt;The odds of false positives should be fairly small

Well it would be application dependent. What if the application is working on a char array, where each transaction updates an element of the array? There would be 64 "logically" separate entities per 64 byte line, and multiple transactions could often hit on the same cache line and cause a lot of false transaction failures.

Perhaps this is a simplistic example that illustrates the worst case, but I'm sure you get the point. It would not surprise me that they decided this was very acceptable to avoid the extra overhead in tracking individual bytes. I think you need to understand that collisions in that workload would be utter horror for multi-core systems already. A cache line needs to be privately owned by a core to be written to. If you constantly write to a cache line from multiple cores, you need to bounce it between them every time a new core gets a write instruction in. A simple "constantly store to a single memory location in a tight loop" slows the *way* down when you run it from multiple threads at the same time. I think I got a 100-time slowdown when I last tried that, but that was on core 2.

Understanding false sharing is important in data structure design. If you want to often update a location, you need to make sure that no other core ever wants to read from the same cache line. Even if this means putting in 63 bytes of padding.   [This blog post](http://software.intel.com/en-us/blogs/2012/02/07/coarse-grained-locks-and-transactional-synchronization-explained) seems to describe it really well with technical details.  It sounds very similar to the method that early versions of ethernet used, i.e. [Carrier Sense Multiple Access](http://en.wikipedia.org/wiki/Carrier_sense_multiple_access_with_collision_detection).  For that reason, I doubt the granularity is seen as any different at all by the transaction.  I'm sure they both see the same segment of cache but also notice when there's a conflict if they're both trying to access it concurrently.

It also seems that its very purpose is to prevent false transaction failures while still enabling use of coarse-grained locks.

Its important to note the use cases for this as well.  If you're using shared memory in your high performance application, you should already be using some sort of locking mechanism to prevent writing to the same segment.  This just makes what most people already do perform better without having to rewrite your data structures into some unmaintainable nightmare. There didn't seem to be many details there to be honest. For example if the hash map was implemented as an array of 32 bit ptrs (4 bytes), then there would be 16 entries per 64 byte cache line, and therefore this false failure effect could still be seen.

&amp;gt;If you're using shared memory in your high performance application, you should already be using some sort of locking mechanism to prevent writing to the same segment. 

The point of using transactional memory is so that you can avoid having to use explicit locks in a shared memory system. Their TM implementation is effectively setting a "lock" for each 64 byte cache line, and if there is a conflict then they enforce sequential execution for the conflicting transactions, whether through an explicit lock or other means. So you could still have this problem with "coarse-grained" locks as they mentioned, it's just down to a 64 byte granularity with much less explicit software overhead.</snippet></document><document><title>Need a language for language evaluation project.</title><url>http://www.reddit.com/r/compsci/comments/18v0cc/need_a_language_for_language_evaluation_project/</url><snippet>My class mate and I are doing a project in our Principles of Programming Languages class and we are to pick a language to investigate and present to the class. The language may not be: Java, C, C++, C#, Lisp/Scheme, Prolog, assembly/machine code, page layout languages, or any language that descends from Al Gol-like languages.

Wondering if anyone has any ideas for interesting languages we could evaluate?

Thanks in advance!!      My suggestions:

 - APL - programming language with a rather unique concept
 - [Erlang](http://www.erlang.org/) - a concurrency-oriented language
 - [io](http://iolanguage.org/) - lightweight object-oriented language and virtual machine
 - [Lua](http://www.lua.org/) - lightweight, embeddable scripting language     [brainfuck](http://en.wikipedia.org/wiki/Brainfuck)

     ++++++++++[&amp;gt;+++++++&amp;gt;++++++++++&amp;gt;+++&amp;gt;+&amp;lt;&amp;lt;&amp;lt;&amp;lt;-]&amp;gt;++.&amp;gt;+.+++++++..+++.&amp;gt;++.&amp;lt;&amp;lt;+++++++++++++++.&amp;gt;.+++.------.--------.&amp;gt;+.&amp;gt;.

^ That's "hello world"  Go (golang), Erlang, Python, Ruby, OCaml, Haskell, Javascript, Scala.  Go (golang), Erlang, Python, Ruby, OCaml, Haskell, Javascript, Scala.         Cobol was the *first* high-level language. Amazing Grace. Wasn't FORTRAN the first? Wasn't FORTRAN the first? Wasn't lisp the first?   Erlang, Scala, or Haskell would be good choices that would give you some good experience. These languages are being used more and more, and they have some good documentation on their design choices.           </snippet></document><document><title>How does the kernel act as a bridge when hardware is different?</title><url>http://www.reddit.com/r/compsci/comments/18s1s1/how_does_the_kernel_act_as_a_bridge_when_hardware/</url><snippet>So, the kernel is some code for managing and interfacing with hardware, and provides the same functionality for the OS/applications.  As I understand it, way back when in the beforetimes, the 'operating system' needed to be developed for each machine that was different hardware wise.

So what I'm wondering is what changed and how is it possible for the kernel to interface with any hardware that we stuff inside a box. I assume it has to do with drivers, but then again you usually don't need a driver for the cpu.  Also, drivers are built by the manufacturer and I would imagine they wouldn't cater to every single operating system under the sun, so I'm uncertain as to if thats the answer either.  Think of a kernel primarily as an API and the drivers as the implementation of that API.  The kernel defines a set of operations that make sense over a general class of hardware (video framebuffer, network interface, storage bus, etc.), and it's the job of each driver to perform those operations in the way that the underlying hardware understands.  Drivers aren't always written by the hardware vendor; sometimes they're written by the organization that maintains the operating system.

Back in the day, operating systems were tied closer to the computer for a couple of reasons.  The most striking was that, before the IBM System 360, individual computers tended to be bespoke works of engineering.  There was very little hardware commonality between even similar computers from the same company.  Secondly, computers themselves were very small (in terms of hardware capacity) compared to modern ones, so all the effort that goes into abstracting away the relatively minor differences in hardware would've been unbearable overhead.

There actually are drivers for the CPU!  CPU features vary from vendor to vendor and model to model, even among compatible families.  Sure, the kernel has to be a program that your particular CPU can run, but it can be very generic with drivers for interfacing with cryptographic acceleration or power management in ways that hide those differences from the programs you'll run on top of the kernel.  So, an "i686" kernel might run on every x86-compatible CPU made since the Pentium Pro, but have CPU-specific modules inside to deal with the new features in your Core i7 or AMD Bulldozer.

Abstract that another layer higher, and you can design an OS that works mostly the same on completely incompatible CPUs (i686 vs ARM, for instance).  The shared code will be the same at the source code level (in the case of C or C++, not assembly), but will compile to something that each particular CPU family knows how to run.

What makes modern operating systems possible is that there is so much commonality in the design of modern hardware.  Nearly every ethernet card works pretty-much the same: it plus into a commonly-available bus, has some configuration registers, and buffers packets between main memory and the network.  Nearly every sound card works similarly.  Nearly every storage card works similarly.  So it's possible to define huge swaths of kernel code in terms of these generalities, and the drivers have to deal with the details like "how does this particular storage card queue I/O commands?"

Also, buses!  Back in the day, it was common for there to be one I/O bus in the system: the bus between the CPU and main memory, and everything that plugged into the system had to speak this bus.  Sometimes this meant that peripherals "lived" in memory&#8212;that is, maybe some range of memory addresses actually "wrote" to buffer or configuration space on an I/O card, instead of bits in RAM.  Sometimes this meant that there was a separate address space just for I/O, but happened to use the same semantics as the processor's memory bus (this is how the original IBM PC worked up through the "local bus" shootout days of the 486&#8212;ISA is just the Intel 8086 memory bus!).

These older and simpler buses meant that adding a device to the system involved the user had telling the OS technical details about every device that got plugged in, because there was literally no way for the computer to know this information on its own.  On a DOS or Windows system, this meant running some setup program and changing some startup files (Google "SET BLASTER" for more information).  On other systems (VMS, some Unixes, most real-time OSes), this meant running a "sysgen" program that would rebuild the OS to recognize your new specific system configuration.  This was a hybrid of both methods: a system-specific OS image with a generic OS design.  Thankfully, those days are mostly behind us.

Fast forward today with all sorts of buses: PCI (incl. PCIe, Cardbus, and Thunderbolt), Firewire, USB, etc.  These all share the common features of device enumeration and configuration.  When you plug in a PCI card or USB device, the bus itself makes a note of it, arbitrates some I/O addresses for communication, and makes that information available for the OS to query.  That gets a LOT of hard work out of the way: you know that there's a device connected, you know what resources it's using, and you usually even know the vendor and device identifiers, so loading a driver (provided you have one) is just a simple matter of looking up the IDs and loading the code. Great post on the topic.  I'd also add that at least on the modern PC platform a lot of initial configuration and machine specific black magic happens through ACPI.  ACPI or it's tables abstracts a lot of very messy chipset specific features for monitoring things like lid switches, batteries and hardware power state transitions.  On top of that it's used to enumerate fixed chipset hardware (mainly legacy ISA devices at this point). Think of a kernel primarily as an API and the drivers as the implementation of that API.  The kernel defines a set of operations that make sense over a general class of hardware (video framebuffer, network interface, storage bus, etc.), and it's the job of each driver to perform those operations in the way that the underlying hardware understands.  Drivers aren't always written by the hardware vendor; sometimes they're written by the organization that maintains the operating system.

Back in the day, operating systems were tied closer to the computer for a couple of reasons.  The most striking was that, before the IBM System 360, individual computers tended to be bespoke works of engineering.  There was very little hardware commonality between even similar computers from the same company.  Secondly, computers themselves were very small (in terms of hardware capacity) compared to modern ones, so all the effort that goes into abstracting away the relatively minor differences in hardware would've been unbearable overhead.

There actually are drivers for the CPU!  CPU features vary from vendor to vendor and model to model, even among compatible families.  Sure, the kernel has to be a program that your particular CPU can run, but it can be very generic with drivers for interfacing with cryptographic acceleration or power management in ways that hide those differences from the programs you'll run on top of the kernel.  So, an "i686" kernel might run on every x86-compatible CPU made since the Pentium Pro, but have CPU-specific modules inside to deal with the new features in your Core i7 or AMD Bulldozer.

Abstract that another layer higher, and you can design an OS that works mostly the same on completely incompatible CPUs (i686 vs ARM, for instance).  The shared code will be the same at the source code level (in the case of C or C++, not assembly), but will compile to something that each particular CPU family knows how to run.

What makes modern operating systems possible is that there is so much commonality in the design of modern hardware.  Nearly every ethernet card works pretty-much the same: it plus into a commonly-available bus, has some configuration registers, and buffers packets between main memory and the network.  Nearly every sound card works similarly.  Nearly every storage card works similarly.  So it's possible to define huge swaths of kernel code in terms of these generalities, and the drivers have to deal with the details like "how does this particular storage card queue I/O commands?"

Also, buses!  Back in the day, it was common for there to be one I/O bus in the system: the bus between the CPU and main memory, and everything that plugged into the system had to speak this bus.  Sometimes this meant that peripherals "lived" in memory&#8212;that is, maybe some range of memory addresses actually "wrote" to buffer or configuration space on an I/O card, instead of bits in RAM.  Sometimes this meant that there was a separate address space just for I/O, but happened to use the same semantics as the processor's memory bus (this is how the original IBM PC worked up through the "local bus" shootout days of the 486&#8212;ISA is just the Intel 8086 memory bus!).

These older and simpler buses meant that adding a device to the system involved the user had telling the OS technical details about every device that got plugged in, because there was literally no way for the computer to know this information on its own.  On a DOS or Windows system, this meant running some setup program and changing some startup files (Google "SET BLASTER" for more information).  On other systems (VMS, some Unixes, most real-time OSes), this meant running a "sysgen" program that would rebuild the OS to recognize your new specific system configuration.  This was a hybrid of both methods: a system-specific OS image with a generic OS design.  Thankfully, those days are mostly behind us.

Fast forward today with all sorts of buses: PCI (incl. PCIe, Cardbus, and Thunderbolt), Firewire, USB, etc.  These all share the common features of device enumeration and configuration.  When you plug in a PCI card or USB device, the bus itself makes a note of it, arbitrates some I/O addresses for communication, and makes that information available for the OS to query.  That gets a LOT of hard work out of the way: you know that there's a device connected, you know what resources it's using, and you usually even know the vendor and device identifiers, so loading a driver (provided you have one) is just a simple matter of looking up the IDs and loading the code. Wow, thanks!  I didn't expect such a lengthy explanation.  Was a good read.  I guess that means there is a lot of interaction between the people who program the OS, the people who program the drivers, and the people who manufacture the hardware itself.  This pretty much explains why years back when I was learning linux for some graphic/sound cards I had to download drivers made by 3rd party people.  

The amount of effort required to make drivers and kernel work together is impressive. Think of a kernel primarily as an API and the drivers as the implementation of that API.  The kernel defines a set of operations that make sense over a general class of hardware (video framebuffer, network interface, storage bus, etc.), and it's the job of each driver to perform those operations in the way that the underlying hardware understands.  Drivers aren't always written by the hardware vendor; sometimes they're written by the organization that maintains the operating system.

Back in the day, operating systems were tied closer to the computer for a couple of reasons.  The most striking was that, before the IBM System 360, individual computers tended to be bespoke works of engineering.  There was very little hardware commonality between even similar computers from the same company.  Secondly, computers themselves were very small (in terms of hardware capacity) compared to modern ones, so all the effort that goes into abstracting away the relatively minor differences in hardware would've been unbearable overhead.

There actually are drivers for the CPU!  CPU features vary from vendor to vendor and model to model, even among compatible families.  Sure, the kernel has to be a program that your particular CPU can run, but it can be very generic with drivers for interfacing with cryptographic acceleration or power management in ways that hide those differences from the programs you'll run on top of the kernel.  So, an "i686" kernel might run on every x86-compatible CPU made since the Pentium Pro, but have CPU-specific modules inside to deal with the new features in your Core i7 or AMD Bulldozer.

Abstract that another layer higher, and you can design an OS that works mostly the same on completely incompatible CPUs (i686 vs ARM, for instance).  The shared code will be the same at the source code level (in the case of C or C++, not assembly), but will compile to something that each particular CPU family knows how to run.

What makes modern operating systems possible is that there is so much commonality in the design of modern hardware.  Nearly every ethernet card works pretty-much the same: it plus into a commonly-available bus, has some configuration registers, and buffers packets between main memory and the network.  Nearly every sound card works similarly.  Nearly every storage card works similarly.  So it's possible to define huge swaths of kernel code in terms of these generalities, and the drivers have to deal with the details like "how does this particular storage card queue I/O commands?"

Also, buses!  Back in the day, it was common for there to be one I/O bus in the system: the bus between the CPU and main memory, and everything that plugged into the system had to speak this bus.  Sometimes this meant that peripherals "lived" in memory&#8212;that is, maybe some range of memory addresses actually "wrote" to buffer or configuration space on an I/O card, instead of bits in RAM.  Sometimes this meant that there was a separate address space just for I/O, but happened to use the same semantics as the processor's memory bus (this is how the original IBM PC worked up through the "local bus" shootout days of the 486&#8212;ISA is just the Intel 8086 memory bus!).

These older and simpler buses meant that adding a device to the system involved the user had telling the OS technical details about every device that got plugged in, because there was literally no way for the computer to know this information on its own.  On a DOS or Windows system, this meant running some setup program and changing some startup files (Google "SET BLASTER" for more information).  On other systems (VMS, some Unixes, most real-time OSes), this meant running a "sysgen" program that would rebuild the OS to recognize your new specific system configuration.  This was a hybrid of both methods: a system-specific OS image with a generic OS design.  Thankfully, those days are mostly behind us.

Fast forward today with all sorts of buses: PCI (incl. PCIe, Cardbus, and Thunderbolt), Firewire, USB, etc.  These all share the common features of device enumeration and configuration.  When you plug in a PCI card or USB device, the bus itself makes a note of it, arbitrates some I/O addresses for communication, and makes that information available for the OS to query.  That gets a LOT of hard work out of the way: you know that there's a device connected, you know what resources it's using, and you usually even know the vendor and device identifiers, so loading a driver (provided you have one) is just a simple matter of looking up the IDs and loading the code. Think of a kernel primarily as an API and the drivers as the implementation of that API.  The kernel defines a set of operations that make sense over a general class of hardware (video framebuffer, network interface, storage bus, etc.), and it's the job of each driver to perform those operations in the way that the underlying hardware understands.  Drivers aren't always written by the hardware vendor; sometimes they're written by the organization that maintains the operating system.

Back in the day, operating systems were tied closer to the computer for a couple of reasons.  The most striking was that, before the IBM System 360, individual computers tended to be bespoke works of engineering.  There was very little hardware commonality between even similar computers from the same company.  Secondly, computers themselves were very small (in terms of hardware capacity) compared to modern ones, so all the effort that goes into abstracting away the relatively minor differences in hardware would've been unbearable overhead.

There actually are drivers for the CPU!  CPU features vary from vendor to vendor and model to model, even among compatible families.  Sure, the kernel has to be a program that your particular CPU can run, but it can be very generic with drivers for interfacing with cryptographic acceleration or power management in ways that hide those differences from the programs you'll run on top of the kernel.  So, an "i686" kernel might run on every x86-compatible CPU made since the Pentium Pro, but have CPU-specific modules inside to deal with the new features in your Core i7 or AMD Bulldozer.

Abstract that another layer higher, and you can design an OS that works mostly the same on completely incompatible CPUs (i686 vs ARM, for instance).  The shared code will be the same at the source code level (in the case of C or C++, not assembly), but will compile to something that each particular CPU family knows how to run.

What makes modern operating systems possible is that there is so much commonality in the design of modern hardware.  Nearly every ethernet card works pretty-much the same: it plus into a commonly-available bus, has some configuration registers, and buffers packets between main memory and the network.  Nearly every sound card works similarly.  Nearly every storage card works similarly.  So it's possible to define huge swaths of kernel code in terms of these generalities, and the drivers have to deal with the details like "how does this particular storage card queue I/O commands?"

Also, buses!  Back in the day, it was common for there to be one I/O bus in the system: the bus between the CPU and main memory, and everything that plugged into the system had to speak this bus.  Sometimes this meant that peripherals "lived" in memory&#8212;that is, maybe some range of memory addresses actually "wrote" to buffer or configuration space on an I/O card, instead of bits in RAM.  Sometimes this meant that there was a separate address space just for I/O, but happened to use the same semantics as the processor's memory bus (this is how the original IBM PC worked up through the "local bus" shootout days of the 486&#8212;ISA is just the Intel 8086 memory bus!).

These older and simpler buses meant that adding a device to the system involved the user had telling the OS technical details about every device that got plugged in, because there was literally no way for the computer to know this information on its own.  On a DOS or Windows system, this meant running some setup program and changing some startup files (Google "SET BLASTER" for more information).  On other systems (VMS, some Unixes, most real-time OSes), this meant running a "sysgen" program that would rebuild the OS to recognize your new specific system configuration.  This was a hybrid of both methods: a system-specific OS image with a generic OS design.  Thankfully, those days are mostly behind us.

Fast forward today with all sorts of buses: PCI (incl. PCIe, Cardbus, and Thunderbolt), Firewire, USB, etc.  These all share the common features of device enumeration and configuration.  When you plug in a PCI card or USB device, the bus itself makes a note of it, arbitrates some I/O addresses for communication, and makes that information available for the OS to query.  That gets a LOT of hard work out of the way: you know that there's a device connected, you know what resources it's using, and you usually even know the vendor and device identifiers, so loading a driver (provided you have one) is just a simple matter of looking up the IDs and loading the code. &amp;gt; Back in the day, operating systems were tied closer to the computer for a couple of reasons.

This, rather unfortunately, is equally true about most modern mobile devices also. There is no real reason why you shouldn't be able to load android on an iphone, but there you are... That has very little to do with OS design.  It's true ARM vendors are annoyingly tight lipped about hardware details so open drivers are an issue but there really isn't anything in an iphone linux doesn't already run on. That has very little to do with OS design.  It's true ARM vendors are annoyingly tight lipped about hardware details so open drivers are an issue but there really isn't anything in an iphone linux doesn't already run on. &amp;gt; Back in the day, operating systems were tied closer to the computer for a couple of reasons.

This, rather unfortunately, is equally true about most modern mobile devices also. There is no real reason why you shouldn't be able to load android on an iphone, but there you are... &amp;gt; Back in the day, operating systems were tied closer to the computer for a couple of reasons.

This, rather unfortunately, is equally true about most modern mobile devices also. There is no real reason why you shouldn't be able to load android on an iphone, but there you are... &amp;gt; Back in the day, operating systems were tied closer to the computer for a couple of reasons.

This, rather unfortunately, is equally true about most modern mobile devices also. There is no real reason why you shouldn't be able to load android on an iphone, but there you are...  modern kernels all have a Hardware Abstraction Layer (HAL) that provides a small footprint to be ported to each platform so the rest of the kernel is hardware-agnostic.  This is key since it allows as little of the kernel to be modified each time you want to run on a different type of cpu.  I'm not talking about supporting the newest chip from intel (although each time intel rolls out a new model little updates are needed here and there to support all the new features).  I'm talking about how apple runs the same kernel for OS X on both macbooks and ipads even though macbooks are x86 and ipads are arm.  Apple maintains 2 different non-portable parts of darwin (the os x kernel), one for intel and one for arm.  It does just enough work to hide the differences between intel and arm so the rest of darwin doesn't have to care what type of cpu its running on.  Allocating memory takes the same basic steps no matter what arch you're talking about, with a tiny bit of arch specific bits to fully get the job done.  Note apple still has to compile both a intel and arm version of darwin when they go to ship both iOS and OS X.

Your question is really three parts:

"way back when in the beforetimes, the 'operating system' needed to be developed for each machine that was different hardware wise"

Before high level system languages like C all OSs were written in assembly which meant they had to be rewritten each time you wanted to run on different hardware.  The C compiler for UNIX meant the vast majority of the OS didn't have to be rewritten.  It was a major shift in how operating systems were designed and built.  UNIX to this day remains the basic blueprint of most OSs (even windows get more and more unix like with each release).  C as a language was designed and developed solely to support the effort to write UNIX.

"how is it possible for the kernel to interface with any hardware that we stuff inside a box. I assume it has to do with drivers"

A kernel is just like any other software, it's just lower level than most people write programs.  Drivers do things like initialize hardware and define the memory addresses various hardware functions can be found at.  The whole point of a driver is to handle communication with a device so no other program has to (including in many cases the rest of the kernel itself).  Drivers provide an abstracted interface and in modern OSs the interface is usually part of a well documented standard so software doesn't need to care about differences between hardware vendors.  The driver itself (and the kernel API) handle that.

POSIX read/write is an excellent example.  There are many types of storage devices.  Hard drives, usb sticks, RAID Arrays, etc.  POSIX defines operations like open, read and write.  Device drivers for various storage devices implement the details of how open, read and write happen on their device.  Then programs just call open, read and write and do not care (or even need to know) how the storage works underneath.

The way device drivers hide the differences between individual pieces of hardware kernels hide the difference between systems.  The kernel abstracts things like memory management and task scheduling so userland (basically everything that is not kernel) can go about its business without knowing anything about the hardware below it.

"Also, drivers are built by the manufacturer and I would imagine they wouldn't cater to every single operating system under the sun"

Not always.  Apple famously controls the drivers that ship with OS X, often writing most of the code with input from various vendors.  Linux drivers are usually a collaborative effort between the vendor and the community.  Sometimes when a vendor won't build an open driver the linux community reverse engineers a driver. That makes sense.  Reverse engineering a driver sounds like a painful task however. Your intuition is correct. Reverse engineering a device driver written
in assembly code is a time consuming task which can be extremely
painful, unless you are the sort of person who enjoys that particular
kind of puzzle -- they call us "masochists" for some reason.  CPUs with the same architectures act the same (more or less, anyway), so a kernel compiled for that specific architecture works on any CPU of that architecture. If a new architecture appears, only a small part of the kernel has to be rewritten in assembly directly (i.e. very specifically for that architecture), the rest of it is written in C, and in theory shouldn't require too much changes in order to be compiled for that architecture.

As for stuff that isn't CPU, the kernel simply has to include the drivers for those peripherals. Either the vendor gives the drivers, or more often, the drivers are written by the kernel developers themselves (possibly following specifications by the vendor). Ah, okay so vendors publish in depth documents about their hardware.  Have an upvote.  Kernels are compiled for a particular CPU, but most popular ones these days are written so they can be portable to multiple CPUs. The kernel is an abstraction layer that, among other things, lets the same drivers and software (AKA the rest of the OS) work on various CPU architectures (though in practice you still need to recompile the driver and software to target the architecture in question, you don't need a different compilation for every variation in CPU features, and for a well designed OS you don't need to change code very much to run on different target platforms; compare though a much less stable situation like DOS where there is no kernel, only some code for loading and running programs).

From the other side, successful CPU vendors tend to be the ones that stick to a pretty standard/stable interface to make OS development easier. Many popular modern day CPUs could be significantly more efficient if they did not contain hardware level emulators for old CPU versions as their default top level interface to the OS. There is a historic pattern of a new, more efficient CPU designs that do not take off because of a relative lack of OS / application portage.

A big factor with the success of Unix (and later Linux) was that it was well designed for portability to novel chip designs (so much so that it's own custom systems programming language is used for writing the vast majority of operating systems today, even the ones that don't aspire to be Unix compatible, like the ones that run on a remote control or a washing machine). interestingly unix was never actually designed for portability.  It just so happened the abstractions they chose (like everything is a file) turned out to be so flexible and adaptable unix became the portable OS Yeah, "everything is a file" is a classic example of good systems design - before the big OO movement really hit, you get exactly the sort of usefulness that makes OO so popular: since everything is implemented to respond to the file operations, one set of consistent tools can work everywhere. In Unix, a file is nearly a protocol in the full modern sense of the word.

Also compare the ubiquity and durability of stdin/stdout to the many other less successful ways people have tried to make programs interoperate. interestingly unix was never actually designed for portability.  It just so happened the abstractions they chose (like everything is a file) turned out to be so flexible and adaptable unix became the portable OS Intresting, but I have a question.  What is the abstraction used if not a file? It's not a matter of systems using abstractions other than files to access hard drives and other mass storage. The neat thing about the Unix "everything is a file" philosophy is that you can use the same APIs to send data out a serial port or out a speaker or to another process or to a file. A less clever OS design would require different APIs for each of those tasks, so each time your OS gains support for a new kind of device, all the programs would need to be re-written to add support for that kind of device. Unix has tried to avoid that problem as much as possible. (Unix systems still have specialized device access APIs, but having a common interface for a basic set of functions is really useful.)      We're actually heading back to the 'beforetimes', with iOS and Android. The hardware is similar, but different enough that you cannot run Android on an iPhone and vice versa.

I'm hoping we'll eventually standardize the OS, and move on to differing 'commercial' interfaces.  Most of these answers are technically correct, but they're all over complicating the question you asked.

We *still do* need to recompile the OS for different CPU architectures.

That's why PPC applications no longer work on the most recent few versions of OS X. And why you choose between 32 and 64 bit versions of operating systems.

Other hardware communicates through their drivers, so it doesn't require a different version of the OS. But the drivers, themselves, still need to be recompiled for different architectures.    If you really want to dig into this at depth I suggest you have a look at the Linux kernel. It is open source and rather well written. There are platform specific details about different architectures and CPUs that are written in assembly at the lowest level to abstract away the differences between them. This way the rest of the kernel has a common interface to work with for the higher level tasks like scheduling. If you're going to read the source you'll need to be fluent in C though.  \# 1 law of software engineering: abstraction layers are the solution to everything    From my understanding back in the day each machine was custom built from literal scratch (wires, logic gates, etc), now a days it is automated and standardized.  For instance most computers we consider "old" is i686 processors which is in nearly all the "old" PC's.  Now a days there are a few more different ones (ARM, IA64, etc).  So because of this there isn't many different drivers that need to be placed within the kernel to be able to run off the CPU.

As for different interfaces like PCI cards, USB controllers, etc.  For the Windows and Mac kernel they will need third party drivers for devices (considering if you installed the OSX on a device that is non-apple) while the Linux kernel has nearly all the drivers on the most common hardware available is built in itself.  Although most manufactures won't support every OS, if its in high demand most of the time someone will write a driver to support the device (fairly typical in the Linux community).  These type of drivers tend to be open source drivers but there are propritary drivers which are common from manufactures.

This may not be fully 100% accurate but from my experience this is the conclusion I have come up with and I hope this info is useful. I wonder how third party developers make the drivers for say the Linux kernel.  I guess they would have to get in contact with manufacturer or read documents they published if any.</snippet></document><document><title>Examples of algorithms optimized for power?</title><url>http://www.reddit.com/r/compsci/comments/18rxoc/examples_of_algorithms_optimized_for_power/</url><snippet>It's said that Google saves money for its massive server farm by optimizing for power usage. I'm familiar with optimizing for speed and memory usage, but I've never seen, for example, a sorting algorithm designed for low power requirements.

Anyone know of some simple algorithm examples for this?  To save on power on modern machines, you code to reduce memory access.  Accessing memory is becoming a couple orders of magnitude more expensive than a floating point operation.  Using cache memory is still expensive, but preferable to main memory.
Expected near term projections put flops at 10pj of work, and a memory request at 1300pj. (currently at a ~2100/80 ratio for a CPU)

A mergesort where cache sized blocks are sorted in place would beat a quicksort in power usage (except when the input is already mostly sorted ).

Moving information across a network is also a crazy power drain.  Umm, I know about algorithms that try to obscure their power usage/cpu usage for security reasons but just for using low power I'd instinctively go with the same kind of algorithms that are fast. Less work cycles results in less power usage I think. I could be massively wrong here though. No, you're on the mark. Algorithms that run faster  and/or use less memory also use less power.

I'm curious of other kinds of optimizations with the same result. There are energy-aware scheduling algorithms. At least in theory. The idea is that the energy-efficiency of a processor varies depending on its load. In particular, running the processor at full speed might be a non-optimal choice. So you want schedules that optimize the overall energy consumption instead of the processing time (or a combination of both objectives). However, I do not know any real world solutions of such approaches.

~~**Edit:** [A paper by Google addressing the energy-efficiency of their data centers (2007).](http://impact.asu.edu/cse591sp11/Barroso07_EnergyProp-clean.pdf)~~

**Edit 2:** Ignore the paper. Not really related to your question (mistook it for something else I read).  Huh? There are plenty of real world solutions for CPUs, enabled by hardware support for dynamic voltage and frequency scaling. I'd bet a dollar your computer's running at something much lower than maximum frequency as you sit here and reddit. There are energy-aware scheduling algorithms. At least in theory. The idea is that the energy-efficiency of a processor varies depending on its load. In particular, running the processor at full speed might be a non-optimal choice. So you want schedules that optimize the overall energy consumption instead of the processing time (or a combination of both objectives). However, I do not know any real world solutions of such approaches.

~~**Edit:** [A paper by Google addressing the energy-efficiency of their data centers (2007).](http://impact.asu.edu/cse591sp11/Barroso07_EnergyProp-clean.pdf)~~

**Edit 2:** Ignore the paper. Not really related to your question (mistook it for something else I read).    My EE knowledge is limited, but do you mean some sort of algorithm to minimize circuits?  Like a Karnaugh Map algorithm? Imagine you're in charge of implementing Java's `sort()` method for Integer ArrayLists on Dalvik, the Android JVM. Phones shouldn't drain their battery too quickly, so your primary constraint is stated as "sorting such that minimal battery power is used". What kind of algorithm do you employ? My imaginary all-purpose O(1) sorting algorithm!

But in all seriousness, I guess that would be a 'counting sort' as it is O(n) in efficiency.  I don't know if that's applicable to power usage, but the extent of my formal education thus far is algorithm analysis and design.   &amp;gt;  I guess that would be a 'counting sort' as it is O(n) in efficiency.  

I believe that it's O(n * k), where k is usually small.  You cannot achieve an O(n) sort for arbritrary elements.  Counting sort / Bucket sort does have real-world applications, such as sorting T-shirts based on sizes Small, Medium,Large, but not integers, because your k there is HUGE! 2^32 -1 &amp;gt; I believe that it's O(n * k)

Google-fu tells me it's O(n + k) and therefore still linear, right? yea, your right, it's O(n + k), but k is 2^32 -1 for an integer, and infinity for an arbritrary number right? 2^n - 1, n-bit memory depending on the architecture?  Imagine you're in charge of implementing Java's `sort()` method for Integer ArrayLists on Dalvik, the Android JVM. Phones shouldn't drain their battery too quickly, so your primary constraint is stated as "sorting such that minimal battery power is used". What kind of algorithm do you employ? On the topic of general mobile programming, I would think there are a lot of other places to focus on first to reduce power usage. I can't imagine your choice of sorting makes more of a difference than constant network access or display lighting will on a battery. </snippet></document><document><title>What exactly is data?</title><url>http://www.reddit.com/r/compsci/comments/18qqqc/what_exactly_is_data/</url><snippet>I'm an undergraduate student taking my second course in CS, which is really the first "real" CS course, and we are learning Java.

Well, I hear the teacher talk about storing information and data in memory, and I also hear different people talk about storing, accessing, or searching for data and information.

Is data or information nothing but binary code that is stored on some physical location somewhere in the world? Am I making it too simple or is this really all it comes down to?  More generally, I think you should be thinking of data as the stuff that your code operates on. That is, you write your Java code to do something to some data. So, all those things that you store in variables in your objects, those are all data. Those text files that you read and write to? All data. The stuff that you show on the screen for someone to read and answer? Those answers? All data. The stuff that operates on those things, that's your code. 

"Data" and "Code" are generally considered the two parts of a program (and, in fact, in some programming languages they are actually split in the program. ie. you have a data section and a code section). Without code, the data is just pieces of information stored somewhere in some way. Without data, the code is just a set of instructions that has nothing to do.  &amp;gt; "Data" and "Code" are generally considered the two parts of a program.

That is probably a fitting explanation for the OPs case. Just remember, usually one mans "code" is another mans "data".

For example if you program in Java, the code you write becomes the data for the virtual machine. Well, that's the next step in thinking about "data". In most cases, the code you write is "data" for a compiler (or interpreter) to operate on to generate "data" that the CPU operates on. And then the idea that his "data" is also something that the "code" can manipulate to create new "data". . .

Also then idea of "data" about "data" (metadata) and how often this is used and how useful it is. 

But, first OP needs to get over the hump of understanding "data".   The problem is that 'data' is a term that is used in several disciplines, but with slightly different meanings.

Before the computer era, 'data' was a statistical concept: a list of specific values of quantitative variables.

After the advent of computing machinery and the theoretical frameworks developed by folks like Shannon, data also came to mean the electronic representation of information.

These two definitions are mostly comparable, different really only in context.

You might want to ask "well ok, if data is the representation of information, then what's information?" that's a bit of a rabbit hole, but it leads to some of the most interesting thinking of the last 100 years. As an oversimplification, information ultimately is about conservation of symmetries. That is, 1 + 1 = 2, even though we might represent 1 or 2 in any arbitrary way in a physical process. If it sounds like I'm reducing CS and physics to a common framework: yes, I am.

This is a really interesting and active area of theory. Shannon's thesis is easy to skim and a great read. More contemporary writers would be Edwin Jaynes and Gregory Chaitin. I've oft wondered if I should place a high priority on reading Shannon's work, and you just convinced me to do so.  Thanks!  I'm a PhD student in Information Systems (it's in the business school, not CS) and my department has very philosophical thinkers (I don't mean that to sound snooty, it's because came from areas not in IS or CS so that's their view on IS). To us, Information and Data are not interchangeable. In fact, there are quite a few different definitions of information, depending on which school of thought you come from. 

I would highly recommend reading the book "The Information". While it is very highly skewed towards the Shannon view of information, it is really interesting to see the evolution and divergence of the term. After that, you can pick up "The Mathematical Theory of Communication". I would also recommend the book Code, which has been mentioned before. 

This will only get you so far. If you read IS articles on Knowledge Management Systems, you'll get an idea of how data &amp;lt;-&amp;gt; information &amp;lt;-&amp;gt; knowledge are all related. If you are interested in some of those, I can give you references.  Seconding 'The Information' recommendation, if you want to get right into the philosophical and scientific details. Fantastic book, very accessible but doesn't try to skip the slippery concepts either.

"... a million zeroes and a million coin tosses lie at opposite ends of the spectrum. The empty string is as simple as can be; the random string is maximally complex. The zeroes convey no information; coin tosses produce the most information possible. Yet these extremes have something in common. They are dull. They have no value. If either one were a message from another galaxy, we would attribute no intelligence to the sender. If they were music, they would be equally worthless.

*Everything we care about lies somewhere in the middle, where pattern and randomness interlace.*" This follows Shannon's view of information as Entropy; Shannon's view of information came from telecommunication. I would argue that coin tosses also convey no information; rather, they are data, much like a string of zeroes. Without a synthesis of what this data is, it is meaningless. Information is synthesized data. 

Additionally, if you take Gadamer's philosophy, everyone interprets data differently. This is what he calls "prejudice". Prejudice is based on personal history, which is why it is interpreted differently. Prejudice, he says, are good.

A lot of what early software information systems dealt with is the ability to allow managers the ability to see data. Initially, they were elated by the ability to now have this amount of visibility into the firm; however, it quickly became obvious that there was an issue with data overload. This lead to the new interest in the field of actual Information, that is, presenting managers with meaningful information, not data. They don't need to see every little detail of the firm, but rather what was important was finding out the useful bits and presenting it to them in a meaningful way.

I agree with your quote at the bottom. This sweet spot is what we call meaningful information.  &amp;gt;I would argue that coin tosses also convey no information; rather, they are data, much like a string of zeroes.

Aye good point, I was trying to keep the quote terse, it's explained more fully in the preceding paragraph. What you say is alluded to in the sentence about the data being dull. One is empty, one is pure noise, neither convey anything useful. I'm a PhD student in Information Systems (it's in the business school, not CS) and my department has very philosophical thinkers (I don't mean that to sound snooty, it's because came from areas not in IS or CS so that's their view on IS). To us, Information and Data are not interchangeable. In fact, there are quite a few different definitions of information, depending on which school of thought you come from. 

I would highly recommend reading the book "The Information". While it is very highly skewed towards the Shannon view of information, it is really interesting to see the evolution and divergence of the term. After that, you can pick up "The Mathematical Theory of Communication". I would also recommend the book Code, which has been mentioned before. 

This will only get you so far. If you read IS articles on Knowledge Management Systems, you'll get an idea of how data &amp;lt;-&amp;gt; information &amp;lt;-&amp;gt; knowledge are all related. If you are interested in some of those, I can give you references.  &amp;gt; I'm a PhD student in Information Systems (it's in the business school, not CS) and my department has very philosophical thinkers

http://consc.net/papers/rock.html I'm a PhD student in Information Systems (it's in the business school, not CS) and my department has very philosophical thinkers (I don't mean that to sound snooty, it's because came from areas not in IS or CS so that's their view on IS). To us, Information and Data are not interchangeable. In fact, there are quite a few different definitions of information, depending on which school of thought you come from. 

I would highly recommend reading the book "The Information". While it is very highly skewed towards the Shannon view of information, it is really interesting to see the evolution and divergence of the term. After that, you can pick up "The Mathematical Theory of Communication". I would also recommend the book Code, which has been mentioned before. 

This will only get you so far. If you read IS articles on Knowledge Management Systems, you'll get an idea of how data &amp;lt;-&amp;gt; information &amp;lt;-&amp;gt; knowledge are all related. If you are interested in some of those, I can give you references.  Can you give an example of information that is not data, and data that is not information?  I always liked to define information as anything that can be exactly copied, and data as a subset of information that exists in a storage device. I'm a PhD student in Information Systems (it's in the business school, not CS) and my department has very philosophical thinkers (I don't mean that to sound snooty, it's because came from areas not in IS or CS so that's their view on IS). To us, Information and Data are not interchangeable. In fact, there are quite a few different definitions of information, depending on which school of thought you come from. 

I would highly recommend reading the book "The Information". While it is very highly skewed towards the Shannon view of information, it is really interesting to see the evolution and divergence of the term. After that, you can pick up "The Mathematical Theory of Communication". I would also recommend the book Code, which has been mentioned before. 

This will only get you so far. If you read IS articles on Knowledge Management Systems, you'll get an idea of how data &amp;lt;-&amp;gt; information &amp;lt;-&amp;gt; knowledge are all related. If you are interested in some of those, I can give you references.   Simplified, you're thinking of [Information hierarchy](http://people.ischool.berkeley.edu/~ray/Affiliates98/img006.gif): also known as [DIKW hierarchy](http://en.wikipedia.org/wiki/DIKW_Pyramid) gives a rough context. This is probably more than you would be expected to know at your level, but think of data as the base building block.

Just like a real brick: with no context, kinda useless on it's own, but when structured together as a wall (information) it serves a clear purpose. Put walls together to build a house (knowledge), then use that house as a template for other houses on your street (wisdom).

ninja edit: [here](http://upload.wikimedia.org/wikipedia/en/9/93/DIKW.png)'s a more complex graph, still should be pretty understandable. /u/Pash91 speaks the truth, and will probably tear apart my simplified view :P


   * The pixels of an image is data.
* Names and numbers in a phone book is data.
* Your contact list in your cell phone is data.
* Website links stored in a search engine is data.
* An Excel spreadsheet is a fancy table for storing numerical data.
* Historical sports statistics is data.
* Medical records contain data.
* Media files that come with a game is data.
* Written text is data.
* A sequence of random numbers is data.
* Weather measurements sampled over time is data.
* Music MP3 files contain data.

Data can be represented by anything. Letters, numbers, or symbols. On a computer it is stored as binary data or text data due to the constraints of the CPU. For hand written and printed documents, there are no such constraints so you could makeup your own representation for the data if you wanted. &amp;gt;On a computer it is stored as binary data or text data

This makes it sound like the computer has two different ways of representing information: binary and text.  All data on a computer is stored in binary, including data interpreted as text.   There's an application distinction between data: string of characters versus binary. Why do you think we distinguish between bytes and characters?

And of course it's all binary in the end. That is *obvious* to any developer with a complete education. It does absolutely no good to boil it down that far because OP won't learn anything by it.

I'm using terminology the OP can understand. He said he's taking his 2nd course, ever. I phrased my post so as not to confuse him. You all would prefer to make very exacting posts using terminology he's just beginning to learn and confuse the fuck out of him, because your need to prove yourself is greater than your desire to help a newbie out.

Your critique is really reaching on this one, but then Redditers usually enjoy being obnoxious contrarians arguing for argument's sake. Criticizing for no good reason. I am insulted that I would even have to explain myself because it should have been obvious to you. There's an application distinction between data: string of characters versus binary. Why do you think we distinguish between bytes and characters?

And of course it's all binary in the end. That is *obvious* to any developer with a complete education. It does absolutely no good to boil it down that far because OP won't learn anything by it.

I'm using terminology the OP can understand. He said he's taking his 2nd course, ever. I phrased my post so as not to confuse him. You all would prefer to make very exacting posts using terminology he's just beginning to learn and confuse the fuck out of him, because your need to prove yourself is greater than your desire to help a newbie out.

Your critique is really reaching on this one, but then Redditers usually enjoy being obnoxious contrarians arguing for argument's sake. Criticizing for no good reason. I am insulted that I would even have to explain myself because it should have been obvious to you. &amp;gt; That is obvious to any developer with a complete education. It does absolutely no good to boil it down that far because OP won't learn anything by it.

I respectfully disagree. OP must understand this concept as a foundation of Computer Science. Understanding that a processor only understands chains of 1s and 0s is fundamental.

I agree it is necessary to explain terms at an elementary level so as not to confuse the OP, but in this case stating that data on a computer is stored as binary or text is, in fact, misleading. * The pixels of an image is data.
* Names and numbers in a phone book is data.
* Your contact list in your cell phone is data.
* Website links stored in a search engine is data.
* An Excel spreadsheet is a fancy table for storing numerical data.
* Historical sports statistics is data.
* Medical records contain data.
* Media files that come with a game is data.
* Written text is data.
* A sequence of random numbers is data.
* Weather measurements sampled over time is data.
* Music MP3 files contain data.

Data can be represented by anything. Letters, numbers, or symbols. On a computer it is stored as binary data or text data due to the constraints of the CPU. For hand written and printed documents, there are no such constraints so you could makeup your own representation for the data if you wanted. * The pixels of an image is data.
* Names and numbers in a phone book is data.
* Your contact list in your cell phone is data.
* Website links stored in a search engine is data.
* An Excel spreadsheet is a fancy table for storing numerical data.
* Historical sports statistics is data.
* Medical records contain data.
* Media files that come with a game is data.
* Written text is data.
* A sequence of random numbers is data.
* Weather measurements sampled over time is data.
* Music MP3 files contain data.

Data can be represented by anything. Letters, numbers, or symbols. On a computer it is stored as binary data or text data due to the constraints of the CPU. For hand written and printed documents, there are no such constraints so you could makeup your own representation for the data if you wanted.  Data is just what it sounds like, a vague term for stored information.
Don't flip out, if its your first course everything will be pretty much what it sounds like. I'm not flipping out at all, I don't think this sort of question will really be of much relevance in this course anyway.

I'm just the type of person who likes to really understand how something physically works. I want to actually understand what is beyond the mere word "data".

One day, I would like to see it all in action.

EDIT: So what exactly is stored information? Is this just binary code stored somewhere? And then what exactly is binary code in terms of physical structure? &amp;gt; So what exactly is stored information? Is this just binary code stored somewhere? And then what exactly is binary code in terms of physical structure?

While UncleMeat's answer is good, I'd like to stress one more point.

As far as the theory of computation is concerned, it need neither be binary nor correspond to any particular physical structure.  Binary is convenient in many ways, but "stored data" could be described as "discriminable state" - the level of water in a jug, marks on a piece of paper, the colour of a ball in an urn, the type and arrangement of knots on a piece of string, the presence or absence of a chad on a punch card, the audible frequencies stored on 1s worth of tape, the state of a qubit ... This is important. As many people have stated, the meaning of data depends who you ask, even restricted to the computing field. Ask a guy who writes hard disk drivers, and data is 1s and 0s. Ask a filesystem guy, and data is files. Ask an Excel user, and data is tables and functions.

Even restricting to hardware concepts, "binary" is too restrictive a definition of data. For example, you can check out [ternary computers](http://en.wikipedia.org/wiki/Ternary_computer).

In short, I think "data" depends on context too much for one answer, but a safe-enough assumption is that you can think of it as a collection of information until some context makes that wrong :-p. This is important. As many people have stated, the meaning of data depends who you ask, even restricted to the computing field. Ask a guy who writes hard disk drivers, and data is 1s and 0s. Ask a filesystem guy, and data is files. Ask an Excel user, and data is tables and functions.

Even restricting to hardware concepts, "binary" is too restrictive a definition of data. For example, you can check out [ternary computers](http://en.wikipedia.org/wiki/Ternary_computer).

In short, I think "data" depends on context too much for one answer, but a safe-enough assumption is that you can think of it as a collection of information until some context makes that wrong :-p. I'm not flipping out at all, I don't think this sort of question will really be of much relevance in this course anyway.

I'm just the type of person who likes to really understand how something physically works. I want to actually understand what is beyond the mere word "data".

One day, I would like to see it all in action.

EDIT: So what exactly is stored information? Is this just binary code stored somewhere? And then what exactly is binary code in terms of physical structure? Warning: oversimplication incoming. 

"Data" is stored in your computer in one of two ways. It is either stored as magnetic state on your hard disk or it is stored as electrical state inside of memory. More specifically, data is stored on a hard disk by literally adjusting the magnetic field of the hard disk in a very particular spot. Based on the orientation of the magnetic field at this location we either have a 0 or a 1. To store data using electricity, we use circuits called [flip-flops](http://en.wikipedia.org/wiki/Flip-flop_(electronics\)). If the electricity is looping one way in the circuit then we call that a 0 and if the electricity is looping the other way then we call that a 1. Your RAM is basically a big ass array of these little circuits.

If you are interested in this sort of material, I **highly** recommend the book CODE by Charles Petzold. It is, in my opinion, the best book on computers ever written and it covers the construction of computers from a very low level in a way that even high schoolers can understand.  Warning: oversimplication incoming. 

"Data" is stored in your computer in one of two ways. It is either stored as magnetic state on your hard disk or it is stored as electrical state inside of memory. More specifically, data is stored on a hard disk by literally adjusting the magnetic field of the hard disk in a very particular spot. Based on the orientation of the magnetic field at this location we either have a 0 or a 1. To store data using electricity, we use circuits called [flip-flops](http://en.wikipedia.org/wiki/Flip-flop_(electronics\)). If the electricity is looping one way in the circuit then we call that a 0 and if the electricity is looping the other way then we call that a 1. Your RAM is basically a big ass array of these little circuits.

If you are interested in this sort of material, I **highly** recommend the book CODE by Charles Petzold. It is, in my opinion, the best book on computers ever written and it covers the construction of computers from a very low level in a way that even high schoolers can understand.  I'm not flipping out at all, I don't think this sort of question will really be of much relevance in this course anyway.

I'm just the type of person who likes to really understand how something physically works. I want to actually understand what is beyond the mere word "data".

One day, I would like to see it all in action.

EDIT: So what exactly is stored information? Is this just binary code stored somewhere? And then what exactly is binary code in terms of physical structure? Some concepts in computer science have precise definitions. "data" is not one of them.

If you really want a definition, I would say that data is sort of the opposite of "code". Code runs and does stuff while data gets read and processed by the code, without "doing stuff" itself. By this definition, the concrete representation of the data or where it gets stored is irrelevant.

But, as you can probably see, this is clearly a contextual definition - for example, the code itself can be seen as data if you look fro the point of view of the operating system. &amp;gt; I would say that data is sort of the opposite of "code"

Don't get too caught up in that - taking it for granted leads to a worldview that neglects some lovely things like self-modifying code, extensible syntax (yes I'm talking Lisp, also Tcl) and the Futamura projections. I'm not flipping out at all, I don't think this sort of question will really be of much relevance in this course anyway.

I'm just the type of person who likes to really understand how something physically works. I want to actually understand what is beyond the mere word "data".

One day, I would like to see it all in action.

EDIT: So what exactly is stored information? Is this just binary code stored somewhere? And then what exactly is binary code in terms of physical structure? I'm not flipping out at all, I don't think this sort of question will really be of much relevance in this course anyway.

I'm just the type of person who likes to really understand how something physically works. I want to actually understand what is beyond the mere word "data".

One day, I would like to see it all in action.

EDIT: So what exactly is stored information? Is this just binary code stored somewhere? And then what exactly is binary code in terms of physical structure?  Data is stored as a series of binary bits, where the [voltage determines the value for that bit](http://imgur.com/Bv2WHCo).

Edit: This isn't the only method to store data - this is how volatile memory is stored. [px403 noted below](http://www.reddit.com/r/compsci/comments/18qqqc/what_exactly_is_data/c8hgg29) that data can also exist on magnetic storage devices (platters inside of a mechanical hard drive.) In this case, [coils of the read arm determine the polarity of the bit, and interpret it as a 1 or a 0 accordingly](http://wiki.answers.com/Q/How_is_data_written_to_and_read_from_a_magnetic_disk).

Data may also be stored in a graphical representation (think barcode or QR code.) So data can't exist on magnetic storage devices like hard drives?     Yes. Data resides as charges in the RAM capacitors or as magnetically charged regions on a hard drive platter.

In CS when we say data it usually means something that is not executable program. In modern architectures this distinction is purely a matter of context since programs and data share the same memory space. Historically speaking this was not always true, and some architectures had separate memory spaces for programs and data. 

Information on the other hand is ambiguous term that as far as I can tell does not have a discrete meaning in CS. However it is usually applied in the sense of "organized data" - something a human can take, digest and understand.

So let's say you have a big file full of unorganized records. That's data. Now sort that file, aggregate it, plot it on a graph and now it is information because it conveys some message to the human.

So to summarize:

* Data is anything in memory that is not a executable program code
* Information is organized data that conveys a message

Note, however that it is perfectly acceptable to use data an information interchangeably. In fact, I frequently do this when I lecture just to switch things up so that I don't end up saying data 18 times per minute. 

I hope that helps. I think that your definition lacks generality. Data does not have to be 'in memory' nor reside in transistors at all. You could use pebbles, bananas or horse-sized ducks to represent numbers/graphs/trees if you wanted to. We just happen to use transistors because they are much, much more efficient that e.g. bananas and vaccum tubes. A book, for instance, is merely a collection of data comprising of dots and dots of ink. The dots, in turn, represent letters which in turn represent words of a language, that all, when subjected to interpretation by the reader, conveys some idea in a formalized manner (that is the language).

Thus anything that represents ideas in some formalized may be considered data IMO. Why not consider executable program code data? They are merely asserted and disasserted signals of the transistors comprising the memory. I'd say they qualify as data for the same reason that we may interpret the bits as binary-represented numbers. They are only instructions because we choose to look at them that way.        I tend to think of data as "values that are always true", e.g. birthdate.
Whereas information is something that is "calculated", e.g. age.
The world around us however does seem to change, and so do the state of things. I always thought that gender was "data", but nowadays with science, it's possible to "change sex" and thus your "gender" becomes a "calculated value", hence information.            IMO Data and Information is interchangeable. It's just talking about storing values, however encoded they are. If it's an integer, or a single character, or a string of characters, or a structure of data. Data and information arn't really interchangeable. having a string of batting results for every player of cricket  in all time won't tell you anything about them.

but by taking that data and data on the bowlers you can work out information on what swings a batter should make vs will make and this then you can build knowledge on that.

data -&amp;gt; information -&amp;gt; knowledge
  </snippet></document><document><title>Open question: I'm an undergrad math student with an interest in CS. What are some topics I should look into? </title><url>http://www.reddit.com/r/compsci/comments/18pq99/open_question_im_an_undergrad_math_student_with/</url><snippet>Hello! 

I'm a math student but I have quite a bit of programming experience as a hobby (mostly in C and more recently in Matlab and Mathematica) but I have an interest in the more "theoretical" aspect of CS. I'm not sure if theoretical is the right word, but what I'm trying to say is that I have no interest in writing Web back/front end, games or mobile apps. As an example, one of my recent project has been a brainfuck compiler written in C. The interpreter part is all good to go but the compiler still needs work. 

I've been looking into Haskell as well as Common Lisp recently as they both seem to be quite popular especially for small compilers and interpreters and Haskell seems to be one of the "go to" languages for mathematicians (although I personally never had to use it for anything). My very limited Lisp-like experience comes from going through Emacs packages but again, I've never really used it for anything else. I know topics in CS aren't really language dependent but something I could explore while learning some Haskell or a Lisp variant would be great. 

I got a copy of The Haskell Road to Math, Logic and Programming which I started reading so hopefully I can get some inspiration from this but I thought I'd ask here just in case. 

Thanks! 

Tldr: Looking for CS topics that could be interesting to a math student, especially if it can be explored with Haskell or Common Lisp (or any Lisp variants)   Former math undergrad here. If you're into set theory, you should read Sipser's [An Introduction to the Theory of Computation](http://www.amazon.com/Introduction-Theory-Computation-Sipser-Edition/dp/B003NI4ZPM/ref=sr_1_4?ie=UTF8&amp;amp;qid=1361159135&amp;amp;sr=8-4&amp;amp;keywords=an+introduction+to+the+theory+of+computation) (you can probably find it in your library... the first edition is something like $25 used on Amazon), followed by [Arora-Barak](http://www.amazon.com/Computational-Complexity-Approach-Sanjeev-Arora/dp/0521424267/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1361159227&amp;amp;sr=1-1&amp;amp;keywords=arora+barak) for which this is a [freely-available draft](http://www.cs.princeton.edu/theory/complexity/) (unfortunately, the draft doesn't contain the figures, which are oftentimes helpful). You could, in theory, skip directly into the latter, but working through Sipser first will much better fuel your intuition.

Another thing to look into is Algorithm design, in which case I recommend checking out the book titled [Algorithms](http://www.cs.berkeley.edu/~vazirani/algorithms.html) by Dasgupta, Papadimitriou, and Vazirani. It's a really nice introduction to algorithms that doesn't shy away from formality. Eventually, you'll want to transition to something more advanced like [CLRS](http://en.wikipedia.org/wiki/Introduction_to_Algorithms) or [Kleinberg-Tardos](http://www.amazon.com/Algorithm-Design-Jon-Kleinberg/dp/0321295358), but DPV itself is a wonderful (and free) starting point.

Crypto, as someone else mentioned, is also a very nice field to look into, but I wouldn't recommend going into it before at least working through DPV up through the NP-completeness chapter.

If you're into math, I'd recommend not wasting your time with studying individual programming languages. [Once you learn one, you effectively learn them all](http://en.wikipedia.org/wiki/Church-Turing_Hypothesis) (perhaps modulo some small changes that come with different paradigms... but nothing truly substantial). Also, to be frank, I'm not really impressed by the math involved in many of these programming language books (such as the one linked to by dmwit). The types of proofs mentioned there are, at best, at the level of a 11th grader who passed a basic course in Euclidean geometry and Algebra 2. Perhaps I'm missing something obvious with these books, but I can't imagine anyone who has taken a legitimate Analysis or (abstract) Algebra class considering these proofs to be anything but trivial.

By the way, if you end up running into any roadblocks in any of these books (including the PL book... I guess...), feel free to PM me. I'm generally addicted to helping people who want to learn, and I promise to do what I can.

Edit: Some other notes on the comments here:

1. GEB is a waste of time. It's intended for an audience much less mathematically inclined than math majors. Please don't waste your time with it. If you want to learn about Godel's theorem, Ernest Nagel has [a decent book](http://www.amazon.com/G%C3%B6dels-Proof-Ernest-Nagel/dp/0814758371/ref=sr_1_1?ie=UTF8&amp;amp;qid=1361160958&amp;amp;sr=8-1&amp;amp;keywords=godel%27s+theorem+nagel) you could use instead.
2. Knuth is probably a good book, but I'm willing to put down $100 that whoever recommends it to you hasn't actually read much of it. I tried reading a bit of it to inspire me back when I was a TA for an undergrad algorithms class and quickly realized it's not a particularly good starting point.
3. AI might be a fun topic, though again I'd recommend reading up on it *after* going through DPV.  It's mathematically lighter than, say, Arora-Barak, but it's perhaps more practical. Russell-Norvig is considered the standard intro tome, and is [effectively free used on Amazon](http://www.amazon.com/gp/offer-listing/0131038052/ref=sr_1_2_olp?ie=UTF8&amp;amp;qid=1361160736&amp;amp;sr=8-2&amp;amp;keywords=russell+norvig&amp;amp;condition=used).
4. Quantum complexity would require a large degree of familiarity with linear algebra, and I can't quite recommend going for it without at least some background in classical complexity. Nonetheless, Fields Medalist Tim Gowers has [a nice set of talks available online](http://sms.cam.ac.uk/collection/545358) on the subject if you're up for the task. I imagine it would be rather tricky to use this as a starting point, but if I recall correctly he doesn't assume *that* much background knowledge in algorithms (it's been about 4 years since I watched the thing, so I could be misremembering). Former math undergrad here. If you're into set theory, you should read Sipser's [An Introduction to the Theory of Computation](http://www.amazon.com/Introduction-Theory-Computation-Sipser-Edition/dp/B003NI4ZPM/ref=sr_1_4?ie=UTF8&amp;amp;qid=1361159135&amp;amp;sr=8-4&amp;amp;keywords=an+introduction+to+the+theory+of+computation) (you can probably find it in your library... the first edition is something like $25 used on Amazon), followed by [Arora-Barak](http://www.amazon.com/Computational-Complexity-Approach-Sanjeev-Arora/dp/0521424267/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1361159227&amp;amp;sr=1-1&amp;amp;keywords=arora+barak) for which this is a [freely-available draft](http://www.cs.princeton.edu/theory/complexity/) (unfortunately, the draft doesn't contain the figures, which are oftentimes helpful). You could, in theory, skip directly into the latter, but working through Sipser first will much better fuel your intuition.

Another thing to look into is Algorithm design, in which case I recommend checking out the book titled [Algorithms](http://www.cs.berkeley.edu/~vazirani/algorithms.html) by Dasgupta, Papadimitriou, and Vazirani. It's a really nice introduction to algorithms that doesn't shy away from formality. Eventually, you'll want to transition to something more advanced like [CLRS](http://en.wikipedia.org/wiki/Introduction_to_Algorithms) or [Kleinberg-Tardos](http://www.amazon.com/Algorithm-Design-Jon-Kleinberg/dp/0321295358), but DPV itself is a wonderful (and free) starting point.

Crypto, as someone else mentioned, is also a very nice field to look into, but I wouldn't recommend going into it before at least working through DPV up through the NP-completeness chapter.

If you're into math, I'd recommend not wasting your time with studying individual programming languages. [Once you learn one, you effectively learn them all](http://en.wikipedia.org/wiki/Church-Turing_Hypothesis) (perhaps modulo some small changes that come with different paradigms... but nothing truly substantial). Also, to be frank, I'm not really impressed by the math involved in many of these programming language books (such as the one linked to by dmwit). The types of proofs mentioned there are, at best, at the level of a 11th grader who passed a basic course in Euclidean geometry and Algebra 2. Perhaps I'm missing something obvious with these books, but I can't imagine anyone who has taken a legitimate Analysis or (abstract) Algebra class considering these proofs to be anything but trivial.

By the way, if you end up running into any roadblocks in any of these books (including the PL book... I guess...), feel free to PM me. I'm generally addicted to helping people who want to learn, and I promise to do what I can.

Edit: Some other notes on the comments here:

1. GEB is a waste of time. It's intended for an audience much less mathematically inclined than math majors. Please don't waste your time with it. If you want to learn about Godel's theorem, Ernest Nagel has [a decent book](http://www.amazon.com/G%C3%B6dels-Proof-Ernest-Nagel/dp/0814758371/ref=sr_1_1?ie=UTF8&amp;amp;qid=1361160958&amp;amp;sr=8-1&amp;amp;keywords=godel%27s+theorem+nagel) you could use instead.
2. Knuth is probably a good book, but I'm willing to put down $100 that whoever recommends it to you hasn't actually read much of it. I tried reading a bit of it to inspire me back when I was a TA for an undergrad algorithms class and quickly realized it's not a particularly good starting point.
3. AI might be a fun topic, though again I'd recommend reading up on it *after* going through DPV.  It's mathematically lighter than, say, Arora-Barak, but it's perhaps more practical. Russell-Norvig is considered the standard intro tome, and is [effectively free used on Amazon](http://www.amazon.com/gp/offer-listing/0131038052/ref=sr_1_2_olp?ie=UTF8&amp;amp;qid=1361160736&amp;amp;sr=8-2&amp;amp;keywords=russell+norvig&amp;amp;condition=used).
4. Quantum complexity would require a large degree of familiarity with linear algebra, and I can't quite recommend going for it without at least some background in classical complexity. Nonetheless, Fields Medalist Tim Gowers has [a nice set of talks available online](http://sms.cam.ac.uk/collection/545358) on the subject if you're up for the task. I imagine it would be rather tricky to use this as a starting point, but if I recall correctly he doesn't assume *that* much background knowledge in algorithms (it's been about 4 years since I watched the thing, so I could be misremembering). That is quite a bit of info! Thanks for the edit as well, I've been taking notes from all the posts so far, good to see different opinions. Skimmed through GEB and you're probably right, it looks pretty interesting but not really what I had in mind. I'll definitely try and read parts of it at one point though.

Thanks a lot for the reply though, lots of stuff to look at! Former math undergrad here. If you're into set theory, you should read Sipser's [An Introduction to the Theory of Computation](http://www.amazon.com/Introduction-Theory-Computation-Sipser-Edition/dp/B003NI4ZPM/ref=sr_1_4?ie=UTF8&amp;amp;qid=1361159135&amp;amp;sr=8-4&amp;amp;keywords=an+introduction+to+the+theory+of+computation) (you can probably find it in your library... the first edition is something like $25 used on Amazon), followed by [Arora-Barak](http://www.amazon.com/Computational-Complexity-Approach-Sanjeev-Arora/dp/0521424267/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1361159227&amp;amp;sr=1-1&amp;amp;keywords=arora+barak) for which this is a [freely-available draft](http://www.cs.princeton.edu/theory/complexity/) (unfortunately, the draft doesn't contain the figures, which are oftentimes helpful). You could, in theory, skip directly into the latter, but working through Sipser first will much better fuel your intuition.

Another thing to look into is Algorithm design, in which case I recommend checking out the book titled [Algorithms](http://www.cs.berkeley.edu/~vazirani/algorithms.html) by Dasgupta, Papadimitriou, and Vazirani. It's a really nice introduction to algorithms that doesn't shy away from formality. Eventually, you'll want to transition to something more advanced like [CLRS](http://en.wikipedia.org/wiki/Introduction_to_Algorithms) or [Kleinberg-Tardos](http://www.amazon.com/Algorithm-Design-Jon-Kleinberg/dp/0321295358), but DPV itself is a wonderful (and free) starting point.

Crypto, as someone else mentioned, is also a very nice field to look into, but I wouldn't recommend going into it before at least working through DPV up through the NP-completeness chapter.

If you're into math, I'd recommend not wasting your time with studying individual programming languages. [Once you learn one, you effectively learn them all](http://en.wikipedia.org/wiki/Church-Turing_Hypothesis) (perhaps modulo some small changes that come with different paradigms... but nothing truly substantial). Also, to be frank, I'm not really impressed by the math involved in many of these programming language books (such as the one linked to by dmwit). The types of proofs mentioned there are, at best, at the level of a 11th grader who passed a basic course in Euclidean geometry and Algebra 2. Perhaps I'm missing something obvious with these books, but I can't imagine anyone who has taken a legitimate Analysis or (abstract) Algebra class considering these proofs to be anything but trivial.

By the way, if you end up running into any roadblocks in any of these books (including the PL book... I guess...), feel free to PM me. I'm generally addicted to helping people who want to learn, and I promise to do what I can.

Edit: Some other notes on the comments here:

1. GEB is a waste of time. It's intended for an audience much less mathematically inclined than math majors. Please don't waste your time with it. If you want to learn about Godel's theorem, Ernest Nagel has [a decent book](http://www.amazon.com/G%C3%B6dels-Proof-Ernest-Nagel/dp/0814758371/ref=sr_1_1?ie=UTF8&amp;amp;qid=1361160958&amp;amp;sr=8-1&amp;amp;keywords=godel%27s+theorem+nagel) you could use instead.
2. Knuth is probably a good book, but I'm willing to put down $100 that whoever recommends it to you hasn't actually read much of it. I tried reading a bit of it to inspire me back when I was a TA for an undergrad algorithms class and quickly realized it's not a particularly good starting point.
3. AI might be a fun topic, though again I'd recommend reading up on it *after* going through DPV.  It's mathematically lighter than, say, Arora-Barak, but it's perhaps more practical. Russell-Norvig is considered the standard intro tome, and is [effectively free used on Amazon](http://www.amazon.com/gp/offer-listing/0131038052/ref=sr_1_2_olp?ie=UTF8&amp;amp;qid=1361160736&amp;amp;sr=8-2&amp;amp;keywords=russell+norvig&amp;amp;condition=used).
4. Quantum complexity would require a large degree of familiarity with linear algebra, and I can't quite recommend going for it without at least some background in classical complexity. Nonetheless, Fields Medalist Tim Gowers has [a nice set of talks available online](http://sms.cam.ac.uk/collection/545358) on the subject if you're up for the task. I imagine it would be rather tricky to use this as a starting point, but if I recall correctly he doesn't assume *that* much background knowledge in algorithms (it's been about 4 years since I watched the thing, so I could be misremembering). Former math undergrad here. If you're into set theory, you should read Sipser's [An Introduction to the Theory of Computation](http://www.amazon.com/Introduction-Theory-Computation-Sipser-Edition/dp/B003NI4ZPM/ref=sr_1_4?ie=UTF8&amp;amp;qid=1361159135&amp;amp;sr=8-4&amp;amp;keywords=an+introduction+to+the+theory+of+computation) (you can probably find it in your library... the first edition is something like $25 used on Amazon), followed by [Arora-Barak](http://www.amazon.com/Computational-Complexity-Approach-Sanjeev-Arora/dp/0521424267/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1361159227&amp;amp;sr=1-1&amp;amp;keywords=arora+barak) for which this is a [freely-available draft](http://www.cs.princeton.edu/theory/complexity/) (unfortunately, the draft doesn't contain the figures, which are oftentimes helpful). You could, in theory, skip directly into the latter, but working through Sipser first will much better fuel your intuition.

Another thing to look into is Algorithm design, in which case I recommend checking out the book titled [Algorithms](http://www.cs.berkeley.edu/~vazirani/algorithms.html) by Dasgupta, Papadimitriou, and Vazirani. It's a really nice introduction to algorithms that doesn't shy away from formality. Eventually, you'll want to transition to something more advanced like [CLRS](http://en.wikipedia.org/wiki/Introduction_to_Algorithms) or [Kleinberg-Tardos](http://www.amazon.com/Algorithm-Design-Jon-Kleinberg/dp/0321295358), but DPV itself is a wonderful (and free) starting point.

Crypto, as someone else mentioned, is also a very nice field to look into, but I wouldn't recommend going into it before at least working through DPV up through the NP-completeness chapter.

If you're into math, I'd recommend not wasting your time with studying individual programming languages. [Once you learn one, you effectively learn them all](http://en.wikipedia.org/wiki/Church-Turing_Hypothesis) (perhaps modulo some small changes that come with different paradigms... but nothing truly substantial). Also, to be frank, I'm not really impressed by the math involved in many of these programming language books (such as the one linked to by dmwit). The types of proofs mentioned there are, at best, at the level of a 11th grader who passed a basic course in Euclidean geometry and Algebra 2. Perhaps I'm missing something obvious with these books, but I can't imagine anyone who has taken a legitimate Analysis or (abstract) Algebra class considering these proofs to be anything but trivial.

By the way, if you end up running into any roadblocks in any of these books (including the PL book... I guess...), feel free to PM me. I'm generally addicted to helping people who want to learn, and I promise to do what I can.

Edit: Some other notes on the comments here:

1. GEB is a waste of time. It's intended for an audience much less mathematically inclined than math majors. Please don't waste your time with it. If you want to learn about Godel's theorem, Ernest Nagel has [a decent book](http://www.amazon.com/G%C3%B6dels-Proof-Ernest-Nagel/dp/0814758371/ref=sr_1_1?ie=UTF8&amp;amp;qid=1361160958&amp;amp;sr=8-1&amp;amp;keywords=godel%27s+theorem+nagel) you could use instead.
2. Knuth is probably a good book, but I'm willing to put down $100 that whoever recommends it to you hasn't actually read much of it. I tried reading a bit of it to inspire me back when I was a TA for an undergrad algorithms class and quickly realized it's not a particularly good starting point.
3. AI might be a fun topic, though again I'd recommend reading up on it *after* going through DPV.  It's mathematically lighter than, say, Arora-Barak, but it's perhaps more practical. Russell-Norvig is considered the standard intro tome, and is [effectively free used on Amazon](http://www.amazon.com/gp/offer-listing/0131038052/ref=sr_1_2_olp?ie=UTF8&amp;amp;qid=1361160736&amp;amp;sr=8-2&amp;amp;keywords=russell+norvig&amp;amp;condition=used).
4. Quantum complexity would require a large degree of familiarity with linear algebra, and I can't quite recommend going for it without at least some background in classical complexity. Nonetheless, Fields Medalist Tim Gowers has [a nice set of talks available online](http://sms.cam.ac.uk/collection/545358) on the subject if you're up for the task. I imagine it would be rather tricky to use this as a starting point, but if I recall correctly he doesn't assume *that* much background knowledge in algorithms (it's been about 4 years since I watched the thing, so I could be misremembering). I highly disagree with that blanket statement regarding programming languages. You don't learn Haskell by learning C or Prolog by learning Java. Hell, ever see Java programmers try to code in C++ or vice versa? And these two languages even fall under the same paradigm!

Theoretically they all break down to the same thing, but you must learn the mechanics behind each and every language in order: 1 gauge what that language is best at and 2, utilize it in a manner that suits its strengths.  Have you taken a class in discrete math yet? A lot of math departments offer one and it's usually taken by CS students too.  Hey! Yes, I did take one already and it was shared with CS students from what I can remember. We used Kenneth Rosen's book (Rosen's Discrete Mathematics) and I think there was a chapter or 2 about cryptography towards the end that wasn't part of the course. I'll check it out. Thanks!  Try some cryptography stuff, especially the [RSA problem](http://en.wikipedia.org/wiki/RSA_problem). This will definitely take you into the "theoretical" realm, since the security of the RSA algorithm is dependent on whether or not it is possible to factor arbitrarily large numbers in efficient time and space on standard architecture. You'll learn about the time complexity classes such as P and NP. 

Another thing I would suggest is [approximation algorithms](http://en.wikipedia.org/wiki/Approximation_algorithm). 

Also, since sorting and searching are the some of the most basic fundamental things you can possibly do with a computer program so it might be fun to study those related algorithms, ie. insertion sort, selection sort, binary search, quick-select, quick sort, merge sort, and linear-time sorting algorithms like radix-sort.  Try some cryptography stuff, especially the [RSA problem](http://en.wikipedia.org/wiki/RSA_problem). This will definitely take you into the "theoretical" realm, since the security of the RSA algorithm is dependent on whether or not it is possible to factor arbitrarily large numbers in efficient time and space on standard architecture. You'll learn about the time complexity classes such as P and NP. 

Another thing I would suggest is [approximation algorithms](http://en.wikipedia.org/wiki/Approximation_algorithm). 

Also, since sorting and searching are the some of the most basic fundamental things you can possibly do with a computer program so it might be fun to study those related algorithms, ie. insertion sort, selection sort, binary search, quick-select, quick sort, merge sort, and linear-time sorting algorithms like radix-sort.     Quantum computability. Not the boring practical aspects but algorithms and computability theory &amp;amp;c Isn't quantum computability the same as classical computability? I think you mean quantum computation, if I'm not wrong. Quantum computability is an extension of classical computability and introduces a few more complexity classes ( BQP and so on ) and just the nature of probability spaces ( postselection ) which give you a bunch more stuff to work on. Full of open questions. Does that mean that if a certain problem is undecidable on a Turing machine, there is still a possibility that it may be decidable on a quantum computer? I had always assumed that *computability wise* (not complexity wise), QCs were equivalent to TMs.  
From the [wikipedia page](http://en.wikipedia.org/wiki/Quantum_computer),  
&amp;gt;Given sufficient computational resources, a classical computer could be made to simulate any quantum algorithm; quantum computation does not violate the Church&#8211;Turing thesis. A problem in NP may be in BQP ( for instance factorization of primes ) which means a classical turing machine can not solve it *in polynomial time*, but a quantum computer can.

If you want to learn about problems that a turing machine **cannot** solve but we can talk about machines that can (for instance a solution to the halting problem) the term you're looking for is [hypercomputation](http://en.wikipedia.org/wiki/Hypercomputation). I'm sorry, but I strongly feel you're confusing computability with complexity.  Artificial Intelligence might be good? There's a lot of theoretical math, as well as logic theory. Also, I know a lot of professors who did their undergrad in math and did their masters/phD studying algorithms. I think the field has turned to Machine Learning rather then AI. My thesis work is in Computer Vision which involves a lot of Machine Learning to train classifiers to differentiate things. It's very math heavy with an emphasis on statistics/probability. I'v never been good at math unfortunately, so my eyes still glaze over a bit in some of papers in my niche when they get into the machine learning aspects.

OP, I'v seen several people carry over from a math major to CS and fit in well in the labs. IMHO, Computer Science should really be called Computational Science, at least when you move beyond learning languages and software engineering best practices. The higher you go, the more math you use. Artificial Intelligence might be good? There's a lot of theoretical math, as well as logic theory. Also, I know a lot of professors who did their undergrad in math and did their masters/phD studying algorithms.     You might look into [Python](http://python.org); it has many of the same capabilities and functional features of Lisp-likes, but the imperative structure of C (though the syntax looks like neither).  If you're going to do anything with C itself, you should probably also learn C++ while you're at it, since it's basically an extension of C.        Automated theorem proving is probably a pretty good area for a math student who is interested in CS.  A fun system to start with might be ACL2, a theorem prover for Lisp.  Coq is also a very neat system if you have enough background in programming languages / type theory to understand it.   Read [G&#246;del, Escher, Bach: An Eternal Golden Braid](http://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach). Seriously. Then read any detailed book or paper on G&#246;del's incompleteness theorems.  Well this is quite the coincidence. I'm subletting an apartment for a year and the guy left his books here (he has a pretty impressive collection) and I remember seeing that title somewhere. Turns out it's sitting right there on the bookshelf. I'll have a look at it. Thanks! </snippet></document><document><title>Learn how a computer really works, build one from logic gates to OS.</title><url>http://www.nand2tetris.org</url><snippet>  If you like this, you'll probably also really enjoy Charles Petzold's [Code: The Hidden Language of Computer Hardware and Software](http://www.charlespetzold.com/code/).  To my knowledge, this is required university coursework to get a bachelors in anything remotely relating to computers. For my CS degree, we covered these concepts as separate parts, but never integrated it into a cohesive whole. Also, we never got all the way through the compilers book, so there's that. A lot of the exposure is cursory. I took only 1 course in building logic gates. Do I understand the basics? Yeah. Could I build something useful? Fuck no.

I have the conceptual mental model from top to bottom but not the knowledge of specific details enough to build the computer. A programmer or software engineer doesn't really need more than the conceptual mental model to do good work. Maybe a computer science researcher does if they're working on algorithms directly impacted by hardware though, but I think those scenarios are few and far between. For my CS degree, we covered these concepts as separate parts, but never integrated it into a cohesive whole. Also, we never got all the way through the compilers book, so there's that. I've thought for a long time that the stereotypical compilers course omitted too much information and should be split into a pair of courses taken in sequence:

* An introductory interpreters course. This would cover lexing, parsing, and a survey of various forms of semantic analysis, including HM type inference.

* A follow-up course on compilers. This would start where the interpreters course left off and would include a survey on optimizations, code generation, and JIT compilation. I think most programs do this, its just that the later course is a grad level course. You simply aren't going to be able to cover the mechanics of lexing, parsing, type checking, and code generation and still have time for interesting optimizations and other important functions like garbage collection.  To my knowledge, this is required university coursework to get a bachelors in anything remotely relating to computers.   This video and project gives me so much hope. This is the perfect CS 101 course, to teach people about it all, so they can figure out an area they are interested in, and understand how it inter-operates with the other resources.

More importantly, it shows people how critical resources are, and how much you can do with limited resources and demonstrates the importance of resource management. At least I hope, since I haven't taken the course. I think this is better for a sophomore level course (which is where most architecture classes fall, from what I've seen anyway). You have to realize there are freshmen who are coming into the program with 0 programming experience at all, and I don't think they're going to appreciate most of what is going on in a processor/OS if they haven't written any code before.

For my undergrad, this course came right after data structures alongside the first systems programming course, which seemed a rather nice place for it.    Disappointed that this relies on simulated hardware instead of real hardware. Can anyone explain to me like I'm five how hard it would be to perform this same feat but with real hardware? Real hardware costs real money, and you don't want to be learning and experimenting when each try costs hours of assembly work and potentially hundreds of dollars of materials.

Transistors aren't something you would build from scratch, and generally speaking logic gates aren't either. You *can* build fairly complex stuff using things like 7400 series chips, but breadboards get cumbersome pretty quickly, and PCBs drive up the cost a lot, especially if you ever make a mistake in the layout. FPGAs are more practical, but really aren't any different from simulating, except that you can wire up your device to real hardware.

Once you've got a stable and debugged design through the use of simulators, it's straightforward to translate it to hardware, either by ordering a custom PCB, or having an ASIC made from the HDL that you simulated. It'll still be expensive, though. Thanks. I figured completely-from-scratch hardware-building would be too onerous, and was hoping something like this thing you said was true:

&amp;gt;Once you've got a stable and debugged design through the use of simulators, it's straightforward to translate it to hardware, either by ordering a custom PCB, or having an ASIC made from the HDL that you simulated. 

Awesome.

&amp;gt; It'll still be expensive, though.

How expensive? I know it depends, but could you make an educated guess, for a custom PCB or ASIC sufficient to support the kind of device you build in this course (i.e. a basic PDA). $100? $500? I'm seriously considering doing this course, and I'd love to extend it into to physical hardware if I can afford it. Thanks. I figured completely-from-scratch hardware-building would be too onerous, and was hoping something like this thing you said was true:

&amp;gt;Once you've got a stable and debugged design through the use of simulators, it's straightforward to translate it to hardware, either by ordering a custom PCB, or having an ASIC made from the HDL that you simulated. 

Awesome.

&amp;gt; It'll still be expensive, though.

How expensive? I know it depends, but could you make an educated guess, for a custom PCB or ASIC sufficient to support the kind of device you build in this course (i.e. a basic PDA). $100? $500? I'm seriously considering doing this course, and I'd love to extend it into to physical hardware if I can afford it.   Seems like more a /r/learnprogramming thing than /r/compsci, but whatever...

I was a bit disappointed by this, though:

&amp;gt; Projects 1-5 focus on building the hardware platform of the computer system. Projects 6-12 build the computer's software hierarchy. Technically, one can build the hardware platform without a programming experience. There is no way though to complete projects 6-12 without knowing to program in some decent language like Java or Python. 

So we're building circuitry, and suddenly Python? Doesn't that skip a ton of layers? The course has you use something like Java or Python to write things like assemblers. You're not really skipping layers, just avoiding the unnecessary hassle of bootstrapping your entire toolchain from scratch.</snippet></document><document><title>Computer science students successfully boycott class final, exploiting a loophole to get perfect scores</title><url>http://www.jhunewsletter.com/2013/01/31/computer-science-students-successfully-boycott-class-final-76275/</url><snippet>  I had a professor who would give really difficult exams and also had a bonus condition that if you achieved a zero you would get 100%. Nobody ever got a zero. Some kids did get 2% on exams trying though. I'm impressed they got this to work, I imagine it would be a bit of a hostile environment at first. Playing with game theory, making sure nobody takes it. &amp;gt;Nobody ever got a zero.

Couldn't you just achieve a zero by not answering any of the questions? Answering every question was a condition of getting the zero flipped to a 100%. Many of which were multiple choice. Like 30-50 of them (half an exam). Some were even entirely multiple choice with 80-100 questions. Ah, free form would be easy to futz.

Q: Describe a covalent bond

A: Pizza. I wouldn't be so sure.  Some spiteful professor might argue that there exists some covalent bond in the chemical composition of pizza. Ion. Answering every question was a condition of getting the zero flipped to a 100%. Many of which were multiple choice. Like 30-50 of them (half an exam). Some were even entirely multiple choice with 80-100 questions. Odds of not getting at least one point are stupid low you would half to know all the answers in order to guess incorrectly  Odds of not getting at least one point are stupid low you would half to know all the answers in order to guess incorrectly  You don't need to know all the answers, you just need to be able to pick one answer that's definitely wrong.

For example:

    What is the integral of x^2
    a. 2x + C
    b. x^3 / 3 + C
    c. banana I assure you that not every question has a multiple choice answer that is completely irrelevant to the question. I assure you that not every question has a multiple choice answer that is completely irrelevant to the question. You don't need to know all the answers, you just need to be able to pick one answer that's definitely wrong.

For example:

    What is the integral of x^2
    a. 2x + C
    b. x^3 / 3 + C
    c. banana You don't need to know all the answers, you just need to be able to pick one answer that's definitely wrong.

For example:

    What is the integral of x^2
    a. 2x + C
    b. x^3 / 3 + C
    c. banana Definitely not a. That polynomial has the wrong order. I see what you mean. Play it safe and pick a. You don't need to know all the answers, you just need to be able to pick one answer that's definitely wrong.

For example:

    What is the integral of x^2
    a. 2x + C
    b. x^3 / 3 + C
    c. banana You don't need to know all the answers, you just need to be able to pick one answer that's definitely wrong.

For example:

    What is the integral of x^2
    a. 2x + C
    b. x^3 / 3 + C
    c. banana Odds of not getting at least one point are stupid low you would half to know all the answers in order to guess incorrectly  Odds of not getting at least one point are stupid low you would half to know all the answers in order to guess incorrectly  I had a professor who would give really difficult exams and also had a bonus condition that if you achieved a zero you would get 100%. Nobody ever got a zero. Some kids did get 2% on exams trying though. I'm impressed they got this to work, I imagine it would be a bit of a hostile environment at first. Playing with game theory, making sure nobody takes it. I had a professor that did this, but with the added incentive of getting 150%. The only person I knew that tried to do it left one answer she wasn't completely sure of blank, hoping it would be overlooked. The professor was nice though, and let her get 100 if she could do it correctly in front of the class afterward. I had a professor who would give really difficult exams and also had a bonus condition that if you achieved a zero you would get 100%. Nobody ever got a zero. Some kids did get 2% on exams trying though. I'm impressed they got this to work, I imagine it would be a bit of a hostile environment at first. Playing with game theory, making sure nobody takes it. I had a professor who would give really difficult exams and also had a bonus condition that if you achieved a zero you would get 100%. Nobody ever got a zero. Some kids did get 2% on exams trying though. I'm impressed they got this to work, I imagine it would be a bit of a hostile environment at first. Playing with game theory, making sure nobody takes it. I had a professor who would give really difficult exams and also had a bonus condition that if you achieved a zero you would get 100%. Nobody ever got a zero. Some kids did get 2% on exams trying though. I'm impressed they got this to work, I imagine it would be a bit of a hostile environment at first. Playing with game theory, making sure nobody takes it. I had a professor who would give really difficult exams and also had a bonus condition that if you achieved a zero you would get 100%. Nobody ever got a zero. Some kids did get 2% on exams trying though. I'm impressed they got this to work, I imagine it would be a bit of a hostile environment at first. Playing with game theory, making sure nobody takes it. Please, explain me something like I am five years old. It is obvious that 0 / 0 is undefined. So, the "loophole" is an undefined yet evident point in the rules. Why do the students know the final score will be 100% and not 0%?  Please, explain me something like I am five years old. It is obvious that 0 / 0 is undefined. So, the "loophole" is an undefined yet evident point in the rules. Why do the students know the final score will be 100% and not 0%?   I had three classes with this professor, and I heard many attempts to make this happen, but no one ever had the guts (or, more likely, coordination) to go through with it until now.

To be perfectly honest, though, my beef with his tests was less about the grading scheme, (in my opinion, it's better than having a test where everyone can fail) but with his choices for material on the test, which usually boiled down to obnoxious trivialities in the C language-family that, if encountered in practice, should be looked up in Google or Stack Exchange, and then replaced with something saner. &amp;gt; which usually boiled down to obnoxious trivialities in the C language-family

I had a teacher who would do this and create the most obfuscated pointer/memory/array code and tell us that once we really knew C then it will all make sense.  As it turned out half of his examples weren't even valid according to ANSI standards.  Just turns out that whatever compiler he was using (IIRC whatever MS had around 2003) was allowing him to get away with it - and would fail with other compilers. That's just plain awful. The stuff he gave us was at least valid enough to make it through gcc, albeit only if you left most of the warnings turned off. IMHO if you're going to engage in language pedantry, you should compile with `-Wall -pedantic` at *least*, and probably also `-ansi` while you're at it, or at least `-std=`something official. #include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;

int main(int argc, char *argv[]) {

	// prints hello world to stdout
	printf("%s\n", "Hello, world!");

	return 0;
}

gcc -c -ansi hello.c

error: expected expression before / token

screw -ansi
:'(
 Well, C99 is "something official", right? #include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;

int main(int argc, char *argv[]) {

	// prints hello world to stdout
	printf("%s\n", "Hello, world!");

	return 0;
}

gcc -c -ansi hello.c

error: expected expression before / token

screw -ansi
:'(
 &amp;gt; error: expected expression before / token

Single-line // comments don't exist in ANSI C, that's why. IMHO if you're going to engage in language pedantry, you should compile with `-Wall -pedantic` at *least*, and probably also `-ansi` while you're at it, or at least `-std=`something official. I don't really see the reason to use `-ansi`. That standard is very old, and you'll miss quite a few of the newer features that make your code better. &amp;gt; which usually boiled down to obnoxious trivialities in the C language-family

I had a teacher who would do this and create the most obfuscated pointer/memory/array code and tell us that once we really knew C then it will all make sense.  As it turned out half of his examples weren't even valid according to ANSI standards.  Just turns out that whatever compiler he was using (IIRC whatever MS had around 2003) was allowing him to get away with it - and would fail with other compilers. &amp;gt;whatever compiler he was using (IIRC whatever MS had around 2003) 

Probably the Intel C++ Compiler.  I had three classes with this professor, and I heard many attempts to make this happen, but no one ever had the guts (or, more likely, coordination) to go through with it until now.

To be perfectly honest, though, my beef with his tests was less about the grading scheme, (in my opinion, it's better than having a test where everyone can fail) but with his choices for material on the test, which usually boiled down to obnoxious trivialities in the C language-family that, if encountered in practice, should be looked up in Google or Stack Exchange, and then replaced with something saner. It's pretty easy to coordinate things via a class mailing list or with systems like [Piazza](https://piazza.com) as article seems to indicate the students did. Of course such organization is public to the class and can be moderated by the instructor. It takes only one student to not do it at it's frakked up for everyone else, which is what's so interesting about it. Especially if the other students will never know which student did it. Have you heard about prisoners dilemma? This is just that on a grand scale. It's not the same as the prisoners dilemma though due to the fact that everyone gains from not "telling" (participating in this case), unless the student was hoping to do better than a percentage of the class assuming the class is graded on a proper curve. Additionally because any student knows if another student enters the room to take the test, then they too will have the opportunity to take it. Whereas the prisoners dilemma, you are told that the others ratted you out thus it's better for you to comply, when that information is probably false. &amp;gt; It's not the same as the prisoners dilemma though due to the fact that everyone gains from not "telling" (participating in this case)

Isn't that exactly the case of prisoners dilemma too? I guess the numbers are a little different, but I would assume the same situation arises. Maybe it's just me that's a little scared of failing the test if anyone else does it properly.

&amp;gt; unless the student was hoping to do better than a percentage of the class assuming the class is graded on a proper curve.

I assumed that to be true. Having only the highest grade relative to the others sounds odd.

&amp;gt; Additionally because any student knows if another student enters the room to take the test, then they too will have the opportunity to take it.

What does this mean? I assume all students enter the room to take the test, they sit silently for the required time, and then hand in blank and/or answered tests.

&amp;gt; Whereas the prisoners dilemma, you are told that the others ratted you out thus it's better for you to comply, when that information is probably false.

That information is not probably false; that's the thing with prisoners dilemma. Most people react by ratting the other one out, so that is what's probably *true*.

For the single person, it makes so much more sense to take the test properly, because they have everything to gain and nothing to lose. For the community, though, it makes much more sense to hand in a blank test, but there's no guarantee everyone else would do it, so individually they have everything to gain and a lot to lose. &amp;gt; What does this mean? I assume all students enter the room to take the test, they sit silently for the required time, and then hand in blank and/or answered tests.

In that situation you are correct. It very much is like the prisoner's dilemma. The situation described in the article, however, stated that the students waited outside of the classroom to see if any student entered to take the test. The assumption was that if anyone entered, they would then all enter and take the test, thus not presenting any dilemma at all.

Edit: Grammar I had three classes with this professor, and I heard many attempts to make this happen, but no one ever had the guts (or, more likely, coordination) to go through with it until now.

To be perfectly honest, though, my beef with his tests was less about the grading scheme, (in my opinion, it's better than having a test where everyone can fail) but with his choices for material on the test, which usually boiled down to obnoxious trivialities in the C language-family that, if encountered in practice, should be looked up in Google or Stack Exchange, and then replaced with something saner. I had three classes with this professor, and I heard many attempts to make this happen, but no one ever had the guts (or, more likely, coordination) to go through with it until now.

To be perfectly honest, though, my beef with his tests was less about the grading scheme, (in my opinion, it's better than having a test where everyone can fail) but with his choices for material on the test, which usually boiled down to obnoxious trivialities in the C language-family that, if encountered in practice, should be looked up in Google or Stack Exchange, and then replaced with something saner.    Strange grading. If a student does not attend the exam it's a failed not 0 points at my university. I don't think any prof can change that, i think it's a rule of the uni.
 But they all showed up so the professor could still argue they attended the exam and got every question wrong by failing to complete them. But we need to go inside, take our place, start the exam and during the exam, the assistants would check who's there and you have to sign your exam. Everything else is a failed. I feel like this is a technicality that could easily be dealt with if their university required it and you had the organization to do the bit they did. Instead of organizing everybody to stand outside, just organize everybody to sit and sign, then leave.  &amp;gt; &#8220;In my courses, all grades are relative to the highest actually achieved score. Thus, if no one showed up and everyone got 0 percent, everyone would be marked as 100 percent,&#8221;

Wait a minute. Zero out of zero is just as much 15% or 128% as it is 100%. The highest score is 0. Thus, everyone gets a 100. I don't see that the second sentence follows from the first.

The quote says "all grades are relative to the highest actually achieved score". That formulation in itself doesn't fully specify what grades the students should get. The problem is that every fraction of zero is zero. Therefore, zero out of zero can bee seen as equal to the highest score, but also just as well half the highest score, or twice the highest score. I don't see that the second sentence follows from the first.

The quote says "all grades are relative to the highest actually achieved score". That formulation in itself doesn't fully specify what grades the students should get. The problem is that every fraction of zero is zero. Therefore, zero out of zero can bee seen as equal to the highest score, but also just as well half the highest score, or twice the highest score. I don't see that the second sentence follows from the first.

The quote says "all grades are relative to the highest actually achieved score". That formulation in itself doesn't fully specify what grades the students should get. The problem is that every fraction of zero is zero. Therefore, zero out of zero can bee seen as equal to the highest score, but also just as well half the highest score, or twice the highest score. Zero divided by zero is undefined.  Still, it worked. The problem is that an undefined value averaged in with all your other grades will result in an undefined semester average. Averaging an undefined semester grade into your GPA will make your GPA equally undefined. This may seem bad at first, but a computer scientist with a resume that said "GPA: NaN/4.0" would certainly stand out. Zero divided by zero is undefined.  Still, it worked. &amp;gt; &#8220;In my courses, all grades are relative to the highest actually achieved score. Thus, if no one showed up and everyone got 0 percent, everyone would be marked as 100 percent,&#8221;

Wait a minute. Zero out of zero is just as much 15% or 128% as it is 100%. That was my first thought as well. As it happens the article says (or at least heavily implies) that he explicitly told the students when explaining the policies that 0 max would be 100%. Yeah, it seems so, because "relative to the highest" doesn't really make it clear what happens when the highest is zero. &amp;gt; &#8220;In my courses, all grades are relative to the highest actually achieved score. Thus, if no one showed up and everyone got 0 percent, everyone would be marked as 100 percent,&#8221;

Wait a minute. Zero out of zero is just as much 15% or 128% as it is 100%. No. The professor grades on a [curve](https://en.wikipedia.org/wiki/Grade_curve).        30 comments in and not one mention of the prisoner's dilemma?  /r/compsci, I am disappointed. Probably because this isn't a prisoner's dilemma. Here if everyone else cooperates, there's no advantage to you defecting (taking the exam), and there's a disadvantage in terms of social repercussions (being "that guy"). So everyone cooperating is a Nash equilibrium here, while it's not in the prisoner's dilemma. A Nash equilibrium must assume you can predict the strategy employed by the other participants.  If it is possible the "that guy" is more interested in class rank than social repricussion, then there is no predicting what he might do.  All it takes is one wildcard or simply the threat of one, and then nobody can be trusted.

It took 7 years before any class pulled it off, so it seems pretty evident that the paranoia is real. There is more than one Nash equilibrium: *nobody* cooperating is also a Nash equilibrium. The point is that once everyone has signaled that they're not going to take the exam, there's no reason for anyone to defect, which is completely different from the Prisoner's dilemma.  If that were the case, how do you explain that it took 7 years before a class did it?  Or in xeonoex's case, 15 years?  Surely that implies that mutual trust is difficult to maintain even when cooperation is the logical and reasonable choice.  And *that* can be modeled as the prisoner's dilemma.  It's simply a variation on the normal scenario. I'm not sure what you're arguing here. Are you disagreeing that everyone cooperating is a Nash equilibrium? Because I don't think that's really in question here; "Nash equilibrium" is a precise term with a precise meaning which this clearly satisfies. If you're arguing that this is a prisoner's dilemma, again, that has a specific game theoretic definition that this doesn't satisfy. It's not a variation, it's a completely fundamental change. A prisoner's dilemma would be more akin to everyone getting a 90 if they all cooperated; then nobody would ever cooperate, because somebody could get a better grade by defecting.  If that were the case, how do you explain that it took 7 years before a class did it?  Or in xeonoex's case, 15 years?  Surely that implies that mutual trust is difficult to maintain even when cooperation is the logical and reasonable choice.  And *that* can be modeled as the prisoner's dilemma.  It's simply a variation on the normal scenario. All it takes is 1 person entering the classroom and everyone else will do it too. If that were the case, how do you explain that it took 7 years before a class did it?  Or in xeonoex's case, 15 years?  Surely that implies that mutual trust is difficult to maintain even when cooperation is the logical and reasonable choice.  And *that* can be modeled as the prisoner's dilemma.  It's simply a variation on the normal scenario. If that were the case, how do you explain that it took 7 years before a class did it?  Or in xeonoex's case, 15 years?  Surely that implies that mutual trust is difficult to maintain even when cooperation is the logical and reasonable choice.  And *that* can be modeled as the prisoner's dilemma.  It's simply a variation on the normal scenario. A Nash equilibrium must assume you can predict the strategy employed by the other participants.  If it is possible the "that guy" is more interested in class rank than social repricussion, then there is no predicting what he might do.  All it takes is one wildcard or simply the threat of one, and then nobody can be trusted.

It took 7 years before any class pulled it off, so it seems pretty evident that the paranoia is real. A Nash equilibrium must assume you can predict the strategy employed by the other participants.  If it is possible the "that guy" is more interested in class rank than social repricussion, then there is no predicting what he might do.  All it takes is one wildcard or simply the threat of one, and then nobody can be trusted.

It took 7 years before any class pulled it off, so it seems pretty evident that the paranoia is real. Probably because this isn't a prisoner's dilemma. Here if everyone else cooperates, there's no advantage to you defecting (taking the exam), and there's a disadvantage in terms of social repercussions (being "that guy"). So everyone cooperating is a Nash equilibrium here, while it's not in the prisoner's dilemma. Probably because this isn't a prisoner's dilemma. Here if everyone else cooperates, there's no advantage to you defecting (taking the exam), and there's a disadvantage in terms of social repercussions (being "that guy"). So everyone cooperating is a Nash equilibrium here, while it's not in the prisoner's dilemma. Not really, because if that one guy takes it then he's guaranteed 100%, regardless of his mark.  But if he doesn't take it and everyone else doesn't, he also gets 100%. So if he knows everyone else will cooperate, he gains no advantage from defecting. But he doesn't know, does he? Maybe someone else defects just in case, so they are pretty much guaranteed the 100% score. Then you better defect too so you don't fall as far behind as the punters who honestly believed everyone would hand in a blank test.

This is exactly the prisoners dilemma on a larger scale. The other guys say they'll hand in blanks, but the safest bet is to actually try doing the test anyway. Probably because this isn't a prisoner's dilemma. Here if everyone else cooperates, there's no advantage to you defecting (taking the exam), and there's a disadvantage in terms of social repercussions (being "that guy"). So everyone cooperating is a Nash equilibrium here, while it's not in the prisoner's dilemma. Moreover, there was a whole horde of people guarding the entrance... Yeah, that sort of ruins the fun.  The professor should take his diabolic psychological experiments more seriously next time. Moreover, there was a whole horde of people guarding the entrance... Cooperation enforced by an implied threat of force. I'm surprised it wasn't a political science class.  Probably because this isn't a prisoner's dilemma. Here if everyone else cooperates, there's no advantage to you defecting (taking the exam), and there's a disadvantage in terms of social repercussions (being "that guy"). So everyone cooperating is a Nash equilibrium here, while it's not in the prisoner's dilemma. There's an advantage to defecting if the course is graded on a curve, and everybody else failing benefits you. There's an advantage to defecting if the course is graded on a curve, and everybody else failing benefits you. 30 comments in and not one mention of the prisoner's dilemma?  /r/compsci, I am disappointed. 30 comments in and not one mention of the prisoner's dilemma?  /r/compsci, I am disappointed. That's because one of the key conditions of the prisoner's dilemma is that the prisoners can't communicate:

&amp;gt; [Each prisoner is in solitary confinement with no means of speaking to or exchanging messages with the other.](http://en.wikipedia.org/wiki/Prisoner%27s_dilemma) Actually, not being able to communicate is irrelevant at the game-theoretic level.

More important are the ability not to be able to enforce agreements and the effective simultaneity of the decisions.     So, the scores were basically all fractions, with the highest score as the denominator. 

Headline should be: CompSci students exploit division by zero bug in grading system. You're being downvoted because it's not division by zero.

The test wasn't worth zero points, just zero points were assigned to everyone with an average of 0.  I think they need more math courses... 0/0 is undefined, not 100%.
That being said, I'm surprised at the fact that they got everyone to do it. The test isn't out of 0 points. Yes, but if you're scaling everything to the highest score, you take a percent of the highest score (e.g. if the highest score were 79, then each student's percentage would be x/79).  In this case, the highest score is 0, so each student's percentage would be x/0; or for this specific set of cases, they would all be 0/0. He doesn't scale it, he simply adds (100 - highest score) to everyone's test. If the highest score is 80, add 20 points to everyone's test: 80/100 -&amp;gt; 100/100, 60/100 -&amp;gt; 80/100, 0/100 -&amp;gt; 20/100.

Though it's not relevant because the professor can grade however he wants, and decided that everyone getting a 0 means that everyone gets a 100. Yeah, all it says in the article is that the teacher grades "relative to the highest actually achieved score" and then talks in terms of percents.

And yes, the professor can grade however they want, I was simply pointing out a mathematical flaw as I saw it. [deleted]  Looks like no one in that class knows anything about game theory or they would have known the proper decision in the prisoner's dilemma is to go take the test.

Edit: guys, if I can't make silly references to compsci topics in r/compsci without everyone jumping down my throat, where do i do it? Thanks, I know this isn't actually the prisoners dilemma. Thanks, I know you know game theory, that's why I made the reference. Now quit acting like hipster bungholes about it.  And chance getting less than 100%?  No, this is a modified prisoner's dilemma in which cooperation produces the best individual outcome, but you have to gamble on the chance of a "that guy" appearing.  You know that guy.  The one who won't give you the answer to problem 3 on the homework because in his mind, the rest of you are his competition, and wouldn't think twice about throwing the rest of the class under the bus "because he studied so why wouldn't he take the final?".

If you have a that guy factor, then it goes back to being an interesting problem.  Do you trust the that guy to play along enough that you would go down with the ship if he backstabs you, or do you say screw it and take the test?  And then if he does play along, you are now the one backstabbing everyone.  Or, what if someone else in your situation chickens out an takes the test?  What if the "that guy" is entertained by your collective predicament and uses his station to further sow the seeds of doubt among the class?

*That*, sir, is computer science! Most of the class showed up, and if someone were to walk in they still could have taken the test. You aren't risking anything by cooperating because as soon as someone defects you can follow along with them, and the social stigma would only be on the first guy to walk in the room.

Unlike the prisoner's dilemma, the participants were able to act after seeing their peers' choices.  And chance getting less than 100%?  No, this is a modified prisoner's dilemma in which cooperation produces the best individual outcome, but you have to gamble on the chance of a "that guy" appearing.  You know that guy.  The one who won't give you the answer to problem 3 on the homework because in his mind, the rest of you are his competition, and wouldn't think twice about throwing the rest of the class under the bus "because he studied so why wouldn't he take the final?".

If you have a that guy factor, then it goes back to being an interesting problem.  Do you trust the that guy to play along enough that you would go down with the ship if he backstabs you, or do you say screw it and take the test?  And then if he does play along, you are now the one backstabbing everyone.  Or, what if someone else in your situation chickens out an takes the test?  What if the "that guy" is entertained by your collective predicament and uses his station to further sow the seeds of doubt among the class?

*That*, sir, is computer science! Looks like no one in that class knows anything about game theory or they would have known the proper decision in the prisoner's dilemma is to go take the test.

Edit: guys, if I can't make silly references to compsci topics in r/compsci without everyone jumping down my throat, where do i do it? Thanks, I know this isn't actually the prisoners dilemma. Thanks, I know you know game theory, that's why I made the reference. Now quit acting like hipster bungholes about it.  Looks like no one in that class knows anything about game theory or they would have known the proper decision in the prisoner's dilemma is to go take the test.

Edit: guys, if I can't make silly references to compsci topics in r/compsci without everyone jumping down my throat, where do i do it? Thanks, I know this isn't actually the prisoners dilemma. Thanks, I know you know game theory, that's why I made the reference. Now quit acting like hipster bungholes about it.  Actually this is a more real prisoners dilemma, covering an important contingency that the game theory scenario does not: if you rat on your friend, there is a high chance that he will kill you. 


Thus, I would think that the utility is much less for deviating, as academics isn't the sole source of utility. The most important thing is that they had people guarding the entrance, such that deviation would be immediately punished. You could probably map this out to a dominant strategy: if you don't take the test, you get 100%. If you do take the test, a frat bro breaks your legs.   Prisoner's dilemma solved by communication and credible threats. Looks like they learned something after all. Prisoner's dilemma specifically does not include communication. Prisoner's dilemma solved by communication and credible threats. Looks like they learned something after all. Not a Prisoner's Dilemma in any way. No one had an incentive to defect. Not a Prisoner's Dilemma in any way. No one had an incentive to defect. ...Screwing over the rest of the class and getting the easiest A of your life.   If I was a student in that class I would have trolled the shit out of them. I would have went in and pretended to take the test to see what the rest of them would do. If they thought I was taking the test the rational choice for them would be to also take it.

Such an opportunity missed! You also wouldn't get invited to any Pike parties for the rest of college.  But to have a 0 you need to know every answer so might as well get a hundred yourself. Wat To make sure you get every question wrong, you need to know the right answer to every question to make sure you don't pick it. If you know every answer you might as well pick it. The "wat" is probably because you didn't reply to http://www.reddit.com/r/compsci/comments/18lusx/computer_science_students_successfully_boycott/c8fwygi but to the article itself. Wrong reply box. Indeed, that makes more sense.  </snippet></document><document><title>The source code for Photoshop 1.0.1 is now officially available in the Computer History Museum </title><url>http://computerhistory.org/atchm/adobe-photoshop-source-code/</url><snippet>  Deep in that page I learned the current Photoshop has over 10,000,000 lines of code. God damn. I work on a fairly robust EMR software with 1/10th that. All our various EMRs we use combined plus server side code is easily well over 10,000,000 lines, but that was made and is maintained by at least 10 or more small teams.  
  
Wow.  [Mac Paint and Quickdraw](http://www.computerhistory.org/atchm/macpaint-and-quickdraw-source-code/) source code is also in the CHM. Neat stuff.  And still not free...   I have a Macintosh Classic that might run that one, but unfortunately I've got it from a school and never found the passwords. Just [download System 6 from Apple](http://download.info.apple.com/Apple_Support_Area/Apple_Software_Updates/English-North_American/Macintosh/System/Older_System/System_6.0.x/) and reinstall it. I have a Macintosh Classic that might run that one, but unfortunately I've got it from a school and never found the passwords.   This is cool and all, but i don't really think it belongs on r/compsci... Maybe r/opensource would be a better location

Edit: wow, I thought people would agree with me on this one.  Read the sidebar... r/compsi is supposed to be for general computer science concepts and theory, not a specific instance of software.  Can someone explain to me how this is related to compsci? It's not open source. sounds [open source](http://www.popphoto.com/gear/2013/02/photoshop-101-source-code-released-free-online) to me Can you modify it and re-release it?  No?  Then it isn't open source. This is cool and all, but i don't really think it belongs on r/compsci... Maybe r/opensource would be a better location

Edit: wow, I thought people would agree with me on this one.  Read the sidebar... r/compsi is supposed to be for general computer science concepts and theory, not a specific instance of software.  Can someone explain to me how this is related to compsci? I believe it belongs here as this link provides a collection of beautifully implemented algorithms. Most computer scientists hold an appreciation of elegant implementation and I believe would enjoy spending some time looking over these.

This is an beautiful collection of source code to read through and I hope the people in this subreddit enjoy it.  Okay.  Thank you for an actual answer.  I still think it would be better off in r/programming since it's implementation related, but I can see the argument for posting it here now.  </snippet></document><document><title>I just discovered the ISBN to BibTeX Converter and figured /r/compsci would also appreciate this tool</title><url>http://manas.tungare.name/software/isbn-to-bibtex/</url><snippet /></document><document><title>Computational Complexity: What if P = NP?</title><url>http://blog.computationalcomplexity.org/2004/05/what-if-p-np.html</url><snippet>  I would be more interested in how the polynomial-hierarchy would collapse. Check out slide 54 of [these lecture notes](http://sma.epfl.ch/~moustafa/Other/Complexityslides/lec13.pdf), which do a more thorough job of explaining then I am about to.  :-)

tl;dr:  if you know that one level of the hierarchy = P, then the next level up also = P because you can peel off the leftmost qualified variable to go back down a step of the hierarchy which you already know = P.  In particular, if (abusing notation here) P = NP is true for any level of the hierarchy (not just the bottom) then everything above, but not below, that level of the hierarchy collapses to it.  Since P=NP is at the bottom then this means that everything collapses to the bottom.  I thought there were current forms of asymmetric crypto that would stand up to P=NP. Am I confused with quantum computing? If I understand correctly, classical one way functions do not exist if P=NP. This means that there are no trapdoor one way functions to use as the base of an asymmetric crypto scheme. Hence, no classical asymmetric crypto.  Truly? This is not my area of expertise but if P ?= NP is settled is there some reason it's impossible to still question whether Exponential time ?= non-deterministic exponential time.

Cause if that was not the case I would imagine it it possible to have a similar one way function where one direction is exponential and the other direction is super exponential. 

 One way functions are typically defined as functions which can be computed in polynomial time but that cannot be inverted with non-negligible probability in polynomial time. This is a useful restriction on the definition since if a function takes exponential time to compute then we cannot really use it for encryption in the first place, even if it is incredibly difficult to invert. 

But this isn't really relevant to the discussion since if P=NP then EXPTIME=NEXPTIME.  &amp;gt;function takes exponential time to compute then we cannot really use it for encryption in the first place, even if it is incredibly difficult to invert.

Wouldn't it just mean you have to have less input?

&amp;gt;But this isn't really relevant to the discussion since if P=NP then EXPTIME=NEXPTIME.

oh that's cool. Is there anyway to explain why in a way that's accessible, or is it necessary to actually crunch all the math before getting any intuition for it? &amp;gt; Wouldn't it just mean you have to have less input?

I guess the keys could be incredibly tiny. To be honest, I don't know how it would turn out if we tried to use one way functions built out of problems in NEXPTIME. I could see it going either way (being feasible if you keep the input size way down or being completely infeasible). 

&amp;gt; Is there anyway to explain why in a way that's accessible, or is it necessary to actually crunch all the math before getting any intuition for it?

To get the details of the proof you are going to need to understand a  bit about complexity theory, but I can give you the overview. The proof type is called a [padding argument](http://en.wikipedia.org/wiki/Padding_argument). The wiki page conveniently shows an outline of a proof that EXPTIME=NEXPTIME if P=NP. 

Basically we are given some language in NEXPTIME and want to show that it is also in EXPTIME. To do this, we pad the length of the strings in the language with an exponential number of meaningless characters. This allows us to decide the padded language in non-deterministic polynomial time (since we bumped up the length of the strings by so much, our running time is now a polynomial of the length of input string instead of an exponential of the length of the input string). Since P=NP, we can construct a deterministic polynomial machine that decides this language. Our exponential algorithm basically just runs the polynomial algorithm we just found except it throws away the pad. Thanks so much for the explanation. My area is computer engineering but complexity theory is often interesting. I actually found your summary more useful then the wiki page. 

If I was to try and restate how the proof works: Given a statement S of length N in NEXPTIME, we form a statement S' made by appending exp(aN) nonsense characters to N. Given an appropriately chosen a, this S' is in NP. Therefore S' is in P. Therefore when we discard the nonsense S is in EXPTIME. 

The only thing that gives me pause is that when you pad a NEXP statement by exp(aN) characters to bring it into NP, there isn't a given 'a'; you need different values of 'a' for different statements in NEXP depending on how exponential they are. On second thought I'm not sure if that really matters. 

This is random, but do you know anything about how factoring and quantum computing relate to complexity theory? My impression of some of the facts:

* factoring is in NP

* it is not known if factoring is in P

* the best known conventional algorithm for factoring is exponential

* Shor's algorithm can factor numbers in poly time on a quantum computer

* it is unknown how exactly quantum computers affect time complexity. 

Not sure if all that is true, but if it is it seems like there is a gaping hole in complexity theory around quantum computing (not that comp sci is a field lacking in open problems, but still...).  Does that seem accurate, am I way offbase, or is that an area you are less acquainted with?   </snippet></document><document><title>Surrogate vs Natural Keys [Database Theory]</title><url>http://www.reddit.com/r/compsci/comments/18gvkl/surrogate_vs_natural_keys_database_theory/</url><snippet>Hello /r/compsci.

Earlier today I asked my teacher in Database design why we haven't used any auto incremented integer primary keys(surrogate keys) in the course. I have done a decent amount of development and this is my first course in databases so I am naturally influenced by less than optimal design choices from being self taught. 

His response was that he thinks surrogate keys are harmful and should be avoided as much as possible, now this came as a pretty big shock to me seing as every framework/example/tutorial/code I didn't write have always used surrogate keys. 

His selling point for using natural keys, keys that are attributes in the business logic, was in part that adding surrogate introduces more data and increases the numbers of joins required to find data. We didn't have time to talk for long so some of his arguments might have not been brought up.

This sparked my interest and I started reading some more about it on the internet. Overall the general opinion seems to advocate using surrogate keys while a smaller number of people argue that PKs should be natural if the attribute chosen is already some kind of identifier e.g part number, social security number. I didn't find any material advocating the usage of only natural keys. 

The requirements for a primary key are:

* It should be non NULL, unique and apply to all rows
* It should be minimal 
* It should be stable over time

As you might have guessed my teacher did not succeed in convincing me that natural keys are the way to go, I wouldn't even say I think it's a good idea to use natural keys in cases were it might seem to make sense like with social security numbers and part numbers. 

I see several problems with natural keys that far outweigh the pros. A natural key is duplicated in many places and since it involves business logic there is a very real chance that it will change value over time which means it has to be updated in every single place it's used as a reference, cascading updates does take care of this problem. I don't like relying on the user for my keys even though the database will validate uniqueness and constraints for me.

My main point how ever is that I really cannot think of any natural keys that are stable enough to be used safely. 


So /r/compsci  what is your opinion?  Do you know of any material advocating natural keys ?

TL;DR
Surrogate or Natural keys? 

 
  This has been discussed heavily. I don't know how much you've already researched, so here are some links. There is a terrifying amount of information being asserted in this thread without a single bit of hard evidence to back it up, so I beg you to *please* research this heavily before making a decision (including being for natural keys, which is my preference).

* Primary Keyvil [Part 1](http://it.toolbox.com/blogs/database-soup/primary-keyvil-part-i-7327), [Part 2](http://it.toolbox.com/blogs/database-soup/primary-keyvil-part-ii-7345), [Part 3](http://it.toolbox.com/blogs/database-soup/primary-keyvil-part-iii-7365).
* [SO](http://stackoverflow.com/questions/159087/composite-primary-keys-versus-unique-object-id-field/160085#160085) on this topic, and [again](http://stackoverflow.com/questions/63090/surrogate-vs-natural-business-keys) and [again](http://stackoverflow.com/questions/707657/picking-the-best-primary-key-numbering-system/718200#718200).
* You may find interesting thoughts in [DBDebunk papers](http://www.dbdebunk.com/)
* The writing of C J Date is extremely interesting. I've just found this new book [Database Design and Relational Theory: Normal Forms and all that Jazz](http://www.amazon.co.uk/Database-Design-Relational-Theory-Normal/dp/1449328016/ref=sr_1_2?ie=UTF8&amp;amp;qid=1360797388&amp;amp;sr=8-2). It also comes with a corresponding 10 hour master class, available on [O'Reilly](http://shop.oreilly.com/product/0636920025900.do).

To answer your question, just like everyone else said - it depends. I personally go with natural keys wherever possible because:

* I work in environments that do not need optimal write performance
* I find it more intuitive
* I find it easier to reason about
* It forces me to think about what constitutes a duplicate row up front. Even if you use a surrogate key this is *essential* for good database design!
* Less indexes to maintain (you'll need a unique index alongside the surrogate key anyway)
* Join data for freeee! For example, rather than joining on `country_id` against `country` to get an `iso_code`, I might just use `country_iso` as my FK - which often does the job just fine.
* A similar point to the above is that it can make debugging easier. Rather than having to laboriously join stuff just to check some assumptions I'm making are correct, I can often just eyeball the data.

A lot of these are very subjective, but most modern database's don't really make either advantageous in their own right (or more, there is no silver bullet). Do your research, understand the problem domain, and worry about the rest of your model first.

People keep saying "oh noes, SSN isn't unique!" Great, it's not, but throwing a surrogate ID on there hasn't solved your problems, unless your only problem was that you needed to use it in a FK, which I highly doubt. If you don't have an answer to "what constitutes a unique row" you should really take a step back and work out what, in the domain of the problem you're actually trying to solve (that is, your application), constitutes a uniqueness. This has been discussed heavily. I don't know how much you've already researched, so here are some links. There is a terrifying amount of information being asserted in this thread without a single bit of hard evidence to back it up, so I beg you to *please* research this heavily before making a decision (including being for natural keys, which is my preference).

* Primary Keyvil [Part 1](http://it.toolbox.com/blogs/database-soup/primary-keyvil-part-i-7327), [Part 2](http://it.toolbox.com/blogs/database-soup/primary-keyvil-part-ii-7345), [Part 3](http://it.toolbox.com/blogs/database-soup/primary-keyvil-part-iii-7365).
* [SO](http://stackoverflow.com/questions/159087/composite-primary-keys-versus-unique-object-id-field/160085#160085) on this topic, and [again](http://stackoverflow.com/questions/63090/surrogate-vs-natural-business-keys) and [again](http://stackoverflow.com/questions/707657/picking-the-best-primary-key-numbering-system/718200#718200).
* You may find interesting thoughts in [DBDebunk papers](http://www.dbdebunk.com/)
* The writing of C J Date is extremely interesting. I've just found this new book [Database Design and Relational Theory: Normal Forms and all that Jazz](http://www.amazon.co.uk/Database-Design-Relational-Theory-Normal/dp/1449328016/ref=sr_1_2?ie=UTF8&amp;amp;qid=1360797388&amp;amp;sr=8-2). It also comes with a corresponding 10 hour master class, available on [O'Reilly](http://shop.oreilly.com/product/0636920025900.do).

To answer your question, just like everyone else said - it depends. I personally go with natural keys wherever possible because:

* I work in environments that do not need optimal write performance
* I find it more intuitive
* I find it easier to reason about
* It forces me to think about what constitutes a duplicate row up front. Even if you use a surrogate key this is *essential* for good database design!
* Less indexes to maintain (you'll need a unique index alongside the surrogate key anyway)
* Join data for freeee! For example, rather than joining on `country_id` against `country` to get an `iso_code`, I might just use `country_iso` as my FK - which often does the job just fine.
* A similar point to the above is that it can make debugging easier. Rather than having to laboriously join stuff just to check some assumptions I'm making are correct, I can often just eyeball the data.

A lot of these are very subjective, but most modern database's don't really make either advantageous in their own right (or more, there is no silver bullet). Do your research, understand the problem domain, and worry about the rest of your model first.

People keep saying "oh noes, SSN isn't unique!" Great, it's not, but throwing a surrogate ID on there hasn't solved your problems, unless your only problem was that you needed to use it in a FK, which I highly doubt. If you don't have an answer to "what constitutes a unique row" you should really take a step back and work out what, in the domain of the problem you're actually trying to solve (that is, your application), constitutes a uniqueness. This has been discussed heavily. I don't know how much you've already researched, so here are some links. There is a terrifying amount of information being asserted in this thread without a single bit of hard evidence to back it up, so I beg you to *please* research this heavily before making a decision (including being for natural keys, which is my preference).

* Primary Keyvil [Part 1](http://it.toolbox.com/blogs/database-soup/primary-keyvil-part-i-7327), [Part 2](http://it.toolbox.com/blogs/database-soup/primary-keyvil-part-ii-7345), [Part 3](http://it.toolbox.com/blogs/database-soup/primary-keyvil-part-iii-7365).
* [SO](http://stackoverflow.com/questions/159087/composite-primary-keys-versus-unique-object-id-field/160085#160085) on this topic, and [again](http://stackoverflow.com/questions/63090/surrogate-vs-natural-business-keys) and [again](http://stackoverflow.com/questions/707657/picking-the-best-primary-key-numbering-system/718200#718200).
* You may find interesting thoughts in [DBDebunk papers](http://www.dbdebunk.com/)
* The writing of C J Date is extremely interesting. I've just found this new book [Database Design and Relational Theory: Normal Forms and all that Jazz](http://www.amazon.co.uk/Database-Design-Relational-Theory-Normal/dp/1449328016/ref=sr_1_2?ie=UTF8&amp;amp;qid=1360797388&amp;amp;sr=8-2). It also comes with a corresponding 10 hour master class, available on [O'Reilly](http://shop.oreilly.com/product/0636920025900.do).

To answer your question, just like everyone else said - it depends. I personally go with natural keys wherever possible because:

* I work in environments that do not need optimal write performance
* I find it more intuitive
* I find it easier to reason about
* It forces me to think about what constitutes a duplicate row up front. Even if you use a surrogate key this is *essential* for good database design!
* Less indexes to maintain (you'll need a unique index alongside the surrogate key anyway)
* Join data for freeee! For example, rather than joining on `country_id` against `country` to get an `iso_code`, I might just use `country_iso` as my FK - which often does the job just fine.
* A similar point to the above is that it can make debugging easier. Rather than having to laboriously join stuff just to check some assumptions I'm making are correct, I can often just eyeball the data.

A lot of these are very subjective, but most modern database's don't really make either advantageous in their own right (or more, there is no silver bullet). Do your research, understand the problem domain, and worry about the rest of your model first.

People keep saying "oh noes, SSN isn't unique!" Great, it's not, but throwing a surrogate ID on there hasn't solved your problems, unless your only problem was that you needed to use it in a FK, which I highly doubt. If you don't have an answer to "what constitutes a unique row" you should really take a step back and work out what, in the domain of the problem you're actually trying to solve (that is, your application), constitutes a uniqueness. This has been discussed heavily. I don't know how much you've already researched, so here are some links. There is a terrifying amount of information being asserted in this thread without a single bit of hard evidence to back it up, so I beg you to *please* research this heavily before making a decision (including being for natural keys, which is my preference).

* Primary Keyvil [Part 1](http://it.toolbox.com/blogs/database-soup/primary-keyvil-part-i-7327), [Part 2](http://it.toolbox.com/blogs/database-soup/primary-keyvil-part-ii-7345), [Part 3](http://it.toolbox.com/blogs/database-soup/primary-keyvil-part-iii-7365).
* [SO](http://stackoverflow.com/questions/159087/composite-primary-keys-versus-unique-object-id-field/160085#160085) on this topic, and [again](http://stackoverflow.com/questions/63090/surrogate-vs-natural-business-keys) and [again](http://stackoverflow.com/questions/707657/picking-the-best-primary-key-numbering-system/718200#718200).
* You may find interesting thoughts in [DBDebunk papers](http://www.dbdebunk.com/)
* The writing of C J Date is extremely interesting. I've just found this new book [Database Design and Relational Theory: Normal Forms and all that Jazz](http://www.amazon.co.uk/Database-Design-Relational-Theory-Normal/dp/1449328016/ref=sr_1_2?ie=UTF8&amp;amp;qid=1360797388&amp;amp;sr=8-2). It also comes with a corresponding 10 hour master class, available on [O'Reilly](http://shop.oreilly.com/product/0636920025900.do).

To answer your question, just like everyone else said - it depends. I personally go with natural keys wherever possible because:

* I work in environments that do not need optimal write performance
* I find it more intuitive
* I find it easier to reason about
* It forces me to think about what constitutes a duplicate row up front. Even if you use a surrogate key this is *essential* for good database design!
* Less indexes to maintain (you'll need a unique index alongside the surrogate key anyway)
* Join data for freeee! For example, rather than joining on `country_id` against `country` to get an `iso_code`, I might just use `country_iso` as my FK - which often does the job just fine.
* A similar point to the above is that it can make debugging easier. Rather than having to laboriously join stuff just to check some assumptions I'm making are correct, I can often just eyeball the data.

A lot of these are very subjective, but most modern database's don't really make either advantageous in their own right (or more, there is no silver bullet). Do your research, understand the problem domain, and worry about the rest of your model first.

People keep saying "oh noes, SSN isn't unique!" Great, it's not, but throwing a surrogate ID on there hasn't solved your problems, unless your only problem was that you needed to use it in a FK, which I highly doubt. If you don't have an answer to "what constitutes a unique row" you should really take a step back and work out what, in the domain of the problem you're actually trying to solve (that is, your application), constitutes a uniqueness. Okay, smartypants. What's the natural key of a blog post?

Yes, using an autoincrement column to index countries is silly.

But the idea that there is always (or even often) a natural key for user-generated data is sillier.

It really depends on your domain, but for many tables in many applications written today, surrogate keys are the only reasonable choice. Okay, smartypants. What's the natural key of a blog post?

Yes, using an autoincrement column to index countries is silly.

But the idea that there is always (or even often) a natural key for user-generated data is sillier.

It really depends on your domain, but for many tables in many applications written today, surrogate keys are the only reasonable choice. What about user and time? Should a user not be allowed to have two posts with the same time?

I claim they most definitely should, **otherwise your time column becomes a glorified, overly-complex auto-increment field**.

Think about this:

What does the time *represent*? Is it simply the time the post was made, never to be changed again? Or does it carry more meaning? In most CMS software, you can change the published time of a post&#8212;for example, you might be entering posts that were previously made in an old version of the software, or you might be entering posts to appear in the future. In this case I think it makes perfect sense to allow duplicate post times.

If on the other hand, you want to enforce unique times, then I do not see how this is any different than simply making the time column an auto-incremented integer ID. Except that it's a more complex, larger column.

Okay so users should be able to have two posts at the same time... so I guess the key should be (user, time, body)? It's reasonable not to allow two posts with identical content... but now your key is absolutely monstrous. Just think what your foreign keys are going to be like!  DB application developer with 17 years experience here.

I find that academics tend to advocate natural keys, and professional developers tend to use surrogate keys. Why? Are the academics smarter than the professionals? No, I think the reason is that natural keys seem like a great idea in theory, but that theory falls apart when confronted by the realities of the messiness of real-world data and situations. I'm sure you've already come across examples of these problems in your own research.

My advice for surrogate keys is the opposite of umlcat's. At least with SQL Server, integer IDENTITY columns will never return duplicate keys even highly concurrent environments. Their monotonically increasing nature makes them ideally suited as a table's clustered index because new values will always be appended to the end of the table on disk and never inserted in the middle of a previously allocated page, which could result in a cascade of page swapping. Integer surrogate keys are only 4 bytes wide, which means more index entries per page compared to any compound or natural key which in turn means less I/O which in turn means higher performance.

If you absolutely must generate probably-unique key values on the clients in highly concurrent scenarios, you should probably use GUIDs, but these are 4x as big as integers, so this will cause system performance to suffer somewhat. You will also need to be prepared to handle the (unlikely) possibility of collisions. Also, unless your clients can generate sequential GUIDs, you may see a lot more disk I/O when inserting new rows that result in needing to swap data off of an already-full page. The problems of using GUIDs as keys are amplified even further when using natural keys, which tend to be very wide compared to integers, often have little to no chance of being entered sequentially, and often have a higher chance of collision.

Depending on your choice of ORM or other DB tools, you may find that natural and/or composite keys are not well supported (or supported at all!) compared to auto-incrementing integer keys. I find this irritating, because there are some cases where a composite key comprised of multiple surrogate keys is actually the best design. So by bringing this point up, I don't mean to defend any tools with this moronic limitation. However, it should be acknowledged that they exist, and you might even be forced to use one. Tool support does matter. Natural keys are nice when they make sense in the domain, but as you've mentioned tooling is probably the main thing holding it back. If using composite keys was just as easy as using a surrogate key, I would use it more. Another thing, if you do Agile development, the DB can undergo significant changes, and it is simply easier to deal with with a non-changing surrogate.  I tend to favor surrogate keys over primary keys -- let's use your example of a social security number:

* SSNs are not unique, since they can be reused after death.
* Not all people have SSNs: one doesn't need to acquire one until he or she gets a job, or you could be dealing with illegal immigrants or international customers.
* SSNs can change: consider a reissuement after identity theft.

There are also advantages to letting the database pick the key. If you're in a scenario where records written together in a short period of time will be accessed together, you can greatly increase the chance that your DB engine won't have to load a bunch of pages off the disk. &amp;gt; There are also advantages to letting the database pick the key. If you're in a scenario where records written together in a short period of time will be accessed together, you can greatly increase the chance that your DB engine won't have to load a bunch of pages off the disk.

Arguments like this are really tricky to justify. While yes, they make a measurable improvement right now, there is no guarantee that they will remain optimal (or advantageous) in the future. One of the main points of relational databases is the logical separation, and when you start bringing the physical model into this, you limit the amount of long-term productivity that separation gives you. However, modelling in this way can often mean a sacrifice now in trade for a "trust" that things can get better in the future.

It's a difficult one. &amp;gt;While yes, they make a measurable improvement right now, there is no guarantee that they will remain optimal (or advantageous) in the future

I've never understood why this is a decision that database maintainers are expected to make.  The database should be fully capable of comprehending how it is used, and over time should only become more and more confident of the most efficient layout for its data, changing its layout as is advantageous.  Keeping query counts and analyzing the variance of what columns are used, what tables are joined, etc over time and restructuring things to perform most efficiently for the real use scenario of a production system is by no means an unsolvable problem. I tend to favor surrogate keys over primary keys -- let's use your example of a social security number:

* SSNs are not unique, since they can be reused after death.
* Not all people have SSNs: one doesn't need to acquire one until he or she gets a job, or you could be dealing with illegal immigrants or international customers.
* SSNs can change: consider a reissuement after identity theft.

There are also advantages to letting the database pick the key. If you're in a scenario where records written together in a short period of time will be accessed together, you can greatly increase the chance that your DB engine won't have to load a bunch of pages off the disk. &amp;gt; I tend to favor surrogate keys over primary keys -- let's use your example of a social security number [...]

The properties you list simply prove that SSNs are not primary keys. How many natural key are actually true primary keys? There are many, many properties that seem like primary keys, but then turn out not to be. And the ones that really are primary keys (like part numbers) are probably just made up and arbitrarily assigned in the same way that surrogate keys are!  It definitely depends on the situation. I use a mix of both at my job where I maintain DB's for the company. I find the more you normalize tables, the more you use surrogate keys, but at the basic level, my tables use natural keys (unless you just can't find a combination of attributes that assure a unique identifier).

I prefer using natural keys where possible. A lot of our data can be uniquely identified by location and time, so most keys are composite. 

In our case, the locations are from a defined list, and so to save space, we give each location name an integer i.d. So in this case we use a surrogate. But an i.d. for the location/time composite data would be useless since we are doing joins by time, or location, or both.

If you're going to be using a natural key in many places in the database then your tables probably are not being normalized properly. That's where surrogate keys come in. &amp;gt; If you're going to be using a natural key in many places in the database then your tables probably are not being normalized properly. That's where surrogate keys come in.

What on Earth? This is *exactly* what normalization gives you. In fact, as you go towards 6NF, you have nothing *but* natural keys. Sorry, but tacking on surrogate keys is *not* normalization.  Surrogate keys can encourage bad relational design. 

If you are aware of that fact  and are disciplined, you can mitigate that risk.

Also, if you think about how your data is used, natural keys can be helpful in organizing your data so that your index overhead is minimized while maximizing the use you can get out of your indexes.  (subset indexes, cardinality considerations and the like).   Again, a little thought can go a long way in this area even if you use surrogate keys.

Natural keys can be useful if you have larger, more complex structures because the identifying data tends to be passed around allowing you to forego joins on occasion.

The main argument against natural keys are that they are not *always* applicable and ORMs make using natural keys hard to work with (which is more of a shame on ORMs than natural keys).

I tend to use natural keys unless I am using a framework that takes care of the database for me. *Surrogate keys can encourage bad relational design.*

I believe you, but could you give an example?   I'd err on the side of surrogate keys in most cases. Your intuition about social security numbers is correct. If you had chosen to use SSN as your key, you'd be up the creek without a paddle. http://ssa-custhelp.ssa.gov/app/answers/detail/a_id/79/~/request-for-a-different-social-security-number The possibility you mentioned of updating the natural key and cascading the update over the references is very bad in all but very special use cases, and should be avoided.

Join tables, however, are one example where a composite key is sufficient, and no surrogate key is necessary. Whether the components of the composite are natural or surrogate, however, is immaterial.

Keep in mind that there's nothing stopping you from indexing, even uniquely, something like a social security column, while still allowing you to separate the identity from the state.

The argument that surrogate keys shouldn't be used because they introduce more data is flimsy. Knowledge of the data and the necessary functionality should be sufficient to make the decision without resorting to blanket statements about imagined performance impact.

The argument that it requires more joins sounds like it is vaguely based on NoSQL thinking, which implies structuring your data (including denormalizing) in ways that are efficient only for your known usage patterns, but I can't really put my finger on what he means. Certainly, simple keys are more efficient to join with than composite keys are, but that is orthogonal to the surrogate/natural key issue. The best partis that when SSN's are your primary key, you invariably end up with some damn fool who creates a web service to get your data, and lets you use SSNs as a search criteria. 

Then he makes it public (if undocumented) over __*http*__.

True story.  My first thought was that a lot of data doesn't *have* a "natural" key that satisfies the requirements of a primary key.  First, I have to say that this has been a very good discussion and I love the variety of opinions.  As for my thoughts, I'll keep them short.

1. If you can, prefer natural keys that are short.  Indices work better with shorter fields.  I've always liked "codes" as good fields.  I believe all RDBMSs don't like having indices on VARCHARS that have to store a "large" portion of the string.
2. Natural keys can help with debugging.  If I'm looking at a table and I want to make sure that Cincinnati is in Ohio, it's easier for me to verify if I have the abbreviation "OH" in the table vs. having to look up the number 43 in a surrogate table.
3. Most RDBMSs support cascading changes to keys, so if you do change a primary key, the change can cascade to everything that uses it, so relying on a surrogate key to prevent having to update all referencing tables isn't quite as necessary.
4. Surrogate keys can improve performance because of their being, hopefully, smaller and at least integer types should be very fast to compare, however I'd do benchmarks before I rely on this knowledge.
5. Consider the size of your application.  A small project most likely has no need for best performance, so sticking with natural keys may help keep things easy to work with.  However, in a data warehouse, see the previous bullet.
6. MySQL specific: with the 5.0 release, IIRC, they implemented a new indexing strategy where you can use multiple indices in a single query on a single table.  The recommendation were were given at the conference was to always use surrogate keys in a data warehouse and take advantage of this new strategy for best performance.  I don't know if this advice has changed, but I wanted to mention it just in case.

In my personal experience, I usually ended up in a hybrid approach where short natural keys were used where appropriate, but surrogate keys were used when the keys got larger.  However, you have to do what's best for you and your project. &amp;gt; I believe all RDBMSs don't like having indices on VARCHARS that have to store a "large" portion of the string.

Possibly, but you (or others) might be interested in reading Hubert Lubaczewski (depesz's) findings on the difference in performance between integer and character based indexes. See [=123 vs ='Depesz'. What is faster?](http://www.depesz.com/2012/06/07/123-vs-depesz-what-is-faster/) and the [followup](http://www.depesz.com/2012/06/08/123%E2%80%B3-vs-depesz-followup/).

&amp;gt; Surrogate keys can improve performance because of their being, hopefully, smaller and at least integer types should be very fast to compare, however I'd do benchmarks before I rely on this knowledge.

This point seems to be missed - I consider it so obvious, but forgot to mention it. Thanks for bringing that up! It can't be stressed enough.

All good points.   Hi. I have worked with DB for about 15 years. I have used both concepts, &amp;amp; sometimes, one its better than other, and viceversa.

Let me remark, that there are two ways to work with surrogate keys.

(1) One way is that when you add a record to a table, the database server generate the key value (like autoincrement fields)...

(2) ... and another, that your application generate, automatically, the key value, and send it, with the rest of the fields.

I have trouble with several databases that generate their own fields like autoincrement fields, due to several problems, like the database generate a duplicated key, specially if people work with paralell programming, concurrent programming, several sever cores, several CPU's.

I suggest that when choosing a surrogate key, generate the key automatically from the application, using a good method, for example, Universal Unique Identifiers.

Another way, its to have a table that stores an integer key for each table, and each time the value its read it, the value its incremented, even if its not used.

Natural Keys, are better when using common data, that requires to be easy identified. Imagine you have a e-store, and you sell products, and you have to store the country where the delivery is made.

Instead of making a new table with new surrogate autoincrement keys, 
for each country, you could use the "2" or "3" letter standard for Internet countries, ("us" for U.S., "ca" for Canada).

(3) Another important thing to consider, in natural keys, is that there exist also 2 ways to handle them. Single Fields, like U.S. Security Numbers, and multiple key, a.k.a. "Composite Key".

Composite Keys, can be used, but, sometimes, it can be difficult to handle. 

Its common to design a header / parent table ("SaleHeader", "Products"), with their own single key field, and a detail / child table ("SalesProducts"), with a key field composed by the header composed by parent keys ("SalesHeader_Key", "Product_Key"), &amp;amp; a consecutive number for each product.

But, if you need to replace the parent key, in case of an error, or want to check all fields, it can be boring. Composite Keys are also slow to read in a query, and some database severs, doesn't even support it.

Cheers.

TL;DR There is not perfect solution, but, Natural are better for short tables, commonly used data, surrogate for big tables. But, avoid keys generated by your server, and avoid composite keys. Hi. I have worked with DB for about 15 years. I have used both concepts, &amp;amp; sometimes, one its better than other, and viceversa.

Let me remark, that there are two ways to work with surrogate keys.

(1) One way is that when you add a record to a table, the database server generate the key value (like autoincrement fields)...

(2) ... and another, that your application generate, automatically, the key value, and send it, with the rest of the fields.

I have trouble with several databases that generate their own fields like autoincrement fields, due to several problems, like the database generate a duplicated key, specially if people work with paralell programming, concurrent programming, several sever cores, several CPU's.

I suggest that when choosing a surrogate key, generate the key automatically from the application, using a good method, for example, Universal Unique Identifiers.

Another way, its to have a table that stores an integer key for each table, and each time the value its read it, the value its incremented, even if its not used.

Natural Keys, are better when using common data, that requires to be easy identified. Imagine you have a e-store, and you sell products, and you have to store the country where the delivery is made.

Instead of making a new table with new surrogate autoincrement keys, 
for each country, you could use the "2" or "3" letter standard for Internet countries, ("us" for U.S., "ca" for Canada).

(3) Another important thing to consider, in natural keys, is that there exist also 2 ways to handle them. Single Fields, like U.S. Security Numbers, and multiple key, a.k.a. "Composite Key".

Composite Keys, can be used, but, sometimes, it can be difficult to handle. 

Its common to design a header / parent table ("SaleHeader", "Products"), with their own single key field, and a detail / child table ("SalesProducts"), with a key field composed by the header composed by parent keys ("SalesHeader_Key", "Product_Key"), &amp;amp; a consecutive number for each product.

But, if you need to replace the parent key, in case of an error, or want to check all fields, it can be boring. Composite Keys are also slow to read in a query, and some database severs, doesn't even support it.

Cheers.

TL;DR There is not perfect solution, but, Natural are better for short tables, commonly used data, surrogate for big tables. But, avoid keys generated by your server, and avoid composite keys. Hi. I have worked with DB for about 15 years. I have used both concepts, &amp;amp; sometimes, one its better than other, and viceversa.

Let me remark, that there are two ways to work with surrogate keys.

(1) One way is that when you add a record to a table, the database server generate the key value (like autoincrement fields)...

(2) ... and another, that your application generate, automatically, the key value, and send it, with the rest of the fields.

I have trouble with several databases that generate their own fields like autoincrement fields, due to several problems, like the database generate a duplicated key, specially if people work with paralell programming, concurrent programming, several sever cores, several CPU's.

I suggest that when choosing a surrogate key, generate the key automatically from the application, using a good method, for example, Universal Unique Identifiers.

Another way, its to have a table that stores an integer key for each table, and each time the value its read it, the value its incremented, even if its not used.

Natural Keys, are better when using common data, that requires to be easy identified. Imagine you have a e-store, and you sell products, and you have to store the country where the delivery is made.

Instead of making a new table with new surrogate autoincrement keys, 
for each country, you could use the "2" or "3" letter standard for Internet countries, ("us" for U.S., "ca" for Canada).

(3) Another important thing to consider, in natural keys, is that there exist also 2 ways to handle them. Single Fields, like U.S. Security Numbers, and multiple key, a.k.a. "Composite Key".

Composite Keys, can be used, but, sometimes, it can be difficult to handle. 

Its common to design a header / parent table ("SaleHeader", "Products"), with their own single key field, and a detail / child table ("SalesProducts"), with a key field composed by the header composed by parent keys ("SalesHeader_Key", "Product_Key"), &amp;amp; a consecutive number for each product.

But, if you need to replace the parent key, in case of an error, or want to check all fields, it can be boring. Composite Keys are also slow to read in a query, and some database severs, doesn't even support it.

Cheers.

TL;DR There is not perfect solution, but, Natural are better for short tables, commonly used data, surrogate for big tables. But, avoid keys generated by your server, and avoid composite keys. &amp;gt; I have trouble with several databases that generate their own fields like autoincrement fields, due to several problems, like the database generate a duplicated key, specially if people work with paralell programming, concurrent programming, several sever cores, several CPU's.

What?  This can't happen or you're doing it wrong, or your db is buggy.  In T-SQL (MS SQL), it's a identity column.  You create it as follows:

    CREATE TABLE Customer (id int identity(1,1) primary key, name nvarchar(100));

In this case, you insert as follows:

    INSERT INTO Customer (name) VALUES ( 'John Doe');

This works on every database I've ever used with different syntax.  MySQL calls it AUTO_INCREMENT.  PgSQL calls it SERIAL.  These databases are designed to do this, even with high load and multithreading.  DBs use locking just like your multithreading code to ensure these things work.

The only way I've ever seen this fail is when people do it wrong by generating them manually like this:

    INSERT INTO Customer (id, name) VALUES ((SELECT max(id) + 1 FROM Customer), 'John Doe');

or worse, query the db for the max key, and add one, and then write back to the db, without creating a transaction, which is a huge recipe for failure.

I've seen *those* fail, but they're a horrible abuse of SQL.  If for some reason you *need* to generate the IDs in the application, and I recommend against this, it's better to let the db do things for you when possible, *then* I'd use the GUID/UUID.

I've been working with DBs for 7 years professionally, including DB2, MySql, PgSql, and MS SQL.  If you're using Oracle, then maybe you're right, it tends to be immensely different than the others.   Man I feel strange being that person who loves UUIDs for almost everything. UUIDs are the worst for keys. They have no meaning, no order, and are too long. A key doesn't need to have meaning, nor order. The 'too long' thing needs both evidence to prove that this is at all a problem, and then analysis to decide if the drawbacks of using a UUID outweigh the advantages, *in a specific problem domain*. Right now, whether you intend to or not, you are spreading Fear, Uncertainty, and Doubt.    There are cases for both, but I've found that I prefer int keys.  MySQL also expects int keys and will create them anyway if you don't and not give you access to them.

Also, what should the natural key be for a customer data record?  Name?  Multiple people have the same name.  Name and Address?  That's going to be over half a dozen columns, or depend on yet another key if the address table is normalized as well.  Probably email, but that's a bizarre key and can also change. To the best of my knowledge, MySQL doesn't "expect" int keys, or rather - MySQL will still uniquely label that row regardless of whether or not you use a surrogate ID. That's an almost entirely internal detail to MySQL. Do you have any information to suggest otherwise?

&amp;gt; That's going to be over half a dozen columns, or depend on yet another key if the address table is normalized as well.

I'm not able to parse this. If a table is well normalized, then all other properties are fully functional depend on the primary key itself. There is no 'depending on another key' with normalized tables, other than perhaps having a foreign key as a subset of the key. But then, what's the problem with that? Can't seem to find an article on this right now, but my understanding is that MySQL always creates integer rowids internally if you don't create a unique integer field.  So not creating it, and using a natural key if it is not an integer will not give you access to the real keys, despite them taking up space in your db.

&amp;gt; other than perhaps having a foreign key as a subset of the key. But then, what's the problem with that?

If you normalize your tables to 3NF, you'd have to split out addresses from your customer table into a seperate table.  If you'd use natural keys for that, you'd have to use a compound key containing all fields in the address table (address_line1, address_line2, city, state, zip), which would be absurd.  You'd have to create a surrogate key for it.  Which gives you:

Customer: name, addressid, ....

Address: id, street, city, state, zip

If you want to use a natural key for Customer, you'll have to use a compound key again.  That doesn't sound like much of a problem until you realize how much of a headache that is to update.  Say you have a family that all live at the same place, and then one of them moves away.  You can't just update the address in the address table, because it will change the other two family member's address who still live there.  But, if you instead add a new record and update the customer table, you've broken every other table that use (name, addressid) as a foreign key to customer.  This breaks the stable over time rule that OP mentioned, because the primary id can change.  Someone could also change their name, phone number, email address, so there's really nothing in a customer record that's stable enough to be used as a natural key, unless you work for a bank or some business that has the ability to request your ssn from you.

I guess what I'm saying is that I've found in most everything I deal with, surrogate keys end up working better almost all the time. &amp;gt; So not creating it, and using a natural key if it is not an integer will not give you access to the real keys, despite them taking up space in your db.

My point is, that again - to the best of my understanding - even if you do use a surrogate autoincrementing primary key, you will *still* have this underlying secondary integer on the row. But I am not at all familiar with MySQL, so this is just an assumption - if it matters to anyone reading this post, please research it yourself and correct me.

&amp;gt; Say you have a family that all live at the same place, and then one of them moves away. You can't just update the address in the address table, because it will change the other two family member's address who still live there.

Correct, but you're not updating addresses directly, you're changing the address of a customer.

&amp;gt; But, if you instead add a new record and update the customer table, you've broken every other table that use (name, addressid) as a foreign key to customer.

We have cascading foreign keys for this, so at least theoretically, this is a solved problem in most DBMS. Can't seem to find an article on this right now, but my understanding is that MySQL always creates integer rowids internally if you don't create a unique integer field.  So not creating it, and using a natural key if it is not an integer will not give you access to the real keys, despite them taking up space in your db.

&amp;gt; other than perhaps having a foreign key as a subset of the key. But then, what's the problem with that?

If you normalize your tables to 3NF, you'd have to split out addresses from your customer table into a seperate table.  If you'd use natural keys for that, you'd have to use a compound key containing all fields in the address table (address_line1, address_line2, city, state, zip), which would be absurd.  You'd have to create a surrogate key for it.  Which gives you:

Customer: name, addressid, ....

Address: id, street, city, state, zip

If you want to use a natural key for Customer, you'll have to use a compound key again.  That doesn't sound like much of a problem until you realize how much of a headache that is to update.  Say you have a family that all live at the same place, and then one of them moves away.  You can't just update the address in the address table, because it will change the other two family member's address who still live there.  But, if you instead add a new record and update the customer table, you've broken every other table that use (name, addressid) as a foreign key to customer.  This breaks the stable over time rule that OP mentioned, because the primary id can change.  Someone could also change their name, phone number, email address, so there's really nothing in a customer record that's stable enough to be used as a natural key, unless you work for a bank or some business that has the ability to request your ssn from you.

I guess what I'm saying is that I've found in most everything I deal with, surrogate keys end up working better almost all the time. I think I recall what you're talking about with MySQL and it has to do with if you don't put a primary key on an InnoDB table, it automatically creates one that's an auto-increment field.  At least that what I recall.  I know for a fact that not all tables are expected to have one.</snippet></document><document><title>MIT researchers design molecular Boolean circuits [x-post from r/science]</title><url>http://www.nature.com/news/how-to-turn-living-cells-into-computers-1.12406</url><snippet /></document><document><title>I am writing a paper about computer interfaces for the disabled user. Do you have any interesting sources for this?</title><url>http://www.reddit.com/r/compsci/comments/18glxl/i_am_writing_a_paper_about_computer_interfaces/</url><snippet>Hi all,

I am writing a paper for one of my Computer Science courses, Computer Human Interfaces. The topics for this paper are completely open-ended and I decided that an interesting topic would be about what is in involved in designing applications so that it suits any (or every) type of disabled user. 

I think I can structure this paper to be argumentative, informative, or any other format as he didn't specify. 

So back to my question: Do you scholars have any interesting (preferably scholarly) articles about this topic? It can relate in any way, I just want to learn as much as I can about this topic before writing it.  I have a bit of a sideways comment. I was in a Walmart a while back  in the video games section. There was a physically and seemingly mentally disabled guy in a wheelchair (that had electric controls but his mother still overrode as I don't think he could navigate a store.) But he was staring at the covers of the various video games with seriously intense interest. But I very much doubt that he had either the physical or mental capacity to complete say a level of Halo. What I think would be great would be to create a simpler version of existing games. Take halo, the idea would be that the game would play through almost regardless of what inputs you used along with the character being much tougher. You would push forward and the character would have a preordained path that they would follow with a huge propensity to stick to the path. If you face an enemy the target would really stick to the enemy. If you fire they get hit, if they fire you take damage but your damage resistance is inversely proportional to your remaining health. Ammunition should be nearly unlimited. All told you would be on a marginally interactive walkthrough of the game. I would think that for many people this would bring them much pleasure to be able to play just "like" everyone else.

I don't see this as a profitable venture but sort of a right-thing-to-do. I can't see it being very hard to take an existing game and make this kind of special version.
 I have a bit of a sideways comment. I was in a Walmart a while back  in the video games section. There was a physically and seemingly mentally disabled guy in a wheelchair (that had electric controls but his mother still overrode as I don't think he could navigate a store.) But he was staring at the covers of the various video games with seriously intense interest. But I very much doubt that he had either the physical or mental capacity to complete say a level of Halo. What I think would be great would be to create a simpler version of existing games. Take halo, the idea would be that the game would play through almost regardless of what inputs you used along with the character being much tougher. You would push forward and the character would have a preordained path that they would follow with a huge propensity to stick to the path. If you face an enemy the target would really stick to the enemy. If you fire they get hit, if they fire you take damage but your damage resistance is inversely proportional to your remaining health. Ammunition should be nearly unlimited. All told you would be on a marginally interactive walkthrough of the game. I would think that for many people this would bring them much pleasure to be able to play just "like" everyone else.

I don't see this as a profitable venture but sort of a right-thing-to-do. I can't see it being very hard to take an existing game and make this kind of special version.
  http://stackoverflow.com/questions/118984/how-can-you-program-if-youre-blind   For the colorblind disablity:

[The use of color so that color blind people \(and everyone else\) can see what you are presenting.](http://www.neitzvision.com/content/excuseme.html) I think I might use color blindness as my topic for this paper. I think it would be wise to narrow the topic down to one type of disability and I don't have a lot of time to write this paper. Also, I have a friend who is 100% color blind (I think) so he would be a valuable source. Thanks for bringing this to my attention.  I think I might use color blindness as my topic for this paper. I think it would be wise to narrow the topic down to one type of disability and I don't have a lot of time to write this paper. Also, I have a friend who is 100% color blind (I think) so he would be a valuable source. Thanks for bringing this to my attention.              </snippet></document><document><title>Free Quantum Mechanics and Quantum Computation Course (Started yesterday, but you can still register)</title><url>https://www.edx.org/courses/BerkeleyX/CS191x/2013_Spring/about</url><snippet>     Unfortunetly I am not able to take the course right now, but does anyone know if it will be offered again next year or semester?  How long does the course run for? Or equivalently, how many lectures are there per week?   As a CS student, do these courses offer anything other than personal enrichment? Yeah, they give a certificate, but what is that really worth?

Does it count against federal financial aide standards? I can't afford any class that doesn't go towards a degree.  all these courses kind of make me wish i wasnt paying &#163;3,500 a year :/ college is awesome, the environment and contacts and friends are worth the cost. make the most of the people, the courses should be easy.</snippet></document><document><title>Ernst-Hugo: A simple automaton with a working assembler.</title><url>http://thegrandmotherblogg.wordpress.com/2013/01/29/a-simple-automaton/</url><snippet /></document><document><title>Regexp derivatives to LLVM [mp4]</title><url>http://mirror.linux.org.au/linux.conf.au/2013/mp4/redgrep_from_regular_expression_derivatives_to_LLVM.mp4</url><snippet>   quite beautiful code ... I'm curious how much it would suffer from being extended to become an alternative to POSIX EREs or PCRE:  the main difficulty here being of course match groups.

Maybe also greediness quantifiers, though I can imagine some cases where conjunctions and negations would make these irrelevant - perhaps the normalisation step could eliminate them from a surface syntax? I have a crazy plan to implement capturing groups by using Antimirov partial derivatives &#8211; extended by Caron et al. to support conjunction and complement &#8211; and Laurikari tagged transitions in order to construct a TNFA and then a TDFA and then LLVM IR. quite beautiful code ... I'm curious how much it would suffer from being extended to become an alternative to POSIX EREs or PCRE:  the main difficulty here being of course match groups.

Maybe also greediness quantifiers, though I can imagine some cases where conjunctions and negations would make these irrelevant - perhaps the normalisation step could eliminate them from a surface syntax? </snippet></document><document><title>Introduction to Parallel Programming - Overview class by NVIDIA</title><url>https://www.udacity.com/course/cs344</url><snippet>  Shouldn't this be in /r/learnprogramming? Either way, it's awesome. It is, I signed up for this class the day they announced it (a few months ago).  After the first unit, I must admit that I am incredibly impressed and looking forward to more content becoming available.

Every other class I've done on udacity has been fantastic too, and you can't beat free!  thanks for the link to this never heard of this website . its gold ! Man, they are great, but they are doing a really poor job at marketing.

There is a class made by Peter Norvig there. thanks for the link to this never heard of this website . its gold !  GPUs that run CUDA are not regular processors with 10s of thousands of cores.  If your algorithm accesses memory too many times, the algorithm slows to a crawl.   If your parallelism is not "fine grained" enough, then using CUDA does not create any advantage over a CPU. GPUs that run CUDA are not regular processors with 10s of thousands of cores.  If your algorithm accesses memory too many times, the algorithm slows to a crawl.   If your parallelism is not "fine grained" enough, then using CUDA does not create any advantage over a CPU.</snippet></document><document><title>What is the fastest data-structure for cross-word like searching?</title><url>http://www.reddit.com/r/compsci/comments/1856zv/what_is_the_fastest_datastructure_for_crossword/</url><snippet>Think of a fixed-size dictionary of words on which you want to perform a lot of lookups like:

    Find all words with "I" on the second position and "K" on the fourth.

In my case words can be of any length. The first thing which comes in mind is Trie, but is it the best option for this problem?  Be careful with demands like "fastest". Are you sure you want to pay the space cost incurred by answering the question literally? The absolute fastest data structure goes like this:

1. Look through the dictionary and find the longest word, and call this length n.
2. Now, all queries are of the form of a function mapping indices in [0..n] to a number in [1..28] representing either a letter, a hole, or an end-of-word marker.
3. It's easy to turn a query of this form into a number between 0 and 28^n in O(1) time.
4. Now your data structure is an array, and querying it is an array lookup, so everything is O(1) time to answer.

The space cost of this approach is quite high. According to this machine's `/usr/share/dict/words`, the longest word is "electroencephalographs", weighing in at 22 characters long. Assuming you can store the answer in a single word somehow (unlikely), that's 10^23 gigabytes. You'll need a 128-bit machine just to be able to address that much memory.

Building the data structure is hell, but querying it is faster than a squirrel on coke.

Edit: possibly looking at the dictionary, concluding that the problem space is finite, and then claiming O(1) time is unconvincing (since after all, nearly any damn algorithm is O(1) when the input space is finite). So the more correct answer is that this solution takes O(n) time, where n is the length of the longest word (but is _independent_ of the query). Okay, more realistically: construct a (minimal) DFA for your dictionary. When the query comes in, construct a minimal DFA for that (this should be easy, no need for an expensive minimization step here) and then perform DFA intersection. Then you can just read off the accepting paths in the result. This should be fast in implementation time (you can probably use off-the-shelf DFA libraries), fast in runtime, and relatively low in space costs. A battle-tested automaton library that can do this intersection operation is OpenFST (the specific operation is [here](http://www.openfst.org/twiki/bin/view/FST/IntersectDoc)). Okay, more realistically: construct a (minimal) DFA for your dictionary. When the query comes in, construct a minimal DFA for that (this should be easy, no need for an expensive minimization step here) and then perform DFA intersection. Then you can just read off the accepting paths in the result. This should be fast in implementation time (you can probably use off-the-shelf DFA libraries), fast in runtime, and relatively low in space costs. Okay, more realistically: construct a (minimal) DFA for your dictionary. When the query comes in, construct a minimal DFA for that (this should be easy, no need for an expensive minimization step here) and then perform DFA intersection. Then you can just read off the accepting paths in the result. This should be fast in implementation time (you can probably use off-the-shelf DFA libraries), fast in runtime, and relatively low in space costs. DFA ? Be careful with demands like "fastest". Are you sure you want to pay the space cost incurred by answering the question literally? The absolute fastest data structure goes like this:

1. Look through the dictionary and find the longest word, and call this length n.
2. Now, all queries are of the form of a function mapping indices in [0..n] to a number in [1..28] representing either a letter, a hole, or an end-of-word marker.
3. It's easy to turn a query of this form into a number between 0 and 28^n in O(1) time.
4. Now your data structure is an array, and querying it is an array lookup, so everything is O(1) time to answer.

The space cost of this approach is quite high. According to this machine's `/usr/share/dict/words`, the longest word is "electroencephalographs", weighing in at 22 characters long. Assuming you can store the answer in a single word somehow (unlikely), that's 10^23 gigabytes. You'll need a 128-bit machine just to be able to address that much memory.

Building the data structure is hell, but querying it is faster than a squirrel on coke.

Edit: possibly looking at the dictionary, concluding that the problem space is finite, and then claiming O(1) time is unconvincing (since after all, nearly any damn algorithm is O(1) when the input space is finite). So the more correct answer is that this solution takes O(n) time, where n is the length of the longest word (but is _independent_ of the query).    I would think a modified Trie with each level of the trie connected by linked list.  The problem with using a trie alone is that to get all words with character X in position Y, you have to traverse every node to depth Y.  With the linked lists you can just skip to a certain depth and iterate.

edit: of course, to make this work your trie also has to have pointers back to the parent node. Another thought would be to create a hash table that is indexed by the tuple (position, letter).  The hash table itself stores a list of all the words that are keyed by that tuple (like a multimap for the stl inclined). I think this is the answer here. The main problem I see with the trie solution is that **you need to perform a lot of unnecessary graph traversal to get to your results**. Once you've gotten to your data elements that satisfy the query conditions, you should be done: you shouldn't need to traverse a node for every subsequent letter for each word in your solution. I'm not convinced that the trie traversal is more expensive than the intersection operations you'd have to do to use a hashtable-based solution. The trie solution probably scales better, but the intersection solution would probably be faster to a point. I'd be interested to see the results if someone took the time to workbench this.  I posted this solution before seeing pserspichaos's solution.

It's O(nk) for k intersections.  Approximately the same cost as traversing the whole list k times. Another thought would be to create a hash table that is indexed by the tuple (position, letter).  The hash table itself stores a list of all the words that are keyed by that tuple (like a multimap for the stl inclined). Not a list in each entry but a tree set or a sorted list.  This way you can calculate the set intersections quickly. I feel like, for a reasonably-sized word list, you're quickly going to hit the point where it's more efficient to just run the check on the elements rather than do the intersection.

For instance, in the ENABLE word list, there are 966 six-letter words with I in the second position, and 325 six-letter words with K in the fourth position. Assuming you have both of those sets handy, wouldn't it be faster to just check the 325 words for I's in position 2, rather than take the intersection (which is size 48)? I would think a modified Trie with each level of the trie connected by linked list.  The problem with using a trie alone is that to get all words with character X in position Y, you have to traverse every node to depth Y.  With the linked lists you can just skip to a certain depth and iterate.

edit: of course, to make this work your trie also has to have pointers back to the parent node. This sounds fine for 1 letter of information. But possibly slow for k letters of information. That is, OP probably wants something that works faster when more letters are known. But with this modified trie, it's not obvious how one would get the conjunction of, say, 2 letter positions known without checking everything that matches one of the positions to see if it also matches the second position.

I wonder if [BDD's](http://en.wikipedia.org/wiki/Binary_decision_diagram) might be a possible solution. They're supposed to support arbitrary conjunction of boolean functions somewhat efficiently. So, you'd have your one big BDD that represents the database of words. Then you'd form a second BDD that represents the letters you know for the word you're trying to find. Compute the conjunction of the two BDDs, and then traverse the result to read out the words that match.

No idea, off-hand, if the performance of this approach would be competitive...

Anybody know more about BDDs than my paltry understanding? Mmmmm...new data structures.  Looks like I have some pleasure reading this weekend.
 I would think a modified Trie with each level of the trie connected by linked list.  The problem with using a trie alone is that to get all words with character X in position Y, you have to traverse every node to depth Y.  With the linked lists you can just skip to a certain depth and iterate.

edit: of course, to make this work your trie also has to have pointers back to the parent node. How does the trie even help you? Wouldn't it just be faster to have a dictionary of words with an index for each letter position? Your search would then just boil down to a set operation: intersection of index SECOND_POSITION['I'] with index FOURTH_POSITION['K'] = pointers to the words you want. 

You know what? On second thought putting this into a trie first might help. My way is easier though. I'd be interested to see someone try implementing it both ways and test how performant they are relative to each other. I think you're right.  What you're suggesting is a good refinement of the multimap idea I proffered in my reply to myself. I would think a modified Trie with each level of the trie connected by linked list.  The problem with using a trie alone is that to get all words with character X in position Y, you have to traverse every node to depth Y.  With the linked lists you can just skip to a certain depth and iterate.

edit: of course, to make this work your trie also has to have pointers back to the parent node. This doesn't really help you.  After you traverse every node to depth Y, You'd have to traverse EVERY node in the trie AFTER depth Y and make a list of all of them...because EVERY possible node is a candidate.  This mean that EVERY query traverses every node.  At that point, why don't you just do a list search?

It also doesn't help you with the intersections. You don't traverse to depth Y.  You use the list to jump to depth Y, then traverse only the relevant subtrees.

In the case where you are looking at characters Y and Z, you jump to Y and jump to Z, then find all paths between the two matching sets.  Again, you're only looking at a set of subtrees.   I suspect it might be some sort of suffix trie. This strikes me as similar to the lookup operations in BLAST, except that I suspect in BLAST they use a very fast constant time lookup for a specific string, and then simply generate all strings matching some particular pattern, and then lookup each one individually. I suspect it might be some sort of suffix trie. This strikes me as similar to the lookup operations in BLAST, except that I suspect in BLAST they use a very fast constant time lookup for a specific string, and then simply generate all strings matching some particular pattern, and then lookup each one individually.    Build independent sets of words.  One for each letter and each position you encounter it in.  For instance you store the word "apple" in the A_0, P_1, P_2, L_3, and E_4 sets.

We can store these sets for easy access in a lot of ways that are all O(1) to retrieve.  Nested arrays work as well as anything.  We can store the strings as C strings, or pointers into some sort of trie; either way, they're constant size.  (We can use these constant size IDs for comparison rather than performing any sort of expensive string comparison.)

When answering a query, you simply intersect all of the relevant sets.

There are `n * l` (`n`umber of words and  average word `l`ength) entries stored in ~~n * l~~ `26 * l` sets.  ~~Can we call the average set size 1?  I think so.~~ Average size n/26.

This solution requires O(nl) space, and requires the intersection of k O(n)-element sets.

Conclusion: I suck at math.   </snippet></document><document><title>What makes captchas hard to decipher ? </title><url>http://www.reddit.com/r/compsci/comments/184sa3/what_makes_captchas_hard_to_decipher/</url><snippet>     I personaly think that captchas has become annoyingly hard for humans as well.

I have actually been thinking about wirting a program to decipher the capcha used on reddit. i think it might be possible to deduce a transformation function from the grid in the background and se if one casn use that to "flatten" the image and then subtract the grid in order to make the text clearer. I'm pretty sure the grid is distorted separately from the text, so flattening the grid will not flatten the text

Ninja edit: i don't mean to sound pessimistic, i actually think this would be a fun project and would be interested in working with you on it   A captcha is hard to decipher because there is no scientifically known easy way to decipher a captcha.</snippet></document><document><title>The Base64 encoder has a fixed point.</title><url>http://fmota.eu/blog/base64-fixed-point.html</url><snippet>  &amp;gt; We could keep going, getting into the nitty gritty about how the bits of information are lost gradually, by looking at the transformation of the characters one by one. But I'm going to stop right there and give you the big picture.

&amp;gt; What matters is that the number of possible prefixes of s decreases as you repeatedly encode it. This means that as strings are repeatedly encoded, their prefixes become more and more similar. Over time, the prefixes of all strings converge onto prefixes of the fixed point.

Wrong!

What happens if we simply shift the core table of the encoder, the mapping from a 6-bit number to a printable character, by some offset (wrapping around)? More to the point, will there always be a single, unique fixed point even for one character?

[The answer is no!](http://ideone.com/EwOXJS) For a lot of offsets there are two fixed points, for the offset of 52 there are three, and for offsets 22 to 40 there are no fixed points: the encoded string goes through a sequence of values with some period.

And that's just if you use a shifted version of the original table, I guess you can do strangest things if you rearrange the values arbitrarily.

So it's purely coincidental that the original base64 encoding table results in a single fixed point, even among shifted variants only 29 of 64 do.

By the way, a curious fact: the fixed point for the table shifted by -1 is a string 'UUUUUUUUUUUUUUUUUUUU....'. Yes! I agree with you!

I'm not claiming that any $phase1$ or $phase2$ will do. But the base64 ones do work. That's why I didn't want to get into the nitty gritty of base64, and the way it gradually loses bits. My argument isn't quite as clean as I wish it was.

It is coincidental that base64 has a fixed point that starts with 'Vm0', but once you have 3 characters of a fixed point, the rest are completely determined, because 3 input characters gives you 4 output characters, and then 5, then 6, and so on. It's fairly likely that any function designed like base64 ends up with at most a few fixed points or cycles. I certainly just proved that such a function can't have more than 64^3 fixed points or cycles.

The 'UUUUUU...' thing is pretty cool.

I'll include a link to your comment in my post.

Lovely username. :-) &amp;gt; That's why I didn't want to get into the nitty gritty of base64, and the way it gradually loses bits.

I don't think that this is the correct way to look at it in the first place.

Consider this: if we had an 8bit-&amp;gt;8bit transformation, given by some table, what would we see in terms of fixed points? Our 256 states would be split in some number of cycles, some of them will be of length 1 (and we would call them "fixed points"), some will have longer lengths.

Now when we have a transformation from 8 bits to 6 bits then some fixed extension back to 8 bits, what happens? We still can have multiple cycles of varying lengths, but now most states belong to a path ending in some cycle but do not belonging to that cycle.

The particular arrangement depends on the transition table, in fact my program was woefully incomplete as it only searched for cycles of length 1, and demonstrated existence of a longer cycle only in cases when there were no cycles of length 1.

So based on my program we can't even tell for sure that for standard base64 all states end up in that single fixed point and not in some longer cycle.

&amp;gt; but once you have 3 characters of a fixed point, the rest are completely determined

Not quite. You have pushed some fixed character to the next triplet, then it has its own transition table that involves that fixed character pushed in, but the two remaining characters are arbitrary.

I actually checked all four-character strings for other attractors under the standard base64, surprisingly there are none (unless my stuff is buggy):

    def main():
        fixpoints = set()
        visited = set()
        for ss in product(b64table, b64table, b64table, b64table):
            s = ''.join(ss)
            current_visited = set()
            while True:
                if s in current_visited:
                    fixpoints.add(s)
                    print s
                    break
                if s in visited: break
                visited.add(s)
                current_visited.add(s)
                s = base64.b64encode(s)[0:len(s)]

It took more than a minute in Python, I guess I could brute force five using C++, 64^5 happens to be one megabit, totally manageable, but then 64^6 will not be at all, and I want 64^9 at least, so I guess I'm going to implement the further steps directly using some characters fed from the left, and Python will be fast enough then.

Then it should be interesting to see if using `urlsafe_b64encode` instead yields multiple cycles. &amp;gt; That's why I didn't want to get into the nitty gritty of base64, and the way it gradually loses bits.

I don't think that this is the correct way to look at it in the first place.

Consider this: if we had an 8bit-&amp;gt;8bit transformation, given by some table, what would we see in terms of fixed points? Our 256 states would be split in some number of cycles, some of them will be of length 1 (and we would call them "fixed points"), some will have longer lengths.

Now when we have a transformation from 8 bits to 6 bits then some fixed extension back to 8 bits, what happens? We still can have multiple cycles of varying lengths, but now most states belong to a path ending in some cycle but do not belonging to that cycle.

The particular arrangement depends on the transition table, in fact my program was woefully incomplete as it only searched for cycles of length 1, and demonstrated existence of a longer cycle only in cases when there were no cycles of length 1.

So based on my program we can't even tell for sure that for standard base64 all states end up in that single fixed point and not in some longer cycle.

&amp;gt; but once you have 3 characters of a fixed point, the rest are completely determined

Not quite. You have pushed some fixed character to the next triplet, then it has its own transition table that involves that fixed character pushed in, but the two remaining characters are arbitrary.

I actually checked all four-character strings for other attractors under the standard base64, surprisingly there are none (unless my stuff is buggy):

    def main():
        fixpoints = set()
        visited = set()
        for ss in product(b64table, b64table, b64table, b64table):
            s = ''.join(ss)
            current_visited = set()
            while True:
                if s in current_visited:
                    fixpoints.add(s)
                    print s
                    break
                if s in visited: break
                visited.add(s)
                current_visited.add(s)
                s = base64.b64encode(s)[0:len(s)]

It took more than a minute in Python, I guess I could brute force five using C++, 64^5 happens to be one megabit, totally manageable, but then 64^6 will not be at all, and I want 64^9 at least, so I guess I'm going to implement the further steps directly using some characters fed from the left, and Python will be fast enough then.

Then it should be interesting to see if using `urlsafe_b64encode` instead yields multiple cycles. &amp;gt; That's why I didn't want to get into the nitty gritty of base64, and the way it gradually loses bits.

I don't think that this is the correct way to look at it in the first place.

Consider this: if we had an 8bit-&amp;gt;8bit transformation, given by some table, what would we see in terms of fixed points? Our 256 states would be split in some number of cycles, some of them will be of length 1 (and we would call them "fixed points"), some will have longer lengths.

Now when we have a transformation from 8 bits to 6 bits then some fixed extension back to 8 bits, what happens? We still can have multiple cycles of varying lengths, but now most states belong to a path ending in some cycle but do not belonging to that cycle.

The particular arrangement depends on the transition table, in fact my program was woefully incomplete as it only searched for cycles of length 1, and demonstrated existence of a longer cycle only in cases when there were no cycles of length 1.

So based on my program we can't even tell for sure that for standard base64 all states end up in that single fixed point and not in some longer cycle.

&amp;gt; but once you have 3 characters of a fixed point, the rest are completely determined

Not quite. You have pushed some fixed character to the next triplet, then it has its own transition table that involves that fixed character pushed in, but the two remaining characters are arbitrary.

I actually checked all four-character strings for other attractors under the standard base64, surprisingly there are none (unless my stuff is buggy):

    def main():
        fixpoints = set()
        visited = set()
        for ss in product(b64table, b64table, b64table, b64table):
            s = ''.join(ss)
            current_visited = set()
            while True:
                if s in current_visited:
                    fixpoints.add(s)
                    print s
                    break
                if s in visited: break
                visited.add(s)
                current_visited.add(s)
                s = base64.b64encode(s)[0:len(s)]

It took more than a minute in Python, I guess I could brute force five using C++, 64^5 happens to be one megabit, totally manageable, but then 64^6 will not be at all, and I want 64^9 at least, so I guess I'm going to implement the further steps directly using some characters fed from the left, and Python will be fast enough then.

Then it should be interesting to see if using `urlsafe_b64encode` instead yields multiple cycles. &amp;gt; We could keep going, getting into the nitty gritty about how the bits of information are lost gradually, by looking at the transformation of the characters one by one. But I'm going to stop right there and give you the big picture.

&amp;gt; What matters is that the number of possible prefixes of s decreases as you repeatedly encode it. This means that as strings are repeatedly encoded, their prefixes become more and more similar. Over time, the prefixes of all strings converge onto prefixes of the fixed point.

Wrong!

What happens if we simply shift the core table of the encoder, the mapping from a 6-bit number to a printable character, by some offset (wrapping around)? More to the point, will there always be a single, unique fixed point even for one character?

[The answer is no!](http://ideone.com/EwOXJS) For a lot of offsets there are two fixed points, for the offset of 52 there are three, and for offsets 22 to 40 there are no fixed points: the encoded string goes through a sequence of values with some period.

And that's just if you use a shifted version of the original table, I guess you can do strangest things if you rearrange the values arbitrarily.

So it's purely coincidental that the original base64 encoding table results in a single fixed point, even among shifted variants only 29 of 64 do.

By the way, a curious fact: the fixed point for the table shifted by -1 is a string 'UUUUUUUUUUUUUUUUUUUU....'.  "If we repeat the base64 encoding process over and over again, starting at any point, we get closer and closer to the fixed point of base64, which is some **infinite** string that begins with s."

wat  

It's not a fixed-point if it keeps growing indefinitely. It depends on how you look at it. This is a pretty standard view in combinatorics on words. Any morphism (or substitution) which satisfies a few basic properties has a constructive fixed point. 

For example, if there exists an element a in the alphabet with f(a) = as, where s is any string, then the limit as n goes to infinity of f^n (a) is a fixed point of f. This forms the basis for lot of dynamical systems and combinatorics research.  Ok, but wouldn't you need to prove that such a limit exists? "If we repeat the base64 encoding process over and over again, starting at any point, we get closer and closer to the fixed point of base64, which is some **infinite** string that begins with s."

wat  

It's not a fixed-point if it keeps growing indefinitely.  Any [Base 64](http://en.wikipedia.org/wiki/Base64) encoded string is a fixed point as all characters are already in the Base64 encoding range. Well subtle differences might exist depending on the flavor; only one on the wiki page extends the length indefinitely (must be the python flavored one). But this is an almost trivial observation.

Is like saying ASCII has a fixed point for ASCII text. Of course it does. Any ASCII text. This is a property not of the encoding but of the transformation that encodes text. You're treating the output of the ASCII -&amp;gt; Base64 transformation as ASCII input to that transformation and applying it repeatedly.

In general, `Base64(Base64(s)) != Base64(s)`. For instance:

    Base64('0') = 'MA=='
    Base64('MA==') = 'TUE9PQ=='
    Base64('TUE9PQ==') = 'VFVFOVBRPT0='

So it's kinda surprising that Base64 has a fixed point. Ah sorry - yeah Base64 taken as a 24-&amp;gt;32 bit mapping. Then the fixed point would be infinite length, assuming the authors metric space is complete (which I don't completely see yet - will have to impose some discrete topology or Hamming metric or something on strings to check it). I suspect then since the mapping maps [0-63] onto a shifted linear range that this property would always follow for any such encoding mapping one linear range to another. Some form of Banach Fixed Point theorem would apply to them all. I'm using a fairly simple compactification of the space of strings: the space of finite and infinite strings. Base64 happens to have a fixed point, but it turns out that encodings of a similar structure are not guaranteed to have a fixed point, or a unique fixed point. [See this comment by moor-GAYZ.](http://www.reddit.com/r/compsci/comments/18234a/the_base64_encoder_has_a_fixed_point/c8b1axm) No, he wrapped around. That is not linear. 

Please explain your metric on strings and why it's complete. I have a math PhD so should understand how you checked it.  The set of real numbers in the interval [0, 1] and the set of (finite + infinite) strings have the same cardinality. Consider a continuous bijection between them, where any string with a given prefix is mapped onto a smaller interval [a,b] with 0 &amp;lt; a &amp;lt; b &amp;lt; 1. Another way to look at it is to look at the string as a sequence of digits in base 256, and interpret that sequence as being after the radix point. (This is analogous to taking a sequence of decimal digits 123456..., and interpreting them as being after the decimal point 0.123456...)

Then, any complete ultrametric over the real numbers should give you a complete ultrametric over the (finite+infinite) strings. Using an ultrametric, you also get the desired property that base64 is an eventually contracting map.
 A continuous bijection requires a topology on each space. What are they? Also, why an ultra metric and not a plain metric? 

You can take the strings as base 256 numbers, and use the standard topology and metric on the reals and the induced ones on strings, and ignore ultra metrics, otherwise you'd need more theorems on metric equivalences. And in either case saying base64 is "eventually" contracting makes no sense. It either is contracting or it isn't. It's one map. &amp;gt; A continuous bijection requires a topology on each space. What are they?

The metric topology. 

&amp;gt; You can take the strings as base 256 numbers, and use the standard topology and metric on the reals and the induced ones on strings, and ignore ultra metrics, otherwise you'd need more theorems on metric equivalences.

That's basically what I'm doing. I just happen to be using an ultrametric because that makes the math easier in my opinion.

&amp;gt; And in either case saying base64 is "eventually" contracting makes no sense.

An eventually contracting map is a map f such that f^k is contracting, for some k. Link to eventually contracting definition? If f^k is contracting f already is.

If you use the metric topology you need the same metric, not a different one. You cannot mix like that.</snippet></document><document><title>Why Lambda-calculus beats Turing Machines for scientific study of programming languages</title><url>http://cstheory.stackexchange.com/a/14795/4984</url><snippet>  OTOH, one of the most important ideas behind Turing machines is that it is a *machine* that can calculate anything. &amp;gt; OTOH, one of the most important ideas behind Turing machines is that it is a machine that can calculate anything.

You can also systematically translate the lambda calculus into a machine. See the [SECD machine](http://en.wikipedia.org/wiki/SECD_machine) for an early example. OTOH, one of the most important ideas behind Turing machines is that it is a *machine* that can calculate anything. This was a great advantage back when machines that calculate anything weren't so obiquitous that "suppose there is a machine that can calculate anything..." was a notion one had to spend time on. But IMHO nowadays we should get away from old metaphoras.  OTOH, one of the most important ideas behind Turing machines is that it is a *machine* that can calculate anything. I don't think the "machine" part is as important as the "automated process" part. The really awesome thing about turing machines is that they provide a very intuitive and natural definition of computability (specially when you also show that there is an universal TM and that its equivalent to the other models). While things like G&#246;del's recursive functions or even the lambda calculus are equally powerful, Turing machines make it obvious that the class of functions computable with these systems line up with what we would intuitively want to consider as "computable".  Is this a question?  A Turing Machine is a useful construct for discussing the idea of computability, but as a *language*?

No, use lambda-calculus.  True, but complexity classes defined by Turing machines and their variants are still the best way to model complexity classes. I was careful to formulate my title in a way that left that implicit: I'm only talking about *programming*, which is concerned with the *notation* of programs, and not saying anything about complexity theory which is concerned with are concerned with computational phenomena independently of their formal description language.

That said, I'm interested in seeing more work on complexity that is not based on Turing Machines (I personally have little sympathy for this framework and am glad to work on other formal objects): implicit complexity, etc. I'm no expert in the area, but I'm not sure your claim is true for the sub-polynomial, or at least sub-logarithmic, classes (where I suspect circuits are a better formalism). While I'm aware of no strong urge to use different foundations (than Turing-Machine) for polynomial classes and above, this is also because at this point the precise machine model is not so important anymore, and results do not actually depend on Turing Machines that much, so could probably be reformulated in other computation models just as well.

Anyhow, this is another debate. I was discussing *programming languages* research. True, but complexity classes defined by Turing machines and their variants are still the best way to model complexity classes. Are the complexity classes on Turing machines not equal to those on lambda calculus? Are the complexity classes on Turing machines not equal to those on lambda calculus? </snippet></document><document><title>Hey Reddit, I made a site for quickly jumping to specific concepts in long CS lectures</title><url>http://teachingtree.co</url><snippet>  Right now it is pretty much focused on Algorithms, Graphics, and AI/Machine Learning, but anybody can add more videos/concepts and hopefully it can grow over time.

(Also, this is my first real website, so if any of you have feedback it is much appreciated. Hope you like it.)

EDIT 3: Thank you everybody for the positive response and all the upvotes! I've already made several changes based on your feedback and I'm excited to see the amount of content grow over the next few weeks/months. Thanks it looks great!  Right now it is pretty much focused on Algorithms, Graphics, and AI/Machine Learning, but anybody can add more videos/concepts and hopefully it can grow over time.

(Also, this is my first real website, so if any of you have feedback it is much appreciated. Hope you like it.)

EDIT 3: Thank you everybody for the positive response and all the upvotes! I've already made several changes based on your feedback and I'm excited to see the amount of content grow over the next few weeks/months. Pretty awesome idea and implementation! Excellent work for a first attempt.

Could you tell us about the technical details and what languages and frameworks were used? How long do you think it took you in total? It's pretty well polished! Right now it is pretty much focused on Algorithms, Graphics, and AI/Machine Learning, but anybody can add more videos/concepts and hopefully it can grow over time.

(Also, this is my first real website, so if any of you have feedback it is much appreciated. Hope you like it.)

EDIT 3: Thank you everybody for the positive response and all the upvotes! I've already made several changes based on your feedback and I'm excited to see the amount of content grow over the next few weeks/months.  This is a great concept. 

When I have my head in a problem, and I need to know if my hunch for a solution is correct, referencing a reputable source for fundamentals laid out by a good lecturer is great!  But it isn't always practical or reliable to jot down what I know about my problem, spend an hour clickhunting through 80 minute lectures to find a 10 minute section, then reimmerse myself in the original problem.

In general, my guess is that the largest incompatibility between university-style lectures and independent, web-based learners is the inundating size of the former.  Crowdsourcing a curated index sounds like a good solution.  Please keep the effort up and spread the word! Thanks! I was running into similar problems myself. Glad I'm not the only one.

Hopefully crowdsourcing is a viable solution to the problem and it doesn't just all fall apart. We'll see! I was on my phone earlier, so I couldn't explore much, but I've had some time to play around more.  The site is really well done.  Good looking and minimal home page, and I like the color palette overall.  I'm more impressed by the usage design choices though.

When viewing a specific video/concept, I like the choice of information displayed to the user - other videos on the same concept and related concepts sound like really practical ways to explore from a single concept.  I also dig that the other concepts covered in the same video are auxiliary tags below everything - that choice reflects the purpose of tag curation before video curation.

Were there any UI elements that you were strongly considering, but decided to rule out in the end? The general structure of the viewing page has stayed more or less the same since the start of the project actually. The only major difference was that I was initially going to have some sort of graphical web for showing related concepts. I still think that would be pretty cool, so if I get enough data points I might play around with the concept (but probably not incorporate it into the meat of the site). Cool.  I support the idea of a graph view of concepts.  If you're ever looking into it and you're not already familiar with [D3.js](http://d3js.org/), it should help out.  I've seen it used quite a few times in the data visualization community.  It looks like a bit of mucking about with json [can get you a force-directed graph](http://bl.ocks.org/4062045) pretty easily.  Bookmarked. Freaking awesome. I love it.  Very well done. Post this to HackerNews, they like this kind of thing and you'll get some traffic out of it. http://news.ycombinator.com/  You've done a great job! Now it's time to have the community help it grow and encompass more CS topics!    This is truly awesome. Well done. As someone who knows Javascript pretty well but almost nothing about ruby/rails, where did you start to learn all this information?    Hmm, the YouTube embeds seem to be breaking randomly and I'm not sure why. Apologies to anybody who checks it out while this is still an issue. Would this be why every video I click does nothing? I just keep seeing uniform grey where the video should be, no "broken link/bad video" warnings or what have you.

Otherwise this sounds, to quote the always eloquent crew of Always Sunny, tits. Yeah, still trying to figure out what the issue is. Seems to be happening a lot for some people and not at all for others.

Sorry about that. I'll let everybody know when it is fixed.  Pretty good. Nice design. My only problem with it is that the same topic is categorized multiple ways. For example, hash codes, hashing, and hash tables are all separate.  Is there a strong focus on Java? I'm taking a Java Data Structures course this semester and my professor sounds like Gilbert Gottfried so I won't be doing much learning in the class itself. There are a few classes that do most of their work in Java, but initially I'm trying to not have specific languages be too strong of a focus. 

Here's a teacher who has a Data Structures class in Java though - [link] (http://teachingtree.co/teachers/show?teacher_name=Jonathan+Shewchuk) - pardon the poor organization.

Most of the concepts that you can search for aren't related to Java specifically but more about the ideas that can be found in a lot of languages. 

Hope it helps! Thank you, sir! This and MIT OpenCourseWare are going to save my grade from Iago. All the best and good luck!   This is awesome. Thanks so much for thinking of this idea!   I love you. I was trying to find something on big omega notation earlier... Thanks. You gotta love that professor's pronunciation of 'omega' as well.        What is the difference between this and a WIKI?  very very nice. Site looks great and I've only had one issue with a video not playing but this was solved by doing a half back page and then refreshing. Very strange little bug, probably an exception in your JS.

Just wanted to check that you are storing Passwords Hashed and preferably salted in your DB. It's a very common thing to overlook when it's your first project but this is good enough I can def see people signing up (I have).  

Also, if you have chance can you modify the embed code for the youtube clips so that they can be full screened? Some of the slides are hard to read in the small video.  I can't wait to start adding stuff :)

Also I feel like a kid in a candy store.  One thing I noticed was say under graphs, it links to a lecture, but I noticed there is at least one other video discussing the same topic. If you have multiple videos with the same tag, maybe have them open up into a list of videos pertaining to that tag/subject. That and maybe a suggested order for things like the AI category if you can possibly do that(this first point is more of a concern for me). But great site! I am enjoying this and going to be attempting to build some of these new Data Structures. :)   Is it all videos?

Would be nice if there were text going with the videos...    Pffft, you have an unhealthy obsession with object oriented programming. Not a single concept pertaining to functional programming was listed.

What about categories, monads, combinators, composition, arrows, and continuations? Hah, I am kind of at the mercy of what is most abundant in college lectures and a lot of them seem to focus on OOP. If you know of any good series that cover the missing topics let me know (I'd just like to learn more as well).

Also, hopefully some functional programmers will check out the site and add more videos/concepts too :) Scumbag Haskell hipster: checks out awesome crowd sourced cs concept index, bitches about missing functional programming.

Seriously: this sounds like a great idea! I will check it out when I'm not on my phone. Do you (OP) plan to index other concepts beside cs as well?</snippet></document><document><title>TCS+ Seminar: Exponential Lower Bounds for Polytopes in Combinatorial Optimization Ronald de Wolf</title><url>http://www.youtube.com/watch?v=r7BizwCDfWo</url><snippet>  </snippet></document><document><title>Research Trends across the pacific divide</title><url>http://www.reddit.com/r/compsci/comments/181b8l/research_trends_across_the_pacific_divide/</url><snippet>Just a quick question, from my own observations it seems that in Europe that research is more focused on type systems/ programming language/ verification while the US is more focused on algorithms/ complexity. Is this fair or my naivety?  TIL that America and Europe are on opposite sides of the Pacific.

Seriously, though.  I can't say that I've really seen much of a geographical divide in research topics, apart from the obvious (locations of various conferences, specific centers of research, etc). </snippet></document><document><title>The 48th known Mersenne Prime has been found</title><url>http://www.mersenne.org/various/57885161.htm</url><snippet>  &amp;gt; On January 25th at 23:30:26 UTC

Oh, that's nice! On January 25th, 1996? Or 2013? Or ... ? But hey, I know what second it happened at least. Looks like they cite something from May 2005. So it's likely to be January 2006 or later. At least we narrowed it down. &amp;gt; On January 25th at 23:30:26 UTC

Oh, that's nice! On January 25th, 1996? Or 2013? Or ... ? But hey, I know what second it happened at least. Wikipedia says 2013. I prefer their leaving out the year to Reddit's vague timestamp. &amp;gt; Wikipedia says 2013.

On their [homepage](http://www.mersenne.org/) they have news from December, 2012 seemingly written before this one, so it's a pretty safe bet.

&amp;gt; I prefer their leaving out the year to Reddit's vague timestamp.

But Reddit has a nice little tooltip if you hover them, that's a long way I'd say. Thanks for making me feel retarded. I've been here over a year and never noticed that.  This is cool. What does knowing this number mean? More interesting than knowing this number is prime is knowing how long it took to *verify* this number is prime: 7.7 days. How long it takes us to verify that a suspected prime is definitely prime (especially of the well-formed Mersenne variety) is an excellent metric for our computing ability. Years ago it would have taken us much longer to verify this particular number was prime, and years from now we should be able to do it much faster. &amp;gt; Jerry Hallett verified the prime using the CUDALucas software running on a NVidia GPU in 3.6 days. Finally, Dr. Jeff Gilchrist verified the find using the GIMPS software on an Intel i7 CPU in 4.5 days and the CUDALucas program on a NVidia GTX 560 Ti in 7.7 days. Indeed this reflects poorly on my own reading comprehension for skimming over that part. Thank you for the context. I thought it was worth quoting in more detail regardless, as it's pretty cool that it was done using something on the order of $1000 worth of commodity hardware.

CUDA's exciting. I thought it was worth quoting in more detail regardless, as it's pretty cool that it was done using something on the order of $1000 worth of commodity hardware.

CUDA's exciting. I thought it was worth quoting in more detail regardless, as it's pretty cool that it was done using something on the order of $1000 worth of commodity hardware.

CUDA's exciting. It's one of the neatest ways for your average user to partake in computing on a level that's otherwise unattainable.  I think you put it well: it's a highly parallel processor that's already sitting under every PC gamer's desk.  That's a big opportunity. I've barely tapped the CUDA cores in my GTX 480. I ran some tasks for a while on it for seti@home but got tired of the heat that bad boy was putting out. I'm hoping to take a CUDA class in the next year to really try my hand at working with 480 parallel cores. My university's also got a server with 4 Tesla C2050s in it, so having access to that is something I definitely want to leverage.  I ran ihashgpu on my cheap asus geforce overnight and it died.  Had to RMA it. Im guessing the stock cooler wasnt enough for 8 hours at 100% utilization.

Just one little caveat with lower end consumer cards ;) I thought it was worth quoting in more detail regardless, as it's pretty cool that it was done using something on the order of $1000 worth of commodity hardware.

CUDA's exciting. More interesting than knowing this number is prime is knowing how long it took to *verify* this number is prime: 7.7 days. How long it takes us to verify that a suspected prime is definitely prime (especially of the well-formed Mersenne variety) is an excellent metric for our computing ability. Years ago it would have taken us much longer to verify this particular number was prime, and years from now we should be able to do it much faster. I don't mean to be pedantic, but without context most of what you said is meaningless. The number was discovered with a supercomputer, but the 7.7 day figure is a result of using a single, consumer-level GPU. Had the original computer done the verification, the number wouldn't be nearly so impressive. And that's the other thing: finding this prime is really not an "excellent metric for our computing ability." Firstly because we could have done it a whole lot faster: the fastest supercomputer in the world could (based solely on published average speeds) have performed the same calculation 183 times faster. Secondly because supercomputers these days are nothing more than clusters. If we wanted an exascale computer, we could have one, it'd just be expensive. Perhaps even a zettascale computer if we really wanted to get spendy. (my very rough calculations estimate that would cost around a few trillion dollars). And lastly because "computing ability" is vague to the point of uselessness. If an alien race invents warp drive on a celeron equivalent, whose "computing ability" is greatest? Algorithms, power consumption, engineering, etc are so important to "computing" that a top speed is a (mostly) pointless benchmark. Anyway I'm not trying to argue just for the sake of it, but without context, discovering mersenne primes doesn't mean much to the world of computing.  I don't mean to be pedantic, but without context most of what you said is meaningless. The number was discovered with a supercomputer, but the 7.7 day figure is a result of using a single, consumer-level GPU. Had the original computer done the verification, the number wouldn't be nearly so impressive. And that's the other thing: finding this prime is really not an "excellent metric for our computing ability." Firstly because we could have done it a whole lot faster: the fastest supercomputer in the world could (based solely on published average speeds) have performed the same calculation 183 times faster. Secondly because supercomputers these days are nothing more than clusters. If we wanted an exascale computer, we could have one, it'd just be expensive. Perhaps even a zettascale computer if we really wanted to get spendy. (my very rough calculations estimate that would cost around a few trillion dollars). And lastly because "computing ability" is vague to the point of uselessness. If an alien race invents warp drive on a celeron equivalent, whose "computing ability" is greatest? Algorithms, power consumption, engineering, etc are so important to "computing" that a top speed is a (mostly) pointless benchmark. Anyway I'm not trying to argue just for the sake of it, but without context, discovering mersenne primes doesn't mean much to the world of computing.  You are severely underestimating the difficulties in building an exascale computer. From the point alone of failure it is a very difficult thing to do. There was a talk about this at IPDPS in 2010 by IBM. 

 More interesting than knowing this number is prime is knowing how long it took to *verify* this number is prime: 7.7 days. How long it takes us to verify that a suspected prime is definitely prime (especially of the well-formed Mersenne variety) is an excellent metric for our computing ability. Years ago it would have taken us much longer to verify this particular number was prime, and years from now we should be able to do it much faster. Somebody correct me if I am wrong, but isn't also very useful for cryptography? This is cool. What does knowing this number mean? It's a benchmark of our computing ability. When we start communicating with aliens, we can roughly compare who's better at computing by comparing the size of our largest known primes. It's a benchmark of our computing ability. When we start communicating with aliens, we can roughly compare who's better at computing by comparing the size of our largest known primes. Until we discover that that alien race never formulated a form of quantified mathematics.  They instead formed their entire intellectual structure based upon complexity.  They simply can't understand why prime numbers hold any fascination for us, all of them being such a low level of complexity.  They want to know how we manage to survive without even being able to design systems with a few tiny trillions of components and predict their aggregate behavior when they build themselves into trillions of trillions of macro-scale bodies.  They'll think our numbers are so very cute, and our ability to talk very exactly about things of such a low level of complexity that their relation to reality is only tangential at best, while they point out to us all we ever needed was our knowledge of quantum mechanics and some principles of how complex systems behave at large scales based upon small-scale properties. Until we discover that that alien race never formulated a form of quantified mathematics.  They instead formed their entire intellectual structure based upon complexity.  They simply can't understand why prime numbers hold any fascination for us, all of them being such a low level of complexity.  They want to know how we manage to survive without even being able to design systems with a few tiny trillions of components and predict their aggregate behavior when they build themselves into trillions of trillions of macro-scale bodies.  They'll think our numbers are so very cute, and our ability to talk very exactly about things of such a low level of complexity that their relation to reality is only tangential at best, while they point out to us all we ever needed was our knowledge of quantum mechanics and some principles of how complex systems behave at large scales based upon small-scale properties. Until we discover that that alien race never formulated a form of quantified mathematics.  They instead formed their entire intellectual structure based upon complexity.  They simply can't understand why prime numbers hold any fascination for us, all of them being such a low level of complexity.  They want to know how we manage to survive without even being able to design systems with a few tiny trillions of components and predict their aggregate behavior when they build themselves into trillions of trillions of macro-scale bodies.  They'll think our numbers are so very cute, and our ability to talk very exactly about things of such a low level of complexity that their relation to reality is only tangential at best, while they point out to us all we ever needed was our knowledge of quantum mechanics and some principles of how complex systems behave at large scales based upon small-scale properties. The idea of an aliean race being able to communicate at interplanetary distances without having developed algebra and calculus up to roughly our level or better is one of the most implausible hard science fiction premises I've ever heard. The sun doesn't seem to need to have formulated a system of algebra to send its wind throughout the solar system. The Sun can't communicate. Sending a signal isn't enormously difficult, sending a controlled signal which can carry a message is. What's your point? Sending a signal and sending a controlled signal are entirely a difference in perspective.  Are ants "sending a controlled signal" when they drop pheremone as they are walking around, dropping a bit more when they're carrying food?  They are most certainly communicating effectively to other ants, but they're not sitting down and algebraically factoring the Shannon entropy of their intended message and formulating it in an efficient manner.  They're just dropping pheremone.  They don't even know they're doing it.  There is no way to mathematical factor out a "message" from it.  It's just a signal.

I wouldn't claim that the Sun communicates, but it's a bit presumptive to claim that we know it does not.  It is technically impossible for any signal to entirely lack information content (its existence conveys at least 1 bit).  Whether we decide that this is "intentional" depends completely and entirely upon whether we can make use of it, and upon no more objective factor than that. &amp;gt;Sending a signal and sending a controlled signal are entirely a difference in perspective. 

No, that is not true.

&amp;gt;Are ants "sending a controlled signal" when they drop pheremone as they are walking around, dropping a bit more when they're carrying food?

Yes, they are.

&amp;gt;I wouldn't claim that the Sun communicates, but it's a bit presumptive to claim that we know it does not.

Since the alternative is that the Sun is sentient, it is an entirely reasonable claim.

And the premise that is beyond belief is the possibility of a controlled message being sent across space, which implies  a) radio (or other electromagnetic wave) broadcast technology with b) sufficient power to have the signal carry across planetary distances and c) control of those broadcasts in a way that can transmit information, each of which is independently improbably without advanced mathematics. And completely impossible without the complex number system at the very least, since you need it for circuitry. &amp;gt;No, that is not true.

How is it not?  Explain, please... Is the following a signal or a controlled signal:

xyzzy

And how could a being, completely ignorant of the facts of human existence, discover such a fact?

&amp;gt;Yes, they are.

Then how do you determine the Sun is not?  Its emissions travel to other locations, and they cause effects where they result.  It is done no more 'consciously' (as far as we know) than the ant does.  Is it because a signal must travel from one type of organism to another of the same type?  So when a human gets a dolphin to jump through a hoop with a hand wave, there is no signal sent in that case?

&amp;gt;Since the alternative is that the Sun is sentient, it is an entirely reasonable claim.

That is not the alternative.  What about 'we dont know'?  What about 'sentience is a subjective property we judge organisms to have based upon our own ability to relate to them'?  There are far more alternatives than 'the sun is sentient' by itself.  Especially since I pointed out ants, and they're not sentient.

&amp;gt; each of which is independently improbably without advanced mathematics

For *US*, yes.  WE are incapable of doing it without advanced mathematics.  We are also incapable of telling other people where food is by dropping pheremones, yet ants do it quite easily.  I see no reason why our limitations should be presumed to be absolute.  I do understand mathematics, by the way.  I do understand what it means to prove that something is mathematically impossible, such as predicting a chaotic system and the like.  And I also understand that there is far more outside of that system of understanding than there is addressed by it.  Yes, WE require mathematics.  We are restricted in a multitude of ways in how WE can accomplish things.  What has that got to do with aliens?  Aliens are most likely so vastly different from us that we would be incapable of noticing that they are alive.

Consider this:  If an alien were to sit above our planet in a ship and observe the Earth, how could it conclude that an intelligent life form exists below?  They are capable of perceiving ultraviolet light, but not visible light, directly (tho they have instruments that can), they do not hear.  They can sense alpha and beta radiation with exquisite sensitivity.  They cannot count.  They communicate with one another through the secretion of specialized viruses which infect each other and alter their brain structure in order to transfer thought (as opposed to our methods of vibrating air to vibrate a membrane in another persons ear to produce signals they have been trained to recognize and associate with concepts).  They have no concept of individuality as we have, because they are capable of transferring any portion of consciousness between individuals and therefore lose all motivation to protect their 'bodies' beyond the ability for that 'body' to be physically separate and to see things that are not 'nearby'. &amp;gt;Sending a signal and sending a controlled signal are entirely a difference in perspective. 

No, that is not true.

&amp;gt;Are ants "sending a controlled signal" when they drop pheremone as they are walking around, dropping a bit more when they're carrying food?

Yes, they are.

&amp;gt;I wouldn't claim that the Sun communicates, but it's a bit presumptive to claim that we know it does not.

Since the alternative is that the Sun is sentient, it is an entirely reasonable claim.

And the premise that is beyond belief is the possibility of a controlled message being sent across space, which implies  a) radio (or other electromagnetic wave) broadcast technology with b) sufficient power to have the signal carry across planetary distances and c) control of those broadcasts in a way that can transmit information, each of which is independently improbably without advanced mathematics. And completely impossible without the complex number system at the very least, since you need it for circuitry. It's a benchmark of our computing ability. When we start communicating with aliens, we can roughly compare who's better at computing by comparing the size of our largest known primes. So it's mathematical penis waving?

I feel like we could get the same results by proving 8=D. You say that like mathematical penis waving at aliens isn't an awesome idea. So it's mathematical penis waving?

I feel like we could get the same results by proving 8=D. So it's mathematical penis waving?

I feel like we could get the same results by proving 8=D. In total sincerity: knowing the precise relationship between P and non-P would be a demonstration of massive technological prowess for any civilization we meet.  the N means nondeterministic. Then what's the P stand for, smartypants?

NP means nonpolynomial. Polynomial problems take O( X^Y ) time or less to solve for complexity X (e.g. X members in a set) and some real number Y. E.g., sorting algorithms are typically O( X log( X ) ), which is smaller than O( X^2 ). I.e.: doubling the number of items to be sorted will never more than quadruple the time needed to sort them.  Then what's the P stand for, smartypants?

NP means nonpolynomial. Polynomial problems take O( X^Y ) time or less to solve for complexity X (e.g. X members in a set) and some real number Y. E.g., sorting algorithms are typically O( X log( X ) ), which is smaller than O( X^2 ). I.e.: doubling the number of items to be sorted will never more than quadruple the time needed to sort them.  Then what's the P stand for, smartypants?

NP means nonpolynomial. Polynomial problems take O( X^Y ) time or less to solve for complexity X (e.g. X members in a set) and some real number Y. E.g., sorting algorithms are typically O( X log( X ) ), which is smaller than O( X^2 ). I.e.: doubling the number of items to be sorted will never more than quadruple the time needed to sort them.  This is cool. What does knowing this number mean? It means that we have computers which can do math with 17-million-digit numbers, which is totally fucking awesome. This is cool. What does knowing this number mean?   Now, surely, *this* MUST be the highest prime number there is! Now, surely, *this* MUST be the highest prime number there is!  How would you prove that there are more Mersenne primes to be found? How would you prove that there are more Mersenne primes to be found? Couldn't all this computing power be used for something with at least a little benefit to society?  I understand people have an interest in large prime numbers, but come on, it's not actually that exciting.  It's not a particularly hard endeavor, just time consuming. Couldn't all this computing power be used for something with at least a little benefit to society?  I understand people have an interest in large prime numbers, but come on, it's not actually that exciting.  It's not a particularly hard endeavor, just time consuming. Yeah, because prime numbers have nothing to do with cryptography, or anything useful like that. o_O If you base your security on Mersenne Primes, you're going to have a bad crime.  Math would be even cooler if 2^x-1 was always prime when x is a prime. That can never be true. Any power of 2 is a multiple of 2 and thus not prime. I assume you meant 2^x - 1. Math would be even cooler if 2^x-1 was always prime when x is a prime.</snippet></document><document><title>Jensen's Inequality application in Computer Science</title><url>http://www.reddit.com/r/compsci/comments/17yjy3/jensens_inequality_application_in_computer_science/</url><snippet>I was just curious as to what the application for Jensen's inequality is in computer science  </snippet></document><document><title>Ed Pegg's Busy Beaver Turmite Challenge</title><url>http://www.conwaylife.com/forums/viewtopic.php?f=11&amp;amp;t=1029</url><snippet> </snippet></document><document><title>The very model of a modern major major (Brian Kernighan explains the exponential growth of computer science as a college major)</title><url>http://www.dailyprincetonian.com/2013/02/04/32590/</url><snippet /></document><document><title>Help understanding an audio processing algorithm - speech segmentation</title><url>http://www.reddit.com/r/compsci/comments/17y8b7/help_understanding_an_audio_processing_algorithm/</url><snippet>The algorithm I'm looking at is a V/C/P (Vowel/Consonant/Pause) classification algorithm on a digital speech signal. It is described as such:

1. Audio data is segmented into 20ms-long non-overlapping frames, 
where features, including ZCR, Energy and Pitch, are extracted.  

2. Energy and pitch curve is smoothed. 

3. The  Mean_En and  Std_En of energy curve are calculated to 
coarsely estimate the background noise energy level, as: 
NoiseLevel = Mean_En - 0.75 Std_En. 
Similarly the threshold of ZCR (ZCR_dyna) is defined as: 
ZCR_dyna = Mean_ZCR + 0.5 Std_ZCR. 

4. Frames are classified as V/C/P coarsely by using the following 
rules, where FrameType is used to denote the type of each frame. 
If ZCR &amp;gt; ZCR_dyna then FrameType =Consonant 
Else if Energy &amp;lt; NoiseLevel, then  FrameType =Pause 
Else FrameType =Vowel   

5. Update the  NoiseLevel as the weighted average energy of the 
frames at each vowel boundary and the background segments. 

6. Re-classify the frames using algorithm in step 4 with the updated 
NoiseLevel. Pauses are merged by removing isolated short 
consonants. Vowel will be split at its energy valley if its duration is 
too long  

I do not understand step #5. Like I don't know how to interpret the wording - is there another way to describe what they are doing? I get that we want to update the NoiseLevel variable and re-run step #4 for every frame, I just don't understand how exactly.

Link to the paper: http://bero.freqvibez.net/public/segs/icassp03_senseg.pdf

Thanks for any help.  Assuming that *vowel=maximal consecutive string of frames with FrameType=V* and *background = pause = as vowel but with string of P frames* (neither seems to be defined in the paper).  It looks like 5 is calculating the arithmetic average of the energy of all frames *f* that satisfy (i) or (ii):

(i) *f* is a vowel boundary, that is, its FrameType is V but it has an adjacent frame with FrameType= C or P.

(ii) *f* is frame with FrameType=P.

Or, equivalently, the weighted average of the average of (i) and the average of (ii).

I'm not knowledgeable in that field, and perhaps I've been "spoiled" by reading too much theoretical computer science, but that paper's writing looks a bit sloppy to me. I mean, surely that algorithm needs a more explicit *while* loop somewhere? 
 
Looks like a conference paper, maybe you could try and find a similar, journal-published paper? Couldn't find a journal paper through google yet, and yes I agree it's not that well-written. 

But thank you for some starting ideas; I can try all 3 and see if any of them help out my results. I just did the algorithm normally up to step 4 and tried segmenting speech with that. It works so-so but there's some incorrect comparisons where I get phrase1 having close segmentation data to phrase2 even though they're saying different sentences (i.e. it's better than when I try and repeat phrase1 and compare to the reference).  Despite the less-than-ideal writing, the paper is interesting.

If I understood it correctly, the rationale for the dynamic update of NoiseLevel is that if you get a random recording/video, the background noise is not constant. 

So you "loop" on step 4 for *n* seconds, go to step 5 and recalculate the NoiseLevel, then go back to step 4 again and continue with the next *n* seconds, until the end of the audio.

But if you can restrict the input -- e.g. by choosing recordings with even voice volume and even background noise -- in theory you could make a simplified initial version that doesn't need the dynamic part. After you get this right then you write the dynamic part.

&amp;gt; It works so-so but there's some incorrect comparisons where I get phrase1 having close segmentation data to phrase2 even though they're saying different sentences (i.e. it's better than when I try and repeat phrase1 and compare to the reference). 

Not sure what you mean by repeating phrase1. You mean your algorithm gives different results when you run it twice with the same input=phrase1 (why?) or you mean you manually recorded 2 audio files phrase1a.wav and phrase1b.wav and they diverge more than phrase1a.wav and phrase2.wav?

Of course you should be asking this to your advisor/professor, not to a random guy who just happened to have studied CS. :-)</snippet></document><document><title>Basics of Computer Science : Understanding State Machines</title><url>http://blog.markshead.com/869/state-machines-computer-science/</url><snippet>  &amp;gt; The understanding that finite state machines and regular expressions are functionally equivalent opens up some incredibly interesting uses for regular expression engines&#8211;particularly when it comes to creating business rules that can be changed without recompiling a system.

My hair tried to escape my head.

By the way, is there a good tutorial about _what to do_ with FSM, when you see that you want to do it like that, but are unsure about how to do it? Like, I was mildly surprised to discover that all Python FSM libraries are shit, because the authors apparently don't understand what people want from a FSM.

EDIT: to clarify, in my experience, FSMs have a pronounced structure, the transition table is emphatically non-random and gets extremely non-random as you add states. You have stuff like, if the state is "after backslash" then do this thing; if the symbol is "backslash" then do that thing. You have these rules covering vast swathes of the cells, in both directions. Yet people who had implemented FSM engines in Python at the time when I went searching apparently did that because they just learned of FSMs and thought that they are awesome, not because they had an actual itch to scratch, an actual real-life problem, so their engines force you to implement a rule for each cell, or a rule for each state or symbol but not both. &amp;gt;My hair tried to escape my head.

I'm not sure if this means you thought it was interesting or crazy.

This might be the type of thing you are looking for:
http://www.ibm.com/developerworks/library/l-python-state/index.html &amp;gt; I'm not sure if this means you thought it was interesting or crazy.

"Welcome to your first day in our corporation, you are assigned to the 'regex business rule' support team, please report to our in-house suicide councillor immediately".

&amp;gt; This might be the type of thing you are looking for:

Nah, yet again, it requires declaring actions on a cell-to-cell basis, and pretends that executing them in a loop is some kind of an achievement. Just because an FSM can be represented as a regex, doesn't mean you'd have to think about it or create it as a regex. I agree with your sentiment.  It doesn't have to be a RE, it's just easy to see their application there. Remember, what you're recording is a state &amp;amp; you want to have some sort of accepting state. 

If the object of type XYZ Can start in State: A or B. 
(not a great example, just an example of NDFSM)
B-*puts key into ignition and is turned*

A -*has key in Ignition and is turned*&amp;lt;-a, or b-&amp;gt; C-*ample fuel is injected into your piston shaft* -&amp;gt; (D) *engine turns over, and starts*

I've seen  FSM, and NDFSM in Python being utilized via a dictionary. You just label your nodes as the key and the values establish possible paths to other possible states it can legitimately travel from that node.  If the object needs to travel to a state that isn't available from that node, you just need to determine what happens: does the object die; Does you object accept the ability to be stuck in its current state if it was requested to travel to a state it can't, and get a tick? I dunno it's up to you, in my opinion. However, I've only seen it as the object is ending in a non-accepting state, so it would just die, but, I see no reason why it would have to.  

The only thing a FSM, NDFSM is supposed to do is keep track of state. 

Edit: Normally, it' just stated that it would travel back to a different state.  Ie if the engine didn't ever turn over and the object would always return back to a, go c after some time, it's best just to say the object after so much time not having ended in D, it won't. . I just think it's interesting to think, in my opinion, an object might be more mailable... It could for example get fixed, but, that might be a lot better represented in the NDFSM, ie can go to Node G: where it's fixed by some other process... You can even use Graph theory's Djstrstra's algorithm to determine if the object can make it to a state given some other other parameter.  It's easy to make things more complex than they ought to be, but, it's probably best not to make them less simple than they should. &amp;gt; You just label your nodes as the key and the values establish possible paths to other possible states it can legitimately travel from that node.

My point was that I believe that this approach generally doesn't work for any somewhat complex FSMs, like, five states or more, and you're better off implementing the logic as a bunch of nested switch (or if-elif in Python) statements.

At least until somebody implements a FSM engine in a way that allows you a comparable level of conciseness (and then maybe some more nice stuff), giving you the ability to describe the contents of entire rows or columns or huge parts of them, like nesting switches does.  Why do you think it breaks down with five or more nodes/states?  I was under the impression python dictionaries are O(1) majority of the time (http://wiki.python.org/moin/TimeComplexity#dict).  If you wanted to get away from the amortized worst case, just utilize a string and initiate memoization to get away from the worst case, but, the majority of the time that doesn't appear to be necessary. Oh, performance is not the problem, _specifying_ the transition table is the hard part. If you have five states and five input symbols (character classes) you get 25 cells that you need to fill, usually without any help against typos. Most of which are the same stuff, but the last time I checked none of Python FSM libraries I looked at addressed this problem by giving a nice to specify large portions of the table, at least not as both rows and columns.

 Actually, you would have 20 "cells you need to fill in" because your vertices have outdegree 4 and there are 5 of them. Why would you use a library for something as simple as this? Rolling your own can't take more than half an hour, and can be optimized for the context and preprocessed however you see fit. I've got one of my own for every language I've programmed in.

Also re: Specifying the transitions, that's why we've got [insert procedural content gen tech here]. &amp;gt; The understanding that finite state machines and regular expressions are functionally equivalent opens up some incredibly interesting uses for regular expression engines&#8211;particularly when it comes to creating business rules that can be changed without recompiling a system.

My hair tried to escape my head.

By the way, is there a good tutorial about _what to do_ with FSM, when you see that you want to do it like that, but are unsure about how to do it? Like, I was mildly surprised to discover that all Python FSM libraries are shit, because the authors apparently don't understand what people want from a FSM.

EDIT: to clarify, in my experience, FSMs have a pronounced structure, the transition table is emphatically non-random and gets extremely non-random as you add states. You have stuff like, if the state is "after backslash" then do this thing; if the symbol is "backslash" then do that thing. You have these rules covering vast swathes of the cells, in both directions. Yet people who had implemented FSM engines in Python at the time when I went searching apparently did that because they just learned of FSMs and thought that they are awesome, not because they had an actual itch to scratch, an actual real-life problem, so their engines force you to implement a rule for each cell, or a rule for each state or symbol but not both.  &amp;gt; Turing Machines are computationally complete and anything that can be computed can be computed on a Turing Machine.

I'm not a computer scientist, but isn't the Church-Turing thesis unproven? More like, it's just assumed to be true.

Good article, either way.

Edit: Some great discussion in the comments below. &amp;gt; Turing Machines are computationally complete and anything that can be computed can be computed on a Turing Machine.

I'm not a computer scientist, but isn't the Church-Turing thesis unproven? More like, it's just assumed to be true.

Good article, either way.

Edit: Some great discussion in the comments below. I believe science actually runs on falsifiability. So, any theory is assumed to be true until proven false. Can someone confirm? Nah, the scientific method, simply, is the development of a theory based on some observations, then you test that theory. If we're to assume anything about a theory prior to testing, it would be that it's false. I agree with you, but I see where doesFreeWillyExist is coming from.

Once you've got some confirming evidence to support your theory, you might as well go on assuming it's valid until  you can prove it isn't.

Like Newtonian gravity (assumed gravity was instantaneous).  It worked fine, described everything we could measure about stuff falling and orbits and stuff.  Until we discovered that gravitational waves only propogate at the speed of light and not faster.

So, once we have evidence for a theory, we keep assuming it's true until we can prove it isn't.  Then we produce a better theory. Oh, absolutely, but we don't call those things scientific fact until we've some significant level of proof. There's no reason that we can't make use of a hypothesis before proving it scientifically, of course. &amp;gt; scientific fact

Facts only exist in math, abstract logic, and constructed-reality theoretical physics ;) I believe science actually runs on falsifiability. So, any theory is assumed to be true until proven false. Can someone confirm? &amp;gt; Turing Machines are computationally complete and anything that can be computed can be computed on a Turing Machine.

I'm not a computer scientist, but isn't the Church-Turing thesis unproven? More like, it's just assumed to be true.

Good article, either way.

Edit: Some great discussion in the comments below. It's a thesis, not a theorem. Basically, what it really states is (roughly and informal)

A is an algorithm &#8660; There is a Turing Machine running A

So what it's really good for is finally defining what an algorithm actually is. It's nothing less (or more) than a Turing Machine. Obviously, this *could* change in the future, if we ever come up with something more powerful. However, I've yet to meet someone who even considers that a possibility.
  </snippet></document><document><title>Help finding a paper -- typical ratio of 1 bits to 0 bits inside a computer</title><url>http://www.reddit.com/r/compsci/comments/17tgzf/help_finding_a_paper_typical_ratio_of_1_bits_to_0/</url><snippet>I remember reading a paper that talked about on average how many bits inside a computer were 1 versus 0, explaining that the vast majority of them at any given time are 0. My google-fu is completely failing me though. I think it might have been linked off this subreddit or maybe linked off something linked off the programming subreddit... anyone know it?  I don't know of a paper, but intuitively it makes sense. OSes typically have functions that return zeroed pages of memory. Software tends to include zeroed data sections, and their strings end with zero bytes.

All of that would tend to create a bias for zero, at the byte-level, and at the bit-level, intuitively. I would think that it would largely be due to leading zeroes in numeric data types  
a few examples:   
500,000 stored as a 32 bit int:  
00000000000001111010000100100000  
here's today's timestamp as a 64 bit:  0000000000000000000000010011110010100010111000011010001111111011  
here's the unicode representation of the character 'A':   
0000000001000001  
EDIT: formatting, typos  Just for fun I looked at a few memory dumps I had lying around to check their percentages. The results:

| Image | Ones | Total | Percent |
| ---------- | ---------- | ---------- | ---------- |
| boomer-win2003-2006-03-17.img | 264172547 | 8521973760 | 3.09989862019946 |
| boomer-win2k-2006-02-27-0824.img | 1480754587 | 8521973760 | 17.3757233793689 |
| ds_fuzz_hidden_proc.img | 452020649 | 2147483648 | 21.048851730302 |
| harassment_case_memory_XP.img | 545437942 | 2143092736 | 25.4509724585245 |
| vista-beta2.img | 1277580545 | 8522006528 | 14.9915461904701 |
| winxpsp2.img | 425822218 | 4294934528 | 9.91452175170387 |
| xp-laptop-2005-06-25.img | 1141450942 | 4293722112 | 26.5841829588808 |
| xp-laptop-2005-07-04-1430.img | 1185346136 | 4293722112 | 27.6064939714478 |
| xpsp3_2cpu.img | 6769243747 | 25282838528 | 26.7740654970496 |
| win7vss.vmem | 463131546 | 2147483648 | 21.5662431903183 |
| Windows XP Professional-Snapshot10.vmem | 1177420908 | 4194304000 | 28.0719019889832 |
| Windows XP Professional-Snapshot9.vmem | 890157242 | 4194304000 | 21.2230024814606 |
| xpsp2.mem.dd | 582134622 | 2143158272 | 27.1624653020493 |
| Total | 16654673631 | 80700997632 | 20.6375064989234 |

Note that these are from various Windows systems, mostly XP, so YMMV!

Source for the bit counting program:

    #include &amp;lt;stdio.h&amp;gt;
    #include &amp;lt;stdlib.h&amp;gt;
    #include &amp;lt;stdint.h&amp;gt;

    static const unsigned char BitsSetTable256[256] =
    {
    #   define B2(n) n,     n+1,     n+1,     n+2
    #   define B4(n) B2(n), B2(n+1), B2(n+1), B2(n+2)
    #   define B6(n) B4(n), B4(n+1), B4(n+1), B4(n+2)
        B6(0), B6(1), B6(1), B6(2)
    };

    int main(int argc, char **argv) {
        uint8_t buf[4096];

        for (int fn = 1; fn &amp;lt; argc; fn++) {
            unsigned long c = 0;
            unsigned long t = 0;
            int nb = 0;

            FILE *f = fopen(argv[fn], "rb");
            if (!f) {
                perror(argv[fn]);
                continue;
            }

            while(!feof(f)) {
                nb = fread(buf, 1, 4096, f);
                for (int i = 0; i &amp;lt; nb; i++) {
                    c += BitsSetTable256[buf[i]];
                }
                t += nb;
            }
            fclose(f);
            printf("%s: %ld %ld\n", argv[fn], c, t*8);
        }
        return 1;
    }
 Those are pretty much all compressed files on disk rather than registers and main memory, which is more what I meant. Should have phrased the question better :P Sorry, I should have clarified. Those are all memory images, the contents of RAM at a given point in time. There's no disk data in there, except for perhaps some of the OS's filesystem cache. It doesn't include CPU register state, but that's a very tiny fraction of the total. Just for fun I looked at a few memory dumps I had lying around to check their percentages. The results:

| Image | Ones | Total | Percent |
| ---------- | ---------- | ---------- | ---------- |
| boomer-win2003-2006-03-17.img | 264172547 | 8521973760 | 3.09989862019946 |
| boomer-win2k-2006-02-27-0824.img | 1480754587 | 8521973760 | 17.3757233793689 |
| ds_fuzz_hidden_proc.img | 452020649 | 2147483648 | 21.048851730302 |
| harassment_case_memory_XP.img | 545437942 | 2143092736 | 25.4509724585245 |
| vista-beta2.img | 1277580545 | 8522006528 | 14.9915461904701 |
| winxpsp2.img | 425822218 | 4294934528 | 9.91452175170387 |
| xp-laptop-2005-06-25.img | 1141450942 | 4293722112 | 26.5841829588808 |
| xp-laptop-2005-07-04-1430.img | 1185346136 | 4293722112 | 27.6064939714478 |
| xpsp3_2cpu.img | 6769243747 | 25282838528 | 26.7740654970496 |
| win7vss.vmem | 463131546 | 2147483648 | 21.5662431903183 |
| Windows XP Professional-Snapshot10.vmem | 1177420908 | 4194304000 | 28.0719019889832 |
| Windows XP Professional-Snapshot9.vmem | 890157242 | 4194304000 | 21.2230024814606 |
| xpsp2.mem.dd | 582134622 | 2143158272 | 27.1624653020493 |
| Total | 16654673631 | 80700997632 | 20.6375064989234 |

Note that these are from various Windows systems, mostly XP, so YMMV!

Source for the bit counting program:

    #include &amp;lt;stdio.h&amp;gt;
    #include &amp;lt;stdlib.h&amp;gt;
    #include &amp;lt;stdint.h&amp;gt;

    static const unsigned char BitsSetTable256[256] =
    {
    #   define B2(n) n,     n+1,     n+1,     n+2
    #   define B4(n) B2(n), B2(n+1), B2(n+1), B2(n+2)
    #   define B6(n) B4(n), B4(n+1), B4(n+1), B4(n+2)
        B6(0), B6(1), B6(1), B6(2)
    };

    int main(int argc, char **argv) {
        uint8_t buf[4096];

        for (int fn = 1; fn &amp;lt; argc; fn++) {
            unsigned long c = 0;
            unsigned long t = 0;
            int nb = 0;

            FILE *f = fopen(argv[fn], "rb");
            if (!f) {
                perror(argv[fn]);
                continue;
            }

            while(!feof(f)) {
                nb = fread(buf, 1, 4096, f);
                for (int i = 0; i &amp;lt; nb; i++) {
                    c += BitsSetTable256[buf[i]];
                }
                t += nb;
            }
            fclose(f);
            printf("%s: %ld %ld\n", argv[fn], c, t*8);
        }
        return 1;
    }
  This paper has a lot of that kind of information. The idea is that we use such a small range of numbers that the upper registers are normally zero.
http://pharm.ece.wisc.edu/papers2/colt_micro_2010.pdf
My final project in ECE involved creating a processor that periodically inverted every bit and worked in a complimentary mode to balance the duty cycle of all of the transistors. To the best of my knowledge nobody has physically built a processor like this yet. That project sounds really interesting. Is there a problem with having unbalanced duty cycles?  Thank goodness there is a problem with it otherwise my final project would be pretty useless. Transistors wear out unevenly when there's such disparity. The transistors get slower over time and the transistors that are always off slow down faster. The only actual metric I can remember is that most processors have a 20% buffer in clock speed so that it will likely still work after 5 years. Huh. Cool. Did you end up publishing anything on it? I'd like to read up on it. Thank goodness there is a problem with it otherwise my final project would be pretty useless. Transistors wear out unevenly when there's such disparity. The transistors get slower over time and the transistors that are always off slow down faster. The only actual metric I can remember is that most processors have a 20% buffer in clock speed so that it will likely still work after 5 years. Interesting... so bit rot is real?  I always presumed it to be just a myth.  This paper is more on the byte level, but it's the closest I could find: Kjelso, M.; Gooch, M.; Jones, S., "Empirical study of memory-data: characteristics and compressibility", *Computers and Digital Techniques, IEE Proceedings*, Volume 145, Issue 1, pp. 63-67. DOI: [10.1049/ip-cdt:19981797](http://dx.doi.org/10.1049/ip-cdt:19981797), IEEE: [675545](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=675545)


 Hmm, that could be it but is there a version that isn't paywalled? I don't recall paying last time.  If most things are stored efficiently, it should be 50-50.  Anything compressed should certainly be 50-50, since the whole point of compression is to leverage patterns. I can't remember the exact ratio, but I think it was closer to 10-90. See astangl42's comment. Yeah, I tried not to imply I actually knew anything.  So it turns out we're terrible at actually using bits efficiently. Yeah, I tried not to imply I actually knew anything.  So it turns out we're terrible at actually using bits efficiently. &amp;gt;Yeah, I tried not to imply I actually knew anything.

It is better to be ignorant than incorrect. I made a correct statement about the distribution of bits when the information is maximized ie space-efficient storage.  ie I made a prescriptive statement, not a descriptive statement. You really store everything compressed? That must must be awful Yeah, I tried not to imply I actually knew anything.  So it turns out we're terrible at actually using bits efficiently. If you buy a new hard drive and fill up half of it immediately, what do you think the rest of the drive is filled with? It's almost mostly 0s. It has nothing to do with how efficiently we pack things. "Empty" ie unallocated, does not necessarily mean all 0s. Yes, except it usually is, except certain things like DVD+R (or was it DVD-R?), refurbished devices, etc. In contrast, deleting a file often just means removing references to it, not changing the bits. Secure delete can either write all 0s, random bits or something else. If you buy a new hard drive and fill up half of it immediately, what do you think the rest of the drive is filled with? It's almost mostly 0s. It has nothing to do with how efficiently we pack things. If most things are stored efficiently, it should be 50-50.  Anything compressed should certainly be 50-50, since the whole point of compression is to leverage patterns. Depends where you want the efficiency.  Storing things as small as possible is one goal, and fast access is another.  Sure you can store something like a persons age in a single byte, but the unaligned memory access is going to take considerably longer than an aligned one.  Given that memory is much cheaper than CPU time in most cases, I'd keep things uncompressed.  </snippet></document><document><title>How would one design an ALU based on many-valued logic?</title><url>http://www.reddit.com/r/compsci/comments/17tybp/how_would_one_design_an_alu_based_on_manyvalued/</url><snippet>For example, ten-valued logic, with:

* 0=False

* 1=10% likely

* 2=20% likely

* 3=30% likely

* 4=40% likely

* 5=50% likely

* 6=60% likely

* 7=70% likely

* 8=80% likely

* 9=True

How would you construct a (2,2) half-adder? I know that with binary logic you use XOR for the sum and AND for the carry, but what would you use here? "Ban-wise" operators? What would they be like?

Edit: Correct me if I am wrong to assign probabilities to the 8 values other than true and false  Well, you'd have to define your operations. What do you expect an adder to do? Add 2 probabilities, with a ceiling of "true"? What about multiple digits? The 1's place are 10% values. Maybe the 10's place are 1% values, etc? 11=11%, 12=21%, 31=13%? Add two single digit integers from 0 up to nine to return a one or two digit integer

EDIT: so since I want to find the sum and not the carry yet, just the ones digit Wait. So are you trying to represent numbers, or probabilities? Your explanation above sounds like you want probabilities, and it's still focused on an essentially boolean logic. I think he is trying to describe a ten-valued, i.e. a denary logic. Wait. So are you trying to represent numbers, or probabilities? Your explanation above sounds like you want probabilities, and it's still focused on an essentially boolean logic. Numbers. Boolean logic is bascially a type of probability logic, no? Just you only deal with certainties. In ternary logic systems, for example, you can have a value of "unknown".  Why would you assign probabilities to each of the values? They all really just represent voltage states, so probabilities have nothing to do with it as far as i can see. You could simply refer to 0 as 0, 1 as 1, 2 as 2 and so forth. It's a matter of representation.

My guess:

If you have ten-values and want to create an adder in hardware, you'd have to define decimal/ten-valued logic as some functions,  e.g. [0-9] x [0-9] -&amp;gt; R where R is some proper range (presumably [0-9]). In short your hardware should be able to distinguish between the different voltage levels and make deterministic choices in accordance with the logic you have defined.

For instance there are ternary computers are based on Kleene logic:
http://en.wikipedia.org/wiki/Ternary_logic

EDIT: I kind of see why choose to interpret the values as probabilities now, but are 10% increments not wrong? Since 0 is false (0%) and 9 is true (100%) shouldn't the increment of probability per level be be (100-0)/(9-0)=11.111...?   </snippet></document><document><title>Really good Algorithms videos</title><url>http://www.reddit.com/r/compsci/comments/17s2oa/really_good_algorithms_videos/</url><snippet>Does anybody know of any really good algorithm videos online. I took a course, but it was really dry and I would like to go through again while enjoying this vital area of CS.  More of a "fun" set of lectures: (UNSW Algorithms and Data Structures)
http://www.youtube.com/course?list=ECE621E25B3BF8B9D1

More of a hardcore set of lectures: (MIT Intro to Algorithms)
http://www.youtube.com/playlist?list=PLUl4u3cNGP61Oq3tWYp6V_F-5jb5L2iHb

I've also just found this MIT course. I haven't seen it yet, so I don't know the quality but it has one of the same teachers as the other MIT course:
http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/lecture-videos/ Thanks i'll take a look. The UNSW lectures look interesting The teacher (Richard Buckland) is entertaining and makes the course fun. The subject matter is covered at a pretty slow pace but I've also had 6+ years of CS education. More of a "fun" set of lectures: (UNSW Algorithms and Data Structures)
http://www.youtube.com/course?list=ECE621E25B3BF8B9D1

More of a hardcore set of lectures: (MIT Intro to Algorithms)
http://www.youtube.com/playlist?list=PLUl4u3cNGP61Oq3tWYp6V_F-5jb5L2iHb

I've also just found this MIT course. I haven't seen it yet, so I don't know the quality but it has one of the same teachers as the other MIT course:
http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/lecture-videos/ More of a "fun" set of lectures: (UNSW Algorithms and Data Structures)
http://www.youtube.com/course?list=ECE621E25B3BF8B9D1

More of a hardcore set of lectures: (MIT Intro to Algorithms)
http://www.youtube.com/playlist?list=PLUl4u3cNGP61Oq3tWYp6V_F-5jb5L2iHb

I've also just found this MIT course. I haven't seen it yet, so I don't know the quality but it has one of the same teachers as the other MIT course:
http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/lecture-videos/ In lecture 13, the *31 year old* professor is wearing the [rock-paper-scissors-lizard-spock teeshirt.](http://youtu.be/s-CYnVz-uh4) More of a "fun" set of lectures: (UNSW Algorithms and Data Structures)
http://www.youtube.com/course?list=ECE621E25B3BF8B9D1

More of a hardcore set of lectures: (MIT Intro to Algorithms)
http://www.youtube.com/playlist?list=PLUl4u3cNGP61Oq3tWYp6V_F-5jb5L2iHb

I've also just found this MIT course. I haven't seen it yet, so I don't know the quality but it has one of the same teachers as the other MIT course:
http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/lecture-videos/  Check out 2 free online courses from Coursera. 

Algorithms I -- starts this Monday

https://www.coursera.org/course/algs4partI

and an Algorithms design class that started last week

https://www.coursera.org/course/algo


 Check out 2 free online courses from Coursera. 

Algorithms I -- starts this Monday

https://www.coursera.org/course/algs4partI

and an Algorithms design class that started last week

https://www.coursera.org/course/algo


 Check out 2 free online courses from Coursera. 

Algorithms I -- starts this Monday

https://www.coursera.org/course/algs4partI

and an Algorithms design class that started last week

https://www.coursera.org/course/algo


 Check out 2 free online courses from Coursera. 

Algorithms I -- starts this Monday

https://www.coursera.org/course/algs4partI

and an Algorithms design class that started last week

https://www.coursera.org/course/algo


   The University of Berkeley has a cool [course on data structures](https://www.youtube.com/watch?v=QMV45tHCYNI) you can check The University of Berkeley has a cool [course on data structures](https://www.youtube.com/watch?v=QMV45tHCYNI) you can check  http://www.youtube.com/user/AlgoRythmics?feature=watch

Sorting algorithms only, but serious fun.   These lectures are also good: http://youtu.be/gwlevtaC-u0 </snippet></document></searchresult>
<searchresult><compsci /><document><title>Can bytecode interpreters make use of SIMD instructions?</title><url>http://www.reddit.com/r/compsci/comments/17q4je/can_bytecode_interpreters_make_use_of_simd/</url><snippet>The job a bytecode interpreter (decode, dispatch, repeat) seems fundamentally at odds with the uniformity required to get speedups from vector operations. Nonetheless, does anyone here know of any clever language implementations which make use of packed register instructions? Or even more radically, has anyone managed to shoehorn a dynamic language runtime onto a GPU? I don't mean compiling specialized fragments of a program to a GPU, but actually running an interpreter on top of wide-vector hardware. 

edit: Sorry, I don't think my question was clear. I'm not asking about just-in-time translators/compilers which target SIMD instruction sets (like the JVM or DAISY). I'm also not asking whether there are any interpreted languages which use precompiled vectorized library functions (such as the array operators of APL/J/K/etc...). 

A dispatch loop for a dynamic language interpreter might look something like this:

    for each instruction in the program:
        switch(instruction.opcode) { 
            case ADD_INT:
                x = stack.pop()
                y = stack.pop()
                stack.push(x+y)
            case MULT_INT:
                ...
        }

Of course, the bytecode format might be dynamically typed (and thus use a generic ADD w/ type tags on the value), or the intepreter might be [directly threaded](http://blog.mozilla.org/dmandelin/2008/06/03/squirrelfish/) instead of using a switch, or it might [use registers](http://markfaction.wordpress.com/2012/07/15/stack-based-vs-register-based-virtual-machine-architecture-and-the-dalvik-vm/) instead of a stack, or a million other design variants. But the basic pattern remains the same. It seems extremely unlikely that this sort of code is amenable to parallelization. However, few things are impossible--- so I'm asking if someone has found a clever way to use SIMD instructions to speed up a program that behaves like the one I sketched out above.

     The standard JVM will generate SSE3 instructions (though not particularly well). The next release of LLVM (which is also a JIT) can generate SSE3 from instructions as well by default. 
 I'm guessing you mean that the JVM's runtime compiler generates SSE instructions for compiled traces-- but I'm curious more about the use of SSE instructions in the implementation of the underlying interpreter.   The interpreter side of it isn't my area, so let me know if this is totally preposterous.  

If you looked at your interpreter as some higher-level version of a superscalar processor architecture, I think you could use SIMD instructions as a time-efficient implementation of the execution part of that architecture.  It would have the same issues with underused throughput if the instruction stream doesn't have exactly the structure of parallelism that the hardware design is expecting.  The Pentium 1 was like this: up to 2 instructions per clock, in order, so you only take advantage of that 2-wide execution if instructions happen to come in just the way the processor can use them.

If, let's say, you had multiple interpreter threads looking ahead in the instruction stream for non-overlapping instructions, your back end could look for batches of independent in-flight instructions that could fill up SIMD operations.  This would make your interpreter an emulated implementation of an out-of-order superscalar architecture.

Whether that would be any faster than a more naive way of doing it is a wide open question.

Would that do what you're asking? As an alternative to having multiple threads, I guess you could also fetch k instructions at once and then  have some very large (probably auto-generated) chunk of logic for when subsets of those are combinable. However, in either case, it's really unclear if it would be a performance win due to the packing/unpacking cost of going in and out of SIMD registers. Also, don't superscalar architectures allow unrelated operations to issue at once? If I have two ALUs then I can run an ADD and MULT simultaneously. With SIMD instructions, however, I would be looking to combine multiple instances of the same opcode. 

Nonetheless, it's definitely the sort of thing I'm interested in. Thanks!        On Android, things such as video decoding is written in a language such as C or C++ or ARM assembly with SIMD instructions (NEON specifically) and compiled into native code.  Both first-party and third-party apps take advantage of this.</snippet></document><document><title>Computer Science PhD trends: Taulbee Survey for 2011-12</title><url>http://blog.vivekhaldar.com/post/42007984829/computer-science-phd-trends</url><snippet>  I find it a bit annoying that you have to scroll down the bottom of the post to make sure that, yes, it is only about research in the US&#185;. Hello, this is not the only country doing CS research !

The last point on aliens is very good, though. I've heard bright indian interns talk about how they would rather try to find a company than a PhD programme, because they would get more help for their visas.

&#185;: actually the CRA statistics range over Canada as well, but it has a rather small number of CS depts compared to the US. &amp;gt; I find it a bit annoying that you have to scroll down the bottom of the post to make sure that, yes, it is only about research in the US&#185;. Hello, this is not the only country doing CS research !

Then do your own damn research, write a blog article about it, and then post it here! It is *fine* to have good-quality, detailed statistics about the US PhD trends. I would like people to be clear about the fact that they are *US PhD trends* (or Noth-America PhD trends), and not just "PhD trends".

And this apply equally to the title picked on reddit than on the blog post itself. This would allow, people interested in PhD trends but less so in North-America-only PhD trends to evaluate their interest in the link.  Sorry it doesn't meet your every expectation, but you're not the intended audience. You totally miss the point, and you insist. The phrasing of the title is wrong. I would not mind if it was a tabloid, but I do mind if it is an article written by a PhD and clearly adressed to other current or future PhDs. It's a blog post.  This isn't some peer reviewed article, it isn't something that has past under the purview of some editor, it's a personal statement.  And you are getting upset that he didn't think about YOU when he wrote it.  The actual report has a lot of interesting statistics. I'll try to do a more full summary with more stats later- for now, I'm on my phone.

* only 11.7% of undergraduate CS degrees and 11.8% of undergrad CE degrees are awarded to women. But twice as high a proportion, 22.3%, of Ph.D. students and graduates are women. In CE, the drop out rate for women from Ph.D. programs is significantly lower than for men, for CS the proportion is identical, and in I, there is a slightly higher drop out rate for women. So don't listen to anyone who claims the gender gap is due to a difference in competence or women wanting to have families instead.

* The 11.7/11.8% is really a problem. It seems that, once in the field, women do just as well or better than men, and they are arguably more likely to stick with it and finish grad school. So why are there so few undergrads? There has been a great deal written about how the culture of many CS programs, and the attitudes of many of the students, drive women away. At the very least it is worth investigating and making an effort to correct this. You can't just chalk it up to "women just inherently don't like science/math/logic/gizmos/whatever" - the gap in CS/CE is substantially worse than in other STEM fields.

* the proportion of tenure track positions, both full and associate, held by women is is less than in non tenure track positions, but the numbers for new hires are more balanced. This could mean hiring practices are getting better, or it could mean that women are not getting promoted or selected for advancement once they get to tenure track positions. The actual report has a lot of interesting statistics. I'll try to do a more full summary with more stats later- for now, I'm on my phone.

* only 11.7% of undergraduate CS degrees and 11.8% of undergrad CE degrees are awarded to women. But twice as high a proportion, 22.3%, of Ph.D. students and graduates are women. In CE, the drop out rate for women from Ph.D. programs is significantly lower than for men, for CS the proportion is identical, and in I, there is a slightly higher drop out rate for women. So don't listen to anyone who claims the gender gap is due to a difference in competence or women wanting to have families instead.

* The 11.7/11.8% is really a problem. It seems that, once in the field, women do just as well or better than men, and they are arguably more likely to stick with it and finish grad school. So why are there so few undergrads? There has been a great deal written about how the culture of many CS programs, and the attitudes of many of the students, drive women away. At the very least it is worth investigating and making an effort to correct this. You can't just chalk it up to "women just inherently don't like science/math/logic/gizmos/whatever" - the gap in CS/CE is substantially worse than in other STEM fields.

* the proportion of tenure track positions, both full and associate, held by women is is less than in non tenure track positions, but the numbers for new hires are more balanced. This could mean hiring practices are getting better, or it could mean that women are not getting promoted or selected for advancement once they get to tenure track positions. Thanks for digging up those statistics.

I'm just a newbie to CS, so I don't have much to say. But from the little I've seen in my school, it seems the issue has become self-perpetuating: Women aren't staying in the major because there aren't any women in the major. My intro course had a dozen girls. The next semester there were five of us. Two years ahead of me there are only two. (But weirdly enough, the grad student population is half women, almost all of them Chinese.) The 'CS is for men' is an American thing; in many other cultures it is gender neutral, which is why when you have a big foreign population, like in grad school, it is more balanced. &amp;gt; The 'CS is for men' is an American thing

It's the same thing in Germany. Out of 200 CS students in my year, less than 5 are female. The 'CS is for men' is an American thing; in many other cultures it is gender neutral, which is why when you have a big foreign population, like in grad school, it is more balanced.  I'm a first-year PhD student who's hoping to one day get a job as a teaching-track faculty.

According to this document, my odds are 1 in 50.

Awesome. Reread the data.

7% started with tenure-track faculty positions.  Meaning their first job was such.

How many were seeking a tenure-track faculty position as their first gig?  I know that's my path.  But what about everyone else?   


This guy is putting a very pessimistic, debbie-downer spin on this.

 Exactly. According to his source, 16.8% of new PhDs got a postdoc position, and with other positions listed in academia (researcher and teaching faculty), the total new PhDs entering into academic careers comes to about 31%. I'd say that's not particularly discouraging.  I am very surprised by the breakdown of the PhD students. There are 119 CS students in theory and algorithms. Surpassing almost all other topics. </snippet></document><document><title>Creating a Simple CFD Program.</title><url>http://www.reddit.com/r/compsci/comments/17oaxy/creating_a_simple_cfd_program/</url><snippet>I was hoping to get some input here on my project that a friend and I are undertaking. 

We've participated in some challenges over the years that deal heavily with aeronautics and optimizing the aerodynamics of aircraft and vehicles. In dealing with these, we often have to use large software packages to perform fluid dynamics on our designs. Our idea is to create a tool written in Python and/or Java in order to simply import an STL, mesh it, and export the lift/drag forces.

Here are the steps that it would have to undertake:

1. Opening GUI
1. Enter volume of fluid
1. Input density of fluid (i.e. air, water, etc.)
1. Import STL
1. Mesh model (given a mesh fine-ness/resolution)
1. Determine velocity of the air and other variables (if necessary - temperature, fir example)
1. SOLVE
1. Export colored image/animation of test (like [this](http://www.padtinc.com/images/cfd-fluent-flow-through-valve.jpg))
1. Export spreadsheet of values of drag, lift, etc.

Now, this is obviously no simple task. We'd like to use a simple programming language mostly to test its boundaries and see if it is possible to do so. As far as libraries go for Python/Java in order to mesh the object, I have looked into MeshPy and others but it seems like it may be advantageous to write out own. Are there any other libraries that may be worth looking into?

Let's also assume that we have a pretty firm grasp on the basics of CFD and what it means/tells us about the object.

**tl;dr - Making a CFD program in Python/Java, need advice on where to start**   1. Repeat this question in /r/CFD, /r/numerical and /r/aerospace.
2. Pick a method to implement. If you're using a loose enough definition for CFD, I'd consider a panel method. 
3. Offload the meshing to another tool and generate some simple meshes. 2-D you can do by hand, 3-d you might look at something like OpenVSP.
4. Write the 2-d solver.
5. Write the 3-d solver.
6. Test the solvers on the meshes you generated separately.
7. Now go back and write your mesh generator.
8. Repeat 2-8 for any other solvers you want to write.
9. Ignore people bitching about your language choices.
10. Keep us posted on your progress. :)

Python libraries that might be useful: the numerical stuff like SciPy. Some sort of GUI toolkit, I don't do so well with those, visualization tools like matplotlib, Chaco, Mayavi. I posted to /r/aerospace and /r/programming but /r/CFD unfortunately looks rather dead.. Thanks for the tips though!

I was thinking of starting by making a 2D airfoil solve (like XFOIL) but in Python and to not make the same mistakes JavaFoil did. From there, I'd like to extrapolate that into 3D CFD. I'm pretty well versed in OpenVSP (had to use it in this last challenge), and I'm curious as to how I could maybe call on OpenVSP to mesh an object, like:

Create STL -&amp;gt; Open in CFD program -&amp;gt; OpenVSP Mesh -&amp;gt; Back to CFD for testing 

</snippet></document><document><title>Cache oblivious algorithms</title><url>http://iainkfraser.blogspot.co.uk/2013/01/cache-money-hoes-attached-code.html</url><snippet> </snippet></document><document><title>Is it possible to prove that a chess implementation is "correct"?</title><url>http://www.reddit.com/r/compsci/comments/17nfws/is_it_possible_to_prove_that_a_chess/</url><snippet>I'm not talking an implementation of an engine, just a simple game of chess where you can enter moves for both sides. By correct I mean the program will reject all invalid moves and will always evaluate the final position correctly (Draw or Win for some side). I understand there are languages like Coq for doing stuff like that but would need some guidance to understand how this works better.   [Here's a great starting point on chess verification](http://corp.galois.com/blog/2011/5/26/formally-verified-chess-endgames.html)

You will be able to prove a given 'game' (aka a full list of moves in some unambiguous chess notation) is legal; but it depends what you mean by implementation.

Given an initial board state and then a move, evaluating whether that move is legal is doable; as each piece has a finite set of possible moves it can make. (Castling, promotion, and en-Passat are a pain, but again there is a finite formula for possible moves for a given piece, so if your 'game' tells you a piece did X, you can check the rules against the previous game state).

[Endgame conditions](http://en.wikipedia.org/wiki/Chess#End_of_the_game) are also a finite set, but I believe you need at least a 50 move memory to discount the 50 move draw condition.

So I suppose if you can prove that you correctly handle every piece's move possibilities and every given endgame condition then you should be able to prove your implementation correct in theory, as there's no 'ambiguity' in chess from a rules standpoint.



 &amp;gt; You will be able to prove a given 'game' (aka a full list of moves in some unambiguous chess notation) is legal; but it depends what you mean by implementation.

By implementation I mean the following: Given a starting position the player can input moves (for both sides) and the program will return the new position after the move on the board has been played or  reject the invalid move. The program can also take a position and evaluate it as a checkmate or a stalemate (We can ignore drawn endgames and the rest as unknown for example). 

&amp;gt; Given an initial board state and then a move, evaluating whether that move is legal is doable; as each piece has a finite set of possible moves it can make. (Castling, promotion, and en-Passat are a pain, but again there is a finite formula for possible moves for a given piece, so if your 'game' tells you a piece did X, you can check the rules against the previous game state).

But my point is how do I find out if my implementation of the possible moves is valid? I would like to prove that it is in fact correct. I was building an engine of my own, but stopped halfway through. I don't know if you want a formal proof, but I planned to run my engine through one of those muli-thousand recorded games database and then planned some more manual comparative testing with other chess engines to get those trickier parts correct. Note that just for checking correctness of moves (king in check complictes things) or producing PGN/FIDE compliant notation (the way ambiguity is resolved) you'll need full move generator, meaning there is no 'lightweight' approach to it.  Unfortunately that's not what I'm looking for. Your method can only disprove an implementation. Seems like what I'm looking for isn't easy at all unfortunately :/ What did you have in mind? I thought that if you wanted to have it verified in something like Coq you wouldn't be asking questions here.  &amp;gt; I thought that if you wanted to have it verified in something like Coq you wouldn't be asking questions here. 

Could you kindly point me to a better place to ask this question? I'll retract, mabye compsci was ok place to ask, I would've added 'formally prove' somewhere in the title just to attract people that know a bit about that subject and not other people that think of other ways of verification. Other places where you could ask might be /r/coq if you're familiar with that or stackoverflow.com. 

EDIT: If I may ask, why do you want to make a formal proof? It seems to me that a formal verification, extremely cumbersome and laborius that it often is, is only well justified with systems that need to be absolutely free of any bugs in situations where lives are in danger or where larger sums of money are involved, chess software usually isn't associated with either group. Unless you weren't introduced to the subject then you'll have to process a graduate class of material. The way I would do it is I would ask on /r/coq if it was appropriate for the case, then proceed with reading the manual (which I'm led to believe is excelent), maybe take an online class as an intro before that - but as I said I would make better investment of my time if I added other features to my program i.e. I'd only attempt formal verification if I was well versed in the subject beforehand.  Yes, it's possible, although it will be much more difficult than just writing the code. Basically, you have three tasks:

1) Write the actual code, in a programming language/system that also supports steps 2 and 3 below

2) Write down the specification: what it means for your program to be correct, in your case when a board/move is valid

3) Prove that the program written in (1) behaves as described in (2).

Basically, it means that you write down the rules of chess *twice*, once in step (1), once in step (2), and prove that they are equal in step (3). Usually, the specification is much simpler to read/write than the code, and can be checked manually for correctness (e.g. be compared with the chess rules).

Integrated systems (programming language + proof system) are [Coq](http://coq.inria.fr/), [Isabelle](http://www.cl.cam.ac.uk/research/hvg/Isabelle/), [Agda](http://wiki.portal.chalmers.se/agda/pmwiki.php) .... Tools for existing programming languages are [Frama-C](http://frama-c.com/) for C, [SPARK](http://www.adacore.com/sparkpro/) for Ada, and others.   </snippet></document><document><title>Computer scientists find new shortcuts for Traveling Salesman problem.</title><url>http://www.wired.com/wiredscience/2013/01/traveling-salesman-problem/all/</url><snippet>  "...showed that the traveling salesman problem is &#8220;NP-hard,&#8221; which means that it has no efficient algorithm...unless a famous conjecture called P=NP is true."

I really dislike this pop-sci presentation of P=NP. It conflates efficient in a technically defined way ("in P") with the more pragmatic efficient ("fast enough").

If false, there could still be some efficient (in the practical sense) algorithm for TSP, like n^(log log log n).

See also: http://rjlipton.wordpress.com/2009/07/03/is-pnp-an-ill-posed-problem/ How would you explain it in two sentences so that the layman would have a general idea of what you're talking about and why it matters? He's essentially saying that even though we can't figure out if Travelling Salesman is a part of a set of easier to solve problems, we can still find faster solutions to it than we know of today. It's important because its a problem that is applicable to a lot of problem domains, such as routing internet traffic. I understood what he was saying.  He was complaining about their treatment of P=NP.  I was asking if he could provide a better concise explanation of P=NP that would be appropriate for Wired's target audience. I don't claim to be a writer so what I come up with won't be good. The words "worst case"  should probably appear in there somewhere, though. I understood what he was saying.  He was complaining about their treatment of P=NP.  I was asking if he could provide a better concise explanation of P=NP that would be appropriate for Wired's target audience. How would you explain it in two sentences so that the layman would have a general idea of what you're talking about and why it matters? "...showed that the traveling salesman problem is &#8220;NP-hard,&#8221; which means that it has no efficient algorithm...unless a famous conjecture called P=NP is true."

I really dislike this pop-sci presentation of P=NP. It conflates efficient in a technically defined way ("in P") with the more pragmatic efficient ("fast enough").

If false, there could still be some efficient (in the practical sense) algorithm for TSP, like n^(log log log n).

See also: http://rjlipton.wordpress.com/2009/07/03/is-pnp-an-ill-posed-problem/ "...showed that the traveling salesman problem is &#8220;NP-hard,&#8221; which means that it has no efficient algorithm...unless a famous conjecture called P=NP is true."

I really dislike this pop-sci presentation of P=NP. It conflates efficient in a technically defined way ("in P") with the more pragmatic efficient ("fast enough").

If false, there could still be some efficient (in the practical sense) algorithm for TSP, like n^(log log log n).

See also: http://rjlipton.wordpress.com/2009/07/03/is-pnp-an-ill-posed-problem/ &amp;gt;It conflates efficient in a technically defined way ("in P") with the more pragmatic efficient ("fast enough").

I've seen this conflation in a number of papers, though. I see it pretty much everywhere in AI: in P is tractable, NP-hard is intractable. Really?  All the AI work I've done was with heuristic approaches to things in NP (or harder classes), since those are the really interesting problems.  &amp;gt; Nevertheless, Saberi said, solving the traveling salesman approximation problem in its full generality will probably require an infusion of new ideas.

Unless I'm misreading what he's saying here, it's certainly not possible (unless P=NP). It's been proven (and it's a fairly simple proof) that it's n-inapproximable. Right?

Perhaps the article misunderstood what Saberi said and scribed it unfaithfully. &amp;gt; Nevertheless, Saberi said, solving the traveling salesman approximation problem in its full generality will probably require an infusion of new ideas.

Unless I'm misreading what he's saying here, it's certainly not possible (unless P=NP). It's been proven (and it's a fairly simple proof) that it's n-inapproximable. Right?

Perhaps the article misunderstood what Saberi said and scribed it unfaithfully. Solving the TSP in full efficiently would require P=NP; solving the TSP approximation would not. For instance, the existence of a poly-time algorithm which comes within 0.00001% of the true answer, or one which computes the true answer in 99.99999% of cases do not require P=NP.

Obviously these would be huge breakthroughs. I believe I remember seeing proof that the general TSP is n-inapproximable; i.e. there is no polynomial time algorithm that can calculate it within a constant factor.

The proof is because if you can approximate TSP to within *any* constant factor, you can solve the Hamiltonian cycle problem. Are you sure the proof applies to probabilistic algorithms?  Learnt about the traveling salesman problem in my discrete mathematics class last semester.  Not too sure what implication this has, but an interesting application of it nonetheless! The way I see it, it doesn't have any implications apart from being able to approximate a metric TSP with a smaller error. It's provable that no such approximation algorithm for the general TSP can exist unless P=NP, in which case no such algorithm is needed.

However, the difference between a 50% error and a 40% error is quite substantial for sufficiently large graphs. It's a good result.

As it stands, the TSP is one of the hardest problems in computer science and finding an efficient algorithm (for the corresponding decision problem) would mean that P=NP, which would be a *major* breakthrough. However, proving that no such algorithm can exist would be a major breakthrough as well. At the moment we haven't really got a clue which is the case, although most people in the field suspect the latter. I think saying "we haven't really got a clue" is a bit unfair to the state of research. As far as I know, all (yes, ALL) the experts in complexity are certain P != NP; see, for example, [Scott Aaronson's reasons to to believe as such](http://www.scottaaronson.com/blog/?p=122) (admittedly some are more technical than others).
 &amp;gt; As far as I know, all (yes, ALL) the experts in complexity are certain P != NP;

A [poll](http://www.cs.umd.edu/~gasarch/papers/poll.pdf) found that 61% thought that P!=NP and 9% thougt P=NP. I think saying "we haven't really got a clue" is a bit unfair to the state of research. As far as I know, all (yes, ALL) the experts in complexity are certain P != NP; see, for example, [Scott Aaronson's reasons to to believe as such](http://www.scottaaronson.com/blog/?p=122) (admittedly some are more technical than others).
 Learnt about the traveling salesman problem in my discrete mathematics class last semester.  Not too sure what implication this has, but an interesting application of it nonetheless! Learnt about the traveling salesman problem in my discrete mathematics class last semester.  Not too sure what implication this has, but an interesting application of it nonetheless! Examples of TSP mentioned in the comments and in the article:

 1. Routing network traffic
 1. Routing UPS trucks
 1. Guiding computer-controlled manufacturing devices (possibly CNC mills, laser cutters, 3d printers, etc)


&amp;gt; [The TSP has several applications even in its purest formulation, such as planning, logistics, and the manufacture of microchips. Slightly modified, it appears as a sub-problem in many areas, such as DNA sequencing.](http://en.wikipedia.org/wiki/Travelling_salesman_problem)  I wonder how the UPS and FedEx routing algorithms fair against this... At least UPS' route efficiency system is a known highly guarded trade secret. UPS and FedEx can use spanning trees. They don't have to hit every city with a single truck. Yes, but each leaf represents the territory of a single truck which must solve a travelling salesman problem for its deliveries for that day. They've just added the additional problem of efficiently dividing up which deliveries should be handled by each truck on top of this other problem. Computationally not any less trivial, but by throwing resources at redundancy they can make inefficient solutions simply less noticeable. It's going to get more nuanced than that once you factor in how well the drivers know their own routes vs. how they would perform on an arbitrarily assigned new route.  Plus UPS can still perfectly solve for the routes they want.  It's expensive, sure, but it's not like it's cryptography cracking expensive. UPS and FedEx can use spanning trees. They don't have to hit every city with a single truck.   What Andrew Wiles quote is paraphrased?

He's the Fermat's Last Theorem guy, but I couldn't find anything similar by him on "quotes" pages, e.g. [his wikiquote page](http://en.wikiquote.org/wiki/Andrew_Wiles)

There is a kinda related [quote maybe by Darwin](http://en.wikiquote.org/wiki/Charles_Darwin):

&amp;gt; A mathematician is a blind man in a dark room looking for a black cat which isn't there. I found it [here](http://www.pbs.org/wgbh/nova/physics/andrew-wiles-fermat.html). I think I have seen him say it on video too, maybe in the documentary?

&amp;gt;Perhaps I can best describe my experience of doing mathematics in terms of a journey through a dark unexplored mansion. You enter the first room of the mansion and it's completely dark. You stumble around bumping into the furniture, but gradually you learn where each piece of furniture is. Finally, after six months or so, you find the light switch, you turn it on, and suddenly it's all illuminated. You can see exactly where you were. Then you move into the next room and spend another six months in the dark. So each of these breakthroughs, while sometimes they're momentary, sometimes over a period of a day or two, they are the culmination of&#8212;and couldn't exist without&#8212;the many months of stumbling around in the dark that proceed them. What Andrew Wiles quote is paraphrased?

He's the Fermat's Last Theorem guy, but I couldn't find anything similar by him on "quotes" pages, e.g. [his wikiquote page](http://en.wikiquote.org/wiki/Andrew_Wiles)

There is a kinda related [quote maybe by Darwin](http://en.wikiquote.org/wiki/Charles_Darwin):

&amp;gt; A mathematician is a blind man in a dark room looking for a black cat which isn't there. It was in the Horizon documentary, right at the beginning around 1 minute in. It's an interesting doc, well worth watching. http://www.youtube.com/watch?v=7FnXgprKgSE  </snippet></document><document><title>Writing Loop Invariants in Whiley [VIDEO]</title><url>http://whiley.org/2013/01/29/understanding-loop-invariants-in-whiley/</url><snippet /></document><document><title>Can MEMS-based navigation systems handle activities like walking- jogging- running-in-place?</title><url>http://www.reddit.com/r/compsci/comments/17jyx6/can_memsbased_navigation_systems_handle/</url><snippet>I noticed that these activities can sometimes produce the same accelerometer signals as, say, walking normally. Are there already ways to work around these problems?  Inertial navigation, especially with MEMS sensors, does not work without some other sort of correction. The sensors are not accurate enough, they are prone to measurement biases, and the numerical integration used in those types of schemes are generally not all that accurate. 

In aircraft navigators, for example, inertial sensors are only used to keep track of the vehicle state in between the GPS and magnetometer readings used to correct the inertial position/velocity and attitude estimates. The idea being that GPS and magnetometers are very slow, but very accurate over time, and inertial sensors are very fast but inaccurate over time, so you fuse them together to get the best of both.

So MEMS based navigation systems can't even handle regular walking well without some other sensors, so if you throw running in place at them they will perform even more poorly.

In an idealized system, however, it would have no problem. In order to remain pretty much in place, any accelerations in one direction would have to be offset in the opposite direction in order for you to remain in place. In an actual system, increased variance in the measurements will certainly degrade performance. The way you handle that is by either getting better sensors (decent ones will be around a few hundred bucks, fairly accurate ones a few grand, the kind accurate enough for military aircraft will be hundreds of thousands) or redundant measurements.  </snippet></document><document><title>Creating a visual representation of software performance</title><url>http://www.reddit.com/r/compsci/comments/17jtiz/creating_a_visual_representation_of_software/</url><snippet>I am trying to figure out the best way to represent software system performance. This visualization will be a tool to debug performance problems. How should such a tool look like?



My current idea is based on [chrome's debug tool](http://3.bp.blogspot.com/-xkkPZXpqatA/UKZlMLDQr9I/AAAAAAAAAvA/52-CWOHoQCw/s1600/Google+Chrome+Developer+Tools+-+Timeline+Panel.png). Each task/thread has it's own line and he length of the bar represents its duration. The bar would have visible segments representing execution time and wait time. Seeing all tasks on one page let's you see whats being executed concurrently. I would also add an overlay of system memory usage over time.



Does anyone have experience with performance modeling?    Look up some of the user manuals and help pages for the old Apple developer tool called Shark.

They did it right. I miss Shark...    as a programmer i think you should just show the Big O notation and expect people to figure it out :D edit: sorry for pissing everyone off, was just trying to be funny and or trolly as per ReinH

also, Conky is a great customizable script for linux that can be used as a resource monitor! Check it out, its really sexy. Can you write me a program to analyze a block of code and report the Big-O for it.  Thanks. Can you write me a program to analyze a block of code and report the Big-O for it.  Thanks. Any reasonably competent programmer should be able to figure that out. Presumably, your block of code was written by a programmer. . .If they weren't reasonably competent, fire 'em and give it to one that is so they can figure it out. Now, figure it out. :-&amp;gt; 

But seriously, there's enough profile apps out there that cover the needs here. Use those as the basis for the "visual" data. Once you have that data, you can start looking at all sorts of different ways to represent it visually and I think that different situations will be best represented by different methods. Maybe a part of it should be a line graph that compares inputs to execution times while another part is just a stacked bar graph that shows system time versus cpu time for different threads. Any reasonably competent programmer should be able to figure that out. Presumably, your block of code was written by a programmer. . .If they weren't reasonably competent, fire 'em and give it to one that is so they can figure it out. Now, figure it out. :-&amp;gt; 

But seriously, there's enough profile apps out there that cover the needs here. Use those as the basis for the "visual" data. Once you have that data, you can start looking at all sorts of different ways to represent it visually and I think that different situations will be best represented by different methods. Maybe a part of it should be a line graph that compares inputs to execution times while another part is just a stacked bar graph that shows system time versus cpu time for different threads. I don't think it's that easy. Proving the complexity of a bit of code can be quite complex. Sure, for the most part it's simply counting but some recursive algorithms can be quite tricky. Automating the task is non-trivial I'd say. We can't even *prove* that a program will halt at all, let alone when it will halt. It is literally impossible to do this, no matter how competent the programmer.

What we *can* do is use tools to record the number of operations of an algorithm and compare that to the number of its argument and then use that data to try to find some correlation via curve fitting and other statistical techniques. You know what we call that? *Profiling.*

Edit: and apparently, [this exists](http://www.reddit.com/r/compsci/comments/17jtiz/creating_a_visual_representation_of_software/c86bxup). Cool. Profiling != Complexity analysis

At least we can prove that we can't prove. as a programmer i think you should just show the Big O notation and expect people to figure it out :D edit: sorry for pissing everyone off, was just trying to be funny and or trolly as per ReinH

also, Conky is a great customizable script for linux that can be used as a resource monitor! Check it out, its really sexy. That's a very naive assumption. Lots of parts of a computer's architecture contribute to performance problems, as do OS issues. Pretty sure this is a troll. Nope, just a computer engineer with 7 years industry experience who knows about computer architecture, that's all.

There is a lot more to performance than just Big-O, once you leave the theoretical space (classroom) and move into the practical space (actually run code on a computer).

A lot.

EDIT: oh, you meant the other guy? 

Whoops!! I agree with everything you just said. And yes, I meant the other guy. :)</snippet></document><document><title>100,000 punch cards later, Flossie lives again</title><url>http://www.mirror.co.uk/news/technology-science/technology/flossie-oldest-working-british-computer-1388601</url><snippet>  &amp;gt;Despite its 25 square foot size, Flossie&#8217;s 16,000 transistors and 4,000 logic boards only result in a puny 2kb memory and 1mhz processing speed &#8211; all of which can now fit on a couple of modern-day 10mm silicon chips.

A single 5mm chip can do that today.

A little ATMega chip that you'd find on an Arduino hobby board is 16 times faster, and has similar memory. http://en.wikipedia.org/wiki/Arduino

Even smaller - the Texas Instruments eZ430-F2013 - a programmer card, and detachable "computer on a chip" bits...
http://www.conrad.de/medias/global/ce/1000_1999/1700/1700/1702/170285_ZB_00_FB.EPS_1000.jpg

It's 16MHz, runs at 1.8 volts! , and has 2KB + 256B Flash Memory, 256 bytes RAM, and has 5 I/O lines.

Yep, that's all on the end of a USB plug!     
I love using them! (PDF of tec specs: http://www.ti.com/lit/ds/slas491i/slas491i.pdf ) What do you use them for? &amp;gt;Despite its 25 square foot size, Flossie&#8217;s 16,000 transistors and 4,000 logic boards only result in a puny 2kb memory and 1mhz processing speed &#8211; all of which can now fit on a couple of modern-day 10mm silicon chips.

A single 5mm chip can do that today.

A little ATMega chip that you'd find on an Arduino hobby board is 16 times faster, and has similar memory. http://en.wikipedia.org/wiki/Arduino

Even smaller - the Texas Instruments eZ430-F2013 - a programmer card, and detachable "computer on a chip" bits...
http://www.conrad.de/medias/global/ce/1000_1999/1700/1700/1702/170285_ZB_00_FB.EPS_1000.jpg

It's 16MHz, runs at 1.8 volts! , and has 2KB + 256B Flash Memory, 256 bytes RAM, and has 5 I/O lines.

Yep, that's all on the end of a USB plug!     
I love using them! (PDF of tec specs: http://www.ti.com/lit/ds/slas491i/slas491i.pdf ) Is it just me, or is the USB connector really bulky by comparison?   How many bytes do you fit on a punch card? Filling 2kB of memory should not take that much. Was the punch cards loaded depending on which routine was being executed? (ie. master programme executed by mainframe operators loading routine A, then routine B, etc)   It would seem the major impediment to keeping a machine like this in working order is that the moving parts and consumable goods are just not available.  I would guess you do a lot of restoring the manufacturing base existing at the time just to demonstrate the thing can run.

Whether that is truly an issue or not, this sure brought a smile to my face.  </snippet></document><document><title>A couple questions for people who work in the Computer Science field.</title><url>http://www.reddit.com/r/compsci/comments/17g1d4/a_couple_questions_for_people_who_work_in_the/</url><snippet>Hey guys, I hope this is the right place for this.

I'm currently a Sophomore in college and I have been assigned an interview essay that pertains to a particular major. I picked Computer Science and I have to interview someone who works in the field I picked. If someone who has a job in the Comp Sci field could take the time to answer a few of these questions, that would be awesome.

1. How many of the things you learned while getting your degree do you use in the job you have now?
2. Was finding a job after graduation difficult? Or is there always a demand for Computer Science students?
3. In the job you have, what do you do on a daily basis?
4. As someone who is majoring in Computer Science, any advice on things you can do while still in school to help out future job prospects?


Thanks to anyone who takes the time to answer these.  Comp Sci is a broad field. You might want to focus on a discipline... That being said, I'm a web developer (Software Engineer) with a CS degree.

1. From my classes, I learned Java (I'm a Java developer). Everything else I use, day-to-day, I learned while in school (but on my own time). The most useful thing I learned at school was how to learn (as opposed to literal material). Knowing how to learn CS topics is something I employ everyday.

2. Finding a job isn't hard if you're willing to start at the bottom. I actually took a job unrelated to my degree. In that job, I offered to "automate" some of the grunt work (using Java). I used that experience as a backbone for applying to my current position.

3. Bug fixes and implement new source.

4. Find a focused field that interests you and research it on your own time. Most universities will give you a taste of a subject. But the real learning comes from your passion and desire to know more about a topic. I love(d) web development. So I bought books and read websites. I made websites and built a portfolio/experience (which is something that employers will ask about). A discipline.. interesting, I'll keep that in mind when I'm writing this. This is exactly the information I was looking for.. thanks! Even expanding on the discipline sides of things, people who use the same language and have the same titles will often do profoundly different things. For example, I am also a software engineer, but I work on real-time systems (C++); to a finer grain, I work on a lower-level component that feeds into customer facing applications that are often written in Java. If customer apps require less meaty processing, then they may be written in an interpreted language like perl or javascript. 

Also, I should probably mention that I don't do Windows development. I have heard that it requires a different bread of engineer, but really couldn't say either way.
 A discipline.. interesting, I'll keep that in mind when I'm writing this. This is exactly the information I was looking for.. thanks! Comp Sci is a broad field. You might want to focus on a discipline... That being said, I'm a web developer (Software Engineer) with a CS degree.

1. From my classes, I learned Java (I'm a Java developer). Everything else I use, day-to-day, I learned while in school (but on my own time). The most useful thing I learned at school was how to learn (as opposed to literal material). Knowing how to learn CS topics is something I employ everyday.

2. Finding a job isn't hard if you're willing to start at the bottom. I actually took a job unrelated to my degree. In that job, I offered to "automate" some of the grunt work (using Java). I used that experience as a backbone for applying to my current position.

3. Bug fixes and implement new source.

4. Find a focused field that interests you and research it on your own time. Most universities will give you a taste of a subject. But the real learning comes from your passion and desire to know more about a topic. I love(d) web development. So I bought books and read websites. I made websites and built a portfolio/experience (which is something that employers will ask about). Whats wrong with web development? You don't enjoy it anymore? Comp Sci is a broad field. You might want to focus on a discipline... That being said, I'm a web developer (Software Engineer) with a CS degree.

1. From my classes, I learned Java (I'm a Java developer). Everything else I use, day-to-day, I learned while in school (but on my own time). The most useful thing I learned at school was how to learn (as opposed to literal material). Knowing how to learn CS topics is something I employ everyday.

2. Finding a job isn't hard if you're willing to start at the bottom. I actually took a job unrelated to my degree. In that job, I offered to "automate" some of the grunt work (using Java). I used that experience as a backbone for applying to my current position.

3. Bug fixes and implement new source.

4. Find a focused field that interests you and research it on your own time. Most universities will give you a taste of a subject. But the real learning comes from your passion and desire to know more about a topic. I love(d) web development. So I bought books and read websites. I made websites and built a portfolio/experience (which is something that employers will ask about). &amp;gt; I actually took a job unrelated to my degree. In that job, I offered to "automate"  some of the grunt work (using Java). I used that experience as a backbone for applying to my current position.

I was just curious was kind of job unrelated to CS involves programming in java? Can you elaborate a little more?  1. Everything (Not counting stuff like Macro Economics, and other similar "soft" skills). Programming? VB.net? Check. PHP? check. SQL? check. AS400? Check.

2. Nope. I settled for a lower paying job, with good experience potential... and don't regret a moment.

3. Data Analyst (Many databases, reports, etc), Programmer (not much, but a couple small programs. Helping add functionality to others programs more than my own), network admin script writing.

4. Experience, Experience, Experience. Make stuff, do things, practice, learn. You want to stand out? Have a game written, a website up or something in the "Successful Project" category - especially that relates to your desired field.  I got a Computer Science 'Traditional' degree, that was heavy in programming, math and algorithm work.  I work as an application developer for a large company and use ABAP and SAP.

1. I use the programming skills I used every day.  It's a different programming language and a different environment, but the logic is the same.  Even things like database design and performance come into play.  It's hard to assign a percentage to 'things you learned in classes that you use every day', but it would be high if I could.

2. I accepted a job offer during the fall semester of my senior year.  I never had problems finding internships or jobs during school.  CS students are in demand, but you probably won't turn any heads walking into an interview and only referring to coursework when answering questions.  Technology is a great field to get into, so I would say there is a decent demand for CS grads.

3. I change/create programs to meet requests in specifications.  It could be something simple like adding something to an output screen, or adding huge functionality to a project.  My job revolves around data, so most of the time I make programs that handle/manipulate large amounts of data.

4.  Try different things to find out what you like.  I'm guessing that since you're majoring in CS, you enjoy playing around programming and technology in general.  Keep doing that.  Make a website from scratch that will display a different cat depending on where a user places their cursor.  Make a script that will tell the date on which the least recent file in a directory was edited.  Have fun with it and document it.  

"So madscientest, tell me about a time that you used programming to make a certain task easier."  

"Well, I started making some scripts as a sophomore to learn more about scripting, and realized that I could automate how I organized my schoolwork.  I saved all my files starting with 2-letter keywords Math:'MA', Programming:'Pg' etc when I was working on computers at school.  When I copied my whole profile folder to my personal computer, I could just run my script and it would put the files into the right folder based on the first 2 letters."

Dumb example, you get the idea.  I just graduated with my CS/EE degrees and had a job lined up 6 months before I even graduated.

1. The basics of programming are what I use the most, my company doesn't even use Java (my main language) but I get by just fine because I can pick it up easily.
2. At our job fairs there is literally more jobs than students. I also help recruit, for a main employer, and we literally are desperate for more programmers. I feel safe saying computer science (not even engineering in general) has the most job opportunities post school.
3. I'm a PM, so I architect code and user experiences. I do code but it's not the main focus of my job.
4. INTERNSHIPS! Holy crap I cannot stress that enough. I had 4 job offers to choose from during my senior year. 3 of which were from the companies I had interned at.

Good luck :) Where do you find internships? See if your school's department of computer science knows anything. If not, it doesn't hurt to ask companies directly if they offer such. The worst they can do is say no which is what they are already saying by not asking. Where do you find internships?  Speaking as someone who has hired lots of programmers (and I am a programmer myself):

When hiring someone, I don't care much about their degree, nor do I care about what computer languages they learned in classes. Some of the best programmers I've ever hired were physics majors!

The most important thing to learn in college is how to learn. Programming languages change every few years.  If you understand programming at a deep level, it is easy to pick up the next popular programming language quickly.

Learn about design.  Not just program design, but also graphic design and interaction design. College is a good place to learn that, and it is difficult to learn on the job.

Learn how to communicate, both written and spoken. You will have to do a lot of that. Publish articles, even online. Give talks at conferences. Learn how to network.

Do lots of programming, on projects you find interesting. Do some internships. Extra points if you contribute to open source software. Good programmers love to program. Programming big projects is different and much harder than small assignments (like the ones they assign in classes).

If you are going to do anything related to graphics, learn math (especially trigonometry). User interfaces often involve graphics. Learning hard sciences (like math and physics) also helps you learn how to learn.

Don't worry too much about preparing for a job.  If you love programming, you will always have a job.  1.  Nothing in articular, but everything.  The degree helped me develop a lot of "common sense" for programming.  I make mistakes, I mess things up, but I'm immeasurably better than I would otherwise be.  I was introduced to things I would never have willingly studied, like higher math, computer engineering/organization, low-level OS stuff, assembly, that ended up being tremendously useful.  Having fleshy squishy teachers was awesome, even if they weren't all very good teachers.

2.  Easy, but I had a good relationship with a small specialized company and they answered when I went knocking.  I was very fortunate.  (This is basically an unhelpful answer.)

3.  Code.  A little bit of support ticket handling.

4.  Have good projects.  Good-sized projects.  

I figure a lot of people in this subreddit will disagree (and I'm probably wrong), but I would recommend writing a large general-purpose library/framework.  CrapThatYouNeed.dylib or whatever.  And make it big and bloated and general, you know, shove networking code in there and filesystem code and hardcore logging utilities and any other crap you can think of.  

Why?  Because I think it mirrors what most applications end up being -- a lot of differently purposed code all smushed together -- but you don't have to worry about the actual app idea and how that will change and evolve over time.  A "general purpose library" is sufficiently well-defined that I think it can grow to an arbitrary size without suffering from the ailments that normally trouble applications -- second system effect, etc.  

Maybe.  I dunno.  That's what I'd advise, but keep in mind that I'm an idiot. &amp;gt; I was introduced to things I would never have willingly studied, like higher math, computer engineering/organization, low-level OS stuff, **assembly**, that ended up being tremendously useful.

Can you give examples for assembly? Just had my test in *computer architecture* today and it feels like "yeah good to know about it but will it ever be directly applied?" &amp;gt; I was introduced to things I would never have willingly studied, like higher math, computer engineering/organization, low-level OS stuff, **assembly**, that ended up being tremendously useful.

Can you give examples for assembly? Just had my test in *computer architecture* today and it feels like "yeah good to know about it but will it ever be directly applied?"   Note: I have a computer engineering degree

1. C/C++, UML, algorithms, data structures, calculus

2. I did not have any trouble finding work

3. Designing, design reviews, requirements document reviews, status meetings, management meetings, and on occasion I get to program

4. Do lots of coding. Find time to do your own projects. Take an active roll in learning.  Join user groups.  Show your future employers that you have the right attitude and you are passionate about what you do. I HATE UML...maybe it was the professor that I had.  Is it really that necessary?  I found it to be so tedious and dull.  Maybe the project 'summarized' in UML wasn't of big enough scope to benefit from UML. I HATE UML...maybe it was the professor that I had.  Is it really that necessary?  I found it to be so tedious and dull.  Maybe the project 'summarized' in UML wasn't of big enough scope to benefit from UML. If you have a job in a field requiring complex data design, UML is all important. I had a job in financial software and our UML diagrams were INSANE (not because stupid design, but necessary features/requirements). That's what I was thinking.  I imagine the benefits are most noticeable in complex designs that include tons of people.   I HATE UML...maybe it was the professor that I had.  Is it really that necessary?  I found it to be so tedious and dull.  Maybe the project 'summarized' in UML wasn't of big enough scope to benefit from UML. Note: I have a computer engineering degree

1. C/C++, UML, algorithms, data structures, calculus

2. I did not have any trouble finding work

3. Designing, design reviews, requirements document reviews, status meetings, management meetings, and on occasion I get to program

4. Do lots of coding. Find time to do your own projects. Take an active roll in learning.  Join user groups.  Show your future employers that you have the right attitude and you are passionate about what you do.   1. Learned programming, databases, data structures. That's about it. Everything else will help me grad school, however.
2. It took me 9 months, but the job I got is fantastic with great pay. However, I had no internships and a lowish GPA.
3. Software development: Coding, design, unit testing, provide support to other teams that call our programs. Also troubleshoot and provide code fixes for problems that are wrong in the live environment.
4. Intern and achieve fluency in multiple languages. I've heard contributing to open source projects you can show to employers are wonderful to have. Do u mind me asking what your GPA was (mine isn't great but i didnt think jobs cared so much). But I guess after college thats all they have to go on?                        </snippet></document><document><title>Good CS TED talks?</title><url>http://www.reddit.com/r/compsci/comments/17bfg7/good_cs_ted_talks/</url><snippet>My university is doing a computer science week (whohooo!) and one of the things we are trying to do is pick a day to show a ton of videos to help teach people about different aspects of computer sciences. Does anyone have a link to, or remember a good TED talk about CS?  </snippet></document><document><title>The landscape of just-in-time compilers for numerical Python</title><url>http://www.phi-node.com/2013/01/a-journey-through-parakeet.html</url><snippet>  "Python is an extremely popular language for number crunching and data analysis."

Now that seems to be stretching it a little... numpy, scipy, matplotlib &amp;amp; co are nice, and I've seen some people use it, but I don't think it comes *anywhere* close in popularity to others like Matlab, C/C++ and fortran... "Python is an extremely popular language for number crunching and data analysis."

Now that seems to be stretching it a little... numpy, scipy, matplotlib &amp;amp; co are nice, and I've seen some people use it, but I don't think it comes *anywhere* close in popularity to others like Matlab, C/C++ and fortran... I think it strongly depends on what community you're coming from. Many of the machine learning folks I know are jumping ship from Matlab to Python (though some of the deep learning folks seem to like Lua more). Every physics lab I know (admittedly that's only three of them) uses Python to pull together simulation/experimental results and do analysis on them. And, of course, the use of C, C++, and Fortran doesn't really exclude Python. Out of curiosity, what's your background?   </snippet></document><document><title>Coeffects: typing context-dependent computations using comonads</title><url>http://www.cl.cam.ac.uk/~tp322/drafts/coeffects.html</url><snippet>  I'm currently working on an extension for the GHC compiler as part of my degree to allow the use of comonads in a similar syntax to how monads are currently used in Haskell (unsurprisingly called codo notation, corresponding with do notation), under the supervision of Dominic Orchard (one of the authors of this paper).

As such, I was interested to see this come up. I'm impressed at the name. I don't think I'd've been able to resist calling it "undo notation", "don't notation", or something similarly silly. I'm impressed at the name. I don't think I'd've been able to resist calling it "undo notation", "don't notation", or something similarly silly.  Can someone explain like I'm a C++ programmer what this means? Context-dependent things all have a similar structure which we can leverage to write and reason about programs that are generic to the kind of context we're dealing with in a more structured way. We call the stuff with the same structure that contexts have "comonads". I still have no idea what you're talking about. So by context dependent do you mean you pass all the stuff about the context that the structure uses as some sort of parameter to your functions?

If you have a function that changes a global variable, doesn't that still change the context of other functions?

What does "generic to the kind of context we're dealing with" mean, and how can something be that as well as context dependant?

I'm not sure if what I'm asking is possible, but I'd think it would be possible to explain this in terms of C/C++; I mean using a map function is a functional programming idea, but you can pretty much implement one in C/C++ using function pointers.  The idea is that computations (functions) execute in some *context*. The context may be global variables that they use, resources like databases or other information (functions that are only available on certain kinds of CPUs or platforms).

In a normal code, you just use the context - you access a variable, call a database, invoke a system call. However, if you run the program on a platform where something is not available, you get a runtime error.

The idea of *coeffects* is that we want to make this dependence explicit - so when calling a function, you will know what it requires (what it accesses) - it will be part of the type (just like the type of arguments is present in the signature).

Comonads are just a mathematical structure that models how context propagates through a program (how variables and resources can be passed around) - when you make two function calls, the context is passed to both of them, when you define a nested function, it can access the outer context (etc.) Using comonads, we can figure out what are the rules for the propagation of the contextual information, which tells us how the correct types (or context annotations that are automatically inferred for your code) should look. I still have no idea what you're talking about. So by context dependent do you mean you pass all the stuff about the context that the structure uses as some sort of parameter to your functions?

If you have a function that changes a global variable, doesn't that still change the context of other functions?

What does "generic to the kind of context we're dealing with" mean, and how can something be that as well as context dependant?

I'm not sure if what I'm asking is possible, but I'd think it would be possible to explain this in terms of C/C++; I mean using a map function is a functional programming idea, but you can pretty much implement one in C/C++ using function pointers.  Can someone explain like I'm a C++ programmer what this means?</snippet></document><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document /><document><title>An efficient time-decaying approximate membership filter</title><url>http://eng.42go.com/a-simple-time-decaying-approximate-membership-filter/</url><snippet /></document><document><title>How do Finite &amp;amp; Pushdown Automatons work?</title><url>http://www.reddit.com/r/compsci/comments/18vahw/how_do_finite_pushdown_automatons_work/</url><snippet>Hello readers of /r/compsci ,

I'm doing my first year at university now and I have hit a snag while studying my Principles of Computing module (I wish they could just call it Computer Science theory..)

Unfortunately I have found no resources through the university or my own searches on line on Finite and Pushdown Automatons

I am having trouble with understand how these work, in particular the Pushdown Automatons are something I do not understand fully, I know they are to do with pushing things onto the stack, but I am having trouble with understand the command 'pop'

Of course if someone else has found some sort of guide or resource via the internet then I would gladly appreciate it if you could show it to me!

   Have you tried wikipedia?

I guess `pop` on PDAs is somewhat non-intuitive in that it takes an argument - where `push X` puts `X` on the top of the stack, `pop X` takes an `X` off the top of the stack *iff it is present*, and otherwise fails.  If the stack is empty or has a non-X at the top, `pop X` fails.  </snippet></document><document><title>So I want to do research...</title><url>http://www.reddit.com/r/compsci/comments/1965gy/so_i_want_to_do_research/</url><snippet>Several years ago I completed my BS in CS from a US university and have been working in the software industry ever since. I've learned a lot in the past few years, but I'm not really passionate about the work I'm doing and I don't feel that my current role will allow me to develop the qualifications necessary to move into a research position. Programming has always been fun, but the topics I loved most in my undergrad studies were always from the more abstract side of CS--algorithms analysis, computation theory, etc. I've been thinking about getting into a master's program for the past couple years, actually, and I think it's time to actually do it.

All of the resources I've found for prospective master's students seems to be intended for people still in their undergrad. I don't have research experience from my undergrad (there was very little research done at all in the CS department at my school). I have not kept in touch with any of my old professors, and many of them have since retired from the faculty so I'm at a loss for who to ask for reference letters. 

So I guess my biggest question is: as someone with just a BS and some professional experience, is grad school still an option since I didn't focus on it during my undergrad? How can I fill in some of the gaps in my experience (such as lack of research work) to help my applications? Are there any resources available for people in my position?

My biggest interests are digital art, robotics and distributed computing. Can anyone recommend some good universities with related programs? I'm willing to relocate anywhere in the world (and love exploring new places and cultures) as long as the program is taught in English (or if it would be realistic to take language courses concurrently with the CS courses if some parts of the program are taught in another language).

Any information or advice is greatly appreciated!</snippet></document></searchresult>
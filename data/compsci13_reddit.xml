<searchresult><compsci /><document><title>How do I benchmark software properly?</title><url>http://www.reddit.com/r/compsci/comments/v6nzn/how_do_i_benchmark_software_properly/</url><snippet>I'm not sure how to conduct a benchmark of software or algorithms that are implemented in an application properly. Is there a particular methodology I can follow or does anyone know of a paper that's a great example of benchmarking or that can explain how to do it properly?  It depends on your particular algorithm.  Speed (often in operations per second) is the most general benchmark, but depending on the type of algorithm you're implementing, you might also want to test output correctness and precision (for algorithms that may not always provide the same output given a certain input, like Monte Carlo algorithms).

Always do everything on the same hardware, but that goes without saying.  Provide your testing code for the world to see too.  I'm always iffy whenever I see people compare different algorithms and not provide the actual code to back it up.

**EDIT**:

teawreckshero and ReshenKusaga are correct, you should always do Big-O and other theoretical analysis of your algorithm first.  Benchmarks for speed are mostly for software development in different environments, such as different browsers. Cool thanks for the answer! How useful is it to replicate other people's benchmarks (even if they're flawed)? Cool thanks for the answer! How useful is it to replicate other people's benchmarks (even if they're flawed)? Unit testing is for making sure it's correct. Big O analysis is for measuring efficiency and scalability.

clarle mentions operations per second....I'm not sure how this is relevant as it will vary on different hardware. Ops/sec would be useful for actually testing the hardware (like an actual cpu or something), but it's arbitrary when it comes to running code. No one is going to want your results presented in the form, "my algorithm had ___ operations in one second on ___ hardware". Doesn't do them any good when translating to other hardware. Big O, however, gives a much better idea of the limits of your algorithm. Unit testing is for making sure it's correct. Big O analysis is for measuring efficiency and scalability.

clarle mentions operations per second....I'm not sure how this is relevant as it will vary on different hardware. Ops/sec would be useful for actually testing the hardware (like an actual cpu or something), but it's arbitrary when it comes to running code. No one is going to want your results presented in the form, "my algorithm had ___ operations in one second on ___ hardware". Doesn't do them any good when translating to other hardware. Big O, however, gives a much better idea of the limits of your algorithm. I don't know, maybe some of those specific results would be useful for at least giving an estimate?  Also, actual tested data is likely taking place in an OS that has full control over thread switching which cannot be predicted or accounted for. If you're trying to time something, odds are you will also be including other threads that have nothing to do with your program as well. There are too many variables.

Example:

startTimer() -&amp;gt;
beginAlgorithm() -&amp;gt; Context Switches interleaved into process total 2 sec -&amp;gt; algorithm Returns with 100 completed in 10 sec "operations"

Result: 100 ops/10 sec. Actual time is 100 ops/8 sec, but you have no way of measuring that 2 sec and it will never be consistent. Also, actual tested data is likely taking place in an OS that has full control over thread switching which cannot be predicted or accounted for. If you're trying to time something, odds are you will also be including other threads that have nothing to do with your program as well. There are too many variables.

Example:

startTimer() -&amp;gt;
beginAlgorithm() -&amp;gt; Context Switches interleaved into process total 2 sec -&amp;gt; algorithm Returns with 100 completed in 10 sec "operations"

Result: 100 ops/10 sec. Actual time is 100 ops/8 sec, but you have no way of measuring that 2 sec and it will never be consistent. I don't quite agree. Firstly it's possible to set the priority/affinity of a process to avoid context switches. Secondly, your testing environment should always be fresh and controlled, such that it's possible to replicate. You want to prove that your analysis is correct and applies to the real world, otherwise they're purely theoretical and never used outside academia. Even for purely theoretical datastructures, such as van Emde Boas trees, you want to do a test on real hardware. As long as it's a pre-emptive OS you will not be getting accurate results. For instance, redrawing the screen and getting input from the keyboard/mouse always take priority no matter how high you set the affinity. And these are just the most obvious threads.

To you second point, you are right. I kind of steered to the side of the main question which was in reference to benchmarking software, not just the algorithms behind it. So yes, obviously you must show how it performs empirically, not just theoretically. But keeping my original point in mind, this would always test the implementation more than the algorithms.   CS:APP had a good section on how to properly benchmark, though I don't remember the details of it. CS:APP is what?    </snippet></document><document><title>I have a question regarding the remanufacture of old processors, if anyone knows the answer.</title><url>http://www.reddit.com/r/compsci/comments/v6b42/i_have_a_question_regarding_the_remanufacture_of/</url><snippet>I doubt this is the right place to ask, but I have always wanted to know just how hard it would be for a company (assuming they had access to the designs) to just manufacture classic videogame consoles, like the N64 or Gameboy, so instead of emulating the games, they actually run it on real hardware, exactly the same as the originals).

Is this even possible, or are these old processors lost to history, becoming forever unfeasible to produce again? Or would it actually not be too much of a problem, like if a firm in China knew they could sell at least 10,000 units, would it not be too much trouble to recreate old gaming hardware that runs *exactly* the same as we all experienced it in our childhoods?

Any interesting insight anyone has to anything relating to these questions, I would love to hear it.   The closest you'll get is by using an FPGA.

The factories that fab'd those chips *don't exist anymore*. Also, specific to the N64, fucken RAMBUS. That shit should never be brought back.

&amp;gt; like if a firm in China knew they could sell at least 10,000 units

Except chips are fab'd by the tens of million. It's the only way they can be as cheap as they are. The actual making of the chip is relatively cheap. The preparing of the production run for a chip is the truly expensive part, and the entire production line has to be devoted to that single chip.

I'm going to take a wild guess here, but for the fab plants to turn a profit, they have to be outputting chips for at least 95% of their lifetime. If they retool them more than a couple of times, the entire plant will generate a loss, and everyone involved goes bankrupt.

While that last paragraph is a guess, I think it's not too far from the mark, as the margins on computer hardware are sooooo tiny. I can tell you with absolute certainty that the entire lifetime of the plant is budgeted for, from before they lay a single foundation right to the demolition of the plant and the safe disposal of the remains. To the special training the local fire departments need, to the cost of council approval, to the inflation of the wages of the employees.

If you think I'm bending the truth there, one of the "cleaning agents" used in chip fab is the almighty Chlorine Triflouride, a chemical so good at oxidising it puts oxygen to shame. This shit burns *sand* - vigorously. If it comes in contact with flesh, it doesn't just burn, it looks like a blowtorch is being applied to the flesh. The hazmat sheet for it makes nuclear weapons look like the kind of thing you give to children to play with. The nearest half dozen fire stations need special training and equipment if you even *think* about CF3. To lift a quote from [everyone's favourite drug chemist](http://pipeline.corante.com/archives/2008/02/26/sand_wont_save_you_this_time.php)...

&amp;gt; It is hypergolic with every known fuel, and so rapidly hypergolic that no ignition delay has ever been measured. It is also hypergolic with such things as cloth, wood, and test engineers, not to mention asbestos, sand, and water-with which it reacts explosively.

So yeah, I don't think there's a chip fab plant on Earth that would take time out of their busy (and dangerous) schedule to do anything that doesn't maximise their already slim profits. The closest you'll get is by using an FPGA.

The factories that fab'd those chips *don't exist anymore*. Also, specific to the N64, fucken RAMBUS. That shit should never be brought back.

&amp;gt; like if a firm in China knew they could sell at least 10,000 units

Except chips are fab'd by the tens of million. It's the only way they can be as cheap as they are. The actual making of the chip is relatively cheap. The preparing of the production run for a chip is the truly expensive part, and the entire production line has to be devoted to that single chip.

I'm going to take a wild guess here, but for the fab plants to turn a profit, they have to be outputting chips for at least 95% of their lifetime. If they retool them more than a couple of times, the entire plant will generate a loss, and everyone involved goes bankrupt.

While that last paragraph is a guess, I think it's not too far from the mark, as the margins on computer hardware are sooooo tiny. I can tell you with absolute certainty that the entire lifetime of the plant is budgeted for, from before they lay a single foundation right to the demolition of the plant and the safe disposal of the remains. To the special training the local fire departments need, to the cost of council approval, to the inflation of the wages of the employees.

If you think I'm bending the truth there, one of the "cleaning agents" used in chip fab is the almighty Chlorine Triflouride, a chemical so good at oxidising it puts oxygen to shame. This shit burns *sand* - vigorously. If it comes in contact with flesh, it doesn't just burn, it looks like a blowtorch is being applied to the flesh. The hazmat sheet for it makes nuclear weapons look like the kind of thing you give to children to play with. The nearest half dozen fire stations need special training and equipment if you even *think* about CF3. To lift a quote from [everyone's favourite drug chemist](http://pipeline.corante.com/archives/2008/02/26/sand_wont_save_you_this_time.php)...

&amp;gt; It is hypergolic with every known fuel, and so rapidly hypergolic that no ignition delay has ever been measured. It is also hypergolic with such things as cloth, wood, and test engineers, not to mention asbestos, sand, and water-with which it reacts explosively.

So yeah, I don't think there's a chip fab plant on Earth that would take time out of their busy (and dangerous) schedule to do anything that doesn't maximise their already slim profits. The closest you'll get is by using an FPGA.

The factories that fab'd those chips *don't exist anymore*. Also, specific to the N64, fucken RAMBUS. That shit should never be brought back.

&amp;gt; like if a firm in China knew they could sell at least 10,000 units

Except chips are fab'd by the tens of million. It's the only way they can be as cheap as they are. The actual making of the chip is relatively cheap. The preparing of the production run for a chip is the truly expensive part, and the entire production line has to be devoted to that single chip.

I'm going to take a wild guess here, but for the fab plants to turn a profit, they have to be outputting chips for at least 95% of their lifetime. If they retool them more than a couple of times, the entire plant will generate a loss, and everyone involved goes bankrupt.

While that last paragraph is a guess, I think it's not too far from the mark, as the margins on computer hardware are sooooo tiny. I can tell you with absolute certainty that the entire lifetime of the plant is budgeted for, from before they lay a single foundation right to the demolition of the plant and the safe disposal of the remains. To the special training the local fire departments need, to the cost of council approval, to the inflation of the wages of the employees.

If you think I'm bending the truth there, one of the "cleaning agents" used in chip fab is the almighty Chlorine Triflouride, a chemical so good at oxidising it puts oxygen to shame. This shit burns *sand* - vigorously. If it comes in contact with flesh, it doesn't just burn, it looks like a blowtorch is being applied to the flesh. The hazmat sheet for it makes nuclear weapons look like the kind of thing you give to children to play with. The nearest half dozen fire stations need special training and equipment if you even *think* about CF3. To lift a quote from [everyone's favourite drug chemist](http://pipeline.corante.com/archives/2008/02/26/sand_wont_save_you_this_time.php)...

&amp;gt; It is hypergolic with every known fuel, and so rapidly hypergolic that no ignition delay has ever been measured. It is also hypergolic with such things as cloth, wood, and test engineers, not to mention asbestos, sand, and water-with which it reacts explosively.

So yeah, I don't think there's a chip fab plant on Earth that would take time out of their busy (and dangerous) schedule to do anything that doesn't maximise their already slim profits. Thanks for this info. This is the kind of insight that I lack that I was referring to :3

The plants are only set up with the presumption that they will need to keep on making millions of the exact same chip (say, over four years?). If sales aren't the required number of millions by year two, then the entire operation will be a loss. And with no one paying licenses to use the hardware per sales unit, you can't take a loss of millions of dollars.

Would it be one day conceivable to do a run of chips that contain like individual thousands of individual cores in nano scale of every processor design from 1971-2003 for example? I can imagine that one company wouldn't mind being the sole distributor of such a chip.

I really don't know anything about it though. It must be frustrating for computer science people hear moronic musings from laypeople. Thanks for this info. This is the kind of insight that I lack that I was referring to :3

The plants are only set up with the presumption that they will need to keep on making millions of the exact same chip (say, over four years?). If sales aren't the required number of millions by year two, then the entire operation will be a loss. And with no one paying licenses to use the hardware per sales unit, you can't take a loss of millions of dollars.

Would it be one day conceivable to do a run of chips that contain like individual thousands of individual cores in nano scale of every processor design from 1971-2003 for example? I can imagine that one company wouldn't mind being the sole distributor of such a chip.

I really don't know anything about it though. It must be frustrating for computer science people hear moronic musings from laypeople. [deleted] Woah, that's much better idea than my "make them all small and put them all in one chip" idea. Could such a FPGA fully "emulate" exactly what the SNES chips did, or old corporate super computers? Doh, ninja edit fail :-)

Not so much emulate, as *actually become* that chip. To my limited knowledge, the only limits are the amount of information you can get about the original chips, and the gate count of the FPGA you're using.

FPGA's are a bit past my knowledge of computers. All I really know is that...

* They're magic.
* They're "programmed" with evil programming languages with names like "Verilog".
* FPGA's are commonly used to make custom, single shot chips that are extremely specialised, and often perform their specialised calculations faster than normal, generic chips.
* Just because it's one FPGA, does not mean you have to use it to simulate *one* chip. It's quite possible to have "multi-core" chips created inside a single FPGA. The only limit is the gate count. Thanks for this info. This is the kind of insight that I lack that I was referring to :3

The plants are only set up with the presumption that they will need to keep on making millions of the exact same chip (say, over four years?). If sales aren't the required number of millions by year two, then the entire operation will be a loss. And with no one paying licenses to use the hardware per sales unit, you can't take a loss of millions of dollars.

Would it be one day conceivable to do a run of chips that contain like individual thousands of individual cores in nano scale of every processor design from 1971-2003 for example? I can imagine that one company wouldn't mind being the sole distributor of such a chip.

I really don't know anything about it though. It must be frustrating for computer science people hear moronic musings from laypeople.  It would be 9999909238x cheaper to just emulate the console on a small modern computer, put it in a console case and just map all the IO connections to USB.  This is so cheap/easy that individuals actually do this in the spare time.  Projects like rasberry pi make this cheaper than ever (25$ for a computer that can easily emulate a ~~PS2~~ N64, with higher quality video).

Why build a plant to make old hardware? Not that you are entirely wrong, but emulating a PS2 is VERY hard. This piece of hardware is an incredibly complex and unusual design. To emulate it on a PC, you need a top of the line, highly overclocked machine. In comparison, Wii emulators run on an average gaming PC.

A Raspberry Pi is barely fast enough for N64 emulators. Fair enough, thanks for the correction.  I just remember running a PS1 emulator on a P4 several years ago, I guess I over corrected. Not that you are entirely wrong, but emulating a PS2 is VERY hard. This piece of hardware is an incredibly complex and unusual design. To emulate it on a PC, you need a top of the line, highly overclocked machine. In comparison, Wii emulators run on an average gaming PC.

A Raspberry Pi is barely fast enough for N64 emulators. You certainly don't need a highly overclocked machine to emulate PS2 games anymore. It would be 9999909238x cheaper to just emulate the console on a small modern computer, put it in a console case and just map all the IO connections to USB.  This is so cheap/easy that individuals actually do this in the spare time.  Projects like rasberry pi make this cheaper than ever (25$ for a computer that can easily emulate a ~~PS2~~ N64, with higher quality video).

Why build a plant to make old hardware? Because emulation isn't the same and chip level emulation requires incredible power (as well as not ever being possible for the chips in the N64, according to the maker of the bsnes emulator).

I kind of suspected that it would be infeasible to ever make old processors again, exactly to spec anyway. It just seems sad that when these old devices die out, that it's possible that no one will ever be able to use them again exactly the same way that millions of people used them when they first came out (or exactly the same way that the directors of the software being ran on them expected they would run). It's a good thing the N64 is built like a tank. I have one that's been running well for the past 15 years now. It takes abuse like a champ. If someone keeps good care of the system, I don't see any reason it couldn't last through 50+ years of moderate use.   the n64 is a classic console?? suddenly i am an antique owner! seriously though, they are easy to pick up for next to nothing, which might give you some idea of the profitability of a clone. It's 16 years old. 16. the n64 is a classic console?? suddenly i am an antique owner! seriously though, they are easy to pick up for next to nothing, which might give you some idea of the profitability of a clone. I guess a flash card for old N64s would be more profitable than a clone at the moment.  NES on a chip? I wonder what sort of space would be requited to do a SoC of the NES at with a 22nm process? Maybe a few mm^2? http://en.wikipedia.org/wiki/File:Famicom_clone_PCB.jpg

No idea what the process was, but you get the idea.      The target is not retro silicon. What you do is you target the latest ARM quad cores with co-processor arrays. 

The licensing costs for these things are tiny. The trick is, it's all SMT. But that doesn't mean you can't do it. 

You start off with a solder pot see. And then you make like a hundred and try and get someone to copy you. 

Check out Adapteva and Tilera. Also look for many-core as opposed to multi-core. Usually emmulator systems would want to use both a multi-core such as a quad-core at high clockspeed. But the real emmulation work should be totally offsourced to a big array of 2Hz ARM cores like 64 or so. 

Another issue is the memory. They often call the memory on these things network-on-a-chip or mesh but the gist of it is they are more like a network available for offloading a lot of work. 

As for a languages, you're wanting to learn as much as you can about CUDA. That's where it's at and that's why Torvalds is yelling about NVidia. They are like a cancer. It's a huge crime they've committed all these years. They sell that crap to schools and they don't allow you to use an open driver. What kind of shit is that.  I started thinking your post was bullshit. After some more time I've realized it isn't, just slightly incoherent. An upvote for you.

Some explanations: SMT = surface mounted technology (your microchips sit on top rather than have pins going through your PCB or circuit board). Solder pot = dip your things into molten solder and then stick them together. Mentioned as using typical hand soldering on SMT is either impossible or a black art and as a way to build a prototype for a possible emulation system. 

2 HZ? A clock speed of twice a second will do fuck all. Maybe if you meant 2 Mhz, but even then, what you have is nothing like the original hardware, and if you are going to emulate it just grab a modern x64/x86 processor. Ok Tilera and Adapteva are super cool after looking it up; but it seems that the main selling point for using a shit-ton of parallel chips is power consumption... which matters for servers and data centres but not really for emulators. Also they use frequencies closer to 500 Mhz.

And CUDA? Fuck that, OpenCL ftw, it's a standard and better. My impression is that it's going to win (but I work at AMD so that just might be the propaganda they feed me talking). They are both API's for doing non graphics calculations on GPUs (or potentially even FPGAs) The target is not retro silicon. What you do is you target the latest ARM quad cores with co-processor arrays. 

The licensing costs for these things are tiny. The trick is, it's all SMT. But that doesn't mean you can't do it. 

You start off with a solder pot see. And then you make like a hundred and try and get someone to copy you. 

Check out Adapteva and Tilera. Also look for many-core as opposed to multi-core. Usually emmulator systems would want to use both a multi-core such as a quad-core at high clockspeed. But the real emmulation work should be totally offsourced to a big array of 2Hz ARM cores like 64 or so. 

Another issue is the memory. They often call the memory on these things network-on-a-chip or mesh but the gist of it is they are more like a network available for offloading a lot of work. 

As for a languages, you're wanting to learn as much as you can about CUDA. That's where it's at and that's why Torvalds is yelling about NVidia. They are like a cancer. It's a huge crime they've committed all these years. They sell that crap to schools and they don't allow you to use an open driver. What kind of shit is that.  I wish I understood any of this.</snippet></document><document><title>What can I be doing during summer to learn the more "theoretical" side of computer science?</title><url>http://www.reddit.com/r/compsci/comments/v5no4/what_can_i_be_doing_during_summer_to_learn_the/</url><snippet>I am a recent high school graduate, who has been programming in C++ since I was 11. I know x86 Assembler, C/C++ and Java. I won't be learning anything new in CS for the first 2-3 semesters in college. The introduction class, algorithms and data structure classes are prerequisites for the rest of the CS curriculum. I have already taken Data Structures and Algorithms this past year, which was the practical, implementation approach. I am currently watching the algorithms class on MIT OCW to get a grasp of the more theoretical aspects. And I can say that I am understanding it; it's just a bit dry. This summer, I will be watching the multivariable calculus, intro to algorithms and Physics III lectures through MIT OCW. I have had experience in robotics through FIRST; I personally wrote an implemented my interpretations of the PID controller and Kalman Filter, but that's about it. I have followed the first week or two of the Udacity Robot Car class, but never really followed through. What fun thing can I do during summer? Euler's Project is on the list, writing an Android app is on the list. Perhaps, I can really dive into parallel processing through the use of my PS3. But those are all "practical" things. 

What can I be doing to learn the more theoretical aspects of CS? To be honest, I really don't know much of what happens on the theoretical side of CS.  Regardless of what you study, don't forget to go outside and have some fun. Regardless of what you study, don't forget to go outside and have some fun.  Write a regular expression parser. Start with strictly regular languages, then expand on it. This is a great way to learn the hierarchy of formal languages, as well as understand the degree to which "regular expressions" in modern languages go far beyond that.

When you're happy with your parser, optimize it. In the process, you can learn a lot about computer architecture, the value of a good testsuite when writing a complex piece of software, and some of the traps and pitfalls of optimizing code for performance.  Well first of all automata and language theory is the basis of everything... a bit of lambda calculus and combinators can't hurt. After you understand the models of computation, check out computability and complexity. That is the basis any CS graduate should understand.

Semantics and verification is another big field, especially concerning static analysis and parallel programming, so you can look a bit into that. Operational/Denotational/Axiomatic semantics, Hoare logic, temporal and modal logics, CCS, bisimulation...

Really, there's so much stuff. What would interest you? Either way, AUTOMATA theory is a total and complete basis from which EVERYTHING is built, so definitely check that out.

If you don't know much math, watch some set theory, discrete math fundamentals, relations, functions, graphs, groups, little bit of universal algebra can't hurt, rewriting...

Oh and also, **learn HASKELL!!!** He has 1 summer, not 200 summers. So? One uni course is usually 12*2 hours, that's 24 hours of material. You can learn that in a week. I don't go to lectures all year, learn the shit, ace the test. It's not hard if you work a little bit :/ If your uni only asks you to work 2 hours a week per course, chances are your degree isn't worth the paper its printed on. So? One uni course is usually 12*2 hours, that's 24 hours of material. You can learn that in a week. I don't go to lectures all year, learn the shit, ace the test. It's not hard if you work a little bit :/ Learning takes time. Cramming will result in you forgetting a lot of material and not having the deep understanding that will go with you for the rest of your life. Well first of all automata and language theory is the basis of everything... a bit of lambda calculus and combinators can't hurt. After you understand the models of computation, check out computability and complexity. That is the basis any CS graduate should understand.

Semantics and verification is another big field, especially concerning static analysis and parallel programming, so you can look a bit into that. Operational/Denotational/Axiomatic semantics, Hoare logic, temporal and modal logics, CCS, bisimulation...

Really, there's so much stuff. What would interest you? Either way, AUTOMATA theory is a total and complete basis from which EVERYTHING is built, so definitely check that out.

If you don't know much math, watch some set theory, discrete math fundamentals, relations, functions, graphs, groups, little bit of universal algebra can't hurt, rewriting...

Oh and also, **learn HASKELL!!!** I agree that language theory underpins all of computation. And it leads nicely into complexity, which is one of my favorite topics. I recommend reading Michael Sipser's *Introduction to the Theory of Computation*. Although I realize that it's very hard to motivate yourself to read a textbook on your own. I agree that language theory underpins all of computation. And it leads nicely into complexity, which is one of my favorite topics. I recommend reading Michael Sipser's *Introduction to the Theory of Computation*. Although I realize that it's very hard to motivate yourself to read a textbook on your own. Well first of all automata and language theory is the basis of everything... a bit of lambda calculus and combinators can't hurt. After you understand the models of computation, check out computability and complexity. That is the basis any CS graduate should understand.

Semantics and verification is another big field, especially concerning static analysis and parallel programming, so you can look a bit into that. Operational/Denotational/Axiomatic semantics, Hoare logic, temporal and modal logics, CCS, bisimulation...

Really, there's so much stuff. What would interest you? Either way, AUTOMATA theory is a total and complete basis from which EVERYTHING is built, so definitely check that out.

If you don't know much math, watch some set theory, discrete math fundamentals, relations, functions, graphs, groups, little bit of universal algebra can't hurt, rewriting...

Oh and also, **learn HASKELL!!!** &amp;gt; Hoare logic

Shut your Hoare mouth!  Well first of all automata and language theory is the basis of everything... a bit of lambda calculus and combinators can't hurt. After you understand the models of computation, check out computability and complexity. That is the basis any CS graduate should understand.

Semantics and verification is another big field, especially concerning static analysis and parallel programming, so you can look a bit into that. Operational/Denotational/Axiomatic semantics, Hoare logic, temporal and modal logics, CCS, bisimulation...

Really, there's so much stuff. What would interest you? Either way, AUTOMATA theory is a total and complete basis from which EVERYTHING is built, so definitely check that out.

If you don't know much math, watch some set theory, discrete math fundamentals, relations, functions, graphs, groups, little bit of universal algebra can't hurt, rewriting...

Oh and also, **learn HASKELL!!!** Well first of all automata and language theory is the basis of everything... a bit of lambda calculus and combinators can't hurt. After you understand the models of computation, check out computability and complexity. That is the basis any CS graduate should understand.

Semantics and verification is another big field, especially concerning static analysis and parallel programming, so you can look a bit into that. Operational/Denotational/Axiomatic semantics, Hoare logic, temporal and modal logics, CCS, bisimulation...

Really, there's so much stuff. What would interest you? Either way, AUTOMATA theory is a total and complete basis from which EVERYTHING is built, so definitely check that out.

If you don't know much math, watch some set theory, discrete math fundamentals, relations, functions, graphs, groups, little bit of universal algebra can't hurt, rewriting...

Oh and also, **learn HASKELL!!!** Well first of all automata and language theory is the basis of everything... a bit of lambda calculus and combinators can't hurt. After you understand the models of computation, check out computability and complexity. That is the basis any CS graduate should understand.

Semantics and verification is another big field, especially concerning static analysis and parallel programming, so you can look a bit into that. Operational/Denotational/Axiomatic semantics, Hoare logic, temporal and modal logics, CCS, bisimulation...

Really, there's so much stuff. What would interest you? Either way, AUTOMATA theory is a total and complete basis from which EVERYTHING is built, so definitely check that out.

If you don't know much math, watch some set theory, discrete math fundamentals, relations, functions, graphs, groups, little bit of universal algebra can't hurt, rewriting...

Oh and also, **learn HASKELL!!!** Do you know what the corresponding classes are on itunes U?     Do this online course: http://www.cis.upenn.edu/~bcpierce/sf/

  You will learn all the theory you need in school. Most of the stuff you mentioned, you know, doing online courses and the like, is pretty impractical.

Seriously, if you want to be valuable to employers when you start applying for internships, you need to show practical experience.

What you need to do is to code an app, web or mobile it doesn't matter. Get a bunch of APIs together and create a cool mashup. 

You need to get involved with the community as well. Go to www.meetup.com and look for hackathons or tech events around your area and attend.

You will learn all the theory you need at school, and not enough practical stuff.   after all your theory studying make sure to read up on how to write good code. knowing about lambda calculus and finite automata is fine and dandy but if you write shit code you aren't any better off   Solve/implement an algorithmic problem that you think is interesting e.g. path finding, parallel sorting, etc. Try and make a solution that someone else hasn't come up with yet. My first project on the PS3 will be porting my multithreaded mergesort I wrote in java last year. I will have to be mindful of the fact that the PS3 has SIMD Processors; I'll have to optimize to take advantage of that. Cool, I wrote a parallel merge-sort recently - it was lock free. Was your implementation lock free? Yes, since each thread never worked on the same piece of data. Unless if I am not mistaken, it is lock free. What about merge? How would you check for locks? I haven't really thought about it until you brought it up. I just assumed that the RAM can be written by more than one instruction concurrently. You can look at it; it's not the most efficient since it's not an inplace algorithm. http://pastebin.com/QbBtm9jM Thanks - interesting implementation - it isn't completely parallel since the last merge will only run on one CPU. You are correct regarding locks, and in this sense you divide the work across n threads which isn't very efficient. At the very least, check if the amount of data to be sorted is &amp;gt; K (for K ~= 8, 16, 32) and only use threads if the amount of data is bigger. </snippet></document><document><title>General questions about computer science education and careers.</title><url>http://www.reddit.com/r/compsci/comments/v52ua/general_questions_about_computer_science/</url><snippet>I am an American student with 3-ish years of college under my belt towards a biology major, and I'd like to switch to something in computers.  I love learning about and working with computers, but I don't know much about the education and career options. I've done lots of research through searching the web and this subreddit, but I'm still left with a few questions:

* **2-year associate's or 4-year bachelor's degree?** The median salary for 2-year educations actually looks pretty good, but I'd really like to have something more specialized.  How much will a 4-year degree expand my career options?

* **Can I get an associate's degree, then build a bachelor's off of that?**

* **Does the school I go to matter? Any recommendations for good schools?**

* **Does the location matter?** I live in Minnesota, but I am in a position right now where I can easily pick up and move to another state.  Is location a big factor in computer science careers?  If so, which locations are best?

**EDIT: * **Are online classes a good option?**  For obvious reasons, it seems like a computer science degree would be doable online.  Is this possible/a good idea? Are there any reputable online programs?  Silicon Valley (by San Francisco) is the most famous for computer science. It houses the world headquarters of Google, Apple, and Facebook, *a large Microsoft complex*, and many more large and small companies.

Someone from my high school got his CS degree in Nebraska and immediately moved to SF because he knew he could find a job there. If you have a good GPA and get involved in research and such, once you get to SF companies will actually mail you and ask you to come into interviews. At least, they did for him.
 Just to be pedantic, the "world headquarters" of Microsoft is in Redmond, WA. There are a number of other large companies in the Seattle area as well, so it would not be a bad place to consider living. Yeah, I know there are a lot here but I don't know how many I could rattle off.  Amazon, obviously, and Google has an office in Kirkland.  And somewhere around here there are a couple Nintendo offices, although I don't know if they do dev work here.  And lots of startups. And Valve is in Bellevue, and Pop Cap is somewhere nearby. I know I've seen IBM and Oracle offices around as well, although I don't know how much dev work they do. Also Nokia!  You might do better to ask this question in /r/cscareerquestions .

Everybody I know who has worked at big companies (Microsoft, Intel, Google, Facebook, IBM, AMD) have had at least a BS and many had more than that.  (Then again, most of the people I know with jobs in computer science are people I know from grad school...)  From a practical standpoint, usually the third and fourth years are where you've completed all of your basic classes and you really start to get taking the fun/interesting/specialized electives. This serves a couple purposes. You get to see if there's something in particular that you really love or really hate working on.  You get a basic background in it, which opens up job opportunities even if it's not specifically what you're interested in.  (Ex: I work in distributed systems, even though that's not what my dissertation was on or anything I put down on my candidate interest form; it gave me a boost in the resume pile because I was one of the few people who had some background in it.)  And finally, in some cases, even if you don't work in the area, classes in different areas may give you context to think about how what you're doing interacts with other things.

To expand on what widdly_scuds said, you will find a large number of tech companies in the SF Bay area, Seattle, Austin (I'm pretty sure.. I know Intel and IBM have offices there), and Boston.  And you'll find a lot of defense contractors in the Washington, DC area, if you're interested in that (and can get security clearance). You might do better to ask this question in /r/cscareerquestions .

Everybody I know who has worked at big companies (Microsoft, Intel, Google, Facebook, IBM, AMD) have had at least a BS and many had more than that.  (Then again, most of the people I know with jobs in computer science are people I know from grad school...)  From a practical standpoint, usually the third and fourth years are where you've completed all of your basic classes and you really start to get taking the fun/interesting/specialized electives. This serves a couple purposes. You get to see if there's something in particular that you really love or really hate working on.  You get a basic background in it, which opens up job opportunities even if it's not specifically what you're interested in.  (Ex: I work in distributed systems, even though that's not what my dissertation was on or anything I put down on my candidate interest form; it gave me a boost in the resume pile because I was one of the few people who had some background in it.)  And finally, in some cases, even if you don't work in the area, classes in different areas may give you context to think about how what you're doing interacts with other things.

To expand on what widdly_scuds said, you will find a large number of tech companies in the SF Bay area, Seattle, Austin (I'm pretty sure.. I know Intel and IBM have offices there), and Boston.  And you'll find a lot of defense contractors in the Washington, DC area, if you're interested in that (and can get security clearance). You might do better to ask this question in /r/cscareerquestions .

Everybody I know who has worked at big companies (Microsoft, Intel, Google, Facebook, IBM, AMD) have had at least a BS and many had more than that.  (Then again, most of the people I know with jobs in computer science are people I know from grad school...)  From a practical standpoint, usually the third and fourth years are where you've completed all of your basic classes and you really start to get taking the fun/interesting/specialized electives. This serves a couple purposes. You get to see if there's something in particular that you really love or really hate working on.  You get a basic background in it, which opens up job opportunities even if it's not specifically what you're interested in.  (Ex: I work in distributed systems, even though that's not what my dissertation was on or anything I put down on my candidate interest form; it gave me a boost in the resume pile because I was one of the few people who had some background in it.)  And finally, in some cases, even if you don't work in the area, classes in different areas may give you context to think about how what you're doing interacts with other things.

To expand on what widdly_scuds said, you will find a large number of tech companies in the SF Bay area, Seattle, Austin (I'm pretty sure.. I know Intel and IBM have offices there), and Boston.  And you'll find a lot of defense contractors in the Washington, DC area, if you're interested in that (and can get security clearance).   Having any degree helps.  

Knowing your way around a few common programming languages is good, knowing something about software design is also good.

Knowing about networks of all types and various bits of hardware is awesome.  

        You don't need a CS degree to work as a programmer. Actually, in many cases it wouldn't teach you how to code properly. Get yourself a software engineering course it would be much more beneficial to your career. And after it you would be able to make an educated guess do you really need a CS degree or not. It may not be mandatory if you're already super talented, but let's not kid ourselves here. Nearly all HR people will toss applications for developer jobs from people who have no CS degree. The few developers I've talked to who had no degree were either in school or planning to go back to school because their situation sucked. I'm saying that CS course and Software Engineering course are different things. If you are planning to do programming and not academic work you better go with SE.  
I don't care how good you at coloring graphs if you can't write clean, concise, well-documented, and testable code. Bonus points if you can do architecture, and talk with people. I'm saying that CS course and Software Engineering course are different things. If you are planning to do programming and not academic work you better go with SE.  
I don't care how good you at coloring graphs if you can't write clean, concise, well-documented, and testable code. Bonus points if you can do architecture, and talk with people. You don't need a CS degree to work as a programmer. Actually, in many cases it wouldn't teach you how to code properly. Get yourself a software engineering course it would be much more beneficial to your career. And after it you would be able to make an educated guess do you really need a CS degree or not. I found most of my SE courses at school to be complete and utter bullshit. Internships are the best way to learn proper SE methods IMO, every company does things differently and I think that exposing yourself to several different companies with varying opinions on design is the best way to gain experience and gain perspective.  &amp;gt; 2-year associate's or 4-year bachelor's degree? The median salary for 2-year educations actually looks pretty good, but I'd really like to have something more specialized. How much will a 4-year degree expand my career options?

If you are going to school to make more money you are doing it wrong.  


&amp;gt; Can I get an associate's degree, then build a bachelor's off of that?

In most cases yes, usually it's easier and shorter to go to a community college for the first year taking your general education requirements, calculus and science.  Then moving onto a senior University to complete your CS education.


&amp;gt; Does the school I go to matter? Any recommendations for good schools?

That depends on what you are going to use the degree for.  If you just want the paper so you can get a good job and make good money it likely doesn't matter, you will have a hard time no matter where you go.  

If you actually want a high quality education then yes the school you go to matters.  The material you learn will likely not be different but the requirements and expectations you meet at just at another level at top schools.  Many schools these days are cutting out the math in CS curriculum making them weaker and easier to attain for those less capable.  

Look for a school that you can afford that is the highest possible rank.  Take a look at their requirements. One way to spot a good school is if they have required classes like Theory of Computation or classes that require non imperative languages.  

&amp;gt; Does the location matter? I live in Minnesota, but I am in a position right now where I can easily pick up and move to another state. Is location a big factor in computer science careers? If so, which locations are best?

It doesn't matter.  You may have to move upon graduation. &amp;gt;If you are going to school to make more money you are doing it wrong.

This is kind of a bullshit response. Doing what you love is a noble goal, but living in poverty fucking sucks. Getting an education to try and enter a lucrative field is perfectly valid. CS is not a subject you need to go to school for to do well.  Plenty of self taught first times out there making 80K/yr with nothing more then a few fun projects and a thirst for wikipedia articles. CS is not a subject you need to go to school for to do well.  Plenty of self taught first times out there making 80K/yr with nothing more then a few fun projects and a thirst for wikipedia articles. The thing is that the only way you can learn something is to teach yourself; no matter how you do it, listening to university lectures or reading books and wikipedia articles.  
The point of going to university is structured courses, course projects and more importantly a professional networking. Sure, this will not make you a decent programmer, but I believe it make your odds better. Some companies actively don't hire CS grads over self taught due to just how horrible most CS grads are.  Something like 90+% can't even do a basic fizzbuzz.  


Edit:  OK it's the majority can't not 90
http://www.codinghorror.com/blog/2007/02/why-cant-programmers-program.html Some companies actively don't hire CS grads over self taught due to just how horrible most CS grads are.  Something like 90+% can't even do a basic fizzbuzz.  


Edit:  OK it's the majority can't not 90
http://www.codinghorror.com/blog/2007/02/why-cant-programmers-program.html &amp;gt;Something like 90+% can't even do a basic fizzbuzz.

I'd like to see a source for this. Maybe not 90% but certainly over 50% http://www.codinghorror.com/blog/2007/02/why-cant-programmers-program.html That article has no statistics about the percentage of CS graduates who cannot program a solution to a "FizzBuzz" type problem. It has some hyperbole about applicants that cannot do it, but those applicants would include all applicants, regardless of background, so that doesn't seem to back up your statement. It actually does it says it in bold

&amp;gt; The majority of comp sci graduates can't. I've also seen self-proclaimed senior programmers take more than 10-15 minutes to write a solution. CS is not a subject you need to go to school for to do well.  Plenty of self taught first times out there making 80K/yr with nothing more then a few fun projects and a thirst for wikipedia articles. I call bullshit. If you have an easy business idea or are a fucking rockstar self-taught programmer you might be able to do that, but there is no substitute for a solid CS education for 99% of developers. Maybe you can get some lucrative web-design jobs without a four-year degee but serious paid development is done almost exclusively by trained professionals. You have a serious disconnect with that is really going on at many companies mainly startups these days. Formal education has essentially been shown to be irrelevant.

It hurts me to say it because I do have a CS degree and worked hard for it but I am just being realistic. I know too many people who never went to college making 80K+/yr writing software to write this off as just a few outliers.  Some of them are even in director positions at major software consulting businesses and they make a lot more than 80K/yr.  
 Those must be the "rock star programmers" I mentioned. If you have a great track record and/or start your own business, you probably can get paid to develop with few credentials. Web development in particular is lucrative and needs little in the way of theory. But if you want to work for a software company, they typically won't even call you unless you have the credentials or tons of experience in lieu of credentials.</snippet></document><document><title>What are some good introductory books on Computer Science?</title><url>http://www.reddit.com/r/compsci/comments/v4z58/what_are_some_good_introductory_books_on_computer/</url><snippet>I'd like to understand the theoretical background, but I struggle with even the most basic concepts, like what's a Turing Machine, and how it relates to the computer I have beside my desk right now.

Recently, I stumbled over the [Chomsky Hierarchy](https://en.wikipedia.org/wiki/Chomsky_hierarchy), and I find it fascinating, but incomprehensible.

So, those who didn't go to a school that teaches CS, how did you start, what books could you recommend?

Thanks!  I'll recommend starting with two books:

* [The New Turing Omnibus, by A. K. Dewdney](http://www.amazon.com/The-New-Turing-Omnibus-Excursions/dp/0805071660/ref=sr_1_1?ie=UTF8&amp;amp;qid=1339860008&amp;amp;sr=8-1&amp;amp;keywords=new+turing+omnibus)
* [Algorithmics, by David Harel](http://www.amazon.com/Algorithmics-The-Spirit-Computing-Edition/dp/0321117840/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1339860052&amp;amp;sr=1-1&amp;amp;keywords=algorithmics)

Once you understand the material in those (and assuming you have some mathematical background in logic and discrete math, or can get it), you might want to look at:

* [Introduction to the Theory of Computer Science, by Michael Sipser](http://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/0534950973/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1339860086&amp;amp;sr=1-1&amp;amp;keywords=theoretical+computer+science+sipser)

Having said all of that, a good grasp of practical computation is a great foundation on which to understand theoretical concepts. For that, I'd recommend:

* [The Little Schemer](http://www.amazon.com/The-Little-Schemer-4th-Edition/dp/0262560992/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1339860113&amp;amp;sr=1-1&amp;amp;keywords=little+schemer)
* [The Seasoned Schemer](http://www.amazon.com/The-Seasoned-Schemer-Daniel-Friedman/dp/026256100X/ref=pd_bxgy_b_text_b)
* [The Structure and Interpretation of Computer Programs](http://www.amazon.com/Structure-Interpretation-Computer-Programs-Engineering/dp/0262510871/ref=pd_sim_b_2)

And write actual programs in Scheme as you go through these books. I'll recommend starting with two books:

* [The New Turing Omnibus, by A. K. Dewdney](http://www.amazon.com/The-New-Turing-Omnibus-Excursions/dp/0805071660/ref=sr_1_1?ie=UTF8&amp;amp;qid=1339860008&amp;amp;sr=8-1&amp;amp;keywords=new+turing+omnibus)
* [Algorithmics, by David Harel](http://www.amazon.com/Algorithmics-The-Spirit-Computing-Edition/dp/0321117840/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1339860052&amp;amp;sr=1-1&amp;amp;keywords=algorithmics)

Once you understand the material in those (and assuming you have some mathematical background in logic and discrete math, or can get it), you might want to look at:

* [Introduction to the Theory of Computer Science, by Michael Sipser](http://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/0534950973/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1339860086&amp;amp;sr=1-1&amp;amp;keywords=theoretical+computer+science+sipser)

Having said all of that, a good grasp of practical computation is a great foundation on which to understand theoretical concepts. For that, I'd recommend:

* [The Little Schemer](http://www.amazon.com/The-Little-Schemer-4th-Edition/dp/0262560992/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1339860113&amp;amp;sr=1-1&amp;amp;keywords=little+schemer)
* [The Seasoned Schemer](http://www.amazon.com/The-Seasoned-Schemer-Daniel-Friedman/dp/026256100X/ref=pd_bxgy_b_text_b)
* [The Structure and Interpretation of Computer Programs](http://www.amazon.com/Structure-Interpretation-Computer-Programs-Engineering/dp/0262510871/ref=pd_sim_b_2)

And write actual programs in Scheme as you go through these books. I'll recommend starting with two books:

* [The New Turing Omnibus, by A. K. Dewdney](http://www.amazon.com/The-New-Turing-Omnibus-Excursions/dp/0805071660/ref=sr_1_1?ie=UTF8&amp;amp;qid=1339860008&amp;amp;sr=8-1&amp;amp;keywords=new+turing+omnibus)
* [Algorithmics, by David Harel](http://www.amazon.com/Algorithmics-The-Spirit-Computing-Edition/dp/0321117840/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1339860052&amp;amp;sr=1-1&amp;amp;keywords=algorithmics)

Once you understand the material in those (and assuming you have some mathematical background in logic and discrete math, or can get it), you might want to look at:

* [Introduction to the Theory of Computer Science, by Michael Sipser](http://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/0534950973/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1339860086&amp;amp;sr=1-1&amp;amp;keywords=theoretical+computer+science+sipser)

Having said all of that, a good grasp of practical computation is a great foundation on which to understand theoretical concepts. For that, I'd recommend:

* [The Little Schemer](http://www.amazon.com/The-Little-Schemer-4th-Edition/dp/0262560992/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1339860113&amp;amp;sr=1-1&amp;amp;keywords=little+schemer)
* [The Seasoned Schemer](http://www.amazon.com/The-Seasoned-Schemer-Daniel-Friedman/dp/026256100X/ref=pd_bxgy_b_text_b)
* [The Structure and Interpretation of Computer Programs](http://www.amazon.com/Structure-Interpretation-Computer-Programs-Engineering/dp/0262510871/ref=pd_sim_b_2)

And write actual programs in Scheme as you go through these books. I'll recommend starting with two books:

* [The New Turing Omnibus, by A. K. Dewdney](http://www.amazon.com/The-New-Turing-Omnibus-Excursions/dp/0805071660/ref=sr_1_1?ie=UTF8&amp;amp;qid=1339860008&amp;amp;sr=8-1&amp;amp;keywords=new+turing+omnibus)
* [Algorithmics, by David Harel](http://www.amazon.com/Algorithmics-The-Spirit-Computing-Edition/dp/0321117840/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1339860052&amp;amp;sr=1-1&amp;amp;keywords=algorithmics)

Once you understand the material in those (and assuming you have some mathematical background in logic and discrete math, or can get it), you might want to look at:

* [Introduction to the Theory of Computer Science, by Michael Sipser](http://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/0534950973/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1339860086&amp;amp;sr=1-1&amp;amp;keywords=theoretical+computer+science+sipser)

Having said all of that, a good grasp of practical computation is a great foundation on which to understand theoretical concepts. For that, I'd recommend:

* [The Little Schemer](http://www.amazon.com/The-Little-Schemer-4th-Edition/dp/0262560992/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1339860113&amp;amp;sr=1-1&amp;amp;keywords=little+schemer)
* [The Seasoned Schemer](http://www.amazon.com/The-Seasoned-Schemer-Daniel-Friedman/dp/026256100X/ref=pd_bxgy_b_text_b)
* [The Structure and Interpretation of Computer Programs](http://www.amazon.com/Structure-Interpretation-Computer-Programs-Engineering/dp/0262510871/ref=pd_sim_b_2)

And write actual programs in Scheme as you go through these books.     If you want to be introduced to basics of programming and don't want to purchase a book, visit codecademy.com , it's great for learning the java language.   </snippet></document><document><title>Looking for a Book Recommendation for the  Lambda Calculus</title><url>http://www.reddit.com/r/compsci/comments/v5hzi/looking_for_a_book_recommendation_for_the_lambda/</url><snippet>I m a mathematician and I d like to know what the classic books are on this subject, as I ve recently taken up programming and have heard much about Lambda Calculus but don't have any good resources on it. I have read half of Sipser's Computation book and was disappointed it had little to say about this.    </snippet></document><document><title>How to motivate yourself enough to read a textbook outside of academia?</title><url>http://www.reddit.com/r/compsci/comments/v4n3e/how_to_motivate_yourself_enough_to_read_a/</url><snippet>I have been aiming to read the textbook 'The Elements of Statistical Learning' for over three years now. It's a heavy text with difficult and complicate concepts in the areas of data mining, interference, and prediction. The problem is that I have a full time job that takes up more than 45 hours of my time each week. What kind of habits / tricks can I use to get this beast read from front to end?

EDIT: I do understand that motivation (or interest?) comes naturally for some; however, I genuinely believe that there is a long term benefit from doing things that are may be uncomfortable or difficult to complete.  Pick a problem from the problem sets and then go back and learn what you  need to solve it.  I started doing this with computer science publications (implementing what the paper describes) and it can be a lot of fun and I feel more productive than just the reading.  Eventually you will have covered the good stuff in the book, though maybe not end to end. My approach is similar, even though it is not intended for the purpose of reading a book: (1) Pick a real-world problem you want to solve. (2) Go read enough material in books to make some progress. (3) Make some progress. (4) Finished? If not, go to step (2). (5) Finished! Go to step (1).

I've ended up reading a good part of Elements of Statistical Learning (it's a good book) that way, and even more scientific papers. 

EDIT: On the topic of Machine Learning, especially for learning purposes (no pun intended), I suggest learning to use [Theano](http://deeplearning.net/software/theano/) (see [here](http://www.deeplearning.net/tutorial/) for a tutorial on implementing some basic ML stuff in it). It makes the nitty-gritty implementation of a lot of stuff much nicer than it would otherwise be. You can just compose your model and it does the annoying stuff of computing gradients and compiling the thing to C or CUDA for you. It still has some sharp edges off the beaten track (e.g. for some operations, it can't compute derivatives of derivatives), but the basics work, it's extensible and works well with the numpy/scipy/matplotlib toolset (which itself is essentially "Matlab for Python", pretty good for this kind of stuff even without Theano). Pick a problem from the problem sets and then go back and learn what you  need to solve it.  I started doing this with computer science publications (implementing what the paper describes) and it can be a lot of fun and I feel more productive than just the reading.  Eventually you will have covered the good stuff in the book, though maybe not end to end.    Why read it if you're not motivated? If you dont find it interesting its not worth your time.  Self improvement, preparing for an interview, expanding your skill set, improve your odds of winning arguments on the internet, etc etc Self improvement, preparing for an interview, expanding your skill set, improve your odds of winning arguments on the internet, etc etc Why read it if you're not motivated? If you dont find it interesting its not worth your time.   Find yourself a study buddy. Also, chains.cc            Be interested in what you're reading?

I never understood these questions. "How do I read a textbook?" Well, just read it. I'm sorry, but seriously, what the fuck? Who are these people?

45 hours a week is essentially 9am-6pm, five days out of seven. That leaves five weekday evenings and the entire weekend, which is more than enough time to read anything.

If you can't read a textbook by opening it and reading it, then you probably have no business being an academic or professional. Go wash dishes for a living and switch to the Twilight series. Be interested in what you're reading?

I never understood these questions. "How do I read a textbook?" Well, just read it. I'm sorry, but seriously, what the fuck? Who are these people?

45 hours a week is essentially 9am-6pm, five days out of seven. That leaves five weekday evenings and the entire weekend, which is more than enough time to read anything.

If you can't read a textbook by opening it and reading it, then you probably have no business being an academic or professional. Go wash dishes for a living and switch to the Twilight series. Actually, I never understood people like you? First of all, I have a master's degree in numerical methods / convex analysis. It's not a new concept to read mathematical textbooks. The motivation in academia was very straight forward, you have a thesis and courses, you will need to read those textbooks in order to graduate. In my opinion the text book reading was not the most interesting part it was the result of learning that material. In other words, the application of learning that material which is exciting.   I generally smoke and read.  certainly an expensive way to learn!</snippet></document><document><title>If you wanted to make a virtual machine that was capable of simulating an actual basic computer, what would be essential instructions you would have to have?</title><url>http://www.reddit.com/r/compsci/comments/v4qyd/if_you_wanted_to_make_a_virtual_machine_that_was/</url><snippet>To further elaborate, imagine the vm had to implement a CPU and attached video and keyboard that would be roughly equivalent to something like an Apple II capable machine.

What would the be minimal stuff that you would have to put in the CPU for example?

Being that this is the compsci reddit, feel free to point me to relevant papers or theory I should look at. I have been a hobbyist programmer for twenty years, but very light on theory and roughly about a first or second year level of CS.  Asking about the minimal necessary isn't really an interesting question.  You can construct a universal Turing machine in as few as 6 instructions.  Such a machine would be hellish to program or use, but it can provably compute anything that any other computer can.

The 6502 that the Apple ][ used had 56 instruction mnemonics.  If you count all the possible permutations of the addressing modes, it was more like 151 total distinct instructions.  ISA design is as much about making a system that humans can actually get their brains around, or at least it was in those days.

The Python VM has I believe 119 opcodes, the JVM has 205, the .Net CLR has 218.  Those are of course operating at a much higher level, but it gives you an idea of the size of things.

By the way, things like video and keyboard are pretty much irrelevant from the standpoint of the instruction set.  Hardware is memory mapped, so that for example writing to or reading from registers of the video controller is just like reading/writing any normal memory address from the standpoint of the CPU -- everything goes over the same bus.  Some architectures have IO ports as well as memory mapping, but that just means you have a couple of extra instructions that take a port number and read or write a value into a register.  It's still hardware independent. Actually, you can do it in one.

See https://en.wikipedia.org/wiki/One_instruction_set_computer I was gonna say. You can solve bsat (any problem) using NAND I believe. damn you! I came here to say nor or nand is the only instruction you need. &amp;gt;nor or nand

Don't you mean nor xor nand? I was gonna say. You can solve bsat (any problem) using NAND I believe. This blew my mind.  And I learned it in a philosophy course. Asking about the minimal necessary isn't really an interesting question.  You can construct a universal Turing machine in as few as 6 instructions.  Such a machine would be hellish to program or use, but it can provably compute anything that any other computer can.

The 6502 that the Apple ][ used had 56 instruction mnemonics.  If you count all the possible permutations of the addressing modes, it was more like 151 total distinct instructions.  ISA design is as much about making a system that humans can actually get their brains around, or at least it was in those days.

The Python VM has I believe 119 opcodes, the JVM has 205, the .Net CLR has 218.  Those are of course operating at a much higher level, but it gives you an idea of the size of things.

By the way, things like video and keyboard are pretty much irrelevant from the standpoint of the instruction set.  Hardware is memory mapped, so that for example writing to or reading from registers of the video controller is just like reading/writing any normal memory address from the standpoint of the CPU -- everything goes over the same bus.  Some architectures have IO ports as well as memory mapping, but that just means you have a couple of extra instructions that take a port number and read or write a value into a register.  It's still hardware independent. Six? Is that a LISP reference?   There are such things as Minimal Instruction Set Computers (MISC) that you could look it if you absolutely want it to be minimal.  Or you could implement a Turing Machine, extremely simple and easy to implement, Turing complete, but a pain to write programs for.

If you want more of a "real world" example then a Reduced Instruction Set Computer (RISC) would be ideal.  See the PowerPC instruction set here: http://www.pds.ewi.tudelft.nl/vakken/in1200/labcourse/instruction-set/

Writing an emulator for a simple CPU is often included in a first year Computer Science course so it's a good exercise. My university must suck, it was in the second year :( No, my university sucks - I'm almost done with my third year, and there were no emulators in sight. No, my university sucks - I've been graduated ten years and never did it! My university must suck, it was in the second year :( My university must suck, it was in the second year :( My university must suck, it was in the second year :( Try writing one yourself for something simple, like a 6502 or z80.  It's a steep learning curve in the beginning, so try to follow (but not copy!) existing emulator source code that is easy to read (i.e. something most likely made to  teach. If you're looking for 6502 stuff, look up Dan Boris). Writing even a simple emulator is a great way to learn the basics of instruction sets, addressing modes, binary encoding, etc.

I tried writing a 6502 emulator my sophomore year and learned a ton from it (then learned more later after reviewing my code and writing more emulators). In second year I've written a MIPS pipeline simulator in SystemC, it was fun. what software can we use to run systemc code?  You only need a single instruction: CMOV  (conditional move).  The advantage of having a single instruction is that you don't need to encode it!  So your program consists only of triples of addresses (test src dst).

You can have "registers" and an ALU, both memory mapped.
Have a look at an example here:
http://paste.lisp.org/display/130083     Looking past the concerns raised by other commenters, I would include the following instructions in a "minimal-but-useful" instruction set:

* LOAD (mem-&amp;gt;reg)
* STORE (reg-&amp;gt;mem)
* MOV (reg-&amp;gt;reg)
* ADD (a += b, set carry flag)
* SUB (a -= b)
* MUL (a *= b)
* AND (a &amp;amp;= b)
* OR (a |= b)
* XOR (a \^= b)
* NEG (a = ~a)
* SHL (a &amp;lt;&amp;lt;= b)
* SHR (a &amp;gt;&amp;gt;= b)
* CMPZ (a == 0?)
* BZ (branch if zero)
* BC (branch if carry)
* B (unconditional branch)

Incidentally, this is pretty much a limited version of the [MIPS instruction set](http://en.wikipedia.org/wiki/MIPS_architecture). :)

Note that this architecture would be prohibitively slow with floating-point numbers due to the lack of hardware support for IEEE754. Also note that an integer divide instruction is missing: This is for the same reason that ARM excludes it (divide-by-constant can be transformed to a multiply-by-constant, and divide-by-value can be implemented in terms of subtract and add). I've always wondered and now seems like a good time to ask:

to support signed vs unsigned integers do you need to have different instructions for signed and unsigned comparisons?

Because I don't see how else you could manage to properly handle proper comparisons of int, negative ints, and uints larger than INT_MAX        You only need nand.

Ninja edit:  only nor will work too.

Another:  Someone beat me to it. NAND is a type of gate, not an instruction.
 its both.  &amp;gt; minimal

This isn't the right way to think about it. Either you fully emulate the operation of a device, or you don't. Going halfway isn't really a meaningful exercise. &amp;gt; minimal

This isn't the right way to think about it. Either you fully emulate the operation of a device, or you don't. Going halfway isn't really a meaningful exercise.</snippet></document><document><title>Methodology versus Method</title><url>http://www.reddit.com/r/compsci/comments/v35lp/methodology_versus_method/</url><snippet>In my voyage through software engineering literature I constantly stumble upon the words "methodology" and "method". Some (incorrectly I have realised) use them interchangeably, while others separate them by their individual definitions.
I have personally, however, difficulties in precisely being able to distinguish their separate meaning. Can someone here help me with a sort of "explain it to me like im five"?

I have read definitions such as:

Methodology:  *"organized collection of concepts, methods, beliefs, values and normative principles ..."*, *"organize sets of behavioral and technical rules into a coherent approach which prescribes how to address major development problems"*

Method: *"specify procedure for accomplishing well-defined task"*, *"'technique' is defined as a way of accomplishing a task. ... A method is a well-defined description of a technique. A method is always documentable whereas a technique need not be."*  The way I see it the methodology is the heuristic and the method is the algorithm.  I dunno about explaining it like you are five, but how about middle school? Think back to 7th grade science class.

When you were faced with a problem, or a question to answer, you used the scientific method (Question -&amp;gt; Research -&amp;gt; Hypothesis -&amp;gt; Test -&amp;gt; Analyze -&amp;gt; Conclude -&amp;gt; Report). This in fact was a methodology.

The steps you actually took (1. Get 2 slices of bread. 2. Get out peanut butter. 3. Spread peanut butter) were the method.

So, a method is exactly how you go about doing something. A methodology is how you would go about constructing a method.

Now, to apply to Computer Science, or more specifically software engineering: You are developing a piece of software for the customer. The *method* in which you code is to:

1. Define Classes
1. Define Variables
2. Define Functions

But, say you are using the Watefall Model (Requirements, Design, Implementation, Verification, Maintenance) to build your product. That, is a *methodology*.


I think the Wikipedia article for [Methodology](http://en.wikipedia.org/wiki/Methodology) does a pretty good job explaining it at a higher level.  I am interested in how this differs from or overlaps with the terms Process, Procedure and Work Instruction.  With these three there is the same general/abstract to specific/concrete spectrum.  Is a Method, as discussed on this page really just the same as a Procedure, being a set of steps (with decisions and branches) to go through to achieve a result?   </snippet></document><document><title>Computer Graphics Book</title><url>http://www.reddit.com/r/compsci/comments/v28pg/computer_graphics_book/</url><snippet>I'm looking to invest in some reading material for the summer, particularly regarding computer graphics. I have a decent understanding of many concepts in this area, so I don't really need an introductory book, but I would like something fairly comprehensive.

I've considered the classic [Computer Graphics: Principles and Practice](http://www.amazon.com/Computer-Graphics-Principles-Practice-Edition/dp/0201848406), though I've heard that it's somewhat dated. That doesn't take it off the table, I'd just prefer something that covers more up-to-date concepts (see quaternions). [3D Computer Graphics](http://www.amazon.com/Computer-Graphics-3rd-Alan-Watt/dp/0201398559/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1339705583&amp;amp;sr=1-1) seems to be recommended  well, but I haven't heard any first-hand experiences.

Does anyone have any experience with these sorts of books such that they could provide their thoughts?

Thanks!

Also, one of the reasons I was holding back on the bible, was that I'd heard a new edition is due soon, but nothing I've found online addresses that.

Edit: Maybe I should mention that this is more in preparation for a research career than a programming one.  Other than expressing an interest in more theory than practicality, you haven't give much to go on -- is your interest more in ray tracing, real-time, physical-based simulation, modelling, animation, etc? Most of the active research in the field is applied research into real-time simulators -- mostly video games. I highly suggest: http://www.amazon.com/Real-Time-Rendering-Third-Edition-Akenine-Moller/dp/1568814240/ref=pd_sim_b_4

The field has evolved so much with the hardware and APIs I'd be deeply suspicious of anything older than 4 years. Most reading I've seen senior graphics developers do is compendiums like GPU Pro or ad hoc individual papers. Yeah, sorry about that. I guess my general goal was to get a book covering as much as possible, like principles and practice, but a bit more relevant.  In my computer graphics courses, we used Shirley's Fundamentals of Computer Graphics (mentioned below) for ray tracing, which you'll probably want to look into first, and then Real-Time Rendering for my GPU programming/interactive graphics class. I think if you get through those two books you'll have a pretty good overall understanding of graphics.

Also if you're into GPU programming and looking for specific effects, I found "GPU Gems" on the Nvidia website to be really helpful. Other than expressing an interest in more theory than practicality, you haven't give much to go on -- is your interest more in ray tracing, real-time, physical-based simulation, modelling, animation, etc? Most of the active research in the field is applied research into real-time simulators -- mostly video games. I highly suggest: http://www.amazon.com/Real-Time-Rendering-Third-Edition-Akenine-Moller/dp/1568814240/ref=pd_sim_b_4

The field has evolved so much with the hardware and APIs I'd be deeply suspicious of anything older than 4 years. Most reading I've seen senior graphics developers do is compendiums like GPU Pro or ad hoc individual papers.  I have to throw in my suggestion

[Fundamentals Of Computer Graphics](http://www.amazon.com/Fundamentals-Computer-Graphics-Peter-Shirley/dp/1568811241)


This is the book we used in my grad level computer graphics course for learning ray-tracing, which is what you'll get into in research rather than doing real-world type work with graphics engines and stuff
 I second this.

Also, check out the UC Berkeley "prelim study list:" the list of need-to-know topics before you can pursue a Ph.D. in graphics: http://www.eecs.berkeley.edu/GradAffairs/CS/Prelims/GR.pdf

It's all pretty basic undergrad-level stuff, so it should be accessible. (That's NOT to say it's "easy" stuff, just undergrad-level, ie, broadly useful.) Its a pretty good book IMO. I might be biased tho since my professor wrote one of the chapters :-) Which? (And who is your professor?)  Physically Based Rendering; From Theory to Implementation by Matt Pharr and Greg Humphreys.  (see www.pbrt.org).  By far my favorite book on graphics, this is focused largely on modeling of light transport, sampling, material and geometry representations etc.  Much more oriented toward physically accurate techniques (e.g. ray tracing) but it should be on the shelf of any graphics expert.  Also, not a book, but you should read James Kajiya's 1986 paper "The Rendering Equation".  It eloquently states the problem of computer graphics. Physically Based Rendering; From Theory to Implementation by Matt Pharr and Greg Humphreys.  (see www.pbrt.org).  By far my favorite book on graphics, this is focused largely on modeling of light transport, sampling, material and geometry representations etc.  Much more oriented toward physically accurate techniques (e.g. ray tracing) but it should be on the shelf of any graphics expert.  Also, not a book, but you should read James Kajiya's 1986 paper "The Rendering Equation".  It eloquently states the problem of computer graphics.  Principles of Digital Image Synthesis.  Free online now.    Realtime rendering, 3rd edition by haines, moller etc, is the only good book on the subject.   I think this really depends on the specific area in graphics that interests you. For instance, the techniques used in creating images for real-time uses such as video games tend to differ from those used for photo-realistic film quality renders. 

If you'd like to actually dive into the principles behind rendering (which broadly apply to both), I'd strongly recommend [Physically Based Rendering, Second Edition: From Theory To Implementation](http://www.amazon.com/Physically-Based-Rendering-Second-Edition/dp/0123750792/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1339713282&amp;amp;sr=1-1). It's largely up to date, and come with source code. 


Since you mentioned that this is intended to prepare you for a research career, there's nothing better than going directly to the source! Reading the seminal papers in rendering can be a very useful exercise. This [list](http://www.cc.gatech.edu/current/doctoral/phdcs-qualifier/graphicsvis/rendering)  is a good one to begin with (I'd use this in conjunction with the mentioned book). 


It's a little harder to stay on top of the latest research for real-time rendering. This [book](http://www.amazon.com/Real-Time-Rendering-Third-Edition-Akenine-Moller/dp/1568814240/ref=pd_sim_b_4) is a reasonable starting point. You may also want to take a look at the latest papers at SIGGRAPH or Eurographics if you intend to pursue research in rendering, but these tend to be fairly advanced since rendering is a relatively mature area in graphics.

Feel free to PM me if you have other questions - I'm working on my PhD in graphics (although not on rendering).      I've read [this one](http://www.arcsynthesis.org/gltut/). I'm curious how it compares to other books, but I've been too lazy to follow up.

One thing I like is that it goes straight to the modern, programmable, real-world pipeline, without the "training wheels" of the fixed-function pipeline. Harder to start with, but I now have a sense that I actually understand what the graphics hardware is doing.</snippet></document><document><title>How to answer a question: a simple system</title><url>http://www.michaelnielsen.org/ddi/how-to-answer-a-question-a-simple-system/</url><snippet>  Sounds like they just outsourced the language parsing to Google's search algoritms, or am I missing something?</snippet></document><document><title>I'm very confused about conducting research in Computer Science. I need help.</title><url>http://www.reddit.com/r/compsci/comments/v0nff/im_very_confused_about_conducting_research_in/</url><snippet>I don't know if this belongs here or not, but I could not think of any other subreddit to post this to.  
I am a graduate (masters) student interested in Data Mining (in particular I am interested in applications of Data Mining to social network analysis, astronomy, music and other cool stuff). I just finished my first year of graduate studies and need to start working towards my thesis.  
The problem is that I have no previous research experience. I don't know how it works. How do people come up with a novel topic/problem to solve? Before I started grad school, I was worried about how I would go about solving a completely new problem. But now that I am at this stage of starting my research, I don't even have a problem to solve. I'm also worried that if and when I do come up with something, it might be more than I can handle. I don't want to start working on something and later realize that the problem is beyond my capabilities (or the time I have is not enough to solve it).  
I'm sure there are other people who have faced this problem. How did you work it out?

Edit: I know this is a very vague and subjective question. I don't expect a concrete answer. But just a push in the right direction would really help.

EDIT 2: Thanks a lot for the helpful comments guys. Really appreciate it. I understand that I need to talk to my advisor more and that he is the best person who can help me narrow down the topic. And I need to read more papers. A lot of people suggested that. I'll get right to it.  
Thanks again!   There are two main places to get research ideas. Both come from reading academic papers in your field.  
  
1.When reading, if you ever think "but what if," or "there's a better way," or "I could apply this technique to this other thing," then you just found a topic.  
2. A lot of papers will explicitly outline possible future work.       Robots suck, mainly cause sensors are inaccurate, try to make them suck less           </snippet></document><document><title>The Turning Centenary conference is taking place in Manchester University next week. I urge any Mancunian computer scientists to consider going to the free public lectures at this once in a lifetime conference!</title><url>http://www.turing100.manchester.ac.uk/index.php/programme/programme</url><snippet>      "Please note: although the total number of tickets available for the publiclecture on 22nd June is 1000, only the first 600 registrants will be seated in the main lecture theatre. All other guests will be seated in a separate lecture theatre, to which the lecture will be streamed live."

Do you know if this will be broadcasted over the internet? Tempted to go but it'll be costly. "Please note: although the total number of tickets available for the publiclecture on 22nd June is 1000, only the first 600 registrants will be seated in the main lecture theatre. All other guests will be seated in a separate lecture theatre, to which the lecture will be streamed live."

Do you know if this will be broadcasted over the internet? Tempted to go but it'll be costly.  This looks neat, too bad it's far away from me. Still, TIL the proper demonym for people from Manchester.  </snippet></document><document><title>SKI combinator calculus in Java</title><url>http://www.reddit.com/r/compsci/comments/uzrm9/ski_combinator_calculus_in_java/</url><snippet>This may sound pretty retarded, but I'm trying to implement some typed [SKI-combinator functions](http://en.wikipedia.org/wiki/SKI_combinator_calculus) (and maybe some other well-known lambda functions) in Java as some kind of experiment.

But I have a problem with the types of some functions. [Here is the typed code that I already have implemented](http://pastebin.com/zz19xx8n).

You can see that I can get the result from *result2* (121) and *result4* (129). But  the line that gives *result3* (125) gives a type error because function *g* and *x* are not correctly typed. And I don't know how to solve it.

I tried to solve it by given *K* only one type parameter, but I can't seem to let the other type disappear. Does anyone know how I can do that?

I don't have this problem if I only give a type to the output of the function: [see it in this example.](http://pastebin.com/azCp08ez).

I know that these combinators (and lambda functions) ar normally untyped. So is it possible in Java to make the SKI-combinators fully typed like I wan't to do? 


*EDIT*: [here is the same done in Haskell.](http://pastebin.com/WUF7LjUj)  Well sure it doesn't work. 'x' is a function from Integer to Object, and you give it Function&amp;lt;...&amp;gt; as an argument.

Also keep in mind Function is **NOT!!!!!!!!!!!** a supertype of Function&amp;lt;T&amp;gt;. I know that.

But I can't seem to find a good type so that all results can be calculated. Well you can't find it because it's not possible in java :) Type erasure is a bitch.

In haskell, type 'a' can specialize to 'b-&amp;gt;c' no problem, in java, nope.

Well actually. What you're trying to do doesn't even make sense. Once you have K: Int -&amp;gt; Obj -&amp;gt; Int, you have to pass the correct type, that's the point of types.

Edit: Oh I see... you're doing K:(Int -&amp;gt; Obj) -&amp;gt; Int -&amp;gt; (Int -&amp;gt; Obj). The currification is backwards

a -&amp;gt; b -&amp;gt; c = a -&amp;gt; (b -&amp;gt; c) not (a -&amp;gt; b) -&amp;gt; c

I just skimmed your code, but this makes more sense

instead of 

    Function&amp;lt;Function&amp;lt;Integer,Object&amp;gt;, Integer&amp;gt; g = new K&amp;lt;Integer, Function&amp;lt;Integer,Object&amp;gt;&amp;gt;().apply(new Integer(7));

do

    Function&amp;lt;Integer,Function&amp;lt;Object,Integer&amp;gt;&amp;gt; g = new K&amp;lt;Integer, Function&amp;lt;Object,Integer&amp;gt;&amp;gt;().apply(new Integer(7)); So it is not possible :(

Isn't the type of Function and Function&amp;lt;T&amp;gt; the same at runtime?

Edit: I tried it in Haskell first, and I could get it typed and let it work.
So there is no (ugly) workaround? It is same at runtime, but not at compile time, that's when you get the error :P It's quite messy due to backwards compatibility, read Effective Java, it's described pretty good there.

Also, I've edited the above comments, so reload them. Maybe it will fix this instance, but in general, I don't think you can do a fully typed solution with java. Thanks for the feedback, now I know that I can stop looking for a solution.

The change you proposed doesn't solve the problem. If I implement it and add a change to the type of x (to fix the type errors I'm getting), then *result2* and *result3* will work. But *result4* will give a type error.

I wil just go with 'it is impossible'

 Wait actually, I think it might be possible.

I wrote it out like this and I think you're simply trying to apply incompatible functions (and I'm sure this wouldn't work even in haskell)

        // g :: Int -&amp;gt; [(Int -&amp;gt; Obj) -&amp;gt; Int]  the part in [] is after application
        // x :: Obj -&amp;gt; [(Int -&amp;gt; Obj)]
        Function&amp;lt;Function&amp;lt;Integer,Object&amp;gt;, Integer&amp;gt; g = new K&amp;lt;Integer, Function&amp;lt;Integer,Object&amp;gt;&amp;gt;().apply(new Integer(7));
        Function&amp;lt;Integer,Object&amp;gt; x = new K&amp;lt;Object, Integer&amp;gt;().apply(new Integer(3)); // here can be new Object() right?

        int result2 = g.apply(x);
        System.out.println(result2);

        //// Should work, but gives an error because the type is wrong.
        int result3 = x.apply(g); // x is of type Int -&amp;gt; Obj, if you apply ((Int-&amp;gt;Obj)-&amp;gt;Int) as first parameter, it can't possibly work.
        // (Int-&amp;gt;Obj)-&amp;gt;Int is not the same as Int-&amp;gt;(Obj-&amp;gt;Int)
 I know my functions are incompatible, because the types don't work out (that is my problem).
The types in my example code are the only typing I could find to make *result4* work.  They also let *result2* work, but not *result3*, and that is where my problem is. I can't seem to find a typing so all three results don't give type errors.

The types of *x* and *g* really look weird in my code, they suppose to have the same type, because they are practically the same function (only an other Integer has been put in). This probably won't help you in the slightest, but I believe the approach you're trying to use *would* work in .net, since it doesn't rely on type erasure. If it's just experimental stuff, you could totally use Mono. </snippet></document><document><title>Personal stories regarding general GRE scores and admission into comp. sci. depts.</title><url>http://www.reddit.com/r/compsci/comments/v09jm/personal_stories_regarding_general_gre_scores_and/</url><snippet>Hello,

I was wondering if any of you had personal stories regarding GRE scores and admissions into comp. sci. depts.? Just looking to gain some insight and perhaps feedback too. I apologize in advance if this thread is too vague or unsuitable.

(My score is V163/Q161=324, and I'm probably going to apply in Canada)  My GRE scores were (V 159, Q 170, W 4.0) and I was admitted to PhD programs at Stanford, UC Berkeley, CMU, and some other schools.

I'm pretty convinced the GRE is a nearly worthless exam. The top schools claim to largely ignore it, but my guess is they use it to dwindle their application pool to a manageable size. Lower ranked schools may admit people based almost entirely on GRE+GPA. The best graduate students are those with the greatest research abilities, which is what the top schools are looking for. Research potential can not be measured by GRE scores. My GRE scores were (V 159, Q 170, W 4.0) and I was admitted to PhD programs at Stanford, UC Berkeley, CMU, and some other schools.

I'm pretty convinced the GRE is a nearly worthless exam. The top schools claim to largely ignore it, but my guess is they use it to dwindle their application pool to a manageable size. Lower ranked schools may admit people based almost entirely on GRE+GPA. The best graduate students are those with the greatest research abilities, which is what the top schools are looking for. Research potential can not be measured by GRE scores. thanks for your feedback. gre is probably not a deciding factor, but will have a weightage i'm sure. at the end of the day, im just hoping to get into a school of my choice (v163/q161, Uni British Columbia or Uni of Toronto). btw, you're gre score is awesome, i kinda fudged my quant. meh, im not applying to stanford, etc, so there's no real point in redoing the gre. or is there? im talking about msc in comp sci of course.  I **BOMBED** English. Don't underestimate it. I believe I got a 780 in math. 300-800 score. 86th percentile. But English...was like 380. 28th percentile.  I was admitted for engineering, but I still wish I would have acknowledged the English portion. Lost a lot of opportunities for scholarships that way. Study for it. Read books. Take the free Kaplan GRE Tests.

I'm not familiar with teh new scoring range (130-170?).  160 seems up there for both though. I think it's also a lot about percentile.  If you're top 10%, you're great. Even top %20 means you're in the top of 1/5th of people trying to better themselves. This isn't the ACT where everyone took it anyway, only determined people take this. hey thanks for your feedback. much appreciated. well, you still got into engineering! do you enjoy what you do now? I just graduated last month and start in August. Right now I intern, but I know I want to be a professor.  I like academia environments, not industry work.  Do Canadian universities ask for GRE scores if you're going into CS? If I recall correctly they don't for math, or at least Waterloo and U of T don't.  I failed GRE pretty badly (V=460 Q=720). I think I'm just not good at time management/studying. I skimmed those thick GRE books and took a couple of self tests but that's all I did to 'study' couple days before the test. Maybe this was a good indication that I was not ready for grad school.

I then got a job, applied to CMU Silicon Valley campus, got rejected, and now I'm working.

I kind of wanted to get M.S. while undergrad CompSci is still fresh in my mind while I work but I guess you don't always get what you want. gre isn't an indicator of programming/development ability at all. i myself am planning on an ms after nearly a 2 yr gap in education. its never too late- if you really want to, go for it.  From what I hear from college profs who do admissions into really exclusive humanities program, all the research has ever correlated a GRE score to is the chance that you'll finish a program.  Generally the higher the score, the greater your chance that you'll finish the program.  Even then, it's not a strong correlation.  Still it's required by many grad admissions programs.

Also, you may find that you're seemingly balanced GRE score (I don't know the new scoring system) is in your favour when applying to schools.</snippet></document><document><title>The Evolution of the Computer Science Degree</title><url>http://h30565.www3.hp.com/t5/Feature-Articles/The-Evolution-of-the-Computer-Science-Degree/ba-p/4456</url><snippet /></document><document><title>Flame is lame</title><url>http://www.f-secure.com/weblog/archives/00002383.html</url><snippet>   This is stupid: they were met with ridicule by people on reddit, who ridicule everything. Most people who actually work in infosec understood from day one, that this is something else. I remember getting a first sample in the lab and taking it apart: we ended up staying overtime just because it was that interesting. The discovery that Flame was pulling a man in the middle against Windows Update came only after everybody got really excited. No, I didn't write the post based on comments in Reddit; in fact I haven't seen any commentary on Flame in Reddit.

I was referencing to articles like

* http://www.thetechherald.com/articles/Is-the-hype-surrounding-Flame-blazing-a-FUD-fueled-trail-of-panic
* http://xato.net/malware/flame-is-kind-of-lame/
* http://idealab.talkingpointsmemo.com/2012/05/flame-malware-mostly-smoke-and-mirrors-say-security-experts.php

Mikko [deleted] No, I didn't write the post based on comments in Reddit; in fact I haven't seen any commentary on Flame in Reddit.

I was referencing to articles like

* http://www.thetechherald.com/articles/Is-the-hype-surrounding-Flame-blazing-a-FUD-fueled-trail-of-panic
* http://xato.net/malware/flame-is-kind-of-lame/
* http://idealab.talkingpointsmemo.com/2012/05/flame-malware-mostly-smoke-and-mirrors-say-security-experts.php

Mikko  I never stumbled across anything portraying Flame as lame. If anything most security entities were impressed and alarmed.       [deleted]</snippet></document><document><title>One Computer with Two DIFFERENT Processors</title><url>http://www.reddit.com/r/compsci/comments/uz03t/one_computer_with_two_different_processors/</url><snippet>Does any implementation of BIOS or EFI support having two different processors on the same board?  I know of having two identical processors (which was quite common before the multi-core era).  Even if the BIOS/EFI and board support it, is there any OS that could make use of both?  If so, how likely would the OS actually efficiently deal with the discrepancy between the two processors?

On a side note, I know that the processor in the Nvidia Tegra 3 SoC utilizes what they call "4-PLUS-1" in which there are 4 main cores as well as one "companion core" running at a lower clock speed in addition to using a low power silicon process.  I assume any device utilizing this SoC is just programmed to put the other cores offline and utilize the 5th core just like how Android enables and disables cores on any other multi-core SoC.

EDIT:  I should have been more specific.  An example of my question would be an x86 box with two processor sockets on the board but two different processors (both x86, both 32-bit OR 64-bit).  The NeXT computers had two processors: a 68030 or 68040, and a DSP56001.  Obviously, the "BIOS" and kernel handled both. (/dev/dsp sent data and programs to the DSP).

Nowadays, any PC with a graphic card has both a CPU (or several), and a GPU (or several).  There are ROMs on the graphic cards, and drivers in the OSes, to deal with the GPU parts, but the BIOS deals with it too (thru a de-facto standardized interface ports).

Sometimes, some other chip may be considered separate processors too.  Sophisticated DMA chips for example, could be loaded with a "program" to implement an I/O operation.  But I don't know what capabilities common DMA chips on PCs have. Thanks for your informative response.  I should have been more specific by mentioning that I meant two regular CPUs (like an x86 box with two processor sockets). The [NeXTdimension addon card](http://en.wikipedia.org/wiki/NeXTdimension), which made the NeXT cube color-capable, had an Intel i860 CPU on board that ran its own operating system. There was also the [SunPCi](http://en.wikipedia.org/wiki/SunPCi) that was an x86 processor for SPARC/Solaris machines. Neat!

Let's also not forget [the plethora of add-in cards for the Apple II](http://en.wikipedia.org/wiki/Apple_II_processor_cards) that would let you do things like run CP/M on an Intel 8088. Neat!

Let's also not forget [the plethora of add-in cards for the Apple II](http://en.wikipedia.org/wiki/Apple_II_processor_cards) that would let you do things like run CP/M on an Intel 8088. There was also the [SunPCi](http://en.wikipedia.org/wiki/SunPCi) that was an x86 processor for SPARC/Solaris machines. Thanks for your informative response.  I should have been more specific by mentioning that I meant two regular CPUs (like an x86 box with two processor sockets).   Define 'different'.  Having slightly different performance characteristics is probably pretty common, especially when you consider a single system, rather than a single board. (For the record, a big NUMA machine like an SGI Altix tends to just look like a n processor machine, even though it's potentially spread over several racks.) As such, all of the commercial Unices, BSD and Linux should be pretty happy - it's just a scheduling tweak after all - and there are machines out there that require it. Some of the issues are not dissimilar to hyperthreading can create fairly similar scenarios.

Well, there's been machines that have had processors of differing types.

Acorn's BBC Micro supported an extensive set of add on second processor boards. Application logic tended to move to the second processor, whilst the first processor (in the base machine) handled. The Acorn RiscPC (from 1994) had an ARM processor and a x86 processor (on swappable cards) which shared the same memory bus. This was intended for DOS/Windows in a window, but it was used for other purposes, not least floating point acceleration. There's also many, many machines that have been traditional processors + DSPs.

Quite a few supercomputers used traditional microprocessors in concert with vector processors (and/or FPGAs). In fact, this architecture is not that dissimilar to the PC's x86+GPU architecture (some supercomputers are x86 PCs with GPUs!).

The PC x86+GPU arrangement is exactly what's been targetted by things like OpenCL (where the same code can run on the CPU or GPU), so I suppose the answer is there's a long history of doing so, and the future appears to be multiple architectures on the same board, even in the same processor package or on the same die.

 Sorry for not being more specific.  The question I had in mind would be 2 x86 processors on one board.  My question would be how different clock speed, manufacturers, and possibly instruction sets would affect a system if it would even run.  I remember multi-processor systems before the multi-core era but I'm sure they all had identical processors running in tandem. Sorry for not being more specific.  The question I had in mind would be 2 x86 processors on one board.  My question would be how different clock speed, manufacturers, and possibly instruction sets would affect a system if it would even run.  I remember multi-processor systems before the multi-core era but I'm sure they all had identical processors running in tandem. Sorry for not being more specific.  The question I had in mind would be 2 x86 processors on one board.  My question would be how different clock speed, manufacturers, and possibly instruction sets would affect a system if it would even run.  I remember multi-processor systems before the multi-core era but I'm sure they all had identical processors running in tandem.  Many old video games consoles had multiple general purpose processors, often with different architectures.

* The Sega Genesis had a 68000 and a Z80.

* The Sega 32X (an add-on for the Genesis) added a two SH2s (32-bit RISC CPUs), giving the system four CPUs and 3 different architectures. Adding the Sega CD on top gave it another 68000, for a five (!) core system.

* The Sega Saturn likewise had two SH2s and a 68000.

* The SNES' sound processor, the SPC-700, was effectively a full-fledged programmable 8-bit CPU, although it was almost always used for nothing but sound.

* SNES games frequently came with additional CPUs on their cartridge. Most famous was the SuperFX in Star Fox, Yoshi's Island, and others. Also well known was also the SA-1, used in Super Mario RPG - but that was just a super-charged 65816,the same processor the base SNES has. Several games included DSPs - Super Mario Kart, Pilotwings, Dungeon Master, and the Mega Man X games. Most surprisingly, an obscure Japanese Shogi game features an ARM!

* Also, the SNES' Super Game Boy included the Game Boy's CPU, a 8080/Z80 variant. Many old video games consoles had multiple general purpose processors, often with different architectures.

* The Sega Genesis had a 68000 and a Z80.

* The Sega 32X (an add-on for the Genesis) added a two SH2s (32-bit RISC CPUs), giving the system four CPUs and 3 different architectures. Adding the Sega CD on top gave it another 68000, for a five (!) core system.

* The Sega Saturn likewise had two SH2s and a 68000.

* The SNES' sound processor, the SPC-700, was effectively a full-fledged programmable 8-bit CPU, although it was almost always used for nothing but sound.

* SNES games frequently came with additional CPUs on their cartridge. Most famous was the SuperFX in Star Fox, Yoshi's Island, and others. Also well known was also the SA-1, used in Super Mario RPG - but that was just a super-charged 65816,the same processor the base SNES has. Several games included DSPs - Super Mario Kart, Pilotwings, Dungeon Master, and the Mega Man X games. Most surprisingly, an obscure Japanese Shogi game features an ARM!

* Also, the SNES' Super Game Boy included the Game Boy's CPU, a 8080/Z80 variant. &amp;gt; The Sega Saturn likewise had two SH2s and a 68000.

I'm guessing the 68000 was for backward compatibility?     X86 would not run.  Dissimilar CPUs add enormous amounts of complexity and none of the X86 motherboards, and operating systems (maybe linux has been tweaked?) support it.  AMD and Intel CPUs have slightly different instruction sets, so even if different clock rates or models from the same manufacturer were supported its unlikely someone would support both AMD and Intel CPUs in the same machine.

As others have mentioned its theoretically possible (even for X86), and other architectures have done it commercially.  But its much simpler to just require all the same CPUs.  You tend to see this more where CPUs are used for more specialized purposes.  The original Nintendo had a separate CPU to run sound while the main one ran graphics for example. Don't you remember mathematical co-processors for 386 and 486? Don't you remember mathematical co-processors for 386 and 486?  </snippet></document><document><title>Qbit kept "alive" for 3 minutes - a huge step-up from mere seconds. </title><url>http://www.extremetech.com/computing/130683-researchers-break-record-keep-quantum-computing-qubit-alive-for-3-minutes</url><snippet>  I'm so happy, but I have no one I could explain it to I'm so happy, but I have no one I could explain it to  What's the practical application of:

&amp;gt;A qubit in a quantum computer is capable of being both 1 and 0 at the same time

? You can structure your computation to perform on both values simultaneously, as opposed to now where you need to perform them in sequence, one after the other.

If you have two qubits, you can perform computations on the following values simultaneously:

    00
    01
    10
    11

And you can see as you add more qubits, the parallelization increases exponentially.  The final computation you extract can only be the computation you performed on one of those cases, so you need to construct your algorithm in such a way that the case you finally observe is the case that yields the correct answer.  The way that can be achieved is to simply run the algorithm numerous times and go with the answer that has the highest occurence.  Doing that will often still be much much faster than performing every combination in sequence. So in essence you could run four threads through the same two qubits at the same time? I guess a way of saying it would be you can run four threads at the same time on two qubits, however, the threads are all independent, so they can't talk to one another, they're also limited as to what they can compute, so they're not full blown general purpose threads, and at the end you will only see the result of one of the threads that executed, so you need to structure your algorithm so that the thread that you do 'observe' is likely to contain the right answer.  And given that this is only likely to be the right answer, you'll want to run it multiple times.

This is why quantum computing is amazing, but only provides speed ups to certain problems, specifically problems that can be described by a quantum process.  It's not like you can just take any algorithm and run it in a quantum computer and get exponential parallelization. Just curious, are there any practical problems that are already known or expected to remain hard even with quantum computers? Just curious, are there any practical problems that are already known or expected to remain hard even with quantum computers? What's the practical application of:

&amp;gt;A qubit in a quantum computer is capable of being both 1 and 0 at the same time

?  What I find really interesting is that they did it in silicon. Elaborate, if you will. I'm curious as to why you noted that.
 Since silicon is already very abundant and used in most technological hardware, the fact that this quantum experiment also utilized silicon shows that this cheap resource can still be useful for a new breed of computers. 

If we had to use some rare element for quantum computers to work, then it wouldn't be as accessible as modern computers already are now.  Elaborate, if you will. I'm curious as to why you noted that.
 The stability of qubits varies quite a bit with the type of qubit you are using. I work with a group using trapped ions, and those have no problem with keeping the qubits around for a long time.    The whole idea of quantum computing has always sounded like a load of bunk to me. I think it's safe to say that the time that a quantum computer can do anything remotely interesting will never come.</snippet></document><document><title>Help with Prior distribution for Hawkes Process (Poisson process with time dependent intensity)</title><url>http://www.reddit.com/r/compsci/comments/uypxg/help_with_prior_distribution_for_hawkes_process/</url><snippet>Hi guys,

I'm working on a project right now that involves running a Markov Chain Monte Carlo analysis on a Poisson cluster process (hawkes). I'm a little confused with how to write the acceptance ratio, however. For reference, there are three parameters involved in this process, and the intensity function relies on t (more on that later)

First off, I've read what appears to indicate that acceptance occurs if a random uniformly distributed number on [0,1) is larger than:

(p(y\*|x) / j(y\*|y)) 

/

(p(y|x) / j(y|y\*)

where p represents the probability of generating our data (x) using the posterior distribution given by y, and j represents the probability of getting to y\* from y. In simple list form

* p() = posterior distribution
* j() = chance of going from t -&amp;gt; t*
* y = current parameters (current state of markov chain)
* y\* = proposed parameters (proposed next state of markov)
* x = data point (I'm using a discrete set of data as opposed to a distribution)

I have a hunch that I'm using the notations wrong, as this is one of the first times I've really dealt with Bayesian statistics

Now, my understanding is that the posterior distribution is based on a prior distribution multiplied times a likelihood function, L.

So, the formula I'm working with has the probability of an event happening at time t depending partly on the summation of t&amp;lt;ti of (t-ti) (the summation of all t-tis for all tis smaller than t) where tis are the times of events that have happened prior to t.

I'm really confused with how to generate the prior distribution. My understanding is that for a prior in a situation like this, one simply uses "dummy" parameters for the distribution. Would I effectively use dummy parameters and sum the probabilities of each data point occurring for each point in the data set, and call that the prior? As near as I can tell, that would just give me some kind of constant, and I don't think the prior is supposed to be a constant.

Thanks for reading this far, and if anything I've said is unclear, ask for clarification! (if you want to help me that is) I'm a novice to bayesian statistics, and I probably misunderstood either a definition or messed up the notation.  I'm afraid I am a bit confused with what you're trying to say in the last 3 paragraphs, but maybe this will clarify,

Your acceptance ratio will be 

p(x,y * ) j(y | y *)
        /
p(x, y) j(y *| y)

Assuming it is easy to calculate the prior on y and the likelihood of x given y, we can factor

p(x,y) = p(y) p(x | y)

Thus, every time you propose a new sample y* from j(y* | y), all the arguments in the ratio should be different.

Does that make sense?  I am not certain what p(y) or p(x | y) is in your model, but if you can get a grasp of those (and are certain you truly are sampling from j(y* | y) and calculating its likelihood properly), the algorithm should function.  I am a junior CIS major and I understood literally none of that except for Monte Carlo, assuming you're talking about a Monte Carlo method.  
Nothing constructive to add, just saying. I am a junior CIS major and I understood literally none of that except for Monte Carlo, assuming you're talking about a Monte Carlo method.  
Nothing constructive to add, just saying.</snippet></document><document><title>Who wants to help me solve a 17th century code?</title><url>http://www.reddit.com/r/compsci/comments/uwrvq/who_wants_to_help_me_solve_a_17th_century_code/</url><snippet>[Edit 2: The Phrase "Compsci Nerds" was removed as it has been deemed insensitive to virgins.]

Gordon Code (Parts 1-3): http://imgur.com/a/isCoU 

Remember me, r/compsci? My name is Matthew Kaminski, and I'm a history student at the Masters Program at UMass Boston. The subject of my thesis is the role of John Rowe and the BSETC merchants in fomenting the Stamp Act riots, non-importation movement, and Tea Party. Several months ago I posted a low-quality photocopy of a Masonic Code on a letter located in the Massachusetts Historical Society to the /compsci subreddit. (Original Here: http://www.reddit.com/r/compsci/comments/twl6j/who_wants_to_help_me_crack_an_18th_century/).  You [Sterling examples of masculinity] did the whole proof of identity thing then, so don't question my existence or credentials. The letter was written by William Gordon, a semi-successful Author, Preacher and Freemason. The letter has a complex code written in the margins, which seems to have several possible solutions. I just got back from the MHA and, after getting permission, took some higher quality photographs for you to examine and perhaps solve. If anyone wants more information on Gordon or wants to read Gordon's book on algorithmic code creation (available online), I would be happy to come back in a few minutes and add some comments. The Gordon Code is available here: http://imgur.com/a/isCoU

[EDIT] TLDR: Help me solve this masonic code and win the respect of the Reddit Community! (Warning: Prize does not actually exist.)
[EDIT] Here are the original low quality scans, to help orient the viewer: http://imgur.com/a/rfLtk  There are two issues here: the first is converting the worn and hand written glyphs to something legible and/or computer readable, then decrypting the message.  You are the only one qualified to handle the first, and I believe that basically everyone in the previous thread told you to do that.  Yet you've returned with basically the exact same situation, albeit higher quality photos!

If you really want help, you need to take the bits you have, develop an alphabet, e.g.

glyph1 = X; glyph2 = Y; glyph3 = X ...

and then after completing the vocabulary you need to transliterate the texts you want decrypted from the handwritten notes to the computer-readable alphabet.  

Once you do this, you can just post the transliterated texts to r/compsci, r/programming, r/machinelearning and whatever other sites you like and I can all but guarantee that you will have a billion answers (of varying quality) in a couple of days, as well as a couple of people that will become very interested in helping you take it further.

At the moment however, you are basically asking for us to do everything for you, including this bit that your experience, not ours, is first and best suited to. Everyone in the previous thread asked me for higher quality photos.  If you just wanted to be the first person to say the same things everyone else said in the last thread, you aren't really being helpful.  I'll do that as soon as I have time. I was on your side until this comment.
 [deleted] [deleted] [deleted] Everyone in the previous thread asked me for higher quality photos.  If you just wanted to be the first person to say the same things everyone else said in the last thread, you aren't really being helpful.  I'll do that as soon as I have time. Could you post a transcript of the letter itself, the plaintext part?  It may not be relevant, but then again it could be...  &amp;gt; sterling examples of masculinity

Token female cs nerd here. Was that really necessary? I'm sorry for making an off topic comment, but this comment of OP's is even more off topic. That's actually a really good point. Sorry for the misogyny, my girlfriend was yelling at me the entire time I was posting this and it really made me cranky to read some of the comments.  I really am a feminist, and I apologize to anyone I offended with my tasteless comments.   Your tone has been derogatory from the start, and whenever someone asks for help to help you, you come across as rude and/or needlessly defensive. If they're going to help you, then *of course* you're going to need to present it in a format they can use. You can't just say "hey monkeys here code solve now." and sit back smugly.  I am interested, but I am a novice at this I will never understand why people who show a desire to learn, even if it's far beyond their skills, receive a downvote for expressing just that. Here's an upvote and never stop being interested.  I will never understand why people who show a desire to learn, even if it's far beyond their skills, receive a downvote for expressing just that. Here's an upvote and never stop being interested.   [deleted] This has nothing to do with my thesis.  This is an interesting code that maybe people who don't 1) Teach Full Time 2) Volunteer 15 hours a week 3) Need to work on a Masters Thesis, might want to solve.  I took time to go make high quality digital photos because everyone who commented on the last post wanted them.  I am not going to profit from this in any way, and I've already spent hundreds of hours [EDIT: Maybe 100 Hours] trying to do what he recommended.  Go back and read the comments on the last post, he really just stole what everyone else said and didn't add anything new.  Sorry about my tiredness and rudeness, but he didn't add anything new and if it were that easy I wouldn't need to post it.  It's probably an algorithm from the 1700s, I thought you guys might be interested but never mind. I'll solve it this summer and Reddit won't get any credit.    What it sounds like you're saying is "Hey, I found this thing that can be beneficial to my degree. I don't have the slightest clue of how to solve it, and I'm not willing to try or learn for myself what is needed to solve it. I ask you to do this for me. I will be offended if you ask me to have some part in deciphering this text, so don't ask me."

Go fuck yourself. I don't know why I'm still responding to these, but here goes:
1) I apologize for being a jerk, but I was told to come back with higher scans and that's why I got angry when that person said. "All you've done is come back with higher scans."  That's what I was asked to do.  I upvoted everyone who said this in the first post: "http://www.reddit.com/r/compsci/comments/uwrvq/who_wants_to_help_me_solve_a_17th_century_code/c4zal8r" because they were being helpful, but I also said I've been trying to do exactly that for months.  I'm not the only person uniquely qualified to make a transliteration, because the reason I'm here IN THE FIRST PLACE is that I'm not sure if certain symbols are the same and written differently or are actually different.  That's why I published it, so people can do it if they want.  For instance, are these all the same symbols or should I count them differently?  http://imgur.com/a/j6BQk#16

2) This is completely unconnected to my Masters Degree, and I'm not making any money off it.  In fact, by making this code public before I solve it, I might be losing the ability to publish it in a historical journal and get actual credit for it.  If I sat on this and didn't publish it here, I guarantee I could write and publish a real book on William Gordon soon.  That probably won't happen now.

3) My dickishness notwithstanding, if you go back and read the comments on the original post from a month ago, people were asking me to do exactly what this person is saying I should do, and quite frankly I can't.  I know that to solve the code you need to transcribe the code and put it into a context that can be analyzed, but there are too many symbols with slight variations to effectively put together a concrete syllabary and transliteration.  Again, I've tried and failed to do what he requested. 

4) This is a gift. I am not asking anyone to do any work.  Since I'm not demanding you do anything, by the same token I don't owe you anything. If you don't want to independently try to solve the code, then don't.  I took time out of my incredibly busy schedule to come back to this community with the higher quality scans that were requested from me, so that someone with more time than me could spend the insane amount of time required to solve it.  This is a unique opportunity to do something that no one else has done in the field of history.  If you don't want to crack the code on your own, then don't try.  I think I got it.  It says "Be sure to drink your Ovaltine."    I'm puzzled. Since you're in Boston, why not attempt to collaborate with people there? Its not like the city isn't literally full of universities and I'm certain there are linguists and cryptographers (check for people in the math dept.) willing to (i) understand the glyphs and (ii) decode any possible encryption.  Good idea actually.  I've been trying to get some historians involved but obviously they're not too interested.  I'll see what I can do, a lot of my BU friends were engineers and linguists so that might be a better bet.   </snippet></document><document><title>How do researchers find topics to write about?</title><url>http://www.reddit.com/r/compsci/comments/uvvfi/how_do_researchers_find_topics_to_write_about/</url><snippet>I am going into my second year of a CS program at my university, and I was hoping to spend my summer writing a very simple, introductory paper on some small topic in theoretical computer science (particularly logic, computation, or discrete mathematics). Since it's my first bit of academic writing, I don't expect it to be some grand work... in fact, I'm hoping to end up with maybe 2-4 pages total, depending on the topic I choose.  
  
My main problem is that I have no idea where to look for topics that I am capable of taking on. I've been reading books in my spare time on computation, but have taken no courses yet, so my understanding is very rudimentary. However, if I find something that suits me, then I would have quite a bit of time to get some background information and maybe write an expository piece on the topic.  
  
So, how can I find small questions to pursue over the summer? It seems to me that so much has been done in computation and logic that there's nothing I could contribute, but hopefully something is out there. Thank you for your advice!  My algorithm for finding topics is something like this:

1. read lots of papers
2. keep reading
3. when I find myself saying "why didn't they just..." or "nah, there's a better way", stop. My algorithm for finding topics is something like this:

1. read lots of papers
2. keep reading
3. when I find myself saying "why didn't they just..." or "nah, there's a better way", stop. My algorithm for finding topics is something like this:

1. read lots of papers
2. keep reading
3. when I find myself saying "why didn't they just..." or "nah, there's a better way", stop. and where do you usually find such papers? and where do you usually find such papers?  The easiest way to find a question to work on is to go to a professor in your department who is researching something that interests you and ask them if you can help them on something, or if they have some ideas for you to work on. Since you're going into your second year, you may not have enough CS knowledge to work on a research project, but at the very least a professor could suggest topics to look into, and most will probably welcome your interest (and see you as a potential research assistant down the road). instead of a professor, ask someone more likely to be doing the work behind the research, i.e. assistants or grad students.  Is it a strong requirement that you do novel work? That's quite hard, because it means you have to know the literature thoroughly to be sure nobody worked on your topic before.

Have you considered writing surveys of existing work instead? Take a subject you're interested in, seriously read as much papers as possible, and write a survey detailing the historical progression of the research (where did it start from? did the goal change over the course of the research? did new questions, or new application areas appear? what are the current hopt topics?). You are guaranteed to benefit a lot from this personally; other practitioners of the field may also like it. 

(For this to work best, you must choose a suitably restricted domain. "logic" or "type systems" is too large a subject for a reasonable survey. "separation logic" or "type inference" may be a more reasonable subdomain, though probably still a bit too large; you'll find out when diving in, in which subtopics you could specialize.)
  My algorithm is different from others:

1. Read a lot of papers.
2. Implement something which you find is cool.
3. As you adapt it to a problem you are interested in, figure out how can you improve the algorithm.
4. Try to figure out formally, what you did, and see if it been done before. If not, then publish!  What sort of things do you do when you implement it? Code something in python or a quick language?        </snippet></document><document><title>Need help identifying a type of hill climbing algorithm</title><url>http://www.reddit.com/r/compsci/comments/uuty7/need_help_identifying_a_type_of_hill_climbing/</url><snippet>So there are plots and a more detailed explanation of what I did here:

http://www.wandertechnologies.com/2012/05/12/272/

But basically I was winging it implementing a stochastic hill climbing algorithm for minimizing a cost function, and later realized that I did it completely wrong, but in a way that worked very very well on noisy surfaces with lots of local minima and maxima, albiet slowly. 

I have trouble believing that I just made this up, but sort of need to know what it is called so I can properly reference what I did in a paper I am working on.  Any guesses?  Hey - it sounds like what you're doing is [random optimization](http://en.wikipedia.org/wiki/Random_optimization).

From that wikipedia article:

&amp;gt;The basic RO algorithm can then be described as:
&amp;gt;
* Initialize x with a random position in the search-space.
* Until a termination criterion is met (e.g. number of iterations performed, or adequate fitness reached), repeat the following:
 * Sample a new position y by adding a normally distributed random vector to the current position x
 * If (f(y) &amp;lt; f(x)) then move to the new position by setting x = y
* Now x holds the best-found position.

Though that's for minimizing, but it's obviously the same for maximizing except that you use the condition f(y) &amp;gt; f(x).

If you're curious, I found this by googling "maximization local search normal distribution". It came up as the first result :) (but I tried a lot of other searches that didn't find it).  Strictly speaking, I don't even think this is a hill climbing algorithm.
Since you move in multiple dimensions at a time, you lose that class...

It certainly isn't simulated annealing, this modifies the score associated with a space at a certain time point by taking it to the power of a decreasing geometric function; you don't do that.

I can certainly draw parallels between what you do and other methods, but they really aren't the same.

**My 'verdict'**:
What you're doing appears to be "finding the maximum in a random sample of size n".

I say random sampling because your method doesn't really take any advantage of the space around the current point, and it only accepts a new point if it is better than the previous sample.

Hope I helped :)!

**edit**:

Also, look at the places where you searched...
Many of the spaces are at the edges of the search space.
Maybe if you were to sample your scaling parameters from a smaller distribution, you wouldn't be searching so far away... Definitely, this clearly is not my background, so it's all very interesting to me at this point.  Thanks  maybe you're thinking simulated annealing? Maybe.  That still seems to be dependent on neighbors though, as opposed to 'jumping' beyond neighbors stochastically to check spots 'near' but not necessarily 'adjacent' to the current location. Are you sure?

http://en.wikipedia.org/wiki/Simulated_annealing

&amp;gt; By analogy with this physical process, each step of the SA algorithm attempts to replace the current solution by a random solution (chosen according to a candidate distribution, often constructed to sample from solutions near the current solution). The new solution may then be accepted with a probability that depends both on the difference between the corresponding function values and also on a global parameter T (called the temperature), that is gradually decreased during the process.
 Are you sure?

http://en.wikipedia.org/wiki/Simulated_annealing

&amp;gt; By analogy with this physical process, each step of the SA algorithm attempts to replace the current solution by a random solution (chosen according to a candidate distribution, often constructed to sample from solutions near the current solution). The new solution may then be accepted with a probability that depends both on the difference between the corresponding function values and also on a global parameter T (called the temperature), that is gradually decreased during the process.
  If I've read this correctly then you're using a stochastic hill climbing algorithm with random restarts, except that instead of taking the first random fitness landscape location you find, you poll random places on the landscape until you find one higher than your current fitness value? (With the locations of where to poll influenced by some knowledge of the landscape - that is where you say if a peak isn't found in one direction then you take and random location in the other).

You compare your algorithm to a single run of a stochastic hill climbing algorithm which seems like an unfair comparison. You might like to compare your algorithm to stochastic hill climbing with (purely) random restarts and see how they do. I have a feeling the purely random restarts will do better. For example you'd expect (given that a stochastic hill climb succeeds 56% of the time) to need only 1 restart this way. This is significantly fewer than the number of green dots for the jumps of your algorithm in the diagram.  </snippet></document><document><title>How long does my unique key have to be?</title><url>http://www.reddit.com/r/compsci/comments/uuhm8/how_long_does_my_unique_key_have_to_be/</url><snippet>I'm making a distributed system and every node needs a unique key. I want the system to be salable to hypothetically millions if not billions of nodes. I'm thinking a 32 digit hexadecimal number. Is this long enough or should I go with something longer?  It sounds like what you need is a [UUID](https://en.wikipedia.org/wiki/UUID).  They're 128 bits, so basically it's exactly what you're already thinking of doing. This. Unique id generation is essentially a solved problem; I'm honestly a little surprised that OP managed to learn about distributed systems without encountering the concept of uuids. It sounds like what you need is a [UUID](https://en.wikipedia.org/wiki/UUID).  They're 128 bits, so basically it's exactly what you're already thinking of doing. Maybe. It depends on the type of distributed system. UUIDs, for example, are not Byzantine fault tolerant. We need more information. What's Byzantine fault tolerance? What's Byzantine fault tolerance? A Byzantine fault is an arbitrary fault that occurs during the execution of an algorithm by a distributed system. It encompasses both omission failures (e.g., crash failures, failing to receive a request, or failing to send a response) and commission failures (e.g., processing a request incorrectly, corrupting local state, and/or sending an incorrect or inconsistent response to a request). When a Byzantine failure has occurred, the system may respond in any unpredictable way, unless it is designed to have Byzantine fault tolerance. [wikipedia]
 Thanks.  So it's related to that Byzantine general problem?  I think I was told that that problem is unsolvable&#8212;does that mean Byzantine fault tolerance is just a practical probabilistic (i.e. the ability to say it won't fail 99.99999 percent of the time or whatever) or there a method that gives an true guaranteed solution? It sounds like what you need is a [UUID](https://en.wikipedia.org/wiki/UUID).  They're 128 bits, so basically it's exactly what you're already thinking of doing.     Is the node a connected computer system? If so, the system's network has a layer 2 (osi model) address called a MAC address. It is unique. You could do a system call to get it and use that. Is the node a connected computer system? If so, the system's network has a layer 2 (osi model) address called a MAC address. It is unique. You could do a system call to get it and use that. Is the node a connected computer system? If so, the system's network has a layer 2 (osi model) address called a MAC address. It is unique. You could do a system call to get it and use that.  If your system requires an online signup, you can assign each customer a unique id using the automatic identity field type of your SQL database.  Or simply use their email address as the unique key.

The SID of the processor or the mac address of their network card should also be unique values you can leverage in your software.  You can get by with shorter strings if you use base 36 (numbers and letters). 36^8 is about 2.8 trillion, 36^6 is 2.2 billion. 9 hex digits gets you past ten billion, if you prefer that though.

edit: Really? For suggesting a string key? This subreddit is absurd. That would be fine in a world where digits all took up equal space. We could get rid of binary computers all together and make all our processors work in base 700!

We're not there yet, and you're suggesting wasting bits for no benefit. You can get by with shorter strings if you use base 36 (numbers and letters). 36^8 is about 2.8 trillion, 36^6 is 2.2 billion. 9 hex digits gets you past ten billion, if you prefer that though.

edit: Really? For suggesting a string key? This subreddit is absurd. Not sure if adding non-numeric is a bad idea or what.

But if it isn't a problem, add both lower and upper case letters as well as digits and you have 62 characters (26+26+10).

Make it 16 characters long and you have 62^16 possibilities.

That is 47 *octillion* possibilities.  16^32 = 340282366920938463463374607431768211456

I'm pretty sure you won't need that many, but basically just go up powers of 16 until you have enough nodes, then that is the number of hexadecimal digits you want.

A long int in C is at least 32 bits, which gives 2^32 = 4294967296 unique ids. I'd imagine that would be plenty for most systems. I know that's enough nodes lol. But is it still very unlikely that there would be a overlap given millions of nodes. Can you not just guarantee that each node will be given a unique key? For example, assign your first node the id 0, your second node 2 etc. It would be much safer and more efficient than using random numbers if that's what you're planning. Or do they need to be random for security reasons?</snippet></document><document><title>Kites on a zebra crossing: an algorithmic challenge</title><url>http://fiber-space.de/wordpress/2012/06/09/kites-on-a-zebra-crossing-an-algorithmic-challenge/</url><snippet>  Anyone care to explain that in a bit simpler terms? I know graphs a bit, but still got lost when he complicates the situation. Yeah I think the stripes and stuff make it harder than it needs to be. The example problem given is equivalent to the one in this image: http://i.imgur.com/whoeN.png

Is there a path from the bottom to the top such that the sequence of letters you pass over can be completely "cancelled"? You can cancel out two consecutive letters if they're lowercase and uppercase of the same letter. So in this case, `abBA` can be completely cancelled, by first cancelling the B's then the A's. `aABb` would also work if it were possible. Sequences `abAB`, `aBBA`, and `abB`, for instance, cannot be completely cancelled. Yeah I think the stripes and stuff make it harder than it needs to be. The example problem given is equivalent to the one in this image: http://i.imgur.com/whoeN.png

Is there a path from the bottom to the top such that the sequence of letters you pass over can be completely "cancelled"? You can cancel out two consecutive letters if they're lowercase and uppercase of the same letter. So in this case, `abBA` can be completely cancelled, by first cancelling the B's then the A's. `aABb` would also work if it were possible. Sequences `abAB`, `aBBA`, and `abB`, for instance, cannot be completely cancelled.  This question is not very well defined. How do the number of edges and number of stripes grow with N? And what about K and the typical number of values per node? Are these fixed? Is a solution that's exponential in K okay?

It would be great if an example problem that's possibly hard would be provided. The number of edges and stripes is not fixed but only delimited by N. The question about K is a good one. K is assumed to be fixed while N may vary. I will add this information in the article.

 &amp;gt; The number of edges and stripes is not fixed but only delimited by N.

Er, exactly. You need to specify how they grow with N. If the number of stripes grows linearly with N, you can do it with dynamic programming. I don't see how dynamic programming could help. Even if the *width* of the stripes ( the maximum number of vertices in a stripe ) is fixed, and the number of stripes grows linearly with N, there might be no subgraph which is a problem solution. This due to words of the kind `aA ... Bb` in terms of your problem description. Maybe you could elaborate a little what you had in mind? Yep I see what you're saying. I wouldn't mind trying to solve the simplest non-trivial case of this problem before trying the full general problem. Would you agree that the following case is as simple as can be without having an obvious solution?

_K = 1. Each stripe has exactly 2 vertices. Each vertex in each stripe is connected to each vertex in the following stripe. Each vertex's number list has either 0 or 1 entries._  Let&#8217;s assume that each vertex numlist is of size 1. We can reduce your problem to that one (giving up the assumption that the DAG is layered as you describe). Then we can solve the problem by dynamic programming as follows.

Define P(u,v) to be the predicate &#8220;there is a valid path from u to v&#8221;. Then P(u,u) is false for all u.

And, isn&#8217;t the following claim true?

Claim: for u != v, P(u,v) is true if and only if one of the following three conditions holds:

i) There is a an edge (a,b) such that P(u,a) is true and P(b,v) is true.

ii) The label of u and the label of v sum to zero, and there are edges (u,a) and (b,w) such that P(a,b) is true.

iii) The label of u and the label of v sum to zero, and there is an edge (u,v).

If indeed this claim holds, it gives you a dynamic programming algorithm running in time, say, O(N^4 ). You can probably improve that.

[edit: added option (iii), fixed option (i)]
 The following label list doesn't fit into the categories i) - iii) 

`[1, -2, 2, -1, -1, 2, -2, 1]`

if we assume that outermost labels ( which are bot `1` ) are assigned to `u` and `v` respectively. It nevertheless describes a path which can be reduced to []. Doesn't it fit into category (i)?  That is, it can be broken into two lists each of which is valid:

      [1, -2, 2, -1]  +  [-1, 2, -2, 1]

(Here the edge (a,b) mentioned in category (i) would be the edge between the two vertices labeled -1.)

 Sure, P(u,a) and P(b,w) are both true but P(a,b) isn't because its label set is [-1,-1] which is not reducable. However this implication might not be necessary. For condition (i), (a,b) only needs to be an edge.  P(a,b) is not part of the condition.

edit:  To explain, a list of labels is valid iff either

i) it is the concatenation of two valid lists of labels

ii) its first and last labels sum to zero, and the remaining list in the middle is a valid list,

iii) or, its first and last labels sum to zero, and the list has length two. Let&#8217;s assume that each vertex numlist is of size 1. We can reduce your problem to that one (giving up the assumption that the DAG is layered as you describe). Then we can solve the problem by dynamic programming as follows.

Define P(u,v) to be the predicate &#8220;there is a valid path from u to v&#8221;. Then P(u,u) is false for all u.

And, isn&#8217;t the following claim true?

Claim: for u != v, P(u,v) is true if and only if one of the following three conditions holds:

i) There is a an edge (a,b) such that P(u,a) is true and P(b,v) is true.

ii) The label of u and the label of v sum to zero, and there are edges (u,a) and (b,w) such that P(a,b) is true.

iii) The label of u and the label of v sum to zero, and there is an edge (u,v).

If indeed this claim holds, it gives you a dynamic programming algorithm running in time, say, O(N^4 ). You can probably improve that.

[edit: added option (iii), fixed option (i)]
 FWIW, here is a Python implementation:

    #!/usr/bin/env python2.7
    
    from functools import wraps
    
    def memo(func):
        cache = {}
        @ wraps(func)
        def wrap(*args):
            if args not in cache:
                cache[args] = func(*args)
            return cache[args]
        return wrap
    
    def first(generator):
        for x in generator: return x
    
    class DAG:
        def __init__(self, edges, vertex_labels):
            self.E = edges
            self.V = set(u for (u,w) in edges) | set(w for (u,w) in edges)
            assert all(isinstance(vertex_labels[v], int) for v in self.V)
            self.labels = vertex_labels
            
            self.in_nbrs = {v:set() for v in self.V}
            self.out_nbrs = {v:set() for v in self.V}
            for (u,w) in edges:
                self.out_nbrs[u].add(w)
                self.in_nbrs[w].add(u)
    
            self.ranks = {}
    
            # rank vertices in topological order
            visited = {v:False for v in self.V}
            self.rank = len(self.V)
            def dfs(u):
                visited[u] = True
                for w in self.out_nbrs[u]:
                    if not visited[w]: dfs(w)
                self.ranks[u] = self.rank
                self.rank -= 1
            for u in self.V:
                if not visited[u]: dfs(u)
    
        def find_valid_path(self, source, sink):
            @memo
            def P(u,w):
                if self.ranks[u] &amp;gt;= self.ranks[w]: return None
                if self.labels[u] == -self.labels[w]:
                    if (u,w) in self.E: return [u,w]
                    pair = first((a,b)
                                 for a in self.out_nbrs[u]
                                 for b in self.in_nbrs[w]
                                 if P(a,b) != None)
                    if pair: return [u] + P(*pair) + [w]
                pair = first((a,b)
                             for (a,b) in self.E
                             if self.ranks[u] &amp;lt; self.ranks[a]
                             and self.ranks[b] &amp;lt; self.ranks[w]
                             and P(u,a) and P(b,w))
                if pair: return P(u,pair[0]) + P(pair[1],w)
    
            return P(source, sink)
    
    G = DAG(edges = set((i,i+1) for i in range(7)),
            vertex_labels = [1, -2, 2, -1, -1, 2, -2, 1])
    
    print G.find_valid_path(0, 7)
     Here's another, perhaps more efficient:


    #!/usr/bin/env python2.7
    
    #http://www.reddit.com/r/compsci/comments/ut6rk/kites_on_a_zebra_crossing_an_algorithmic_challenge
    
    from collections import deque, defaultdict
    
    class EdgeSet:
        def __init__(self, edges = set()):
            self._edges = dict()
            self._into = defaultdict(set)
            self._outof = defaultdict(set)
            for e in edges: self.add(e)
        def add(self, (u, w), edge_label=True):
            self._edges[u,w] = edge_label
            self._outof[u].add((u,w))
            self._into[w].add((u,w))
        def into(self, v): return self._into[v]
        def outof(self, v): return self._outof[v]
        def get_label(self, u, w): return self._edges[u,w]
        def __iter__(self): return self._edges.__iter__()
        def __contains__(self, edge): return edge in self._edges
    
    class DAG:
        def __init__(self, edges, vertex_labels):
            self.edges = EdgeSet(edges)
            self.vertices = set(x for e in edges for x in e)
            self.labels = {v:vertex_labels[v] for v in self.vertices}
            self.outof_with_label = defaultdict(set)
            for (u,w) in edges:
                self.outof_with_label[u, vertex_labels[w]].add((u,w))
    
        def find_valid_path(self, source, sink):
            valid_pairs = EdgeSet()
            Q = deque()
    
            class Found(Exception):
                pass
            
            def add_to_valid_pairs(pair, witness):
                if pair in valid_pairs: return
                valid_pairs.add(pair, witness)
                Q.append(pair)
                if pair == (source, sink): raise Found
    
            try:
                for u in self.vertices:
                    for (u,w) in self.outof_with_label[u, -self.labels[u]]:
                        add_to_valid_pairs((u, w), [u,w])
    
                while Q:
                    x,y = Q.popleft()
                    
                    for (a,x) in self.edges.into(x):
                        for (y,b) in self.outof_with_label[y, -self.labels[a]]:
                            add_to_valid_pairs((a, b), [a, (x,y), b])
    
                    for (y,a) in self.edges.outof(y):
                        for (a,b) in valid_pairs.outof(a):
                            add_to_valid_pairs((x, b), [(x,y), (a,b)])
    
                    for (b,x) in self.edges.into(x):
                        for (a,b) in valid_pairs.into(b):
                            add_to_valid_pairs((a, y), [(a,b), (x,y)])
    
            except Found:
                def expand_path(u,w):
                    for x in valid_pairs.get_label(u,w):
                        if isinstance(x, tuple):
                            for v in expand_path(*x): yield v
                        else:
                            yield x
                return list(expand_path(source, sink))
    
    G = DAG(edges = set((i,i+1) for i in range(7)),
            vertex_labels = [1, -2, 2, -1, -1, 2, -2, 1])
    
    print G.find_valid_path(0, 7)
  [Is this your blog?  I don't know if I should make a comment there, or here...]

My instinct tells me this is NP-complete.  Or worse!  (check out the Post Correspondence Problem)

A few reductions might make this simpler:

1.  Change to a single number per vertex.  Verticies with an empty list can be changed to two vertices that immediately cancel (e.g. [-1] --&amp;gt; [1]), and the connectivity from the vertices going into that vertex will be distributed to the vertices pointed to by that vertex.  Vertices with a list of *n* numbers will add *n*-1 layers, one per element in that list.  example:  --&amp;gt; [-3,1,2] --&amp;gt;   becomes  --&amp;gt; [-3] --&amp;gt; [1] --&amp;gt; [2] --&amp;gt;.

2. Do away with the Zebra part.  Solving this for an arbitrary kite is a polynomially equivalent problem.


I'm not sure where to go from there, but this is a delicious problem, and I will be thinking about it for the rest of the weekend at least!

EDIT:  (another random thought)  This sort of problem reminds me of group theory, specifically of "free groups."  A valid path must result in the zero element in the additive group of integers... [Is this your blog?  I don't know if I should make a comment there, or here...]

My instinct tells me this is NP-complete.  Or worse!  (check out the Post Correspondence Problem)

A few reductions might make this simpler:

1.  Change to a single number per vertex.  Verticies with an empty list can be changed to two vertices that immediately cancel (e.g. [-1] --&amp;gt; [1]), and the connectivity from the vertices going into that vertex will be distributed to the vertices pointed to by that vertex.  Vertices with a list of *n* numbers will add *n*-1 layers, one per element in that list.  example:  --&amp;gt; [-3,1,2] --&amp;gt;   becomes  --&amp;gt; [-3] --&amp;gt; [1] --&amp;gt; [2] --&amp;gt;.

2. Do away with the Zebra part.  Solving this for an arbitrary kite is a polynomially equivalent problem.


I'm not sure where to go from there, but this is a delicious problem, and I will be thinking about it for the rest of the weekend at least!

EDIT:  (another random thought)  This sort of problem reminds me of group theory, specifically of "free groups."  A valid path must result in the zero element in the additive group of integers...  </snippet></document><document><title>Why Do Some Programming Languages Live and Others Die? | Wired Enterprise | Wired.com</title><url>http://www.wired.com/wiredenterprise/2012/06/berkeley-programming-languages/</url><snippet>  I also like Paul Graham's writing on the topic http://paulgraham.com/popular.html When I read this, I think that Clojure is the language that he is describing. Maybe the only missing element is the killer app, the popular program written in Clojure that makes everybody want to use it. I believed he used either CLisp or Arc while he was managing Viaweb. Clojure appeared after his time. I mean, Clojure meets the goals he lays out in many ways.  I am not too familiar with Clojure, but does it add anything else significant to the Lisp family beyond concurrency and the JVM? When I read this, I think that Clojure is the language that he is describing. Maybe the only missing element is the killer app, the popular program written in Clojure that makes everybody want to use it.</snippet></document><document><title>microprocessor design (lecture,etc) with complete code?</title><url>http://www.reddit.com/r/compsci/comments/usi8w/microprocessor_design_lectureetc_with_complete/</url><snippet>Hi i have been studying microprocessor  and computer architecture, do you know were i can download a complete project (vhdl,verilog,etc) ?

By complete project i mean control-unit, ALU, Program counter, ROM and instruction set.

Thanks for read !   There's a new course at MIT which ran this spring which had a lot of code for various bits of a computer architecture. The code is in bluespec.

[6.S078](http://csg.csail.mit.edu/6.S078/6_S078_2012_www/index.html)      where are you studying for that specific area? microarchitecture using the harris &amp;amp; harris book. </snippet></document><document><title>Crypto breakthrough shows Flame was designed by world-class scientists</title><url>http://arstechnica.com/security/2012/06/flame-crypto-breakthrough/</url><snippet>  [deleted]  Out of pure curiosity I'd love to get a look at some of the actual code, any ideas on where to find it?  Nice try, Iran.  

Actually, it's all over the pastebins.   I'm too lazy and paranoid to link. 


EDIT: There also exists a torrent for a Windows VM with VSTUDIO 2008 (with the correct C++ compiler config) and the stuxnet project ready for build, allegedly. 
  We're living in a world where some of our best cryptographers are blackhats. I think defining "black hat" may be tough in this scenario.

\*puts on flame-retardant suit\*

They may be saving millions (billions?) of lives by disrupting the creation of nuclear weapons. Agreed that the term black hat is tough, but let me try to justify my statement:

if the 'best and brightest' cannot be assumed to be moral actors, that is, if they'd engage in systems disruption, it will have a knock-on effect: "If 'they're' not above this kind of behavior, I'm not either."  It has the potential to inspire people who don't have the mandate to save the millions/billions of lives by disrupting nuclear weapons.  That mandate is not publicly disclosed, even.

Unlike a nuclear weapon, this tool is self-replicating and can be modified.  That seems like a terrible gambit to me: nuclear weapons are way off in the opposite quadrant.  This tool could be modified into some exquisitely novel. Agreed that the term black hat is tough, but let me try to justify my statement:

if the 'best and brightest' cannot be assumed to be moral actors, that is, if they'd engage in systems disruption, it will have a knock-on effect: "If 'they're' not above this kind of behavior, I'm not either."  It has the potential to inspire people who don't have the mandate to save the millions/billions of lives by disrupting nuclear weapons.  That mandate is not publicly disclosed, even.

Unlike a nuclear weapon, this tool is self-replicating and can be modified.  That seems like a terrible gambit to me: nuclear weapons are way off in the opposite quadrant.  This tool could be modified into some exquisitely novel. Flame is not self-replicating at this point. By self-replicating i meant it's ability to infect other host systems.  From [Wikipedia](http://en.wikipedia.org/wiki/Flame_malware#Usage): "Once a system is infected, Flame can spread to other systems over a local network or via USB stick." Agreed that the term black hat is tough, but let me try to justify my statement:

if the 'best and brightest' cannot be assumed to be moral actors, that is, if they'd engage in systems disruption, it will have a knock-on effect: "If 'they're' not above this kind of behavior, I'm not either."  It has the potential to inspire people who don't have the mandate to save the millions/billions of lives by disrupting nuclear weapons.  That mandate is not publicly disclosed, even.

Unlike a nuclear weapon, this tool is self-replicating and can be modified.  That seems like a terrible gambit to me: nuclear weapons are way off in the opposite quadrant.  This tool could be modified into some exquisitely novel. I think defining "black hat" may be tough in this scenario.

\*puts on flame-retardant suit\*

They may be saving millions (billions?) of lives by disrupting the creation of nuclear weapons. Iran is decades away from having nuclear missiles, and years away from even having the warheads. There are [tremendous engineering problems to overcome](http://www.reddit.com/r/askscience/comments/nivoa/what_about_the_nuclear_process_makes_it_so/c39iojd). I'm hunting around for sourced estimates on how far off Iran actually is from developing nukes... It seems like they don't have anything close to weapons-grade uranium, and in any case they'd be relying on something like a truck bomb.  Better wait until they are fully capable of developing nukes before stopping them, right? Better wait until they are fully capable of developing nukes before stopping them, right? What would motivate Iran to develop nukes? Seeing what happened to Qaddafi...

If he had nukes Nato wouldn't dare to intervene. &amp;gt;If he had nukes Nato wouldn't dare to intervene.

Nato "intervention" is the core of the matter.

Iran understands that it isn't a matter of whether Nato will intervene in their affairs, but when. Gen. Wesley Clark made no secret of ongoing US plans for the region- he said that the agenda was to topple 7 governments within 5 years.

Iraq &#8211; Check

Libya &#8211; Check

Somalia &#8211; Check

Lebanon &#8211; Check  (see assassination of Rafic Hariri and the Cedar Revolution)

Sudan &#8211; Check  (Southern Sudan became a sovereign State on 9 July 2011)

Syria &#8211; Currently in progress

Iran &#8211; The saber rattling and war propaganda has reached a fever pitch.

 Of course, the Chinese and Russians also know about the plan and is the reason they are siding with the Iranians and Syrians.

It's taking a little longer than originally planned, but Obama hasn't deviated from the path set on by Bush.  So what's your point?
Let's assume what you say is true, that's a great motivation to develop nukes.
BTW, your point on Lebanon is partially invalid, Rafic was probably killed by Hezbollah &amp;amp; Syria. My point is that if you take away the threat of Nato intervention, you take away the primary motivation for the Iranians to pursue nuclear weapons. 

If we were to close the bases in the countries we've annexed/allied with and just bring our troops home, we would be better off by far for implementing such a simple, straightforward, and peaceful solution. 

If we continue these policies, we will soon find ourselves engaged in a third world war. What would motivate Iran to develop nukes? What would motivate Iran to develop nukes? Iran is decades away from having nuclear missiles, and years away from even having the warheads. There are [tremendous engineering problems to overcome](http://www.reddit.com/r/askscience/comments/nivoa/what_about_the_nuclear_process_makes_it_so/c39iojd). I'm hunting around for sourced estimates on how far off Iran actually is from developing nukes... It seems like they don't have anything close to weapons-grade uranium, and in any case they'd be relying on something like a truck bomb.  That is garbage!

There is nothing magical about a nuclear bomb.    All you really need is the weapons grade materials.   Downvoters: Actually, he's right. From wikipedia (although unsourced)

&amp;gt;With regard to the risk of proliferation and use by terrorists, the relatively simple design is a concern, as it does not require as much fine engineering or manufacturing as other methods. With enough highly-enriched uranium (not itself an easy thing to acquire), nations or groups with relatively low levels of technological sophistication could create an inefficient&#8212;though still quite powerful&#8212;gun-type nuclear weapon.

Little Boy was such a weapon. gsnedders below is correct that the scientists didn't test it (both because it was so simple and there wasn't enough material for a second bomb), but more weapons have been built (by South Africa, again according to Wikipedia).

Most nations don't build these any more, because they're considerably less efficient and less safe than implosion weapons (which can basically only ever detonate if their explosive lenses are triggered). I don't think it's relevant. It's in dispute if they are trying to manufacture it, not actually to get a dirty bomb. Why would they *not* manufacture it, if it was *easy*? They are two different things. That doesn't address the question... That is garbage!

There is nothing magical about a nuclear bomb.    All you really need is the weapons grade materials.   a bomb is a lot different than a missile. One can make a dirty bomb easily - atomic bomb with more difficulty, H bomb with great difficulty, missiles capable of delivering h bomb with extreme difficulty. Iran is decades away from having nuclear missiles, and years away from even having the warheads. There are [tremendous engineering problems to overcome](http://www.reddit.com/r/askscience/comments/nivoa/what_about_the_nuclear_process_makes_it_so/c39iojd). I'm hunting around for sourced estimates on how far off Iran actually is from developing nukes... It seems like they don't have anything close to weapons-grade uranium, and in any case they'd be relying on something like a truck bomb.  More importantly, I don't see any proof that they're currently working on building nuclear bombs.  Which raises the question: Are the microsoft's engineers that bad to still use md5, or do they use it on purpose knowing it would be easier to hack? Which raises the question: Are the microsoft's engineers that bad to still use md5, or do they use it on purpose knowing it would be easier to hack?  The real [culprit](http://24.media.tumblr.com/tumblr_m4veujPd2z1r39apfo1_400.gif). Please keep the garbage out of the serious subreddits. You're equating humor with garbage and I don't know how I feel about that-- or more importantly, the implications that sort of viewpoint has on your general mental wellbeing.

I wouldn't usually bother to comment on something which might otherwise be insignificant, but you apparently felt strongly enough about the post to publicly express your opinion of it as garbage [where a simple down-vote would have sufficed] in a subreddit where there's no rule against humorous comments. Justifiably, I felt your potential condition might be serious enough that a response in the hopes of prompting healthy self-reflection  was probably warranted. 

I didn't want to risk offending a stranger though (even if it was in his best interest), so I went through your post history, and I'm afraid to say it's dangerously devoid of both imagination and humor. This might be indicative of depression or something, in which case you should seek professional help. Otherwise, I don't know, maybe just work on it?

TLDR; Lighten up a little.

Best of luck.  





\* *Also, please note that the post wasn't entirely cutesie. It was partially prompted by the article's tone, and was meant to highlight the possibility that perhaps researchers were just underestimating some especially clever hacker [Egos are things, after all]. And also I had the gif lying around.* \* The real [culprit](http://24.media.tumblr.com/tumblr_m4veujPd2z1r39apfo1_400.gif). The downvotes make me sad, this is cute, haha.  While I don't deny the likely hood of a nation state behind this, the idea that an individual couldn't do this is simply bogus.  Especially if you know a bit about mathematicians.   Those guys aren't normal!    The idea that a single individual had the knowledge to develop, test, and deploy, several zero days, a hash collision that's never been done before, and the motive to do it, is pretty much impossible.  Unless this person had some wizard like knowledge and access to a super computer, it was 100% devolved by a nation-state, and I know of one who has a cyber organization with several people of this caliber and the resources to do it. There's a lot of room between "an individual" and "a nation state", right? I mean you don't need to be a nation to put 5 experts in a room with about $50K of computing hardware and that'll probably get you quite far. While I don't deny the likely hood of a nation state behind this, the idea that an individual couldn't do this is simply bogus.  Especially if you know a bit about mathematicians.   Those guys aren't normal!    I like to think I'm normal. :(</snippet></document><document><title>Computational Geometry help required</title><url>http://www.reddit.com/r/compsci/comments/ur8n6/computational_geometry_help_required/</url><snippet>What I need is someone to explain to me how you solve the diameter problem with a WSPD (well separated pair decomposition). I know the basics, like what a WSPD is, but I'm having trouble finding any papers that will explain an algorithm or basic theory behind solving the diameter problem.

The diameter problem is: Given a set of points on the plane, find the two point which are furthest apart.

I particularly want to use a WSPD because I can get a (1-eta)-approximation to the problem in O(nlogn) complexity. Any help here would be appreciated.  Try posting to [r/compgeom](www.reddit.com/r/compgeom)</snippet></document><document><title>Recommendations on a good intro to cs book for my GF who would like a little more understanding of what I do. </title><url>http://www.reddit.com/r/compsci/comments/upxn9/recommendations_on_a_good_intro_to_cs_book_for_my/</url><snippet>My girlfriend asked me for a recommendation on an intro to CS book that she could read such that she would get a better understanding of what I do. However, I am unsure which book to pick. I don't think it should be too technical of a book, since the goal is not to progress to more advanced stuff. Rather, I would like it to be more of a fun semi easy read that still gives her some idea of what we do. Also, she has fairly little math background. 

My own research is in AI and relates to algorithms and complexity as well. Ideally I would like that to be reflected in the choice of book, and I would like her to get a bit of an understanding of algorithm design and generally the existence of hard problems and so on.

Thanks!  I generally stay away from math pop/compsci pop books since they lose technical precision and sometimes give more confusing explanation for things than the technical explanation. With that being said, from what I read there are very good sections in James Gleick's [The Information: A History, a Theory, a Flood ]( http://www.amazon.com/The-Information-History-Theory-Flood/dp/0375423729). From the sections I read it gives a pretty comprehensive overview of the history and development of the theory of computation. This book puts people and their respective work into perspective, people including: Babbage, Lovelace, Leibniz, Turing, Shannon, Godel, Kolmogorov/Chaitin. I thought the section on Turing, Godel, Kolmogorov was very well done. I can't really speak for the rest of the book (when I first started reading the book I skipped to Chapter 12).

This book requires very little formal background, but the lay reader won't be able to fully appreciate the ideas/concepts presented in the book without one. Some sections give technical explanations in a layman explanation. From what little I read the book gives a  general overview of the progress made, and the significance of the development of key ideas within the field. Based on the one or two chapters I read, I'd definitely recommend it to your girlfriend (so far chapter 12 is my favorite -- like I said I can't speak much beyond the Turing/Kolmogorov stuff). I was amused by the history of Godel, and his interaction with Chaitin.

I think I am going to pick up where I left off. I study computational complexity theory, and the history of this field is just as interesting as the theorems we prove. I generally stay away from math pop/compsci pop books since they lose technical precision and sometimes give more confusing explanation for things than the technical explanation. With that being said, from what I read there are very good sections in James Gleick's [The Information: A History, a Theory, a Flood ]( http://www.amazon.com/The-Information-History-Theory-Flood/dp/0375423729). From the sections I read it gives a pretty comprehensive overview of the history and development of the theory of computation. This book puts people and their respective work into perspective, people including: Babbage, Lovelace, Leibniz, Turing, Shannon, Godel, Kolmogorov/Chaitin. I thought the section on Turing, Godel, Kolmogorov was very well done. I can't really speak for the rest of the book (when I first started reading the book I skipped to Chapter 12).

This book requires very little formal background, but the lay reader won't be able to fully appreciate the ideas/concepts presented in the book without one. Some sections give technical explanations in a layman explanation. From what little I read the book gives a  general overview of the progress made, and the significance of the development of key ideas within the field. Based on the one or two chapters I read, I'd definitely recommend it to your girlfriend (so far chapter 12 is my favorite -- like I said I can't speak much beyond the Turing/Kolmogorov stuff). I was amused by the history of Godel, and his interaction with Chaitin.

I think I am going to pick up where I left off. I study computational complexity theory, and the history of this field is just as interesting as the theorems we prove. I generally stay away from math pop/compsci pop books since they lose technical precision and sometimes give more confusing explanation for things than the technical explanation. With that being said, from what I read there are very good sections in James Gleick's [The Information: A History, a Theory, a Flood ]( http://www.amazon.com/The-Information-History-Theory-Flood/dp/0375423729). From the sections I read it gives a pretty comprehensive overview of the history and development of the theory of computation. This book puts people and their respective work into perspective, people including: Babbage, Lovelace, Leibniz, Turing, Shannon, Godel, Kolmogorov/Chaitin. I thought the section on Turing, Godel, Kolmogorov was very well done. I can't really speak for the rest of the book (when I first started reading the book I skipped to Chapter 12).

This book requires very little formal background, but the lay reader won't be able to fully appreciate the ideas/concepts presented in the book without one. Some sections give technical explanations in a layman explanation. From what little I read the book gives a  general overview of the progress made, and the significance of the development of key ideas within the field. Based on the one or two chapters I read, I'd definitely recommend it to your girlfriend (so far chapter 12 is my favorite -- like I said I can't speak much beyond the Turing/Kolmogorov stuff). I was amused by the history of Godel, and his interaction with Chaitin.

I think I am going to pick up where I left off. I study computational complexity theory, and the history of this field is just as interesting as the theorems we prove.  CODE by Charles Petzold. While the book looks really good, it seems to be more of an intro to how computers work than to what CS is. I'm looking for something more focused around algorithms, complexity and such. While the book looks really good, it seems to be more of an intro to how computers work than to what CS is. I'm looking for something more focused around algorithms, complexity and such. You are absolutely right about that. However, as with any subject it is fruitful to get a basic understanding of the world surrounding it; in which it 'lives' and functions and on which it to some degree depends, so to speak. And for that purpose, I really can't think of any book that comes anywhere close to being as good as the one mentioned above.

That being said though; if she really want to understand your field and what you do - why don't you simply set aside a lot of time for sitting sown and talk to her about it, let her prod you with any questions she might have, however stupid they might seem to you? (And that is not a pejorative question - any "expert", wheter male or female, are prone to finding some questions from non-experts stupid). My apologies though if you have already tried that approach and found it not satisfactory :)

 While I agree with you I think I should stick to my main thing for now, this could come later if she feels like getting some more context. In my opinion algorithms as a more abstract topic can be enjoyed without knowing much about how a computer works.

For your second point, see my response to kqr below. Personally, I think someone without an innate understanding of how computers work will have a very hard time appreciating algorithms. To you and me the purpose of algorithms may seem very obvious but to someone who is interested in real things or emotions might not get very excited over loops, branching and recursion since they can see no point to them.    this. http://www.laurenipsum.org/

"A story about computer science 
and other improbable things.
Laurie is lost in Userland. She knows where she is, or where she's going, but maybe not at the same time. The only way out is through Jargon-infested swamps, gates guarded by perfect logic, and the perils of breakfast time at the Philosopher's Diner. With just her wits and the help of a lizard who thinks he's a dinosaur, Laurie has to find her own way home."

&#8220;Don&#8217;t look so sad! I do want to help you,&#8221; said Tinker. &#8220;Maybe we can do a trade. It so happens I&#8217;m in the market for a particular algorithm.&#8221;

&#8220;But I don&#8217;t have any algorithms, either.&#8221; said Laurie.

&#8220;That&#8217;s not a problem,&#8221; said Tinker. &#8220;You can compose new ones any time you want, with a little bit of thinking.&#8221; I bought a copy of this. It was adorable, but it's less of an introduction and more of an appetizer.  [Godel, Escher, Bach](http://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567/ref=sr_1_1?ie=UTF8&amp;amp;qid=1339088631&amp;amp;sr=8-1), 832 pages of ultra-accessible pop compsci. That's a light read.   She should be his ex-gf by that afternoon.   It's written basically at a high school level. It's just long. [Godel, Escher, Bach](http://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567/ref=sr_1_1?ie=UTF8&amp;amp;qid=1339088631&amp;amp;sr=8-1), 832 pages of ultra-accessible pop compsci. I was considering this book. I haven't read it yet myself, and I was unsure of the amount of CS in there as opposed to philosophy and cog sci. [Godel, Escher, Bach](http://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567/ref=sr_1_1?ie=UTF8&amp;amp;qid=1339088631&amp;amp;sr=8-1), 832 pages of ultra-accessible pop compsci. pop pseudosci    I know this is not what the thread is about, but why can't you explain to her yourself what the things you're doing are about? Wouldn't that be more satisfying to both of you? It doesn't have to be time-consuming; you can take half an hour a week or something if you're pressed for time. Let her have a hands-on experience instead of only reading from a book.

You might have thought about that already, but in case you haven't, I've just trown it out there now. Teaching is a wonderful thing, and being teached by someone you love about a thing he/she enjoys a lot is one of the best kinds of getting teached.    If you introduce your girlfriend to CS,
you're gonna have a bad time.  The ["Headfirst" series](http://headfirstlabs.com/) contains ok info while still being enjoyably poppy, might be worth a look?   Dude, if you have a girlfriend that cares enough about what you do to read ANY CS book, I'd say you better try to keep her!

I have a hard time getting my wife to stay awake for 5 minutes when talking about a project of mine..  Word. She sounds awesome. ;)  The Art of Computer Programming? The Art of Computer Programming? You are kidding?</snippet></document><document><title>What is the best way of to and fro data syncing between two disparate system?</title><url>http://www.reddit.com/r/compsci/comments/uqtoy/what_is_the_best_way_of_to_and_fro_data_syncing/</url><snippet>One is written in python and the other in php. Server is in python and client written in php, transactional system is written in php and installed on multiple client sites. All transactional data is send over to a central server and server also will send master data all through out the clients.  XML If XML doesn't work, add more. Repeat. You forgot the "just like violence" part.  rsync. man: rsync copies files either to or from a remote host, or locally on the current host (it does not support copying files between two remote hosts). Thanks man looking in too it. Also I'm considering git maybe.  </snippet></document><document><title>The greatest Project Euler problem of all time.</title><url>http://www.reddit.com/r/compsci/comments/unmvg/the_greatest_project_euler_problem_of_all_time/</url><snippet>You have a key generator for adding friends on this hip new social networking site. You generate a key and give it to your IRL friends and then you're friends on the site.

They key has 49 characters.

The first 9 are integers, which change every time you generate.
The second 7 are integers which stay the same every time.
Then there is an underscore followed by 32 hex characters, these also vary every time you generate.

What could the key represent?


In all seriousness: this is Project Euler's friend system. My friends and I can't figure out why its so complicated. This is a site for programmers, so they probably know what they're doing. Why couldn't they just use a request and confirm system? Do they just have this to be awesome? Is this the super secret question you have to answer to win it all?  Complete speculation, but the first is probably the request ID, the second is the ID of the requester, and the last is probably a hash of the two that incorporates some secret salt so that they cannot be tampered with by modification.

As to why they'd implement it like that, does the site already have user to user private messaging?  Assuming it does not, implementing a feature like this becomes a lot easier if you can offload that job onto existing protocols (IM, email, SMS).  Using this token strategy, all you have to implement is a form that generates a token and a form that receives and verifies a token.  You don't have to reinvent the wheel implementing things like checking for new messages or new friend requests each time a visitor views a page, for example.
 This makes the most sense. The first value, the request ID, is necessary so that the token can't be "replayed" or used by multiple people. The 32-byte hex value is probably a SHA-256 HMAC. The 32 byte hex value is probably a 32 byte hex value, is the most accurate guess any of us can give.  If you were assuming a HMAC, 32 bytes would be MD5, wouldn't it?  At least that's what the gods of wiki [tell me](http://en.wikipedia.org/wiki/HMAC#Examples_of_HMAC_.28MD5.2C_SHA1.2C_SHA256_.29) Complete speculation, but the first is probably the request ID, the second is the ID of the requester, and the last is probably a hash of the two that incorporates some secret salt so that they cannot be tampered with by modification.

As to why they'd implement it like that, does the site already have user to user private messaging?  Assuming it does not, implementing a feature like this becomes a lot easier if you can offload that job onto existing protocols (IM, email, SMS).  Using this token strategy, all you have to implement is a form that generates a token and a form that receives and verifies a token.  You don't have to reinvent the wheel implementing things like checking for new messages or new friend requests each time a visitor views a page, for example.
  The website is for programmers with strong mathematical background.

Very very strong mathematical background. **Euler programming problem**: 

Find the smallest prime where the first four digits are consecutive, in base 23.

**Actual programming problem:**

"The client wants a report of their inventory based on the product's manufacturing date."

"But manufacturing date isn't recorded when they enter the data?"

"Well, actually it is.  They have been putting the manufacturing date into the serial number field because they decided they don't need serial numbers."

"So, they want it organized by the serial number field, which has dates in it?"

"Yup"

"This one says '3/10/02'? Is that March 10th 2002, or Oct 2nd 2003?  This one says 'Tursdaay 5', and this one just says 'Peter'?"

"Yeah, I don't know, but the sales guys told them we could do it.  Oh, and they need it by the end of the month."

"So, tommorrow?" Browser cookies can have expiration dates, but while there is a spec because of IE and Mozilla sites can just about send anything they want.  Due to this I ended up spending a good chunk of time on this problem

https://github.com/icefox/parse_cookie_date

I am particularly proud of the part after you sparate the string and have to figure out things like is 3/10/02 March 10th 2002, or Oct 2nd 2003.  After iterating on the problem a fair amount I ended up on a very elegant solution. that works on all of the edge cases.

https://github.com/icefox/parse_cookie_date/blob/master/main.cpp#L318 This is beautiful.     // Example: 31 11 06
    // 31 can't be a day because 11 and 6 don't have 31 days

You're damn right it is. That said, I would have brute-forced it. Try all 6 permutations (with a hand-written order of preference), pick the first that's valid.

    int[][] prefs = {{0, 1, 2},/*ISO*/,
                     {2, 1, 0},/*Backwards ISO*/
                     {2, 0, 1},/*USA*/
                     ...};
    for(auto order in prefs) {
        date d(unknown[order[0]], unknown[order[1]], unknown[order[2]])
        if (validate(d)) return d;
    }
    //No permutations are valid if we haven't returned by now.

6 permutations is not many, after all. It also means you don't need to do clever things like "set the year/month/day that have been deduced and reduce the set as we go along to deduce more". Not nearly as satisfying, though... **Euler programming problem**: 

Find the smallest prime where the first four digits are consecutive, in base 23.

**Actual programming problem:**

"The client wants a report of their inventory based on the product's manufacturing date."

"But manufacturing date isn't recorded when they enter the data?"

"Well, actually it is.  They have been putting the manufacturing date into the serial number field because they decided they don't need serial numbers."

"So, they want it organized by the serial number field, which has dates in it?"

"Yup"

"This one says '3/10/02'? Is that March 10th 2002, or Oct 2nd 2003?  This one says 'Tursdaay 5', and this one just says 'Peter'?"

"Yeah, I don't know, but the sales guys told them we could do it.  Oh, and they need it by the end of the month."

"So, tommorrow?" The answer to the Euler problem is: 305857, which is 12343 in Base 23 (I hope that wasn't a real problem, I wouldn't want to spoil it). Haha. You're awesome!

I made it up, trying to make it as ridiculous and inane as possible yet seem plausible.

And yet somehow I knew that if there was a reasonable answer, there would be someone who would see it as a challenge. :)

I'm actually kind of shocked that there is an answer that doesn't need to be expressed in scientific notation.... &amp;gt;I'm actually kind of shocked that there is an answer that doesn't need to be expressed in scientific notation....

Yeah me too. My implementation basically just iterates through numbers where the first four digits are consecutive (in base 23) starting with 1234, and checking each number if it's prime. I was really expecting to have to rewrite it to be smarter--I definitely was not expecting it to finish in less than a second. **Euler programming problem**: 

Find the smallest prime where the first four digits are consecutive, in base 23.

**Actual programming problem:**

"The client wants a report of their inventory based on the product's manufacturing date."

"But manufacturing date isn't recorded when they enter the data?"

"Well, actually it is.  They have been putting the manufacturing date into the serial number field because they decided they don't need serial numbers."

"So, they want it organized by the serial number field, which has dates in it?"

"Yup"

"This one says '3/10/02'? Is that March 10th 2002, or Oct 2nd 2003?  This one says 'Tursdaay 5', and this one just says 'Peter'?"

"Yeah, I don't know, but the sales guys told them we could do it.  Oh, and they need it by the end of the month."

"So, tommorrow?"  7 constant characters seem like they'd be a user ID of some kind. 9 character might be a timestamp - have you found any patterns there? 32 hex sounds like a UUID, but could be some other unique identifier.  No idea what they are doing, but "This is a site for programmers, so they probably know what they're doing" made me laugh. The problem with "programming" is that everyone that knows how to program fancies himself a "programmer".   
That's like everyone who knows how to write fancying themselves a "writer". The problem with "programming" is that everyone that knows how to program fancies himself a "programmer".   
That's like everyone who knows how to write fancying themselves a "writer". No idea what they are doing, but "This is a site for programmers, so they probably know what they're doing" made me laugh. No idea what they are doing, but "This is a site for programmers, so they probably know what they're doing" made me laugh.   </snippet></document><document><title>What happend when I made this post? [Network question]</title><url>http://www.reddit.com/r/compsci/comments/uof28/what_happend_when_i_made_this_post_network/</url><snippet>I'm studying networks for my finals and I have some troubles with putting everything together.

So I was wondering: Can anyone give me a full overview of what happens when I made this post?

Lets say that I'm in Europe connected to a router at home and I want to write this post. And suppose that the Reddit servers are based in the US (where I guess they are).

What steps/actions need to be taken, and what protocols are most likely to be used?
  Install Wireshark. That is a fantastic response. Not sure why someone would downvote it. No reply to this post could give as much detail as wireshark could give. No it is not. It is a good response, not a fantastic one. See the analogy that reliciler gave. Wireshark would have never given me this! That is a fantastic response. Not sure why someone would downvote it. No reply to this post could give as much detail as wireshark could give. Install Wireshark.  So I guess that at first I need to send a DNS request. This will be sended by a UDP packed via IP to my homenetwork router. The router will use [NAT](http://en.wikipedia.org/wiki/Network_address_translation) to translate the packet and send it to my ISP DNS-servers.

My homenetwork router will receive a DNS reply by UDP via IP and needs to translate this reply via NAT to send it to my computer.

At this point I have the IP-adress of the Reddit servers and I can send my request to open the website.
How will this request be send (UDP/TCP)? What networks will it most likely pass? And how do [OSFP](http://en.wikipedia.org/wiki/Open_Shortest_Path_First) and [BGP](http://en.wikipedia.org/wiki/Border_Gateway_Protocol) fit in this scenario? So I guess that at first I need to send a DNS request. This will be sended by a UDP packed via IP to my homenetwork router. The router will use [NAT](http://en.wikipedia.org/wiki/Network_address_translation) to translate the packet and send it to my ISP DNS-servers.

My homenetwork router will receive a DNS reply by UDP via IP and needs to translate this reply via NAT to send it to my computer.

At this point I have the IP-adress of the Reddit servers and I can send my request to open the website.
How will this request be send (UDP/TCP)? What networks will it most likely pass? And how do [OSFP](http://en.wikipedia.org/wiki/Open_Shortest_Path_First) and [BGP](http://en.wikipedia.org/wiki/Border_Gateway_Protocol) fit in this scenario? Type *traceroute www.reddit.com* in your console to see the "hops" your request makes. I know about traceroute, but as far as I know it doesn't show any specific protocols used.
I guess that some protocols used by ISP's can't be shown this way (due to tunneling etc.). And these are the things that I wan't to know more about (OSFP, BGP, ...).
 Roughly speaking BGP (border gateway protocol) is how routers know which router to send a packet too (I am not a network guy so the following could all be wrong). Imagine a simple network... your home router connected to a router at your ISP. The ISP's router is also connected to two other routers... one that handles all of the US websites and one that handles the rest of the world (ROW). The ISP's router uses BGP to talk to the the US router and the ROW to acquire a map of which IP addresses each router handles. The list of IP addresses is a mix IP addresses and masks (eg. 205.0.0.0 and 255.0.0.0... means any IP address starting with 205).... not the full list of all known IP addresses. When one ISP installs a connection to another ISP (roughly speaking "a phone line") they reach an real world agreement to allow their routers to talk to each other using BSG (i.e. routers won't just accept BSG stuff from any old place). BSG is only involved at the "border" of two networks (i.e. where two ISP "meet"/"share a connection").

So... when you make a specific request BGP isn't actually directly involved. The ISP's routers (and routers around the world) have already initialized themselves and "know" how to route a packet.

The actual browser request is send via TCP/IP using the HTTP protocol to a specific IP address and port number. The protocol defines two standard/default ports... 80 for http: and 443 for https:.  The browser uses the DNS system to translate domain names into IP addresses). The protocol includes commands list "GET" (get the content of a specific resource), "HEAD" (get the header information of a specific resource), and so forth. In general the browser will do a "GET" on the resource and then subsequent "GET"s for images, scripts, style sheets, etc (whether a browser does a GET on additional resources is not part of the HTTP standard... it's part of the normal expected behavior of a browser). 

The response from a webserver always starts with a headers a blank link and then a status line (eg. ignoring headers, something like "200 OK" or "404 Not Found"). Following the status line is the content of the requested resource (for GET's, other http commands may or may not actually send content).

Headers are used both when sending a request and receiving a request. They look like "header name: header value", and there's a raft of standard headers, each with specific purposes. 

For more details on how the HTTP protocol works start with Wikipedia and then look at the related RFC documents.

To come back to your question about "what happens when I write this post"... it looks roughly like:

1. The browser queries your computer for reddit.com's IP address
2. If your computer knows the answer it provides it, otherwise your computer asks its configured DNS server (which in turn may ask other DNS servers).
3. Your browser opens TCP/IP port 80 to reddit's IP address and issues a GET request for the "/submit" resource
4. The webserver responds with a page of HTML
5. Your browser issues additional GET requests to fetch images, style sheets, etc
6. Your browser draws the page and shows it to you
7. You fill out the form and click "submit"
8. Your browser opens TCP/IP port 80 to reddit's ip address and issues a POST request to the "/submit" resource and includes all the data you filled out on the form.
9. The webserver responds with a status of "301 Redirect" and a new url of "/r/compsci/comments/uof28/what_happend_when_i_made_this_post_network/"
10. The browser knows that a 302 status means the webserver would like it to issue a GET request on the specified URL
11. The browser does a GET request on "/r/compsci/comments/uof28/what_happend_when_i_made_this_post_network/" and displays the results to you (goto step 4)

Behind all of the above is the low level routing work that routers do to ensure that a network packet addressed to IP x.x.x.x actually gets to IP address x.x.x.x. This generally involves between half a dozen or two dozen separate routers.  I know about traceroute, but as far as I know it doesn't show any specific protocols used.
I guess that some protocols used by ISP's can't be shown this way (due to tunneling etc.). And these are the things that I wan't to know more about (OSFP, BGP, ...).
 </snippet></document><document><title>Can anyone explain to me the advantages of Content Addressable Memory?</title><url>http://www.reddit.com/r/compsci/comments/unh2t/can_anyone_explain_to_me_the_advantages_of/</url><snippet>I have been reading around a bit about computational models and memory organizations used in modern computers, and I don't understand CAM. I know that it is good for high-speed searching algorithms, but I don't understand why. If you need to look through the whole memory segment and check if every word matches the one you're looking for, isn't that super slow?  The point is it checks every memory location "in parallel".

Or, more likely, to make it less stupidly expensive, content is only allowed to be in a small number of locations based up a hash function or just by only considering a few of its bits, and it checks all of those locations "in parallel".    </snippet></document><document><title>Whiley | Exploring The Verification Corner</title><url>http://whiley.org/2012/06/06/exploring-the-verification-corner/</url><snippet /></document><document><title>Calibrated Graph Problem - anyone have any clever ideas?</title><url>http://i.imgur.com/szWcQ.png</url><snippet>  I don't understand why we need w_i and k_i? Isn't the restriction just that e_i = k_i-w_i so all you need is a single value (say k_i) which must be equal to e_i? You're right. The data set I'm working from had both so I left it in the problem. That's a much nicer way to represent the problem! I'm also tempted to say there won't be a 'nice' representation for all sets of edges (finding just one might be more interesting).

Just as an example, suppose all vertices had values (0,2) (using your original notation), except 2, say x and y, which have (0,1). Then obviously the only solutions are the paths from x to y. The problem is there are O(n!) of these, since the path can traverse the other vertices in any order.

I imagine that in general this problem would have hopelessly unrepresentable solution sets.

EDIT: Just realized this was r/compsci, not r/math.. heh. In that case, finding all edge sets is not polynomially computable, in which case if you're going to compute it at all, you may as well just check every edge set anyway.

   Little background: As far as I know, I made this problem up (made the picture to try to explain it). I'm sure others have solved it before but I have been having a ton of trouble finding similar problems online. If anyone has any ideas for reformulating the problem s.t. it's easier to solve, I'd love to hear it! Any interesting (related) observations, algorithms, or analysis are very welcome! Why the integer pairs? This problem is more-or-less an adaptation of another problem. I should've simplified it further to just be one integer! I'm just not sure what the integer pairs serve. I'm no CS graduate though, just a programmer.

 Using single integer values is definitely better. This problem is actually just related to a chemistry problem I'm working on. The first number is the number of electrons and the second is the number of electrons required to complete a valence shell. This could've been simplified to the number of covalent bonds required for each atom (which I will certainly do now) - equivalent to the number of edges on the graph.  Now you just need to check if the molecules you generate are physically possible in three dimensions! (no easy task, that one) Haven't even gotten the generation down yet! But my next step is to calculate the appropriate angles, so maybe I'll just go for 3D! Any recommended references?  Sorry to reply in someone else's thread :)

[Balloon](http://users.abo.fi/mivainio/balloon) is a nice program to generate 3D conformers from just the connectivity.

If you want to code it yourself, I'm not totally sure how - I've just finished doing some 2D layout stuff, which is hard enough. One approach might be to use Jmol's [minimize command](http://chemapps.stolaf.edu/jmol/docs/#minimize) on a rough model, as it's quite nice to have a visual method.

In general, it's probably not necessary to get a chemically accurate 3D model, as that is quite a bit of work. A 3D 'sketch' model with unrealistic bond lengths (IE : edges) is probably fine for most things. Also, a crude model can then be refined by other software. Haven't even gotten the generation down yet! But my next step is to calculate the appropriate angles, so maybe I'll just go for 3D! Any recommended references?  As a computer scientist with just a bit of physics and chemistry knowledge, it is my impression that simply evaluating whether or not an arbitrary configuration is stable is a very hard problem within the realm of P. Chem. I'd start by searching for related work in physics/chemistry journals, restating the problem in a mathematical way (as you did in the OP), and then attempting to reduce the problem to a well-explored problem. Little background: As far as I know, I made this problem up (made the picture to try to explain it). I'm sure others have solved it before but I have been having a ton of trouble finding similar problems online. If anyone has any ideas for reformulating the problem s.t. it's easier to solve, I'd love to hear it! Any interesting (related) observations, algorithms, or analysis are very welcome! There's something I do not quite understand about your problem formulation:
&amp;gt; Find all sets of edges where w_i + e_i = k_i for every vertex in the graph

Did you mean to say "Find all sets of *vertices* where w_i + e_i = k_i"? Going by your example, this appears to be the case.

Assuming that, why not give each vertex a single integer L_i and look for all vertices that have e_i = L_i? Your original problem could be reduced to this by setting L_i = k_i - w_i.
 You're right, the problem should just be to find the set of graphs requiring a certain number of edges for each vertex. The usual formulation is that the set of integers {e_i} is a [degree sequence](http://mathworld.wolfram.com/DegreeSequence.html). There is a theorem known as the [Havel-Hakimi theorem](http://math.stackexchange.com/questions/61361/what-condition-need-to-be-imposed-on-havel-hakimi-theorem-to-check-for-connected) that gives you *a* graph (but not necessarily connected - see the link).

I could mention that there are ways to generate *all possible* graphs given a degree sequence, but they are quite complicated, and I don't know of an efficient way to do so. I did try using integer partitions of the degrees, which sort of worked.  The usual formulation is that the set of integers {e_i} is a [degree sequence](http://mathworld.wolfram.com/DegreeSequence.html). There is a theorem known as the [Havel-Hakimi theorem](http://math.stackexchange.com/questions/61361/what-condition-need-to-be-imposed-on-havel-hakimi-theorem-to-check-for-connected) that gives you *a* graph (but not necessarily connected - see the link).

I could mention that there are ways to generate *all possible* graphs given a degree sequence, but they are quite complicated, and I don't know of an efficient way to do so. I did try using integer partitions of the degrees, which sort of worked.  [This](http://www.cse.msu.edu/~cse835/Papers/On%20realizing%20all%20simple%20graphs%20with%20a%20given.pdf) is what I was looking for! Thank you so much!</snippet></document><document><title>Help with call-by-value Y Combinator</title><url>http://www.reddit.com/r/compsci/comments/uljwk/help_with_callbyvalue_y_combinator/</url><snippet>I was reading this Wikipedia article http://en.wikipedia.org/wiki/Fixed-point_combinator#Y_combinator and I understand what's happening where Y is applied to a function g.

But then, where the article talks about the call-by-value Y Combinator, I am lost.  I don't understand what it is, how it works, or how to apply it.  Can someone explain it to me?  Do you understand the difference between call-by-name and call-by-value?

What's going to happen if the argument is evaluated before its applied to the function in the Y combinator? What about the Z combinator? I don't understand how the Y combinator can even exist when the language is call-by-value. Won't it expand infinitely under a strict evaluation policy? You *need* call-by-name. I don't understand how the Y combinator can even exist when the language is call-by-value. Won't it expand infinitely under a strict evaluation policy? You *need* call-by-name.  Can anyone provide me with a good guide to Lambda Calculus in its entirety?

I've just finished a CS Major but the one thing I never really got was Lambdas. I'm hoping that if I went over it now it might all click. Can anyone provide me with a good guide to Lambda Calculus in its entirety?

I've just finished a CS Major but the one thing I never really got was Lambdas. I'm hoping that if I went over it now it might all click. Well you could read [the original paper](https://www.fdi.ucm.es/profesor/fraguas/CC/church-An%20Unsolvable%20Problem%20of%20Elementary%20Number%20Theory.pdf). Not recommended :) Can anyone provide me with a good guide to Lambda Calculus in its entirety?

I've just finished a CS Major but the one thing I never really got was Lambdas. I'm hoping that if I went over it now it might all click.  </snippet></document><document><title>Summer research question</title><url>http://www.reddit.com/r/compsci/comments/um30g/summer_research_question/</url><snippet>Hey everyone,

I am currently doing summer research for my college. I am trying to develop a  Dbus library for the GIMP such that I can incorporate scripting development environments such as DrRacket.

My question is: I have no idea how to make a Dbus library, could someone point me in the right direction to learn how?

Thanks  [deleted]  http://www.freedesktop.org/wiki/Software/dbus#Documentation ?

I'm not trying to be snarky -- this is where I would start. yeah, I already read that one ... it gives an idea, but doesn't go into detail. thanks though</snippet></document><document><title>I have a math problem but don't know the field of math to inquiry further. Any help would be appreciated. </title><url>http://www.reddit.com/r/compsci/comments/uk801/i_have_a_math_problem_but_dont_know_the_field_of/</url><snippet>What I have are areas within a 2-D coordinate system (irregularly shaped and not in a pattern). What I want is to be able to select a point in this a 2-D coordinate system and select the area containing this point. Any ideas of a solution, or the "name" of the field within math which address these topics. Thanks. 

Edit: Example problem- Pretend I am given a coordinate (latitude, longitude) in the US. I have a map of the US. Now, I suppose I need to "section off" states (somehow) then identify which state contains the coordinate. 

I wonder a method to solve this problem with high efficiency (for a website). 

EDIT 2: Good guys /r/math suggest Jordan Curve Theorem (http://en.wikipedia.org/wiki/Jordan_curve_theorem) and says it works for polygons (http://en.wikipedia.org/wiki/Point_in_polygon#Winding_number_algorithm)

Great, now which is better?

Thanks again everyone  Computational Geometry.

The general problem is called "Point location", trapezoidal map is often the solution you want.

Get this book: http://www.cs.uu.nl/geobook/ (a "bible" of comp. geometry) or google somewhere for the algorithm [*cough* first link *cough*](https://www.google.com/search?sugexp=chrome,mod=15&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&amp;amp;q=Computational+Geometry%3A+Algorithms+and+Applications+pdf), but seriously pick it up some day. Computational Geometry.

The general problem is called "Point location", trapezoidal map is often the solution you want.

Get this book: http://www.cs.uu.nl/geobook/ (a "bible" of comp. geometry) or google somewhere for the algorithm Also look at [Hanan Samet's](http://www.amazon.com/Foundations-Multidimensional-Structures-Kaufmann-Computer/dp/0123694469/ref=sr_1_1?ie=UTF8&amp;amp;qid=1338956727&amp;amp;sr=8-1) book, which catalogs lots of approaches to these sorts of problems.     Look into existing GIS/geospatial.online mapping software, You probably don't need to reimplement this from the math and an approximation is probally fine. A few search terms: geohashing, geolocating spatial databases (see postGIS, geospatial mysql), GeoDjango.

If you are interested in the math one really cool way to approach something like this is to define a complex function with a zero or pole at your point/points such that you can count the number of points inside each region with a line integral around it. Its been a while since I did this but this looks like the [same idea](http://en.wikipedia.org/wiki/Argument_principle). This. For a production web site you shouldn't be writing custom algorithms like these unless you know one of the many available out of the box solutions don't work. This is especially true if you've never implemented and it seems novel to you.

That being said, you should definitely put your curiosity to good use and write your own version of the algorithm for fun. Just be sure not to use it for any real project ;)   Computational geometry. Some helpful topics for this are triangulation, and using the discriminant to find if a point is in a triangle. I took a course about a year ago on this so ill edit if im wrong. But triangulation is very important  What you are probably looking for is a QuadTree data structure.

 What you are probably looking for is a QuadTree data structure.

 Interesting (chick stroke). I edited the description a bit in case you have the time. Thanks.  You can also use an octree because a region has no more than 8 neighbors.

1 2 3  
4 # 5  
6 7 8    &amp;gt;Edit: Example problem- Pretend I am given a coordinate (latitude, longitude) in the US. I have a map of the US. Now, I suppose I need to "section off" states (somehow) then identify which state contains the coordinate.

If this is actually the problem you want to solve then you could do it easily enough with the Google Maps API. Basically you would define a number of polygons (e.g. one for each state) which are overlays on the map. When someone clicks anywhere within the polygon, you perform whatever action is required. See here: https://developers.google.com/maps/documentation/javascript/overlays#Polygons

&amp;gt;I wonder a method to solve this problem with high efficiency (for a website).

If it's not a Google Map you're using, but say an image, then you could use an image map and define your shapes using 'area' tags. Again, these can have some onClick action or whatever. See here: http://www.w3schools.com/tags/tag_area.asp

Maybe I'm oversimplifying your problem - but make sure you're not reinventing the wheel here. If you're working with geographical coordinates then there is almost certainly an API or library or whatever that will do what you want. In fact, if what you're doing is for a web site then there probably is something out there that you can use instead of implementing it yourself.

EDIT: if you don't have someone clicking, but want to find out for a coordinate which you get some other way, then you could use the method here http://alienryderflex.com/polygon/ which was also linked to by zhay.</snippet></document><document><title>SSDs and distributed data systems</title><url>http://blog.empathybox.com/post/24415262152/ssds-and-distributed-data-systems</url><snippet /></document><document><title>What skills do I need before tackling The Algorithm Design Manual?</title><url>http://www.reddit.com/r/compsci/comments/ulllh/what_skills_do_i_need_before_tackling_the/</url><snippet>I&#8217;ve been reading a book called &#8220;How To Win At College&#8221; by Cal Newport of [Study Hacks](http://calnewport.com/blog/) fame. The book is broken up into a series of tips and tricks that are meant to ultimately lead one to greater success as a university student. One of the tips in the book is &#8220;Do something better than anyone you know.&#8221; I&#8217;ve decided that I want this skill to be the ability to solve algorithmic problems by applying algorithm theory to real-world problems and I&#8217;ve decided on The Algorithm Design Manual by Skiena as my introduction to algorithms.

We all agree that computer science is not programming, but I believe a firm grasp on basic CS concepts can make anyone a better developer. This relationship is similar to that of a journalist who studies English. While a journalist can have a great career without studying English, there is certainly an argument that theory (English) is just as important as application (Journalism). For programmers, this is the difference between studying theoretical computer science (algorithms, data structures, AI, etc) and learning programming languages (Python, C, etc).

My goal is to become proficient at engineering optimized algorithms for solving real-world problems. This won&#8217;t happen by diving right into a book on algorithms and that is something that I fully understand. My question is this: What do I need to know before I start tackling The Algorithm Design Manual? To be a good algorithm designer one certainly needs to know how to analyze the time complexity of an algorithm. What math skills, specifically, are required to do this well? I&#8217;ve heard that going through a book on discrete mathematics would be a good idea. What else would you recommend? Statistics and probability, perhaps?   If you want a really thorough introduction to algorithms you should probably look at [CLRS](http://en.wikipedia.org/wiki/Introduction_to_Algorithms). All the required math is explained in the appendix.

Still, even with the explanations you will probably spend more time trying to get the proofs than actually learning algorithm design (because you might give up early).

Working through Skiena's book first is really not a bad idea. I've only glossed over it, but it seems he introduces all (or most of) the required math in a linear fashion building up your knowledge for algorithm analysis. On the other hand IIRC he glosses over many things such as thorough analysis of randomized algorithms.
So, I don't think you need much prior special math knowledge for Skiena's book. CLRS is much more of a reference book than Skiena's is. I'm sure it's a fantastic book, but I wouldn't consider using it as an actual introduction to the subject. Kleinberg and Tardos's book *Algorithm Design* is a better alternative if you're less interested in rigor and more in the intuition behind things. 

I'd actually recommend reading bits and pieces of CLRS, such as their intro to random variables and so forth. If you want more math, consider skimming at least the opening chapters of *Concrete Mathematics*.

You may want to get into programming contests. I learned more about computer science theory from that than all the classes I took. It's far from real-world application, but as a computer scientist it'll develop an enormous fraction of the skills you want to train. Everything from abstracting a problem to analyzing complexity and proving optimality. I think sites like TopCoder and Interview Street are what I'm looking for. I want to be good at taking problem statements and returning solutions, and that's what these sites are made for.

EDIT: Would you recommend *Algorithm Design* before or after *The Algorithm Design Manual*?     </snippet></document><document><title>In praise of impractical programming.</title><url>http://www.niemanlab.org/2011/11/in-praise-of-impractical-programming/</url><snippet>  I liked a course called "Principles of programming languages" at my uni, which taught (in addition to the principles itself) a few of these so called 'impractical' programming languages, from functional languages to logic languages. Waterloo?

I took that course a year ago. The most interesting course I ever took, ranking above RTOS and Compiler. Tampere University of Technology :). But I agree, that's one of my favourite courses so far.  Good article, but I thought most universities did similar things?

We started with Pascal with a heavy focus on Hoare logic, doing babysteps (secure babysteps). Then we moved onto C for seeing OS thingies, CLisp to see another kind of things and Java/PHP/etc to see "modern" web thingies. 

I thought this was the standard (Im finishing the career this year). I was previously a Comp Sci professor at a major US university, and can say that we, as well as most other universities in our state and region, provide classes that expose students to functional programming, but the vast majority of students can obtain their undergraduate CS degree having learned nothing more than Java and a bit of basic web programming. 

 I was previously a Comp Sci professor at a major US university, and can say that we, as well as most other universities in our state and region, provide classes that expose students to functional programming, but the vast majority of students can obtain their undergraduate CS degree having learned nothing more than Java and a bit of basic web programming. 

 I was previously a Comp Sci professor at a major US university, and can say that we, as well as most other universities in our state and region, provide classes that expose students to functional programming, but the vast majority of students can obtain their undergraduate CS degree having learned nothing more than Java and a bit of basic web programming. 

 Thats kind of sad. Moreso considering its compsci (mine was a mix between compsci and software engineering since they couldnt have both).

But it seems that most students (and the general market) prefer just to focus on apps and practical programming, so it seems the normal focus. I was previously a Comp Sci professor at a major US university, and can say that we, as well as most other universities in our state and region, provide classes that expose students to functional programming, but the vast majority of students can obtain their undergraduate CS degree having learned nothing more than Java and a bit of basic web programming. 

 I was previously a Comp Sci professor at a major US university, and can say that we, as well as most other universities in our state and region, provide classes that expose students to functional programming, but the vast majority of students can obtain their undergraduate CS degree having learned nothing more than Java and a bit of basic web programming. 

 Good article, but I thought most universities did similar things?

We started with Pascal with a heavy focus on Hoare logic, doing babysteps (secure babysteps). Then we moved onto C for seeing OS thingies, CLisp to see another kind of things and Java/PHP/etc to see "modern" web thingies. 

I thought this was the standard (Im finishing the career this year).  Without making any kind of judgement on the greater point of the article...

&amp;gt; because the language is not narrowly designed towards a specific purpose like matrix manipulations or building operating systems.

After my 16-or-so-years of programming experience, I've come to believe that a language having a specific purpose is a good thing in practice, as there really is no general-purpose programming language that could imaginably fit all areas where currently domain-specific languages are employed, without introducing a huge paradigm mismatch or reduce the language to use to the lowest common denominator (which often ends up being C.)

&amp;gt; Lisp is designed for representing symbols

Lisp can't even do basic symbolic computations &amp;amp; manipulations (in a more practical way than any other turing-complete language can do it.) That there matrix manipulation language you scoffed at earlier actually [can...](http://www.mathworks.se/products/symbolic/)

&amp;gt; which means it&#8217;s capable of representing anything.

Does that even mean anything? I'm pretty sure that doesn't mean anything. &amp;gt; as there really is no general-purpose programming language that could imaginably fit all areas where currently domain-specific languages are employed

That's what macros, metaprogramming, and language extension is for. Works well for various Lisps (Clojure, Racket, CL, etc.), Scala, and others.

&amp;gt; Lisp can't even do basic symbolic computations &amp;amp; manipulations (in a more practical way than any other turing-complete language can do it.) That there matrix manipulation language you scoffed at earlier actually can...

Sure it can. You can find libraries to do these things in various Lisps. That said, I have no idea what "representing symbols" has to do with modern Lisp implementations.

&amp;gt; Does that even mean anything? I'm pretty sure that doesn't mean anything.

Nope. Maybe it's an allusion to how SICP chooses to represent everything using quoted s-expressions. Which is unfortunate because it's not pedagogically useful or useful in practice to think of everything as a list (note how real Lisp programs use objects, records, vectors, and so on). &amp;gt; That's what macros, metaprogramming, and language extension is for. Works well for various Lisps (Clojure, Racket, CL, etc.), Scala, and others.

Those are nice, but only address a very very tiny part of what makes up a special-purpose language, namely the syntax. You also want things like certain semantic properties, certain properties of the implementation etc etc. You could "style" your lisp dialect that way to resemble languages that are typically used to query databases or program micro-controllers, but that doesn't make it actually suitable to do the aforementioned tasks. Your database-macros will still be written in a language that doesn't make a lot of guarantees and hence can't be compiled to database queries efficiently, and your language implementation will still make it too big to fit into the ROM of a typical microcontroller.

&amp;gt; Sure it can. You can find libraries to do these things in various Lisps.

Yeah, but as I said, just like in any other language, nothing beyond that. The author of the article however seems to claim that this is somehow a particularly outstanding feature of lisp that gives it special powers. &amp;gt; Those are nice, but only address a very very tiny part of what makes up a special-purpose language, namely the syntax. 

That's not really true. For example, Racket's macro system is powerful enough to write a type system or a language with entirely different semantics (e.g., a lazy language, Algol, etc.) Clojure people are also trying to do something similar with types. Well, it's blurring the lines quite a bit between what constitutes writing a new language and extending the old one, but I think the problems I've raised will still apply regardless. To use it as a database query language, you need to have an efficient compiler that compiles it to primitives the database backend can understand anyway, and you probably don't want to tack an interpreter for a turing-complete language to your database. And the problem that you can't change the implementation properties won't go away regardless. Unless, that is, you make your program output assembly or something to that effect that decouples it from the original implementation -- but then you've effectively just implemented a compiler (which you also can do in any other language, and there are domain-specific tools for that as well that make it very convenient) &amp;gt; Well, it's blurring the lines quite a bit between what constitutes writing a new language and extending the old one

Exactly. See [Languages as libraries](http://www.ccs.neu.edu/racket/pubs/pldi11-thacff.pdf).

&amp;gt; but I think the problems I've raised will still apply regardless

Why?

&amp;gt; To use it as a database query language, you need to have an efficient compiler that compiles it to primitives the database backend can understand anyway

Yes, and you can implement such a compiler as a library. What's the issue here?

&amp;gt; and you probably don't want to tack an interpreter for a turing-complete language to your database

I think most SQL implementations are Turing-complete.

&amp;gt; And the problem that you can't change the implementation properties won't go away regardless

What are you trying to say here? You can write your own macros on top.

&amp;gt; Unless, that is, you make your program output assembly or something to that effect that decouples it from the original implementation

You can make your output *anything you like*. You can compile a language down to C API calls without producing anything resembling the usual "output" from a compiler, yet all the usual steps of a compiler (lexing, parsing, semantic analysis, symbol tables, all sorts of optimizations, "code" generation) are still present. That's what I've been doing for my masters thesis, for example.

&amp;gt; but then you've effectively just implemented a compiler (which you also can do in any other language, and there are domain-specific tools for that as well that make it very convenient)

And a lot of Lisps *give you* those tools. That's what that homoiconic thing is about. &amp;gt; Yes, and you can implement such a compiler as a library. What's the issue here?

&amp;gt; You can make your output anything you like.

&amp;gt; And a lot of Lisps give you those tools. That's what that homoiconic thing is about.

That was what I was trying to avoid, because then I'm not really using lisp anymore, I've implemented a compiler, so that doesn't really count (in my book, because I'd count that as domaine-specific language then). Certainly lisp is very well suited to do this kind of thing (implementing a domaine-specific language) but other languages can do it as well, really, so that's not really the outstanding feature the author originally purported lisp to have. What the author of the article implied was more along the lines "domaine-specific languages are kinda obsolete, now that we have lisp", (which I disagree with,) while you're arguing more along the lines "lisp is an excellent language to implement domaine-specific languages" (which I agree with, but there are others that are just as well-suited)

(And also some domaine-specific languages will be impossible/impractical to implement in lisp, for instance you couldn't feasibly implement a basic dialect that's supposed to run on a microcontroller in lisp, because your runtime environment would be too large, and you wouldn't feasibly implement a language to program microcontrollers in lisp either, because that would basically require you to have a comprehensive knowledge on how to build a compiler (with efficient bytecode backend and all that), so instead you'd just use one of the existing ones (like C, ThingML or something like that, for instance) The point is that the distinction between languages and libraries is a false one, perpetuated by mainstream languages like Java. Saying that you aren't in Lisp (or any other language) because you have a DSL is a bit disingenuous.

And other languages do do DSLs, but none as well as Lisp with its macros.

Obviously there are some scenarios Lisps or other "high-level" languages aren't suited for, but surely we're talking about the 99% use case here. &amp;gt; The point is that the distinction between languages and libraries is a false one, perpetuated by mainstream languages like Java. Saying that you aren't in Lisp (or any other language) because you have a DSL is a bit disingenuous.

Well, I draw the line when you start outputting code rather than just executing it. But regardless what interpretation you chose, what the original article claimed doesn't really hold up..

&amp;gt; And other languages do do DSLs, but none as well as Lisp with its macros.

If you want to make a DSL and compile it to something, outputting, say, assembly or so, there are others that do just as good or even better -- there are domaine-specific (hurr) languages that are made for just that, like compiler-compiler frameworks that allow you to just plop in a grammar and a few primitives you're basically good to go -- the rest is handled for you. (Can't really give any specific examples as I haven't really used these yet, but I believe a bunch of VMs/compiler frameworks come with these, like parrots PEG (or so) and llvm)

&amp;gt; Obviously there are some scenarios Lisps or other "high-level" languages aren't suited for, but surely we're talking about the 99% use case here.

Well, the point of domaine-specific languages is already that they are used for that remaining 1%, for the other 99% you can use any general-purpose programming language whatsoever anyway. If you define it like that, you can claim that *any* general purpose programming language obsoletes all domaine-specific languages -- but that's not really the point. Well, it's blurring the lines quite a bit between what constitutes writing a new language and extending the old one, but I think the problems I've raised will still apply regardless. To use it as a database query language, you need to have an efficient compiler that compiles it to primitives the database backend can understand anyway, and you probably don't want to tack an interpreter for a turing-complete language to your database. And the problem that you can't change the implementation properties won't go away regardless. Unless, that is, you make your program output assembly or something to that effect that decouples it from the original implementation -- but then you've effectively just implemented a compiler (which you also can do in any other language, and there are domain-specific tools for that as well that make it very convenient) &amp;gt; but then you've effectively just implemented a compiler (which you also can do in any other language, and there are domain-specific tools for that as well that make it very convenient)

That is what macros are. They are a controlled hook into the compiler with many convenient tools for manipulating syntax.
 Yeah, I wasn't really arguing against that, I was more arguing against that domaine-specific languages are obsoleted by using lisp (or that lisp is the only necessary tool to implement domaine-specific languages). (See my reply to sid0) Note: I don't disagree with you about the original article. It makes some bogus-sounding claims.

Lisps are certainly not the only (or necessary) tools for implementing DSLs. However, the point is that its language design happens to be particularly suited for DSLs. You could maybe say the same of Scala (and various other languages), but certainly not of, say, Java.

Also, I don't see why you're distinguishing between DSLs implemented in Lisp as opposed to anything else. In particular, this:

&amp;gt; Well, I draw the line when you start outputting code rather than just executing it.

doesn't make sense because macros do exactly that: *output code*. Macros are just transformations on syntax that hook into a particular compiler framework (i.e., the macro expander). Lisp macros definitely do not execute their input. Without making any kind of judgement on the greater point of the article...

&amp;gt; because the language is not narrowly designed towards a specific purpose like matrix manipulations or building operating systems.

After my 16-or-so-years of programming experience, I've come to believe that a language having a specific purpose is a good thing in practice, as there really is no general-purpose programming language that could imaginably fit all areas where currently domain-specific languages are employed, without introducing a huge paradigm mismatch or reduce the language to use to the lowest common denominator (which often ends up being C.)

&amp;gt; Lisp is designed for representing symbols

Lisp can't even do basic symbolic computations &amp;amp; manipulations (in a more practical way than any other turing-complete language can do it.) That there matrix manipulation language you scoffed at earlier actually [can...](http://www.mathworks.se/products/symbolic/)

&amp;gt; which means it&#8217;s capable of representing anything.

Does that even mean anything? I'm pretty sure that doesn't mean anything.</snippet></document><document><title>Are all linear codes (as in coding theory) ideals/modules over some finite-dimensional algebra? (Or allow similar algebraic structure)</title><url>http://www.reddit.com/r/compsci/comments/uix6n/are_all_linear_codes_as_in_coding_theory/</url><snippet>It's known that cyclic codes are ideals over polynomial (quotient) rings R = F[x]/(x^n - 1), and this is a finite dimensional algebra over F (F being some finite field).

Wikipedia mentions some codes, called Linear codes, are vector spaces over a finite-fields, so let's restrict to those. Does every linear code allow an action by an algebraic structure (monoid/semigroup/algebra/etc.)?

Googling around yields fairly basic references, and sometimes cryptic ones. Wikipedia mentions that modular group-representation theory is used in coding theory, but without references. I would love to know if anyone can provide advanced references for coding theory from an abstract algebra point-of-view. Thank you.  I think you are arriving at modern representation theory in a very roundabout way.   A linear code is a subspace of a vector space of a finite field.  The full vector space is acted on transversely by GL(n, F_q).  You might get more results by looking for structures like Grassmanians and flag varieties and how they might relate to codes.  I would be very very surprised if such a general result exists since it seems like it would rely on a very large breakthrough in representation theory and algebraic geometry [Weil Conjectures](http://en.wikipedia.org/wiki/Weil_conjectures).  (Though I'm no expert on coding theory and I'm certainly no expert in algebraic geometry).  You might find interesting results for subclasses of linear codes that aren't cyclic using some of the more modern techniques.  (Relating cyclic codes to ideals was modern in like the 50-60's) Thank you for this lovely answer. I'm actually hoping to study representation theory/algebraic geometry, I enter gradschool this September. 

I was actually motivated by a representation-theoretic theorem, a certain ''no-go'' theorem from the representation theory of finite dimensional algebras (or quiver theory). 

It is known since the '80s that certain algebras have ''universal'' representation theories, in the sense that they contain a copy of the module category of any and all finitely-generated algebra. So if you could produce a classification of all their representations, this would yield one for all finitely-generated algebra in existence, a herculean task. Such algebras are called ''Wild'', the theorem in question is the Tame-Wild dichotomy.

An interesting fact is that Wild algebras are somewhat ubiquitous. For example, if you take a finite group G whose p-sylow subgroup is not cyclic, then the group algebra F_q[G] is Wild if p divides |G|. (q = p^k, p &amp;gt; 3) This pretty much  makes classifying the representations of such groups over F_q unthinkable.

I was thinking, if some families of codes were naturally identified with, say, representations of finite groups over F_q, then the Tame-Wild dichotomy theorem might have something interesting (or depressing) to say about coding theory.

 Check out this paper then.

http://arxiv.org/abs/1204.5547</snippet></document><document><title>Computer Architecture Simulator?</title><url>http://www.reddit.com/r/compsci/comments/uhgh1/computer_architecture_simulator/</url><snippet>Hello,

Does anyone know of any good computer architecture simulators?  You know, for simulating how to build/use the ALU and other components with gates and such.

In my Computer Science class, the simulator is terrible and extremely hard to use.  I'd like to be able to suggest a new one.

Thanks!  We used logisim, which worked alright. Logisim is a great graphical tool for simulating logic circuits - and it's used by plenty of universities around the globe (http://ozark.hendrix.edu/~burch/logisim/usage.html). I introduced it in a computer architecture course at the University of Copenhagen and it was well received by the students.

In case the OP's class is focusing on the MIPS architecture using the popular textbook 'Computer Organization and Design' by Patterson and Hennessy, I recommend this Logisim library: https://github.com/andersbll/logisim-diku. It contains high-level MIPS components that makes it possible to build a relatively advanced processor (e.g. with pipelining) without loosing the overview because of too many wires! We used logisim, which worked alright. We used logisim, which worked alright.   http://www.xilinx.com/products/design-tools/ise-design-suite/ise-webpack.htm

I did some work with this free version of design software. It will work if you can figure out how to use it. I had some trouble with getting my entire project built and routed etc, but I figured it out after a bit. It was definitely powerful software and reasonably intuitive. I've been a bit unimpressed (UI wise) with anything I've used that does this though, so its a bit of a crapshoot. I'm being forced to use ISE 9.2i and EDK right now...dear lord, it is painful to work with this software. http://www.xilinx.com/products/design-tools/ise-design-suite/ise-webpack.htm

I did some work with this free version of design software. It will work if you can figure out how to use it. I had some trouble with getting my entire project built and routed etc, but I figured it out after a bit. It was definitely powerful software and reasonably intuitive. I've been a bit unimpressed (UI wise) with anything I've used that does this though, so its a bit of a crapshoot.  Well hopefully this link will help.  http://vip.cs.utsa.edu/simulators/ and how to use them by watching this video  http://higheredbcs.wiley.com/legacy/college/silberschatz/0470128720/sample/index.html  Dr. Robbins is amazing. So are his simulators, don't know what I would have done without 'em.  We use Modelsim in our architecture class, it works pretty well. It works, but ye gods its IDE is terrible.

And the TCL errors.

Oh man, the TCL errors.          [simplescalar](http://www.simplescalar.com/) is a fantastic tool. It emulates mips instruction set. I did my major project on it and highly recommend it. It has been used in vast amount of research papers too.</snippet></document><document><title>To Dissect a Mockingbird: A Graphical Notation for the Lambda Calculus with Animated Reduction</title><url>http://dkeenan.com/Lambda/index.htm</url><snippet>  So, I don't know if it's me, but I've always found these diagrams really hard to understand. I think the primary problem I'm having is that I can't actually mentally animate the reduction. It would be nice if someone could dynamically generate the animations.  &amp;gt; Ideally these movies would be shown in real time on a computer screen with many more in-between frames to give the appearance of smooth motion. You may be able to get some sense of motion if you can cause your sight to snap suddenly from each frame to the next. Alternatively you could copy the movie, paste it onto light card, cut out the individual frames and make a 'flick-picture'. The later movies in this document leave out more and more in-between frames to save space, so the 'flick-picture' approach will not work and we will be forced to imagine the motion based on the more detailed movies we will have already seen.

I'm going to carry on reading this, but seriously? If only we had a way of animating our pictures. Here is the first "movie", fig 3:

http://imgur.com/6kSJl

Made in 5 minutes by cropping fig3 into 9 "frames" then:

$ convert -delay 50 img0* -loop 0 output.gif To Dissect a Mockingbird has been around since before gifs were supported. Seriously. &amp;gt; Ideally these movies would be shown in real time on a computer screen with many more in-between frames to give the appearance of smooth motion. You may be able to get some sense of motion if you can cause your sight to snap suddenly from each frame to the next. Alternatively you could copy the movie, paste it onto light card, cut out the individual frames and make a 'flick-picture'. The later movies in this document leave out more and more in-between frames to save space, so the 'flick-picture' approach will not work and we will be forced to imagine the motion based on the more detailed movies we will have already seen.

I'm going to carry on reading this, but seriously? If only we had a way of animating our pictures. Here is the first "movie", fig 3:

http://imgur.com/6kSJl

Made in 5 minutes by cropping fig3 into 9 "frames" then:

$ convert -delay 50 img0* -loop 0 output.gif   I prefer [this graphical notation](http://worrydream.com/AlligatorEggs/). :D   *"I know some of those words" he lied.*   I have a bit of a question...

He states that the Omega bird (defined as omega(omega), where omega = \x.xx) can never listen to anything.  I might be doing this wrong, but doesn't the following hold:

    Omega(Idiot) :=
    omega(omega(Idiot)) :=
    omega(Idiot(Idiot)) :=
    omega(Idiot) :=
    Idiot(Idiot) :=
    Idiot

?  Wouldn't this imply that the Omega bird listens to what it hears? I believe it would rather be

    Omega(Idiot) :=
    (omega(omega))(Idiot) :=
    (omega(omega))(Idiot) ....

You got the associativity wrong. </snippet></document><document><title>Homemade mechanical computer: the FIBIAC</title><url>http://chrisfenton.com/the-fibiac/</url><snippet>  Looks cool, although if it only supports increment and decrement the programming capabilities will be something like Brainfuck. still turing complete :)</snippet></document><document><title>How hard would it be to build a computer and a high level compiler for it if the computer ran on Trinary (or other Multi-valued) Logic?</title><url>http://www.reddit.com/r/compsci/comments/udsyt/how_hard_would_it_be_to_build_a_computer_and_a/</url><snippet>So, someone was saying that computers these days have tolerances such that we can easily detect much more than a single on/off or high/low in the circuits.  Obviously, the current machines, compilers, and manufacturing processes are built around binary circuits, and there's no compelling reason to change that.

But how hard would it be?  We can build the circuits, but could we build them as small and efficiently as current chips?  If we can, is the design of the chips difficult such that advances would be slow?  Would it be hard (or impossible) to build an instruction set that could be translated by a modern compiler?  Would we need new languages, or would we just port C/C++ etc. over?  The most efficient base for computing is *e*. And ternary logic is closer to *e* than binary. This is why the Soviets put a lot of effort into ternary computing. However, the logic circuits are a lot more complicated, which is why their computing projects did not succeed in replacing binary stuff.  &amp;gt; The most efficient base for computing is *e*

Why? I found that this article from American Scientist explained it well [Third Base](http://www.americanscientist.org/issues/issue.aspx?id=3268&amp;amp;y=0&amp;amp;no=&amp;amp;content=true&amp;amp;page=5&amp;amp;css=print) &amp;gt; The most efficient base for computing is *e*

Why? It has to do with information theory and entropy. 

http://en.wikipedia.org/wiki/Entropy_%28information_theory%29    
http://ee.stanford.edu/~gray/it.pdf     
http://www.cl.cam.ac.uk/teaching/0809/InfoTheory/HighLitedErrsNotes.pdf     

the paper that started it all:  
http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf  [deleted] It has to do with information theory and entropy. 

http://en.wikipedia.org/wiki/Entropy_%28information_theory%29    
http://ee.stanford.edu/~gray/it.pdf     
http://www.cl.cam.ac.uk/teaching/0809/InfoTheory/HighLitedErrsNotes.pdf     

the paper that started it all:  
http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf  It has to do with information theory and entropy. 

http://en.wikipedia.org/wiki/Entropy_%28information_theory%29    
http://ee.stanford.edu/~gray/it.pdf     
http://www.cl.cam.ac.uk/teaching/0809/InfoTheory/HighLitedErrsNotes.pdf     

the paper that started it all:  
http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf  I can see that it would be more efficient, but wouldn't it be horrendously impractical to use *e* as a base when storing information? Yep. Sometimes math and reality don't line up well, which you pointed out quite well. I have no clue how you'd have a fractional base. Watching this youtube, it still is hard getting my idea around fractional dimensions, so perhaps a fractional base is similar?  
http://www.youtube.com/watch?v=js9nIgHA5FU  

Nah, screw it, call me old fashioned but I'm sticking to integers for dimensions and bases.   It has to do with information theory and entropy. 

http://en.wikipedia.org/wiki/Entropy_%28information_theory%29    
http://ee.stanford.edu/~gray/it.pdf     
http://www.cl.cam.ac.uk/teaching/0809/InfoTheory/HighLitedErrsNotes.pdf     

the paper that started it all:  
http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf  The most efficient base for computing is *e*. And ternary logic is closer to *e* than binary. This is why the Soviets put a lot of effort into ternary computing. However, the logic circuits are a lot more complicated, which is why their computing projects did not succeed in replacing binary stuff.  &amp;gt;the Soviets put a lot of effort into ternary computing. 

Oh wow, after reading about this just now, apparently they weren't just using Ternary, but [*Balanced* Ternary](http://en.wikipedia.org/wiki/Balanced_ternary), where the digits are "-1", "0" and "1".  I never thought of doing a number system like that!  Apparently it has a number of computational advantages.   I think it has a lot of advantages until you figure out two's complement, and once you figure out two's complement then balanced ternary doesn't provide anything new. The most efficient base for computing is *e*. And ternary logic is closer to *e* than binary. This is why the Soviets put a lot of effort into ternary computing. However, the logic circuits are a lot more complicated, which is why their computing projects did not succeed in replacing binary stuff.  The most efficient base for computing is *e*. And ternary logic is closer to *e* than binary. This is why the Soviets put a lot of effort into ternary computing. However, the logic circuits are a lot more complicated, which is why their computing projects did not succeed in replacing binary stuff.  Logical followup... In an analog computer couldn't one actually USE base *e*?     As far as I know, on the software side nothing would really be different. Addresses and instructions would have to be in powers of 3 (or 4 or what have you) but nothing else would be any different (in fact since most of the time those things are referred to in hexadecimal anyway it is already as though the software runs in base 16)

The real difference would come in the math. Computers use a lot of shortcuts based around their binary nature, from bitshifting to boolean logic. Boolean logic in general would be completely meaningless, and that's the basic foundation of all arithmetic and logic in computers! It wouldn't be impossible, but an entirely new set of rules would have to be derived. Imagine working out all of the things in your average digital logic class from scratch, and then resizing them to the scale of a microchip. disagree that boolean logic would be affected by this in any way shape or form.

The c convention that 0 means false and anything else means true would not be affected by anything.

Boolean logic i.e. everything being either TRUE or FALSE would not change, so all the logic structures in programmes would be the same.

All numbers would now be in base 3, so bitshifting would change (it would still be a valid operation except it would now multiple/divide the number by 3. 

Current code that assumes bitshifting works on powers of two would remain identical however compilers would be translating it into trinary/ternary arithmetic under the hood.

The biggest thing that would be different would be the transistors inside chips. There would be a lower noise margin inside the mosfets (you would have two forbidden regions in trinary/ternary and smaller acceptable value ranges). 

This could actually increase the achievable clockspeed as the propagation delay could be a little smaller if it has to move the voltage less to register a logic value change. However there would be more precision required and more vulnerability to parasitic spikes so perhaps the logic level changes would have to be ramped more slowly.

in summary the compiler changes would be easy, the chip changes would be harder, and as a computer user or programmer you would probably not be exposed to much difference if any. What about your ANDs ORs NANDs NORs etc? Wouldn't new equivalents have to be devised or can they be adapted? If you stay with two-valued logic and adopt the convention that 0 is false and any nonzero value is true, then no, you won't have to change at any level.

You could, however, adopt multivalued logic to go with your ternary hardware (for example, TRUE, FALSE, and DONT_CARE), in which case you might need to design new logic gates to deal with that correctly in hardware. Or TRUE, FALSE, and SOMETIMES, where SOMETIMES is essentially the same as saying random() % 2. What about TRUE, FALSE, and UNKNOWN? Like in SQL. If you stay with two-valued logic and adopt the convention that 0 is false and any nonzero value is true, then no, you won't have to change at any level.

You could, however, adopt multivalued logic to go with your ternary hardware (for example, TRUE, FALSE, and DONT_CARE), in which case you might need to design new logic gates to deal with that correctly in hardware. What about your ANDs ORs NANDs NORs etc? Wouldn't new equivalents have to be devised or can they be adapted? disagree that boolean logic would be affected by this in any way shape or form.

The c convention that 0 means false and anything else means true would not be affected by anything.

Boolean logic i.e. everything being either TRUE or FALSE would not change, so all the logic structures in programmes would be the same.

All numbers would now be in base 3, so bitshifting would change (it would still be a valid operation except it would now multiple/divide the number by 3. 

Current code that assumes bitshifting works on powers of two would remain identical however compilers would be translating it into trinary/ternary arithmetic under the hood.

The biggest thing that would be different would be the transistors inside chips. There would be a lower noise margin inside the mosfets (you would have two forbidden regions in trinary/ternary and smaller acceptable value ranges). 

This could actually increase the achievable clockspeed as the propagation delay could be a little smaller if it has to move the voltage less to register a logic value change. However there would be more precision required and more vulnerability to parasitic spikes so perhaps the logic level changes would have to be ramped more slowly.

in summary the compiler changes would be easy, the chip changes would be harder, and as a computer user or programmer you would probably not be exposed to much difference if any. disagree that boolean logic would be affected by this in any way shape or form.

The c convention that 0 means false and anything else means true would not be affected by anything.

Boolean logic i.e. everything being either TRUE or FALSE would not change, so all the logic structures in programmes would be the same.

All numbers would now be in base 3, so bitshifting would change (it would still be a valid operation except it would now multiple/divide the number by 3. 

Current code that assumes bitshifting works on powers of two would remain identical however compilers would be translating it into trinary/ternary arithmetic under the hood.

The biggest thing that would be different would be the transistors inside chips. There would be a lower noise margin inside the mosfets (you would have two forbidden regions in trinary/ternary and smaller acceptable value ranges). 

This could actually increase the achievable clockspeed as the propagation delay could be a little smaller if it has to move the voltage less to register a logic value change. However there would be more precision required and more vulnerability to parasitic spikes so perhaps the logic level changes would have to be ramped more slowly.

in summary the compiler changes would be easy, the chip changes would be harder, and as a computer user or programmer you would probably not be exposed to much difference if any. I'd imagine that if we threw out binary that we'd have to throw out hexadecimal as well.
With a byte in binary, we might have values where the weight of the digits would be 8-4-2-1. I'd imagine that with ternary bits, the digit weight must be 27-9-3-1. In order to represent a byte in a single digit similar the functionality of hex, we would need to use a base large enough to support 80 different values, if my math is right. &amp;gt; In order to represent a byte in a single digit similar the functionality of hex, we would need to use a base large enough to support 80 different values, if my math is right.

I think you mean "nibble" instead of byte there, fwiw. And though we might need 80 (or 81) distinct digits for a four-trit representation, there's no reason to use four trits. Or, for that matter, straight ternary.

If we instead use balanced ternary, the largest four-trit value is 1,1,1,1, which is (1+3+9+27) or 40. Unfortunately, that's still more than decimal numbers + alphabet.

So take the largest three-trit value in balanced ternary: 1,1,1. That's 1+3+9, or 13, which we can easily represent as C in triskadecimal (0-9ABC).

Of course, in balanced ternary, you can also have negative numbers. So the smallest number in three trits isn't 0, it's -1,-1,-1, or (-9-3-1), or -13. Fortunately, since it's balanced, we don't need an additional digit symbol. Unfortunately, we do need some way of indicating subtractive values if triskadecimal is to work.

Let's take the number 400. In balanced ternary, that's 1,-1,-1,0,-1,1,1. In triskadecimal, that would be 1 -B -5. Or, perhaps, 1~~B5~~. Or we could use underlines or overlines. In any case, it should be doable.

Triskadecimal is less capable of making English words (a la 0xDEADBEEF), but would be well-suited as a compact representation of balanced ternary.

If we go with non-balanced ternary, it's a different story. The highest three-trit value is 222, or (2+6+18), or 26. Again, we can do this with numbers and letters: 0-9A-P. Not ideal, since it includes both 0 and O, but doable.

And both representations are more efficient than hexadecimal. Non balanced ternary could skip numerals completely, and just use A-Z.  No problem with 0 vs O then. disagree that boolean logic would be affected by this in any way shape or form.

The c convention that 0 means false and anything else means true would not be affected by anything.

Boolean logic i.e. everything being either TRUE or FALSE would not change, so all the logic structures in programmes would be the same.

All numbers would now be in base 3, so bitshifting would change (it would still be a valid operation except it would now multiple/divide the number by 3. 

Current code that assumes bitshifting works on powers of two would remain identical however compilers would be translating it into trinary/ternary arithmetic under the hood.

The biggest thing that would be different would be the transistors inside chips. There would be a lower noise margin inside the mosfets (you would have two forbidden regions in trinary/ternary and smaller acceptable value ranges). 

This could actually increase the achievable clockspeed as the propagation delay could be a little smaller if it has to move the voltage less to register a logic value change. However there would be more precision required and more vulnerability to parasitic spikes so perhaps the logic level changes would have to be ramped more slowly.

in summary the compiler changes would be easy, the chip changes would be harder, and as a computer user or programmer you would probably not be exposed to much difference if any. I was more referencing how actual logic gates would no longer have any meaning. A physical AND gate makes sense for boolean logic, but in ternary logic, what is 1 AND 2? In C and other languages that use 0 = false other = true, there would be no change. Logic gates could operate using the same principle, but since all arithmetic operations depend on logic gates, this would mess those up. You would end up needing new ways to construct the arithmetic and logic circuits.  disagree that boolean logic would be affected by this in any way shape or form.

The c convention that 0 means false and anything else means true would not be affected by anything.

Boolean logic i.e. everything being either TRUE or FALSE would not change, so all the logic structures in programmes would be the same.

All numbers would now be in base 3, so bitshifting would change (it would still be a valid operation except it would now multiple/divide the number by 3. 

Current code that assumes bitshifting works on powers of two would remain identical however compilers would be translating it into trinary/ternary arithmetic under the hood.

The biggest thing that would be different would be the transistors inside chips. There would be a lower noise margin inside the mosfets (you would have two forbidden regions in trinary/ternary and smaller acceptable value ranges). 

This could actually increase the achievable clockspeed as the propagation delay could be a little smaller if it has to move the voltage less to register a logic value change. However there would be more precision required and more vulnerability to parasitic spikes so perhaps the logic level changes would have to be ramped more slowly.

in summary the compiler changes would be easy, the chip changes would be harder, and as a computer user or programmer you would probably not be exposed to much difference if any. &amp;gt; There would be a lower noise margin inside the mosfets (you would have two forbidden regions in trinary/ternary and smaller acceptable value ranges).

How feasable would it be to use balanced ternary, with +1 at +1.8V (or whatever your logic high is), 0 at 0V, and -1 at -1.8V ? Well that would not be any different from calling it 0, 1.8, 3.6v. So either you have a 3.6v range with roughly the same size noise margin as before (but higher power consumption, propagation time?), or if you implemented it with a range of 1.8v then you would be required to have something like half the noise margin. I had assumed the latter in my post but perhaps you would need to go down the route you describe.  The word is ternary, not trinary. And it's been done. By Russian mathematicians. In 1958. [Setun](http://en.wikipedia.org/wiki/Setun). TIL.  I always figured the words were used depending on the context:

* Primary, Secondary, Ternary
* Unary, Binary, Trinary

Guess I was wrong. :)

Edit: Looks like after secondary is tertiary, not ternary.  So I was really wrong. Where does tertiary fit into this then? The word is ternary, not trinary. And it's been done. By Russian mathematicians. In 1958. [Setun](http://en.wikipedia.org/wiki/Setun). Also, most modern flash cells are MLC (multi-level cell) which have 4 possible states per cell instead of just 2. But that's a power of two so isn't that isomorphic to just encoding 2 bits rather than a single one? Yes, it is. But there's nothing special about ternary either. There's no calculation it can do that binary can't, nor any calculation it can do better. It's just that converting between binary and ternary isn't quite as clean as between binary and quaternary. Yes, it is. But there's nothing special about ternary either. There's no calculation it can do that binary can't, nor any calculation it can do better. It's just that converting between binary and ternary isn't quite as clean as between binary and quaternary. The word is ternary, not trinary. And it's been done. By Russian mathematicians. In 1958. [Setun](http://en.wikipedia.org/wiki/Setun). wikipedia disagrees: http://en.wikipedia.org/wiki/Ternary_computer

&amp;gt; A **ternary computer** (also called **trinary computer**) is a [computer](http://en.wikipedia.org/wiki/Computer) that uses [ternary logic](http://en.wikipedia.org/wiki/Ternary_logic) (three possible values) instead of the more common [binary logic](http://en.wikipedia.org/wiki/Binary_logic) (two possible values) in its calculations.

(formatting re-added because I felt like it.) Citation Needed Well it is /called/ a trinary computer by some, but that doesn't make it right. Oxford's for one does not carry it:

http://oxforddictionaries.com/spellcheck/?region=uk&amp;amp;q=trinary and 
http://oxforddictionaries.com/spellcheck/?region=us&amp;amp;q=trinary

(Sadly,) if it's used often enough, it becomes right, because that's just how language evolves.
 Well it is /called/ a trinary computer by some, but that doesn't make it right. Oxford's for one does not carry it:

http://oxforddictionaries.com/spellcheck/?region=uk&amp;amp;q=trinary and 
http://oxforddictionaries.com/spellcheck/?region=us&amp;amp;q=trinary

(Sadly,) if it's used often enough, it becomes right, because that's just how language evolves.
 Well it is /called/ a trinary computer by some, but that doesn't make it right. Oxford's for one does not carry it:

http://oxforddictionaries.com/spellcheck/?region=uk&amp;amp;q=trinary and 
http://oxforddictionaries.com/spellcheck/?region=us&amp;amp;q=trinary

(Sadly,) if it's used often enough, it becomes right, because that's just how language evolves.
 My point was really just that just because wikipedia says something, doesn't automatically make it correct or fact. The word is ternary, not trinary. And it's been done. By Russian mathematicians. In 1958. [Setun](http://en.wikipedia.org/wiki/Setun). Actually, trinary is also a word. For instance, we say "trinary star system" not "ternary star system". Both are correct wrt star systems.  Trinary? Powers of 3? Or do you mean off, on, and disconnect?
 You should read what wikipedia has to say about [balanced ternary](http://en.wikipedia.org/wiki/Balanced_ternary) it's pretty cool.
Consider each ~~tit~~ trit has three states (-1, 0, 1)
Counting would go like this

    0  0  0 // zero
    0  0  1 // one
    0  1 -1 // two (3-1)
    0  1  0 // three
    0  1  1 // four (3+1)
    1 -1 -1  // five (9-3-1)
 
And so on, negative numbers fit into the scheme beautifully with none of this two's complement nonsense. 

--- 
BTW what's with _disconnect_? The three values should be `TRUE`, `FALSE` and `FILE_NOT_FOUND` You should read what wikipedia has to say about [balanced ternary](http://en.wikipedia.org/wiki/Balanced_ternary) it's pretty cool.
Consider each ~~tit~~ trit has three states (-1, 0, 1)
Counting would go like this

    0  0  0 // zero
    0  0  1 // one
    0  1 -1 // two (3-1)
    0  1  0 // three
    0  1  1 // four (3+1)
    1 -1 -1  // five (9-3-1)
 
And so on, negative numbers fit into the scheme beautifully with none of this two's complement nonsense. 

--- 
BTW what's with _disconnect_? The three values should be `TRUE`, `FALSE` and `FILE_NOT_FOUND` Huh, that's actually a really neat way of representing numbers. How do floating-points work?

Incase anyone didn't get lanzkron's last remark, [reference is here](http://thedailywtf.com/Articles/What_Is_Truth_0x3f_.aspx) Floating point numbers are basically in standard form. The bits (or for this trits) allocated for the number are split up into two, with one part holding the significant digits and one holding the exponent:

Significant digits &#215; base ^ exponent

Both the numbers are stored as integers, so they would follow the convention above. In binary the base is 2, so with trinary it would be 3. You should read what wikipedia has to say about [balanced ternary](http://en.wikipedia.org/wiki/Balanced_ternary) it's pretty cool.
Consider each ~~tit~~ trit has three states (-1, 0, 1)
Counting would go like this

    0  0  0 // zero
    0  0  1 // one
    0  1 -1 // two (3-1)
    0  1  0 // three
    0  1  1 // four (3+1)
    1 -1 -1  // five (9-3-1)
 
And so on, negative numbers fit into the scheme beautifully with none of this two's complement nonsense. 

--- 
BTW what's with _disconnect_? The three values should be `TRUE`, `FALSE` and `FILE_NOT_FOUND` What's wrong with two's complement?  Sure you have to wrap your head around it, but it works beautifully. Two's compliment has the nasty side effect of halving the largest representable positive signed integer. With balanced ternary you'd no longer have to worry about 'signed' and 'unsigned'. You'd just have integers. What's wrong with two's complement?  Sure you have to wrap your head around it, but it works beautifully. You should read what wikipedia has to say about [balanced ternary](http://en.wikipedia.org/wiki/Balanced_ternary) it's pretty cool.
Consider each ~~tit~~ trit has three states (-1, 0, 1)
Counting would go like this

    0  0  0 // zero
    0  0  1 // one
    0  1 -1 // two (3-1)
    0  1  0 // three
    0  1  1 // four (3+1)
    1 -1 -1  // five (9-3-1)
 
And so on, negative numbers fit into the scheme beautifully with none of this two's complement nonsense. 

--- 
BTW what's with _disconnect_? The three values should be `TRUE`, `FALSE` and `FILE_NOT_FOUND` two's complement fixes the -0 problem.  Would that be a problem for balanced ternary? You should read what wikipedia has to say about [balanced ternary](http://en.wikipedia.org/wiki/Balanced_ternary) it's pretty cool.
Consider each ~~tit~~ trit has three states (-1, 0, 1)
Counting would go like this

    0  0  0 // zero
    0  0  1 // one
    0  1 -1 // two (3-1)
    0  1  0 // three
    0  1  1 // four (3+1)
    1 -1 -1  // five (9-3-1)
 
And so on, negative numbers fit into the scheme beautifully with none of this two's complement nonsense. 

--- 
BTW what's with _disconnect_? The three values should be `TRUE`, `FALSE` and `FILE_NOT_FOUND` Trinary? Powers of 3? Or do you mean off, on, and disconnect?
 Frankly, I have no idea what the third (or more) value(s) would be for.  In traditional Trinary logic it seems like it is used for 'indeterminate', but that's not useful.  On the other hand, if we could keep binary logic on a three-valued circuit, are there efficiencies to see from that?  That's at least part of what I'm asking. Trinary? Powers of 3? Or do you mean off, on, and disconnect?
 true, false, and file not found, actually.

edit: doh others beat me to it by hours Trinary? Powers of 3? Or do you mean off, on, and disconnect?
  My guess is compatibility with existing software etc would be a nightmare. Plenty of code hardwires the binary assumption, with " &amp;lt;&amp;lt; 3" being shorthand for "*8' etc. 

So, even if you ported a C compiler, and even if you recompiled existing binaries, I think logic errors would appear. but the compilers would just reimplement this under the hood so that existing code would work the same. that is the efficient bit-shift would be implemented by some other technique that would be less efficient.

If this model became prevalent, new languages might be written that used ternary bitshift as a standard operation.

 but the compilers would just reimplement this under the hood so that existing code would work the same. that is the efficient bit-shift would be implemented by some other technique that would be less efficient.

If this model became prevalent, new languages might be written that used ternary bitshift as a standard operation.

 &amp;gt;so that existing code would work the same

Yeah that's the theory - and I'm saying in practice there would be bugs. What can a compiler do with this:

x = x &amp;lt;&amp;lt; 3;

It can't know whether the semantics of that should now be x *= 8; or x *= 27;

No one has ever written code where the binary nature of the hardware was in question - so assumptions like "1 &amp;lt;&amp;lt; 3 == 8" were perfectly safe.

I imagine gaming hardware and software completely falling over. Think of all the optimisations in mipmapping etc that use 2^N sized textures etc. &amp;gt;so that existing code would work the same

Yeah that's the theory - and I'm saying in practice there would be bugs. What can a compiler do with this:

x = x &amp;lt;&amp;lt; 3;

It can't know whether the semantics of that should now be x *= 8; or x *= 27;

No one has ever written code where the binary nature of the hardware was in question - so assumptions like "1 &amp;lt;&amp;lt; 3 == 8" were perfectly safe.

I imagine gaming hardware and software completely falling over. Think of all the optimisations in mipmapping etc that use 2^N sized textures etc. The c compiler would know that x&amp;lt;&amp;lt;3 &amp;lt;==&amp;gt; x *= 8 because that's what its always meant. (However assembly code might change but its not portable so thats fine). nothing would change there. However the implementation under the hood would be less efficient, as it would be implementing binary bitshift on a trinary system. In fact if we were going to switch over, its possible that processors might include a binary bitshift operator to keep performance of existing code at a reasonable level.

yep I agree performance would be worse for a lot of algorithms that assume base 2. My guess is compatibility with existing software etc would be a nightmare. Plenty of code hardwires the binary assumption, with " &amp;lt;&amp;lt; 3" being shorthand for "*8' etc. 

So, even if you ported a C compiler, and even if you recompiled existing binaries, I think logic errors would appear.        Since nobody has answered your last question, you couldn't use C or C++.  Both of them require numbers to be in binary, so low-level work would be rather inefficient.  You'd need a new low-level language that has ternary primitives built into the language. I'd propose extending C++ by adding in ternary operators (call it C+++), the compiler would have to translate binary ones to ternary but that is doable. &amp;gt; extending

[Not possible](http://www.reddit.com/r/compsci/comments/udsyt/how_hard_would_it_be_to_build_a_computer_and_a/c4uskmb).  You could write a different but similar C++-like language, but it wouldn't be backwards compatible and so wouldn't be an extension. Since nobody has answered your last question, you couldn't use C or C++.  Both of them require numbers to be in binary, so low-level work would be rather inefficient.  You'd need a new low-level language that has ternary primitives built into the language. I'm not sure i follow. They're both languages built with an assumption of binary representations, but nothing in them requires it.

Existing software would probably need to be changed, though. From the C standard, section 6.2.6.1.3:

&amp;gt; Values stored in unsigned bit-fields and objects of type unsigned char shall be
represented using a pure binary notation.^40

&amp;gt; 40) A positional representation for integers that uses the binary digits 0 and 1, in which the values
represented by successive bits are additive, begin with 1, and are multiplied by successive integral
powers of 2, except perhaps the bit with the highest position. (Adapted from the American National
Dictionary for Information Processing Systems.) A byte contains CHAR_BIT bits, and the values of
type unsigned char range from 0 to 2^CHAR_BIT &#8722; 1.

And again, from section 6.2.6.2:

&amp;gt; 1. For unsigned integer types other than unsigned char, the bits of the object
representation shall be divided into two groups: value bits and padding bits (there need
not be any of the latter). If there are N value bits, each bit shall represent a different
power of 2 between 1 and 2^(N-1), so that objects of that type shall be capable of
representing values from 0 to 2^N &#8722; 1 using a pure binary representation; this shall be
known as the value representation. The values of any padding bits are unspecified.

&amp;gt; 2. For signed integer types, the bits of the object representation shall be divided into three
groups: value bits, padding bits, and the sign bit. There need not be any padding bits;
there shall be exactly one sign bit. Each bit that is a value bit shall have the same value as
the same bit in the object representation of the corresponding unsigned type (if there are
M value bits in the signed type and N in the unsigned type, then M &#8804; N ). If the sign bit
is zero, it shall not affect the resulting value. If the sign bit is one, the value shall be
modified in one of the following ways:

&amp;gt;    * the corresponding value with sign bit 0 is negated (sign and magnitude);
&amp;gt;    * the sign bit has the value &#8722;(2^N ) (two&#8217;s complement);
&amp;gt;    * the sign bit has the value &#8722;(2^N &#8722; 1) (ones&#8217; complement).

&amp;gt;     Which of these applies is implementation-defined, as is whether the value with sign bit 1
and all value bits zero (for the first two), or with sign bit and all value bits 1 (for ones&#8217;
complement), is a trap representation or a normal value. In the case of sign and
magnitude and ones&#8217; complement, if this representation is a normal value it is called a
negative zero.

The C++ standard has similar language in it.

There's no reason C or C++ code couldn't run in a virtual machine, but virtual machines are pretty terrible at low-level work, so you'd need a different language for that.</snippet></document><document><title>Native vs interpreted applications, is there a strict definition? </title><url>http://www.reddit.com/r/compsci/comments/uel6x/native_vs_interpreted_applications_is_there_a/</url><snippet>I'm having trouble understanding the difference the term seems to be broad and there doesn't seem to be a consensus. Can somebody explain it to me like I'm 12 year old just beginning to learn computers?      </snippet></document><document><title>Does this class of problem exist? I'll call it "optimal-complete" for lack of a better term.</title><url>http://www.reddit.com/r/compsci/comments/uendq/does_this_class_of_problem_exist_ill_call_it/</url><snippet>Are there problems out there where finding an optimal bound on either the running time or some metric in optimization problems requires finding an algorithm to solve the problem optimally?   Certainly there are, if I'm understanding you. Upper bounds on running time are for algorithms, not problems. There are plenty of problems where we haven't proven that the best available algorithm is optimal. Matrix multiplication is a good (and kind of topical) example (see http://rjlipton.wordpress.com/2011/11/29/a-breakthrough-on-matrix-product/). Right, bounds are for algorithms, but they can also be for groups of algorithms. So you can ask the question "Out of all algorithms, what is the fastest algorithm that solves problem x". 

Take comparison sorting. Comparison sorting isn't an algorithm, you can phrase it as a problem by saying given a list where all that's known is certain comparison relations, sort that list. You can show that of all algorithms which solve it, the best you can do is nlogn average time. But the important part is you can show it without referring to any particular  sorting algorithm. You could have shown the optimal comparison sorting algorithm is nlogn without even knowing of any algorithm that runs in nlogn.

Certain algorithms can be shown to be optimal by induction or assuming that it's not optimal and coming to a contradiction, but you need the algorithm beforehand in order to do that. I'm asking if there are any problems where the optimal running time of an algorithm solving it cannot be deduced without already having an algorithm which solves it. Right, bounds are for algorithms, but they can also be for groups of algorithms. So you can ask the question "Out of all algorithms, what is the fastest algorithm that solves problem x". 

Take comparison sorting. Comparison sorting isn't an algorithm, you can phrase it as a problem by saying given a list where all that's known is certain comparison relations, sort that list. You can show that of all algorithms which solve it, the best you can do is nlogn average time. But the important part is you can show it without referring to any particular  sorting algorithm. You could have shown the optimal comparison sorting algorithm is nlogn without even knowing of any algorithm that runs in nlogn.

Certain algorithms can be shown to be optimal by induction or assuming that it's not optimal and coming to a contradiction, but you need the algorithm beforehand in order to do that. I'm asking if there are any problems where the optimal running time of an algorithm solving it cannot be deduced without already having an algorithm which solves it. How do you show that an O(nlogn) bound for sorting is attainable without explicitly showing an algorithm?  The proofs I'm aware of just show a lower bound, and then use merge sort to show the bound is attainable.  But just showing a lower bound is not enough, since you could also say that O(1) is trivially a lower bound on sorting. Computer science students usually learn this in an analysis of algorithms course.  There's a great wikipedia article on [comparison based sort](http://en.wikipedia.org/wiki/Comparison_sort).  The basic idea of the proof is to show that the number of comparisons necessary to obtain the  required information about the proper order of the elements is, in the worst case, always O(n lg n).  This is commonly done by sketching out a decision tree, and showing that a tree that makes a sufficient number of decisions to sort the elements must have hight of \Omega(n lg n).  This approach to the proof is sketched out [here](http://oucsace.cs.ohiou.edu/~razvan/courses/cs404/lecture09.pdf). Sorry, I think my notation was confusing, but the it seems like proof shows that a sorting algorithm must be \Omega(n lg n), not that a sorting algorithm that's O(n lg n) exists. Ahh, I completely see that now reading the original question more carefully.  You're asking if there is an existence proof of an O(n lg n) sorting algorithm that does not include the algorithm which, itself, attains that bound.  I'm not aware of any such proof, but I'd be interested in seeing one.</snippet></document><document><title>Favorite CS related quotes from professors (or coworkers)?</title><url>http://www.reddit.com/r/compsci/comments/ucj9t/favorite_cs_related_quotes_from_professors_or/</url><snippet>There are plenty of quotes from big names that have been floating around the internet forever, but I'm curious what little bits of hilarity or wisdom have been dropped in your presence that the world should know about.

&amp;gt; The biggest issue you'll face in security, and in life, is your own stupidity.

-- Jonathan Walpole teaching Operating Systems

&amp;gt; What happens in the left subtree, stays in the left subtree. (Discussing parsing)

&amp;gt; Java does its best to keep us from shooting ourselves in the foot, whereas C tells us that knives and guns are fun! (Discussing type systems)

-- both Andrew Tolmach teaching Compilers

Both from Portland State University.

What are yours?      Weeks of coding can save you hours of planning. Weeks of coding can save you hours of planning. Weeks of coding can save you hours of planning. Weeks of coding can save you hours of planning.     &amp;gt; In our business, one in a million is next Tuesday.

Gordon Letwin

http://blogs.msdn.com/b/larryosterman/archive/2004/03/30/104165.aspx  "...and THAT's Djikstra's shortest-path algorithm! If that doesn't excite you, then you should think about switching majors."

-- Glenn Downing, University of Texas, Spring 2005 I hope he was showing something like the one-or-two-character difference between Dijkstra's and Prim's algorithms. 

Dijkstra's algorithm on its own really isn't that exciting. He showed that by simply swapping the underlying data structure from a stack to a queue, you can change the algorithm from being a breadth-first search to a depth-first search. *That's* exciting.   "Why do I teach MIPS instead of x86 in this class? So, suppose you have a small house. One day you decide you want to make the house bigger, so you go to the house next door, blow it up, and pile the rubble on top of your house. Then you decide to make it even bigger, so you go down the street, blow up another house, cart the rubble over, and dump it on your house. Then you drive into the city, blow up a skyscraper, put the rubble on a bunch of dump trucks, cart them over to your house and pile the rubble on top. This, in essence, is the x86 architecture."

-- Stephen Edwards teaching Fundamentals of Computer Systems at Columbia University  "I've determined that powerpoint is non-deterministic"   This was my fav comment in this thread, but it was deleted. I don't care, it's awesome and should be shared. 

By: /u/is_this_4chon

The scene: Final exam of CS237 - Assembly Language Programming.
The final exams are handed out to the 9 remaining students in the class (week 1 estimates, 60).
After 5 minutes of silence, a student can be heard zipping up backpack and then taking exam to front of class where professor Hagar is sitting.
Student slams the exam papers on the desk and loudly says " Everyone hates you and can't wait until you retire. You are the worst fucking professor."
The kid storms out and slams the door. Hagar is visibly red but says nothing.
After a chuckle, I continue with the mind fucking exam. After 5 minutes of silence Hagar finally speaks up:
Well, somebody has to be.
 The set of professors is well-ordered. Wrong. The set of professors has a unique least element. This was my fav comment in this thread, but it was deleted. I don't care, it's awesome and should be shared. 

By: /u/is_this_4chon

The scene: Final exam of CS237 - Assembly Language Programming.
The final exams are handed out to the 9 remaining students in the class (week 1 estimates, 60).
After 5 minutes of silence, a student can be heard zipping up backpack and then taking exam to front of class where professor Hagar is sitting.
Student slams the exam papers on the desk and loudly says " Everyone hates you and can't wait until you retire. You are the worst fucking professor."
The kid storms out and slams the door. Hagar is visibly red but says nothing.
After a chuckle, I continue with the mind fucking exam. After 5 minutes of silence Hagar finally speaks up:
Well, somebody has to be.
  Not exactly a CS quote, but it can be used in a similar context:

&amp;gt; Electronics are magical things, powered by smoke and sparks! Don't believe me? Pour water over it and watch the smoke and sparks leave the device. Won't work after that, the magic is gone.

* Electronic Principals Air Force Instructor in a very heavy Cajun accent Not exactly a CS quote, but it can be used in a similar context:

&amp;gt; Electronics are magical things, powered by smoke and sparks! Don't believe me? Pour water over it and watch the smoke and sparks leave the device. Won't work after that, the magic is gone.

* Electronic Principals Air Force Instructor in a very heavy Cajun accent  "In theory, there's no difference between theory and practice.  But in practice there is."

-- Attributed to Yogi Berra

  "There is a standard procedure for solving any mathematical problem. First, you pound your head on the wall until you figure it out. Then, you fix the wall." -- Giampiero Pecelli  "64bit Computing is just around the corner. Brace yourselves for 64bit numbers." Old lecturer in the early noughties.

"Saying Java is good because it works on all operating systems is like saying anal sex is good because it works on all genders." "64bit Computing is just around the corner. Brace yourselves for 64bit numbers." Old lecturer in the early noughties.

"Saying Java is good because it works on all operating systems is like saying anal sex is good because it works on all genders." that's not a fair comparison. anal sex IS good because it works on all genders. I'm going to write anal sex programmer on my job application. [deleted] that's not a fair comparison. anal sex IS good because it works on all genders. that's not a fair comparison. anal sex IS good because it works on all genders. Just because it works doesn't make it a good thing. I don't intend to make a huge discussion about this, but many says it's a good thing too, especially for server back-end. Wait, you have anal sex with the back-ends of servers? I didn't know that that was even possible. "64bit Computing is just around the corner. Brace yourselves for 64bit numbers." Old lecturer in the early noughties.

"Saying Java is good because it works on all operating systems is like saying anal sex is good because it works on all genders."  Co-worker, while having his code reviewed:

&amp;gt; The compiler accepts it, why can't you? In all seriousness though, I'd have smacked him with a LISP book.  "The worst thing you can encounter while programming is segfaults.  The worst thing you can encounter while programming is race conditions."
-coworker while learning VHDL   Student A: "K what's the difference between TCP and UDP again?"

Student B: "This is TCP. SYN" [initiates a handshake]

Student B: "This us UDP" [smacks student A in the head] That's not quite right. There are two main differences that I can think of. In UDP, but not in TCP: 

1. in the order in which they were sent. Packets may not arrive

2. There is no guarantee that all of

If you want to avoid these, TCP is generally the thing to use.  &amp;gt; In C++, friends can touch your private parts.

It's an old pun that has various forms, but it makes the 5th grader in me chuckle.
 Swap "parts" for "members" and it's more accurate and just as funny! &amp;gt; In C++, friends can touch your private parts.

It's an old pun that has various forms, but it makes the 5th grader in me chuckle.
 &amp;gt; the 5th grader in me

&#3232;_&#3232;   "Don't try and be tricky. Coding is tricky enough to begin with, and we don't need /your/ super brain trying to make it any harder." My God, this is 90% of my programming philosophy in a single quote. Nice one. "Don't try and be tricky. Coding is tricky enough to begin with, and we don't need /your/ super brain trying to make it any harder." I think I'm going to write this down and look at it everytime I sit down to code something.    This was after one of our professors got extremely ill and another had to fill in teaching his classes. 
&amp;gt;So I didn't expect to have to teach this class until next year so I'm not really sure what I'm doing. First person to gain root access and show me how get's an A on the first test and can leave early.

Network Security class.

This is also the same professor who in my four years with I only saw laugh once. It was in our Functional Languages and Parsing class and I was only half paying attention without any real clue what was going on and he asked what the next step was and awkwardly stared at me. I responded with "Just mod that"
&amp;gt; ha...*Awkward silence and creepy stare*. No. So, did anyone get to leave early? Yeah the one kid who made a game that was essentially checkers but with frogs that had M1000s attached to them for some reason.

He was also the same kid who said to always pick a female character in your MMOs because you're going to be constantly staring at their backend. 

 Yeah the one kid who made a game that was essentially checkers but with frogs that had M1000s attached to them for some reason.

He was also the same kid who said to always pick a female character in your MMOs because you're going to be constantly staring at their backend. 

  Yay Portland! I'm at UP. 

Now onto teh funneh quotes

&amp;gt;It's like if Sam takes your order and hits you over the head with a hammer and when you wake up you have  your cheeseburger sitting in front of you.  


-- Andrew Nuxoll, discussing how operating systems deal with read and write calls.  -Software architecture class for Jr.Sr. CS majors.
Prof: "Some bugs actually become features over the course of a projects development."
Smartass Student: "Sounds like Microsoft's entire product line."
Prof: "No, sounds like your entire Senior Design product line."  Private - Your parts are kept to yourself.
Public - Anyone can play with your parts.
Protected - Only your children can play with your parts.

... on discussing class visibility modifiers in Java.

I will always find this funny.

(Prof. Bernstein at JMU)   &amp;gt;Think of it this way: threads are like salt, not like pasta. You like salt, I like salt, we all like salt. But we eat more pasta.

&amp;gt;-- Larry McVoy

And my favourite:

&amp;gt;The question of whether computers can think is just like the question of whether submarines can swim.

&amp;gt;-- Edsger W. Dijkstra a Dijkstra quote in a thread about quotes from unknowns?

Or he is your co-worker / professor? &amp;gt;Think of it this way: threads are like salt, not like pasta. You like salt, I like salt, we all like salt. But we eat more pasta.

&amp;gt;-- Larry McVoy

And my favourite:

&amp;gt;The question of whether computers can think is just like the question of whether submarines can swim.

&amp;gt;-- Edsger W. Dijkstra  There are only three things you need to learn in this class:

1) 2^10=1024
2) the sum of the first n numbers is n/2*(n+1)
3) the answer to any design question is "it depends"

-Owen Astrachan There are only three things you need to learn in this class:

1) 2^10=1024
2) the sum of the first n numbers is n/2*(n+1)
3) the answer to any design question is "it depends"

-Owen Astrachan  "AI is more or less the set of problems we don't know how to solve."
-- high school cs teacher Wow. CS in high school. Sounds amazing. Haha, sort of. I went to a magnet school in north Jersey. I was technically enrolled in a computer science "academy" thing there, but really none of the teachers were particularly good or really knew what they were talking about, and the classes went at a pace that anyone could pretty easily surpass by self-teaching. It wasn't until my senior year that I had a teacher that actually knew their stuff (he was an ex-stanford professor, and the guy the quote's from), and he wasn't even officially cs (he was in the math dept.). But those classes I took senior year were really good.

edit: spelling (**stand**ford) I don't know how they ever managed to get hard drives to work inside a magnet school.    "The problem with Computer Science, is Computers"
-Programming Abstractions Prof.

"Oh No! The Spinny Rainbow Wheel of Death"
-My (future) Advisor, On the first day of my first CS class I've always called it the baby beach ball of death, but I like that one too.  &amp;gt;Java does its best to keep us from shooting ourselves in the foot, whereas C tells us that knives and guns are fun! (Discussing type systems)

There are a lot of variations of this one. For example:

&amp;gt;C makes it easy to shoot yourself in the foot. C++ makes it hard to shoot yourself in the foot, but if you do then you blow your whole leg off.

It's impossible to write a language that people can't fuck up, so I think it's ok to chuckle at these kinds of quotes even if you like the language in question. It doesn't make you a hypocrite. [There's one for every language out there](http://www-users.cs.york.ac.uk/susan/joke/foot.htm). And I much prefer the Java version on that page. [There's one for every language out there](http://www-users.cs.york.ac.uk/susan/joke/foot.htm). And I much prefer the Java version on that page. [There's one for every language out there](http://www-users.cs.york.ac.uk/susan/joke/foot.htm). And I much prefer the Java version on that page. &amp;gt;Java does its best to keep us from shooting ourselves in the foot, whereas C tells us that knives and guns are fun! (Discussing type systems)

There are a lot of variations of this one. For example:

&amp;gt;C makes it easy to shoot yourself in the foot. C++ makes it hard to shoot yourself in the foot, but if you do then you blow your whole leg off.

It's impossible to write a language that people can't fuck up, so I think it's ok to chuckle at these kinds of quotes even if you like the language in question. It doesn't make you a hypocrite. &amp;gt;Java does its best to keep us from shooting ourselves in the foot, whereas C tells us that knives and guns are fun! (Discussing type systems)

There are a lot of variations of this one. For example:

&amp;gt;C makes it easy to shoot yourself in the foot. C++ makes it hard to shoot yourself in the foot, but if you do then you blow your whole leg off.

It's impossible to write a language that people can't fuck up, so I think it's ok to chuckle at these kinds of quotes even if you like the language in question. It doesn't make you a hypocrite. I remember one that went something like, "C gives you just enough rope to hang yourself, and a few extra feet for good measure." &amp;gt;Java does its best to keep us from shooting ourselves in the foot, whereas C tells us that knives and guns are fun! (Discussing type systems)

There are a lot of variations of this one. For example:

&amp;gt;C makes it easy to shoot yourself in the foot. C++ makes it hard to shoot yourself in the foot, but if you do then you blow your whole leg off.

It's impossible to write a language that people can't fuck up, so I think it's ok to chuckle at these kinds of quotes even if you like the language in question. It doesn't make you a hypocrite.   Some people, when confronted with a problem, think "I know, I'll use regular expressions." Now they have two problems.

--alt.religion.emacs Some people, when confronted with a problem, think "I know, I'll use regular expressions." Now they have two problems.

--alt.religion.emacs     "Don't worry it's eaaaaaasy..."  
-Professor explaining an upcoming midterm exam which ended up with a class average of 28%.  
[EDIT: He actually said it with the long drawn out "ea"]
  Professor: "I almost feel professionally dirty when I teach Python." i'm sure that's not a joke. She's a rather book-centric individual and constantly hates on the language... but she also teaches it. Figure that out. &amp;amp;#3232;\_&amp;amp;#3232; She's a rather book-centric individual and constantly hates on the language... but she also teaches it. Figure that out. &amp;amp;#3232;\_&amp;amp;#3232; She's a rather book-centric individual and constantly hates on the language... but she also teaches it. Figure that out. &amp;amp;#3232;\_&amp;amp;#3232;   Premature optimization is the root of all evil.

-- An old prof who may have been quoting someone else Premature optimization is the root of all evil.

-- An old prof who may have been quoting someone else Premature optimization is the root of all evil.

-- An old prof who may have been quoting someone else   &amp;gt; It's "recur," not "recurse." To recurse is to curse again.
-- Robby Findler   Not exactly a quote, but a conversation from class...

The lecture slide in an intro to C class displays:

&amp;gt; typedef LLNode * (*llnfp_t) (LLNode *, int );

A student, shocked by the barrage of eccentric-looking syntax, says "Why would they do that?"

The instructor Bill Leahy replies immediately: "There was a lot of dope in the 60's."
   My calc is a bit rusty but my prof would say something along the lines of "the limit of an engineering major as gpa approaches 0 is a business major". My calc is a bit rusty but my prof would say something along the lines of "the limit of an engineering major as gpa approaches 0 is a business major".  "Normalization: The key, the whole key, and nothing but the key" - I'm pretty sure someone came up with this before me. "Normalization: The key, the whole key, and nothing but the key" - I'm pretty sure someone came up with this before me.   &amp;gt;"C++ walks into a bar, slaps a waitress on the ass, and demands a drink. C turns to the bartender and says, 'I can't believe how he treats women like objects.'" -- A CS friend of mine    &amp;gt; On two occasions I have been asked, 'Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?' I am not able rightly to apprehend the kind of confusion of ideas that could provoke such a question. 

- Charles Babbage  Google fixed that, though. You can often find the right answers for the wrong questions. It was a valid question. Google fixed that, though. You can often find the right answers for the wrong questions. It was a valid question. That's because Google can  sometimes figure out what we mean based on what we said.      Algorithms are a mispronunciation alluding to their creator: Al Gore. People called them Al-Gore-isms, then it changed to algorithms...

Day 1, Class 1 of Analysis of Algorithms with Dr Baas. Good times.             Grow feathers and go shit in a tree

-- My supervisor (who prefers to remain anonymous =) I can't decide if this counts as a unmatched left parenthesis I can't decide if this counts as a unmatched left parenthesis My editors syntax highlighting says otherwise      All problems in computer science can be solved with a layer of indirection.

CSE stands for common sense engineering.

(More of an EE quote but anyhow) There are three sorts of people in this world, resistive, capacitive and inductive.       [Alan Perlis](http://www.cs.yale.edu/quotes.html) had a lot of great CS quotes.  My favorite is "Any noun can be verbed", although it's not really a CS quote.

              </snippet></document></searchresult>
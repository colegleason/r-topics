<searchresult><compsci /><document><title>
Internally deterministic parallel algorithms can be fast [pdf]</title><url>http://www.cs.cmu.edu/~guyb/papers/BFGS12.pdf</url><snippet>  Is there an English translation somewhere? To someone not already familiar with this subfield this paper is totally unapproachable. In what way are existing parallel algorithms nondeterministic? "Internally deterministic" sounds as if the operations are deterministic within a single thread of the computation, which has always trivially been true. If you write a program with multiple threads of execution in a typical imperative programming language, you will have absolutely no guarantees about the order in which you read and modify shared mutable state. That sort of chaos leads to bugs from race conditions, so an incremental step toward determinism/sanity/predictability is using a  locking construct on mutable state. Within a locked region of code you then have local predictability about which statement runs before another statement, but you still have quite a bit of nondeterminism since you don't know which thread will acquire a particular lock at a given time. 

An even more deterministic program will be 'externally' deterministic--- for a given input you always get back the same output. 

This paper proposes going even further towards predictability: 

&amp;gt; We adopt a strong notion of determinism here, often called internal determinism. Not only must the output of the program be deterministic, but all intermediate values
returned from operations must also be deterministic.

They accomplish this by limiting you to some pre-baked data structured and constructs which have nice properties when accessed/used concurrently. 

 &amp;gt; you will have absolutely no guarantees about the order in which you read and modify shared mutable state

Yes, but "internally deterministic" suggests that they mean that things are deterministic within a single thread, which has always been true, even in typical imperative languages.

&amp;gt; Within a locked region of code you then have local predictability about which statement runs before another statement,

Within a single thread, you have determinism even in _unlocked_ regions of code.

&amp;gt; Not only must the output of the program be deterministic, but all intermediate values returned from operations must also be deterministic.

This seems meaningless. Either the values returned by intermediate operations can be affected by other threads, in which case the OS scheduler means you will not have determinism, or they do not, in which case you are just saying you have determinism within a single thread, which is true for every language ever. If your threads are all incrementing a shared counter but never looking at its value (and, of course, the increment is done in a thread-safe manner) then your algorithm is, by the definition of the paper, 'internally deterministic'. More generally, they're talking about the class of algorithms which use commutative operations along with fork/join parallelism. \

I don't know if that is a usefully large class of algorithms but I guess they think it is, since they went to the trouble of creating a whole bunch of commutative primitives/data structures, implementing algorithms on top of them, and publishing a paper about it. 

Anyway, I agree that they didn't do a good job explaining their particular notion of determinism or showing enough examples of what does and doesn't qualify. Their definition in terms of the trace of a computation DAG isn't intuitive since it's not totally clear which values they do and don't consider 'intermediate'.  &amp;gt; If your threads are all incrementing a shared counter but never looking at its value (and, of course, the increment is done in a thread-safe manner) then your algorithm is, by the definition of the paper, 'internally deterministic'

If you never look at its value it's deterministic even if you increment it in a non-thread-safe way :P Downvoting this nonsense now...</snippet></document><document><title>A new reddit for Capability-Based Security</title><url>http://www.reddit.com/r/capabilities</url><snippet /></document><document><title>A Uni&#64257;ed Theory of Garbage Collection [pdf]</title><url>http://atlas.cs.virginia.edu/~weimer/2008-415/reading/bacon-garbage.pdf</url><snippet>  &amp;gt;We present a formulation of the two algorithms that shows that
they are in fact duals of each other. Intuitively, the difference is that
tracing operates on live objects, or &#8220;matter&#8221;, while reference counting
operates on dead objects, or &#8220;anti-matter&#8221;.

Ugh. &amp;gt;We present a formulation of the two algorithms that shows that
they are in fact duals of each other. Intuitively, the difference is that
tracing operates on live objects, or &#8220;matter&#8221;, while reference counting
operates on dead objects, or &#8220;anti-matter&#8221;.

Ugh. Goofy naming aside, I think this is a very nice/compact way to summarize garbage collection: 

&amp;gt;Tracing garbage collection traverses the object graph forward,
starting with the roots, to &#64257;nd the live data. Reference counting
traverses the object graph forward, starting with the anti-roots (the
set of objects whose reference counts were decremented to 0), to
&#64257;nd dead data. &amp;gt;We present a formulation of the two algorithms that shows that
they are in fact duals of each other. Intuitively, the difference is that
tracing operates on live objects, or &#8220;matter&#8221;, while reference counting
operates on dead objects, or &#8220;anti-matter&#8221;.

Ugh. They should have called it "doesn't-matter".</snippet></document><document><title>Great Ideas in Theoretical Computer Science</title><url>http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-080-great-ideas-in-theoretical-computer-science-spring-2008/</url><snippet>   CMU's class is called Great Theoretical Ideas in Computer Science. I wonder who took the name from whom </snippet></document><document><title>What does randomness look like? [via HN]</title><url>http://www.empiricalzeal.com/2012/12/21/what-does-randomness-look-like/</url><snippet>     Seems kind of obvious. Yes, with randomness there will occasionally be clusters of unlikely events. </snippet></document><document><title>How to explain Hash DoS to your parents by using cats</title><url>http://www.anchor.com.au/blog/2012/12/how-to-explain-hash-dos-to-your-parents-by-using-cats/</url><snippet>     So let's come up with a "good" hash function for cat names. Any ideas? I wonder how insecure it'd be to take the first character of the sha-1 of the cat's name. There are too many cats named "Lucky" and "Socks".   You would have to hash their name together with fur color and gender. Maybe throw in the breed somewhere. I wonder how insecure it'd be to take the first character of the sha-1 of the cat's name. Assuming you mean the hex representation, it means that (on average) you'd go through 16 cat names to find a collision. For the cat example, it's not likely that the other cat daycare would realistically figure out your hash function. In the filesystem that the cat stuff is a metaphor for, you're not going to have a lot more than a couple dozen "buckets" for the hashes to be sorted into. Using more buckets means it would take more time to brute-force a collision. Okay, so to generalize, let's say we take the first k characters of the sha-1 (and yeah, let's say the hex representation). Does truncating the sha-1 make it significantly easier to invert? If not, then we could presumably truncate it for cases where we don't want as many buckets as there are possible sha-1 hashes. (So for the cat example, maybe 1 or 2 characters, for a filesystem probably more or maybe they just use the whole string) So let's come up with a "good" hash function for cat names. Any ideas?  I thought SHA-1 was broken, and to be avoided?  </snippet></document><document><title>Cache and I/O Efficient Functional Algorithms</title><url>http://www.cs.cmu.edu/~rwh/papers/iolambda/short.pdf</url><snippet /></document><document><title>Illustrating mechanical assemblies: a method to take a 3D model, identify parts, interactions and motions, and automatically produce diagrams and animations showing how it works.</title><url>http://www.engineering.ucl.ac.uk/news/an-automatic-explanation-how-things-work/</url><snippet /></document><document><title>Just submitted my grad school applications. Would it help at all in getting admitted to email professors to enquire about their labs and potential openings??</title><url>http://www.reddit.com/r/compsci/comments/156o8n/just_submitted_my_grad_school_applications_would/</url><snippet>I just submitted my applications and I was wondering if I should email some of the professors at the universities I applied to, whose work really appeals to me, and who I'd like to do research with.

Will this help my chances at all?  Yes, let me explain why.

When you submit an application to a university, you're submitting your application to the university, not the supervisor. A committee is in charge of choosing the top 'x' potential graduate students. Then, the professors looking for new students in their labs look through the top 'x' potential students and tell the committee that they want specific students.

If you had previously contacted a potential supervisor, and you've gotten their interest, then the potential supervisor will look for your name in the list of potential students. Otherwise, they might miss your application by running into another applicant who is also qualified, or just miss you when looking at 'x' applications. Do professors of undergrad also do this? 
 Do professors of undergrad also do this? 
 Yes, let me explain why.

When you submit an application to a university, you're submitting your application to the university, not the supervisor. A committee is in charge of choosing the top 'x' potential graduate students. Then, the professors looking for new students in their labs look through the top 'x' potential students and tell the committee that they want specific students.

If you had previously contacted a potential supervisor, and you've gotten their interest, then the potential supervisor will look for your name in the list of potential students. Otherwise, they might miss your application by running into another applicant who is also qualified, or just miss you when looking at 'x' applications.   I'm kind of surprised by everyone saying yes to this.  I got into the Ph.D. programs at several top 20 CS grad schools (currently attending one of the four #1 ranked programs) and never emailed anyone because many profs I was interested in working with had messages on their homepage to the effect of "Prospective graduate students: please do not contact me, instead simply apply through the department website."

Though not all of them had something like this, it was several of them from several different schools, and as a result I assumed that no contact was very much the norm, and it would just piss people off if I did try contacting the others.  I also asked my undergraduate thesis advisor what she thought, and she told me that I shouldn't send personal emails about admissions to any professors.  

Just my experience though...apparently everyone else thinks I'm wrong so...do what you want I guess? I'm kind of surprised by everyone saying yes to this.  I got into the Ph.D. programs at several top 20 CS grad schools (currently attending one of the four #1 ranked programs) and never emailed anyone because many profs I was interested in working with had messages on their homepage to the effect of "Prospective graduate students: please do not contact me, instead simply apply through the department website."

Though not all of them had something like this, it was several of them from several different schools, and as a result I assumed that no contact was very much the norm, and it would just piss people off if I did try contacting the others.  I also asked my undergraduate thesis advisor what she thought, and she told me that I shouldn't send personal emails about admissions to any professors.  

Just my experience though...apparently everyone else thinks I'm wrong so...do what you want I guess? Can you give more detail on your stats and research area at all? Might vary drastically from program to program. I assumed the same thing you did and just submitted all of my grad school applications last week without any contact or anything like that. Sure, I can give a little bit of detail (not willing to give much more than what's here): my area is AI, here are any 'stats' I can think of:

* GPA: 3.89
* GRE: 800 math, 590 verbal, 6.0 writing
* Papers/research experience: worked with a prof for 2 years, resulted in undergrad thesis, had one conference submission in the works (but not published or even submitted during applications)
* Undergrad: majored in CS at another top 20 school
* Recommendations: had 2 that were likely good, 1 that was likely mediocre  &amp;gt;I just submitted my applications and I was wondering if I should email some of the professors at the universities I applied to, whose work really appeals to me, and who I'd like to do research with.

Yes, actually, you've done this backwards and potentially wasted your grad school application money.  You should e-mail potential professors first, and find out if they have openings in your research interest, and if they're interested in you as an applicant.  If they say yes *then* you give them money to apply.   If you have good contacts in your undergrad you should use those professors to ask on your behalf (if they know people you're interested in).  

Some professors can't (or won't) talk to you like that, so then you have to spend money on the application, but if they can't take on grad students right now applying at all is a waste of money.  

The way this happens here is the list of applicants (all of them) gets sent out to professors, and then they take the ones they want from the list.  If two profs want the same person they talk about it a bit and go from there.  

You asked somewhere else

&amp;gt;My GPA isn't too great (3.3)  ...Do you think I have a shot at places in the top 20 like UC San Diego or Cal Tech?

Without talking to a professor first and having them agree to take you and pick your name out of the list.  No, you don't have a chance.  I'm not being harsh, having someone like you and pick your name out of the list is far easier than having them guess based on your application that they'd like to take you.  We get literally hundreds of applications for about 40 spots, so professors don't read every application, even if they're looking for students.  They skim them for the ones that are interesting or that they recognize.  

&amp;gt;I just submitted my applications 

While not unheard of, applying right now for a january or february or even may start is less than ideal for a full time grad position.  Grants have either already been decided, or aren't going to be decided for some time.  In the US particularly the austerity bomb (fiscal cliff if you prefer) may pose some challenges.  I'm not in the US so I'm not 100% sure, but grant money is the sort of thing likely on the chopping block.   If you're applying for a september start then disregard this paragraph, every school has its own timings.  But if our group had grant money we would try and fill in september since that's when the funding starts.  The only reason we'd take someone part way through is if a bunch of the people we accepted went somewhere else, and now we have an unfilled opening.  It happens of course.  
 &amp;gt;Without talking to a professor first and having them agree to take you and pick your name out of the list. No, you don't have a chance.

That's not necessarily true, my background sounds very similar to his and I managed it. It may be more unlikely but I wouldn't say no chance. Just be sure to apply at a safety school Illmatic &amp;gt;I just submitted my applications and I was wondering if I should email some of the professors at the universities I applied to, whose work really appeals to me, and who I'd like to do research with.

Yes, actually, you've done this backwards and potentially wasted your grad school application money.  You should e-mail potential professors first, and find out if they have openings in your research interest, and if they're interested in you as an applicant.  If they say yes *then* you give them money to apply.   If you have good contacts in your undergrad you should use those professors to ask on your behalf (if they know people you're interested in).  

Some professors can't (or won't) talk to you like that, so then you have to spend money on the application, but if they can't take on grad students right now applying at all is a waste of money.  

The way this happens here is the list of applicants (all of them) gets sent out to professors, and then they take the ones they want from the list.  If two profs want the same person they talk about it a bit and go from there.  

You asked somewhere else

&amp;gt;My GPA isn't too great (3.3)  ...Do you think I have a shot at places in the top 20 like UC San Diego or Cal Tech?

Without talking to a professor first and having them agree to take you and pick your name out of the list.  No, you don't have a chance.  I'm not being harsh, having someone like you and pick your name out of the list is far easier than having them guess based on your application that they'd like to take you.  We get literally hundreds of applications for about 40 spots, so professors don't read every application, even if they're looking for students.  They skim them for the ones that are interesting or that they recognize.  

&amp;gt;I just submitted my applications 

While not unheard of, applying right now for a january or february or even may start is less than ideal for a full time grad position.  Grants have either already been decided, or aren't going to be decided for some time.  In the US particularly the austerity bomb (fiscal cliff if you prefer) may pose some challenges.  I'm not in the US so I'm not 100% sure, but grant money is the sort of thing likely on the chopping block.   If you're applying for a september start then disregard this paragraph, every school has its own timings.  But if our group had grant money we would try and fill in september since that's when the funding starts.  The only reason we'd take someone part way through is if a bunch of the people we accepted went somewhere else, and now we have an unfilled opening.  It happens of course.  
 I'm applying for the Fall. How screwed am I if I email professors at the places I've applied to this week? Is it bad if I email more than one professor at a given university?  Yes, obviously, do you even have to ask? :)

Our department gets hundreds of applications every year, and while there's no actual committee, the professors go through the list, check the candidates that appeal to them (based on stated research interests), and give the thumbs up/down based on what's in the application material. And believe me, application material is always unreadable, and almost always completely non-descript.

If you actually have some professors whose work you're actually interested in, then you contact them with precise reasons *why* you find their work interesting, ideally pointing to work you have already done in the same area. That's how you stick out of the pile.

Don't bother sending in "form letters" with platitudes just to make a connection if you can't actually articulate why you're interested in the work and why you'd be a good candidate; we're getting hundreds of those as well, and they go to the spam folder. Ah thank you that helps a lot. My GPA isn't too great (3.3) partly because I double majored in Mathematical Statistics but I have a good amount of research experience. Some of the modeling and data analysis research at the schools I'm applying to looks really awesome, so hopefully my interest in the subject matter really helps.

Do you think I have a shot at places in the top 20 like UC San Diego or Cal Tech? Ah thank you that helps a lot. My GPA isn't too great (3.3) partly because I double majored in Mathematical Statistics but I have a good amount of research experience. Some of the modeling and data analysis research at the schools I'm applying to looks really awesome, so hopefully my interest in the subject matter really helps.

Do you think I have a shot at places in the top 20 like UC San Diego or Cal Tech? Yes, obviously, do you even have to ask? :)

Our department gets hundreds of applications every year, and while there's no actual committee, the professors go through the list, check the candidates that appeal to them (based on stated research interests), and give the thumbs up/down based on what's in the application material. And believe me, application material is always unreadable, and almost always completely non-descript.

If you actually have some professors whose work you're actually interested in, then you contact them with precise reasons *why* you find their work interesting, ideally pointing to work you have already done in the same area. That's how you stick out of the pile.

Don't bother sending in "form letters" with platitudes just to make a connection if you can't actually articulate why you're interested in the work and why you'd be a good candidate; we're getting hundreds of those as well, and they go to the spam folder.        </snippet></document><document><title>Lesser known compiler optimizations?</title><url>http://www.reddit.com/r/compsci/comments/155p3c/lesser_known_compiler_optimizations/</url><snippet>Certain optimizations (such as Common Subexpression Elimination, Dead Code Elimination, etc...) line up well with our intuition of what it means to optimize code and can be found in pretty every kind of compiler. 

Other optimizations aren't often implemented due to their cost or complexity (i.e. loop rearrangement via polyhedral analysis) but are still at least well known. 

Do you know of some optimizations which haven't gained much publicity but you think are powerful, theoretically interesting, or at least useful for particular domains?   I'm a huge fan of stream fusion, an optimization which eliminates intermediate data structures when you are doing lots of operations on streams. I'm a huge fan of stream fusion, an optimization which eliminates intermediate data structures when you are doing lots of operations on streams. Got any good links on the topic? I'm a huge fan of stream fusion, an optimization which eliminates intermediate data structures when you are doing lots of operations on streams.  I think people dramatically underestimate what JITs (say, the JVM) is capable of, especially when it comes to inlining.

Java code makes use of interface calls all the time, but the JVM almost never executes an actual interface call, because by the time methods gets inlined a few times, you get to a call site that nearly always has only a small number of possible targets (determined via profiling), and frequently just 1! Which, of course, allows for more inlining!

On top of this inlining feeding back on itself, it can inline across module boundaries, which is a monstrously huge benefit.  Java is infamous for its monstrous frameworks that look like they should perform like dogs, but 80% of the overhead of all that abstraction just gets annihilated by inlining. (And then everyone decides they're dogs anyway, due to memory usage, but that's a different problem... that might actually be solved somewhere in the neighborhood of Java 9... [maybe](https://blogs.oracle.com/jrose/entry/value_types_in_the_vm)) People also tend to dramatically overestimate what chunk of a program even *needs* optimization.

Take OCaml for example - the compiler actually does very little optimization, and yet the language is still very fast.

The reason for this is basically the run-time environment - it includes a very mature, well tuned memory allocator and garbage collector. This is really the biggest issue with performance on a language like OCaml. You're allocating things constantly, so that needs to be blazing fast - and it is. Once you've got efficient heap management in place most of the rest of the language translates fairly straightforwardly to efficient machine code. Of course, the difficulty making the memory system concurrent lead them to punt on the idea of parallelism within a single address space for a long time. 

The interesting thing is that the ability to do this relies on certain high-level semantics of the language; It involves all sorts of tricks that an implementation of C's malloc just plain can't do. (Even ignoring the fact that precise garbage collection is undecidable for languages like C.)

Happy cake day. I think people dramatically underestimate what JITs (say, the JVM) is capable of, especially when it comes to inlining.

Java code makes use of interface calls all the time, but the JVM almost never executes an actual interface call, because by the time methods gets inlined a few times, you get to a call site that nearly always has only a small number of possible targets (determined via profiling), and frequently just 1! Which, of course, allows for more inlining!

On top of this inlining feeding back on itself, it can inline across module boundaries, which is a monstrously huge benefit.  Java is infamous for its monstrous frameworks that look like they should perform like dogs, but 80% of the overhead of all that abstraction just gets annihilated by inlining. (And then everyone decides they're dogs anyway, due to memory usage, but that's a different problem... that might actually be solved somewhere in the neighborhood of Java 9... [maybe](https://blogs.oracle.com/jrose/entry/value_types_in_the_vm)) I think people dramatically underestimate what JITs (say, the JVM) is capable of, especially when it comes to inlining.

Java code makes use of interface calls all the time, but the JVM almost never executes an actual interface call, because by the time methods gets inlined a few times, you get to a call site that nearly always has only a small number of possible targets (determined via profiling), and frequently just 1! Which, of course, allows for more inlining!

On top of this inlining feeding back on itself, it can inline across module boundaries, which is a monstrously huge benefit.  Java is infamous for its monstrous frameworks that look like they should perform like dogs, but 80% of the overhead of all that abstraction just gets annihilated by inlining. (And then everyone decides they're dogs anyway, due to memory usage, but that's a different problem... that might actually be solved somewhere in the neighborhood of Java 9... [maybe](https://blogs.oracle.com/jrose/entry/value_types_in_the_vm))  -funswitch-loops

moves conditionals outside a loop if the condition does not depend on the state of the loop.

i.e.
    for(i=0;i &amp;lt; end; ++i)
    {
        if(flag)
          data[i] += i;
        else
          data[i] *= i;
    }

becomes:

    if(flag)
        for(i=0; i &amp;lt; end; ++i)
            data[i] += i;
    else
        for(i=0; i &amp;lt; end; ++i)
            data[i] *= i;

 -funswitch-loops

moves conditionals outside a loop if the condition does not depend on the state of the loop.

i.e.
    for(i=0;i &amp;lt; end; ++i)
    {
        if(flag)
          data[i] += i;
        else
          data[i] *= i;
    }

becomes:

    if(flag)
        for(i=0; i &amp;lt; end; ++i)
            data[i] += i;
    else
        for(i=0; i &amp;lt; end; ++i)
            data[i] *= i;

  RVO is my personal favorite.  Mainly because it can change the behavior of your program.
  http://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html

Ctrl+F: -O3 Great link. I don't understand why you are being down voted.  I think that people are interpreting (or misinterpreting) the response as being snippy, as in:  Why are you asking *us*?  Look it up for yourself! DDD: Wasn't trying to be snippy; I was genuinely trying to be helpful.

The GCC documentation has a pretty comprehensive list and description of compiler optimizations. Since the OP was interested in "lesser known" ones, these probably won't be the standard ones used in -O or -O2. DDD: Wasn't trying to be snippy; I was genuinely trying to be helpful.

The GCC documentation has a pretty comprehensive list and description of compiler optimizations. Since the OP was interested in "lesser known" ones, these probably won't be the standard ones used in -O or -O2. Great link. I don't understand why you are being down voted.  Great link. I don't understand why you are being down voted.  http://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html

Ctrl+F: -O3 Just a heads up. -O3 doens't always provide a speedup compared to -O2. In fact depending in how your code is structured it might produce a worst result than -O2. Just a heads up. -O3 doens't always provide a speedup compared to -O2. In fact depending in how your code is structured it might produce a worst result than -O2.   Completely unrelated, but whyforcome you have that amazingly nifty fixed point combinator after your username? (It's not Y...what is that, a non standard one? ) It's not a fixed point combinator, it's just an infinite loop ;-)  Hah - nice. :)

How'd'ya get it in your username?</snippet></document><document><title>I'm wondering: why does encryption software ask for both random number generation and a password? Wouldn't it make decryption impossible on any system that doesn't have the same pool of entropy?</title><url>http://www.reddit.com/r/compsci/comments/156ce2/im_wondering_why_does_encryption_software_ask_for/</url><snippet>   Most likely the entropy is being used to generate the "salt" or "initialization vector".  All modern encryption algorithms operate on blocks of input - 8 or 16 byte blocks.  As you note, encryption algorithms must be deterministic given a key and an input block (else they would be undecryptable), which means that, if the same 8 (or 16, etc.)-byte block appears in multiple places in the input, it will encrypt to the same 8-byte block in the output.  So, for example, if you used DES to encrypt the input stream "AAAAAAAABBBBBBBBCCCCCCCCAAAAAAAA" using the key "password", will encrypt to 3c0ce178cadc949837902c748faa43d36ce50b6db33d3f653c0ce178cadc9498 (hexadecimally speaking).  Notice that, since the input started and ended with the same 8 bytes, the output also starts and ends with the same 8 bytes: 3c0ce178cadc9498.  Although this is not decryptable to an eavesdropper, it's a very useful bit of information, especially if patterns repeat in multiple places in the source stream.

To combat this, secure encryption algorithms combine blocks together so that this repetition disappears.  CBC is the simplest such combination method - it XOR's the current block with the contents of the previous block.  This way, the repetition disappears.  This creates a problem for the first block, though - how do you chain the first block with the previous block?

The answer is to pad every output with a random sequence of bytes called the initialization vector (the process is called "salting").  These bytes are recognized and discarded by the decryptor, but they must be passed, unencrypted, to the receiver so that it can perform the decryption correctly.

So you're correct - the decryption system DOES (in a sense) need the same pool of entropy, but the practice of attaching IV's actually provides it.  </snippet></document><document><title>Does the impossibility of solving the halting problem depend on the validity of the Church-Turing thesis?</title><url>http://www.reddit.com/r/compsci/comments/155me3/does_the_impossibility_of_solving_the_halting/</url><snippet>I apologise in advance if this is not the subreddit for such questions. I asked this [question](http://www.reddit.com/r/compsci/comments/13rccg/are_the_results_of_theoretical_computer_science/) 24 days ago here. Despite the many answers there, I just wanted to confirm if what I understood was correct. **Does the impossibility of solving the halting problem depend on the validity of the Church-Turing thesis?**    
I initially [asked](http://www.reddit.com/r/AskComputerScience/comments/154i61/does_the_impossibility_of_solving_the_halting/) this over at /r/askcomputerscience, but there weren't any replies.  Without spending a lot of time looking into it, my immediate response is "no".  The Halting Problem is independent of the mechanics of the underlying computational model - it can be stated in terms of LC without reference to TMs, or vice versa (as Turing originally did, I believe).

cypherx's response is also relevant:  the HP is expressed in terms of a particular computational model - it requires being able to compose machines (terms) and reason about their behaviour.  The fact that it's common to express the HP in terms of TM, then use Church-Turing to show that it can be embedded in LC, is mostly due to history.  But it seems to me that the HP can be equally stated in pure LC, Post machines, universal CA, etc without any reference to other computational models.

*Edit: I said Curry-Howard when I meant Church-Turing.  Oops.* In theory of computation, we often talk about "oracle machines" that have more power than an ordinary Turing machine.  My understanding is that the Church-Turing thesis basically states that these "oracle machines" cannot be physically implemented.  In other words, a Turing machine is as powerful as the most powerful computational model.

We know the halting problem is proved to be undecidable for Turing machines (no Turing machine can solve the halting problem for other Turing machines in general).  I don't think anyone has proved that some more powerful machine couldn't solve the Halting problem for Turing machines.  The Church-Turing thesis states that such a "more powerful machine" cannot exist, so it seems that the answer to OP's question is actually *yes*. Thanks - you've clarified what OP is getting at and I think I missed.  I'll try and state it in terms that make it more obvious to me, since I had to re-read your comment a few times to properly grasp it:

HP states that an Oracle for &amp;lt;formal system&amp;gt; cannot be realised in &amp;lt;formal system&amp;gt;.  It leaves open the possibility for a stronger formal system to exist that can construct an oracle.  CT claims that there exist no stronger formal systems than Turing Machines: thus, Oracles cannot exist at all.

So the answer to OP's question is yes, if you take a wider view of "the impossibility of solving the halting problem" than I did.  My narrower worldview missed the point :-). In theory of computation, we often talk about "oracle machines" that have more power than an ordinary Turing machine.  My understanding is that the Church-Turing thesis basically states that these "oracle machines" cannot be physically implemented.  In other words, a Turing machine is as powerful as the most powerful computational model.

We know the halting problem is proved to be undecidable for Turing machines (no Turing machine can solve the halting problem for other Turing machines in general).  I don't think anyone has proved that some more powerful machine couldn't solve the Halting problem for Turing machines.  The Church-Turing thesis states that such a "more powerful machine" cannot exist, so it seems that the answer to OP's question is actually *yes*.  You are correct, Alpha_Q, the unsolvability of the Halting Problem is dependent on the truth of the Church-Turing Thesis, as follows "Every effectively calculable function is a computable function", where "computable" means computable via Turing Machine, and "effectively calculable" refers to the functions that can, by any method, be algorithmically (i.e. mechanically) instantiated. While Turing Machines are creatures of logic, the instantiation criterion brings the CTT within the realm of constructive mathematics, and thus of physics, because any algorithm constructed as such would have an isomorphic physical counterpart (e.g. a set of logic gates). The Physical Church-Turing Thesis makes this explicit, and strengthens the claim to "Any physically realizable calculable function can be computed via Turing Machine (or equivalent)". 

There are, however, other logics (with, for example, transfinite-induction) that can solve the Halting Problem as formulated for Turing Machines, because they allow for the construction of oracle machines with continuous as opposed to discrete tapes (and thus the Diagonalization Argument which proves the Halting Problem incomputable fails, because the Continuous-Tape Machine has real, aleph-one precision).

This then opens an avenue to the realization of super-Turing computation in the physical world, as it is yet an open question as to whether the manipulable substrate of the Universe is discrete (quantum mechanics, chromodynamics, etc. would say so) or continuous (general relativity would oppose). By implicitly making claims of two different fields, mathematical logic and physics, the Church-Turing Thesis ascends further, into the realm of the meta-physical, and demonstrates the integrated nature underlying the foundations of, at least, these two disciplines. A definitive statement from either side would have equally manifest repercussions for the other. I don't agree. First, what is your formal definition of "effectively calculable"? It seems like it is an informal concept, which means that the Church-Turing Thesis is not a formal statement, but an informal statement making the bridge with a formal theory. Why would, then, a formal theorem (the halting problem is unsolvable), depend on an informal statement?

What is you definition of the unsolvability of the halting problem? If formulated as "There is no turing machine that decides whether a given machine halts on a given input"&#185;, I don't think it depends on the Church-Turing thesis. Do you have an alternative formulation? What is the interest of this different formulation, does it tell us anything more interesting, or does it just bundle the Church-Turing thesis inside the theorem?

&#185;: this formalizations have been shown equivalent to "in the untyped lambda-calculus, there is no provably normalizing lambda-term that takes (an encoding) a lambda-term as argument and decides if it is terminating". I don't agree. First, what is your formal definition of "effectively calculable"? It seems like it is an informal concept, which means that the Church-Turing Thesis is not a formal statement, but an informal statement making the bridge with a formal theory. Why would, then, a formal theorem (the halting problem is unsolvable), depend on an informal statement?

What is you definition of the unsolvability of the halting problem? If formulated as "There is no turing machine that decides whether a given machine halts on a given input"&#185;, I don't think it depends on the Church-Turing thesis. Do you have an alternative formulation? What is the interest of this different formulation, does it tell us anything more interesting, or does it just bundle the Church-Turing thesis inside the theorem?

&#185;: this formalizations have been shown equivalent to "in the untyped lambda-calculus, there is no provably normalizing lambda-term that takes (an encoding) a lambda-term as argument and decides if it is terminating".  Is there a general-case halting problem which we can talk about without reference to a particular computational model? When you say "Halting Problem" aren't you already implicitly talking about some particular variant (i.e. "will this TM halt?" "will this lambda term normalize?" etc...)?  We don't need to frame it any more generally than that, since the entire point of the Church-Turing thesis is that all sufficiently powerful models of computation are equivalent with respect to the set of functions they can compute. Any other model of computation you wish to consider could be simulated on a Turing machine.  Please define precisely what you mean by:

1. "the impossibility of solving the halting problem"
2. "the Church-Turing thesis"

I think the first point is what most people argue over. With different notions of what it means, the answer is going to be either "yes" or "no". "the impossibility of solving the halting problem" = The very fact that it cannot be algorithmically determined whether or not a TM (or equivalent) halts on a given input. The problem is that the word "algorithmically" is vague/under-defined, and every time we try to give it a definition it turns to be equivalent to a TM.   </snippet></document><document><title>Can divide-and-conquer algorithms be parallelized well?</title><url>http://www.reddit.com/r/compsci/comments/155jlg/can_divideandconquer_algorithms_be_parallelized/</url><snippet>Think for example of the Cooley-Turkey FFT, or Merge Sort. Say we have an input size of N, with M processors/compute-units available for multithreading. 

For a simple O(N^2) DFT, which does N dot products of vectors of length N, we can make parallel the output of every element of the vector X, where X = Wx (W is the Vandermonde matrix). So our running time can become: O(N^2 / M). One of those simple "throw more CPUs at the problem". 

But for those more complicated divide-conquer-merge algorithms, I'm wondering how throwing more CPUs at the problem can help. These algorithms will have run times looking like O(NlogN), but I don't see clearly that we can improve this to O(NlogN / M) like in the simpler algorithm. 

Assuming we can't parallelize, then this would suggest sticking with the NlogN algorithms until M approaches N/logN, right? ;) 

Would love to know what you guys think, cheers.    This really depends on the problem and solution.  You can determine how parallelizable the algorithm is by analyzing a [data dependency graph](http://en.wikipedia.org/wiki/Dependency_graph) of the algorithm.

An embarrassingly parallel example is the merge sort, but the quicksort does not have the same easily parallelizable property.
 This really depends on the problem and solution.  You can determine how parallelizable the algorithm is by analyzing a [data dependency graph](http://en.wikipedia.org/wiki/Dependency_graph) of the algorithm.

An embarrassingly parallel example is the merge sort, but the quicksort does not have the same easily parallelizable property.
 I've heard quicksort called a "conquer &amp;amp; divide" algorithm b/c at each step most/all the work must be done before the recursive call. This property also makes a parallel implementation less effective. Huh? Quicksort does O(n) work (partition) and then makes two recursive invocations which do work summing to O(n) * (lg(n) - 1) (assuming a favourable partition). The recursive invocations are thus most of the work, and can be run in parallel since they are independent of each other. Before you can make your recursive calls, you first have to rearrange all the elements in the array such that the pivot is the middle and the LHS of the pivot is all less than the pivot and the reverse for the RHS.  </snippet></document><document><title>Top ten algorithms in data mining (2007) [pdf]</title><url>http://some-docs.googlecode.com/files/Top%2010%20algorithms%20in%20data%20mining.pdf</url><snippet>   This is a great place to start if you are looking to gain an awareness some of the more popular data mining algorithms. There is a book of the same name that goes into further detail and provides more detailed explanations for each algorithm. Any particular methods worth watching a couple youtube videos about?     Maybe it's just me, but I *hate* when they showcase it, with that 'math writing' (my brain just collapsed, i cant seem to recall wtf it's called.) instead of just some pseudo code! not everyone knows this stuff. It's likely easier to learn the mathematical notation than it is to convince people with strong backgrounds in math, writing for people with strong backgrounds in math, to abandon that notation in favor of something more accessible to you. It's likely easier to learn the mathematical notation than it is to convince people with strong backgrounds in math, writing for people with strong backgrounds in math, to abandon that notation in favor of something more accessible to you. That's silly. Why'd they cut their audience like that, I don't realize. Not every developer has a strong background in Math.

Thanks for the downvote, though. 1- I didn't downvote you.

2- My point is that developers aren't the audience, computer scientists are. Mathematical notation is efficient, meaningful communication for computer scientists in a similar way to how code is efficient, meaningful communication for developers. In much the same manner as you would expect your readers to understand what a for loop is, and what variable assignment means semantically, because your readers are developers, the authors of papers on algorithms expect their readers to understand mathematical notation because their readers are computer scientists. Totally makes sense, didn't look at it that way.
I'm just sad to see so much good material fly above my head, because I can't read those damn .. glyphs. All I see in there is summation, norms, and dot products. Maybe you can pick these up from Khan Academy's [Linear Algebra videos](https://www.khanacademy.org/math/linear-algebra/v/vector-dot-product-and-vector-length)? Totally makes sense, didn't look at it that way.
I'm just sad to see so much good material fly above my head, because I can't read those damn .. glyphs. That's silly. Why'd they cut their audience like that, I don't realize. Not every developer has a strong background in Math.

Thanks for the downvote, though. The point is that the people working on this level of stuff work at a mathematical level. It's not their job to distil this into pseudo or example code for you. Maybe it's just me, but I *hate* when they showcase it, with that 'math writing' (my brain just collapsed, i cant seem to recall wtf it's called.) instead of just some pseudo code! not everyone knows this stuff. Presenting algorithms such as these for the first time as pseudocode would make zero sense... They're mathematical models that people develop algorithms (and code) around.

I'm trying to come up with an analogy that makes sense... It would be like trying to define integration with some python code that does Riemann sums. Do you know what I mean? So you're saying that showing the end goal makes zero sense?
Right. It's fine if all you want to do is blindly copy code and get something that sort of works okay, but it won't help you understand what is going on at all. Which, in my opinion, is much more important.  It's fine if all you want to do is make blind assumptions, it just doesn't work.
See what I did there? No, I don't. A lot of the algorithms described are incredibly complex in code, taking hundreds to thousands of lines of code in libraries, yet can be explained concisely in a few lines of equations. Furthermore if you try to implement any without knowing the math behind it, you won't know why it isn't working and will get stuck. I've worked with many of these algorithms before.   I have a lot to learn. it is a short read but it went straight over my head.  Wat? Top ten algorithms in data mining!</snippet></document><document><title>Maximal k-Edge-Connected Subgraphs from a Graph - Implementation</title><url>http://www.reddit.com/r/compsci/comments/1549d0/maximal_kedgeconnected_subgraphs_from_a_graph/</url><snippet>I came across this paper: http://www.edbt.org/Proceedings/2012-Berlin/papers/edbt/a36-zhou.pdf
 while doing a course on SNA (Social Network Analysis). I want to detect communities using mutual friend info on fb data. I want to find out if there is a good implementation of the naive approach to finding maximal k-Edge-Connected subgraphs from a graph. Algorithm enthusiasts of Reddit, could you please point me to any?</snippet></document><document><title>Guaranteed node global uid (guid) with only unique connections in a graph?</title><url>http://www.reddit.com/r/compsci/comments/154erc/guaranteed_node_global_uid_guid_with_only_unique/</url><snippet>Working with a "stealth" startup and they have an interesting math/programming challenge.  Looking for help brainstorming.

They are building a graph, and want each node to have a global unique ID.  However, selection of the unique id will not have a registry to check to make sure it's unique.  This guid can be structured however they need, and it could be long, even up to a few kb if needed. Shorter is better, obviously.

The estimate of total nodes will be plus or minus one order on 1 trillion.  New nodes will be added only with connections to existing nodes, each one will have a guid already (the graph is fully connected), but new nodes will only access data/guid value from connected nodes.  At the time we need the guid, there will be ~5-20 connections with other local nodes.

TLDR question:

So the question is - can we make an algorithm thats local in the graph that makes a new id we can argue is **provably unique** over the whole graph?  The inputs are the guid values of the 5-20 initial nodes the new node is connected to, the output is a guid for the new node.

If it cannot be done (our current efforts think this will not be provabe) is there a way to use local info that makes a "probably unique" guid choice more likely to be unique (connected) guid values?  A two or three part serial number would do it. CPU/core ID# + process/thread ID# + either a local counter, nonce, fine-grained timestamp, or cpucycle count. In practice, any random 128 bit GUID will be way larger than you need. btw the scheme I described is *provably* unique btw the scheme I described is *provably* unique  &amp;gt;A universally unique identifier (UUID) is an identifier standard used ...  to uniquely identify information without significant central coordination. In this context the word unique should be taken to mean "practically unique" rather than "guaranteed unique". Since the identifiers have a finite size it is possible for two differing items to share the same identifier. The identifier size and generation process need to be selected so as to make this sufficiently improbable in practice. Anyone can create a UUID and use it to identify something with reasonable confidence that the same identifier will never be unintentionally created by anyone to identify something else. Information labeled with UUIDs can therefore be later combined into a single database without needing to resolve identifier (ID) conflicts.

[Implementations](http://en.wikipedia.org/wiki/Universally_unique_identifier#Implementations)  We're still waiting to hear from you why you can't use an existing provably unique scheme. </snippet></document><document><title>Bayes or chi-squared? Or does it not matter?</title><url>http://webcache.googleusercontent.com/search?q=cache:YqpW5WJQoZkJ:www.inference.phy.cam.ac.uk/mackay/Chi2.ps+&amp;amp;cd=1&amp;amp;hl=en&amp;amp;ct=clnk&amp;amp;gl=us</url><snippet>  I couldn't stand the mangled math, so I converted the original to a pdf [here](https://docs.google.com/open?id=0B4zQ4U4wZZhgTkFmay0xekNfdzQ), if anyone else is interested.  The author started with the hypothesis that chi-squared is taught, preferred, and wrong (within a hypothetical population of undergrad stats students); then went seeking examples to magnify the difference.

In my experience, chi-squared is taught to undergrad stats classes as introductory bi-variate analysis - its shortcomings are widely discussed, and alternatives are taught in upper-level courses.  Bayesian analysis is taught in introductory compsci or machine learning courses, and its shortcomings are also discussed (though not so widely as in the case of chi-squared in stats classes).  

The *real* education takes place in analyzing a data set and constructing multiple hypotheses to test, even in multiple analytic frameworks - be it chi-squared, bayesian, two-tailed, fourier, or other -- you need to understand the strengths and weaknesses of the model you ultimately choose (or weight vs other models) and also understand the predilection of the data for meaningful analysis within each model.

**tl,dr: DUH** Yeah, so the classes I've taken up this semester have taken me up to the "real education" part. Where do I get my hands on this sort of data and how do I start 'playing' around with these sorts of sets? </snippet></document><document><title>Limits on Approximation of NP-Complete Problems</title><url>http://aaronschild.wordpress.com/2012/12/16/hardness-of-approximating-independent-set/</url><snippet> </snippet></document><document><title>Would taking an Intro to Logic course help me become a better programmer?</title><url>http://www.reddit.com/r/compsci/comments/151idd/would_taking_an_intro_to_logic_course_help_me/</url><snippet>Howdy, I am a sophomore undergrad student, currently pursuing a degree in Computer Science. Ideally I would like to become a SE, or perhaps work on a startup with some business-major friends (did I mention I was a college student?). Anyway, I was wondering whether taking a course such as Intro to Logic would help me wrap my head around some higher concepts in CompSci. It seems that logic is the bridge between math and programming, and I feel this class may help. The only thing is, the class is HARD, and I am already taking a pretty heavy courseload next semester. What do you guys think, is it worth it?  The answer to your question is "yes or no", which can be simplified to "yes". The answer to your question is "yes or no", which can be simplified to "yes". The answer to your question is "yes or no", which can be simplified to "yes". The answer to your question is "yes or no", which can be simplified to "yes". No it can't, that's "yes and no".

edit: Appears i'm being downvoted, someone care to explain what i'm missing? Rules of inference are pretty clear on this. And means both must be true. No is not true. Therefor, Yes AND No = False.

However for or, only one must be true (regular or both can be true, exclusive or only if one and only one is true), therefor Yes OR No = True because Yes is true. Even if it's an exclusive or, No is not true, so it's still True. So Yes And No = No, And Yes Or No = Yes, and Yes Xor No = Yes.  I am astonished that you have managed to get to the second year of a CS degree without being _obliged_ to learn about formal reasoning. What school are you at? I actually just got into CompSci this year after taking the introductory class (basic Java programming). I'm at Rutgers, we have a sweet program here. You'll almost certainly have to take discrete mathematics, which covers formal logic. My program requires both a discrete mathematics course and a class called 'Logic, Set Theory and Proofs'. You'll almost certainly have to take discrete mathematics, which covers formal logic. Yeah, I thought most university CS programs included a Discrete Math or Algebraic Structures course (usually in the Mathematics college) as well as a Digital Logic course where you play with chips on breadboards (usually in the Physics, CS, or Engineering college). Yeah, I thought most university CS programs included a Discrete Math or Algebraic Structures course (usually in the Mathematics college) as well as a Digital Logic course where you play with chips on breadboards (usually in the Physics, CS, or Engineering college). I actually just got into CompSci this year after taking the introductory class (basic Java programming). I'm at Rutgers, we have a sweet program here. I actually just got into CompSci this year after taking the introductory class (basic Java programming). I'm at Rutgers, we have a sweet program here. I actually just got into CompSci this year after taking the introductory class (basic Java programming). I'm at Rutgers, we have a sweet program here. Just like what other people here are saying. I'd be surprised if you can get a bachelor degree in computer science without having to take a "logic course" I actually just got into CompSci this year after taking the introductory class (basic Java programming). I'm at Rutgers, we have a sweet program here. I actually just got into CompSci this year after taking the introductory class (basic Java programming). I'm at Rutgers, we have a sweet program here. I am astonished that you have managed to get to the second year of a CS degree without being _obliged_ to learn about formal reasoning. What school are you at? I think a lot of schools forgo the logic in their computer science programs. 
Although I didn't get in, the main reason I wanted to go to Penn State for undergrad was because they have a very successful CS/Philo dual degree program. &amp;gt; I think a lot of schools forgo the logic in their computer science programs.

Really? We even did it in my *high school* comp sci course.  &amp;gt; I think a lot of schools forgo the logic in their computer science programs.

Really? We even did it in my *high school* comp sci course.  My high school vocational school had a basic 2-year technical course on computers. It wasn't even close to computer science: it mostly covered desktop hardware installation and repair, basic networking, and basic Linux server stuff. Even that course had a decent introduction to digital logic, where we learned about truth tables, logical connectives, made simple counters driven by timers and switches that drove 7-segment displays, etc. Mine was how binary numbers worked (though I don't think we got into two's complement, just how to count in binary), basic logic, and object-oriented programming in Java. I think a lot of schools forgo the logic in their computer science programs. 
Although I didn't get in, the main reason I wanted to go to Penn State for undergrad was because they have a very successful CS/Philo dual degree program. Interesting actually.  I could see how the philosophy classes would teach a method of abstract thought, but still, it's strange you forgo a discrete mathematics course. Interesting actually.  I could see how the philosophy classes would teach a method of abstract thought, but still, it's strange you forgo a discrete mathematics course. Interesting actually.  I could see how the philosophy classes would teach a method of abstract thought, but still, it's strange you forgo a discrete mathematics course. My school offers philosophy classes in formal logic, where they do truth tables, etc. While I share your sentiment and frankly find the whole "skipping discrete" idea baffling, I can see someone picking up the relevant math on the fly if you understood the rules of logic from the philosophy side.  I think a lot of schools forgo the logic in their computer science programs. 
Although I didn't get in, the main reason I wanted to go to Penn State for undergrad was because they have a very successful CS/Philo dual degree program. I can understand that; I did CS as a second bachelor's-type thing, but my first degree included a philosophy minor. It made the first part of Discrete Math a breeze because formal logic is applicable to everything. I think a lot of schools forgo the logic in their computer science programs. 
Although I didn't get in, the main reason I wanted to go to Penn State for undergrad was because they have a very successful CS/Philo dual degree program. I think a lot of schools forgo the logic in their computer science programs. 
Although I didn't get in, the main reason I wanted to go to Penn State for undergrad was because they have a very successful CS/Philo dual degree program. &amp;gt;very successful CS/Philo dual degree program.

CS/Philo....sophy? You've got to be kidding me? Sure, why not? I actually think math, philosophy, and computer science are more related than say, computer science and engineering, in some ways. They are the only three sciences that can exist independent of what the physical laws of the universe are, and independent of humans. Bull.  Yup, they are the three subjects that have their roots in pure logic and reasoning. I don't think I have to state my case here for math being immaterial, and hopefully you will understand that philosophy is also just based on logic and built up using immaterial concepts and reasoning. 
Computer science, well sure, to get a computer working you need to understand and use the laws that exist in our physical universe, you need to build transistors and optical drives and things. But that is what a computer engineer does. computer science is implementing algorithms and data structures at a conceptual level. It requires no experience-based knowledge to do computer science, only knowledge of the workings of logic, which are universal and immaterial. I say bull to the fact that math, philosophy, and CS are more related than CS and engineering.

CS + math is engineering. Philosophy doesn't share much at all. I am astonished that you have managed to get to the second year of a CS degree without being _obliged_ to learn about formal reasoning. What school are you at? I am astonished that you have managed to get to the second year of a CS degree without being _obliged_ to learn about formal reasoning. What school are you at? At RPI Intro to Logic is considered a philosophy course; it is not a named requirement for a BS in Computer Science, though we are required to fulfill broad "humanities and social sciences" requirements, which it would contribute to.  Short answer, yes it will help.  Even if OP wasn't a compsci major, I think everyone should know basic logic. It would make the world much better. A-fucking-men. Can I get a a-fucking-men to the a-fucking-men? My final essay, in my own Logic class, was the argument that Logic should be mandatory in all public schools in the U.S. Even if OP wasn't a compsci major, I think everyone should know basic logic. It would make the world much better. With one delusional premise you can prove anything, so skill in logical procedure can easily lead to a false sense of control. I have no idea how to learn or teach human premise making, but human reason starts there. There *is* some cool CS work on machine premise generation.  I think a basic logic class would be great. At the very least, it'll help with reasoning about code. If you decide to use functional programming (and you should--it's awesome), a good base in formal logic will be even more useful. As an illustration, both type theory and the lambda calculus started in logic; languages based on these concepts (Haskell, OCaml) remain closely related to logic. I find using languages like this, coupled with a basic understanding of the theory, makes you far more productive and makes your code far more maintainable. These are particularly important qualities for a startup--you'll be able to prototype faster and convert your prototype code to more stable code far more easily.

Besides, part of the fun of college is biting off more than you can chew :P.

In the future, I'd also consider taking a class on set theory. it's pretty central to essentially all math and comes up quite a bit in any mathematically oriented branch of CS. It's also a good introduction to mathematical reasoning and will make you more adapt at thinking abstractly. I think a basic logic class would be great. At the very least, it'll help with reasoning about code. If you decide to use functional programming (and you should--it's awesome), a good base in formal logic will be even more useful. As an illustration, both type theory and the lambda calculus started in logic; languages based on these concepts (Haskell, OCaml) remain closely related to logic. I find using languages like this, coupled with a basic understanding of the theory, makes you far more productive and makes your code far more maintainable. These are particularly important qualities for a startup--you'll be able to prototype faster and convert your prototype code to more stable code far more easily.

Besides, part of the fun of college is biting off more than you can chew :P.

In the future, I'd also consider taking a class on set theory. it's pretty central to essentially all math and comes up quite a bit in any mathematically oriented branch of CS. It's also a good introduction to mathematical reasoning and will make you more adapt at thinking abstractly.     I don't know what field intro to logic is in (philosophy? math?)

I know a kid who is a senior econ major at my school (who has also taken some compsci), who took an intro to logic philosophy course and loved it because it taught him a lot about just thinking. I'm sure that's why he also loved his compsci classes.

I think you're probably talking about a more mathematical class. I think that type of formal logical thinking is definitely useful for allowing you to practice logical skills, it is definitely something that has the potential to carry over into your programming abilities, logic is a transferrable skills, just like writing cleary in a history class is also a transferrable skill. I don't know what field intro to logic is in (philosophy? math?)

I know a kid who is a senior econ major at my school (who has also taken some compsci), who took an intro to logic philosophy course and loved it because it taught him a lot about just thinking. I'm sure that's why he also loved his compsci classes.

I think you're probably talking about a more mathematical class. I think that type of formal logical thinking is definitely useful for allowing you to practice logical skills, it is definitely something that has the potential to carry over into your programming abilities, logic is a transferrable skills, just like writing cleary in a history class is also a transferrable skill.     I took a formal logic course (from the philosophy department) and while I don't think it's really helped my programming specifically, it definitely gets you in a good mindset and if you enjoy compsci then you'll absolutely love the course.  Worst case scenario you don't get anything out of it.

I found logic a walk in the park, even though others struggled.  Managed to get 105% on a test because everyone else failed and it was scaled up.  If you like logic where you use it currently, just take it.  You'll very likely not regret it.                                No. if you don't yet know everything taught in that course, you should stick with thee humanities  </snippet></document><document><title>Incremental regular expressions [article and library, xpost /r/programming]</title><url>http://jkff.info/articles/ire</url><snippet /></document><document><title>Question about quantum computers?</title><url>http://www.reddit.com/r/compsci/comments/14z04r/question_about_quantum_computers/</url><snippet>I definitely want to make career out of something computer science related. I don't know a lot about quantum computing , so i my question is, how fundamentally different are they from traditional binary computers? and when they become widely used or if they replace the old, will I have to relearn everything i know about how computers work?  Quantum computers are fundamentally different than classical computers in some ways but not in others.

The set of computable functions is the same for both classical computers. We know this because we can fully simulate a quantum computer on a classical computer and vice versa. In some sense, this means that quantum computers are not "more powerful" than classical computers because there is literally nothing that a quantum computer could do that a classical computer could not do if given enough time.

However, there are problems that quantum computers can solve fundamentally faster than classical computers. The best example of this is unsorted list search. In this problem you are given an unsorted list and must determine if some item is in the list. On a classical machine you must read every element in the list in the worst case (if the item is not there). This algorithm (read everything in the list) grows linearly with the size of the list. If you double the size of the list, the algorithm takes twice as long to complete (simplification, but don't worry). A quantum algorithm for this problem, called [Grover's Algorithm](http://en.wikipedia.org/wiki/Grover's_algorithm) solves this problem in N^(1/2) time. This means that if you increase the size of the list by a factor of X, the algorithm takes X^(1/2) times as long to complete. This is *fundamentally* faster than a classical computer. There is provably no classical algorithm that could solve this particular problem in N^(1/2) time. 

What is really interesting is that we don't actually know how much faster quantum machines are than classical machines. We know they offer at least a polynomial speedup but what we *really* want is an exponential speedup. If we had that then all of a sudden a huge number of relevant problems become feasible to solve. Currently, the fastest known classical algorithm for factoring integers takes exponential time to complete but we have a polynomial quantum algorithm for integer factorization (Shor's Algorithm). If we could prove that classical integer factorization *must* take exponential time then we would be able to say that quantum machines can give an exponential speedup. Unfortunately, this proof is at least as hard as P!=NP (since it would imply P!=NP). 

It is not widely believed that quantum computers will completely replace classical computers. They are extremely difficult to program and wouldn't provide much benefit to the kinds of things people do on their PCs. That said, it is very hard to predict the future of computation. Nobody thought that computers would be used for anything other than massive calculations in the 50s, for example. 

Even if quantum computers completely replaced classical computers, the lessons learned in CS are not going anywhere. Things will be different, but the field wont be replaced with something completely new. Cryptography changes dramatically, for example, but most the principles would survive the transition. A field like software verification doesn't change must unless we find some fast quantum SAT solver or something.  good answer! i'd like to point out that while the set of computable functions is the same for QCs, "there is literally nothing that a quantum computer could do that a classical computer could not do if given enough time" is not true.

for example, there are distributed quantum protocols that break classical impossibility results. there's also quantum cryptography, which afaik can do things entirely outside the range of classical cryptography. quantum computer science is a surprisingly varied field! What is an example of a distributed quantum protocol that breaks a classical impossibility result. I'll admit that I am not very well versed in quantum computing (I do security research) but couldn't a classical computer (or network of classical computers) simulate a network of quantum computers? 

Could you also expand on the quantum crypto stuff. As far as I know, the big difference between quantum and classical crypto is that the hardness assumptions change.  there's a distributed task called leader election, where you have a network of computers elect a single computer from among themselves. if you want a leader election protocol to work on any network shape while having a guaranteed bound on number of rounds, then it becomes classically impossible. this is basically because if you have a computers separated across a network generating probabilities independently of each other, you can't guarantee that they won't generate the same random results and do the exact same thing. on a highly symmetric network, this would result in either multiple computers being elected leader or no computers being elected leader, both of which aren't allowed.

but if you share and measure entangled states, you get these correlated probabilities that can circumvent the classical result!
there's a rather in depth paper you can read here: http://arxiv.org/abs/0712.4213

i don't know any specifics about crypto, this is just what i've heard from people in the field. maybe i shouldn't have brought it up, sorry! What is an example of a distributed quantum protocol that breaks a classical impossibility result. I'll admit that I am not very well versed in quantum computing (I do security research) but couldn't a classical computer (or network of classical computers) simulate a network of quantum computers? 

Could you also expand on the quantum crypto stuff. As far as I know, the big difference between quantum and classical crypto is that the hardness assumptions change.  good answer! i'd like to point out that while the set of computable functions is the same for QCs, "there is literally nothing that a quantum computer could do that a classical computer could not do if given enough time" is not true.

for example, there are distributed quantum protocols that break classical impossibility results. there's also quantum cryptography, which afaik can do things entirely outside the range of classical cryptography. quantum computer science is a surprisingly varied field! How is that true, if a classical computer can simulate a quantum computer? How is that true, if a classical computer can simulate a quantum computer? a single classical computer can simulate a single quantum computer, but a network of quantum computers can harness entanglement in a way classical computers cannot.

see my reply to UncleMeat for more information :) Would it be possible to provide hardware to a set of classical computers that allows them to take advantage of entanglement in the same way? (Where the hardware itself is not itself a quantum computer) a single classical computer can simulate a single quantum computer, but a network of quantum computers can harness entanglement in a way classical computers cannot.

see my reply to UncleMeat for more information :) I would say, then, that a single classical computer cannot fully simulate a single quantum computer. Simulation does not require that the classical computer expose the same mechanism as a quantum one could, in the same way that a classical computer would not have to expose cogs and moving parts to simulate a mechanical computer.  But if a mechanical computer has some important behavior that is caused by its cogs and moving parts, then a full simulation would have to simulate that behavior. Which of course is possible, but so is simulating quantum key exchange.

Actually securely interacting with other nodes outside the simulation is not within the scope of a simulation, otherwise it would be the real thing, not a simulation.  If that's the case, then I would maintain that a classical computer cannot simulate a quantum computer. Quantum computers are fundamentally different than classical computers in some ways but not in others.

The set of computable functions is the same for both classical computers. We know this because we can fully simulate a quantum computer on a classical computer and vice versa. In some sense, this means that quantum computers are not "more powerful" than classical computers because there is literally nothing that a quantum computer could do that a classical computer could not do if given enough time.

However, there are problems that quantum computers can solve fundamentally faster than classical computers. The best example of this is unsorted list search. In this problem you are given an unsorted list and must determine if some item is in the list. On a classical machine you must read every element in the list in the worst case (if the item is not there). This algorithm (read everything in the list) grows linearly with the size of the list. If you double the size of the list, the algorithm takes twice as long to complete (simplification, but don't worry). A quantum algorithm for this problem, called [Grover's Algorithm](http://en.wikipedia.org/wiki/Grover's_algorithm) solves this problem in N^(1/2) time. This means that if you increase the size of the list by a factor of X, the algorithm takes X^(1/2) times as long to complete. This is *fundamentally* faster than a classical computer. There is provably no classical algorithm that could solve this particular problem in N^(1/2) time. 

What is really interesting is that we don't actually know how much faster quantum machines are than classical machines. We know they offer at least a polynomial speedup but what we *really* want is an exponential speedup. If we had that then all of a sudden a huge number of relevant problems become feasible to solve. Currently, the fastest known classical algorithm for factoring integers takes exponential time to complete but we have a polynomial quantum algorithm for integer factorization (Shor's Algorithm). If we could prove that classical integer factorization *must* take exponential time then we would be able to say that quantum machines can give an exponential speedup. Unfortunately, this proof is at least as hard as P!=NP (since it would imply P!=NP). 

It is not widely believed that quantum computers will completely replace classical computers. They are extremely difficult to program and wouldn't provide much benefit to the kinds of things people do on their PCs. That said, it is very hard to predict the future of computation. Nobody thought that computers would be used for anything other than massive calculations in the 50s, for example. 

Even if quantum computers completely replaced classical computers, the lessons learned in CS are not going anywhere. Things will be different, but the field wont be replaced with something completely new. Cryptography changes dramatically, for example, but most the principles would survive the transition. A field like software verification doesn't change must unless we find some fast quantum SAT solver or something.  &amp;gt;[quantum computers] are extremely difficult to program

Could you elaborate? What makes it harder?

Is it a technical problem (loading up a new program must be done with hardware modifications), a cognitive problem (quantum computation is more complex to understand), or maybe a cultural problem (there are no high level quantum programming languages and well established practical familiarity yet)? Quantum computers are fundamentally different than classical computers in some ways but not in others.

The set of computable functions is the same for both classical computers. We know this because we can fully simulate a quantum computer on a classical computer and vice versa. In some sense, this means that quantum computers are not "more powerful" than classical computers because there is literally nothing that a quantum computer could do that a classical computer could not do if given enough time.

However, there are problems that quantum computers can solve fundamentally faster than classical computers. The best example of this is unsorted list search. In this problem you are given an unsorted list and must determine if some item is in the list. On a classical machine you must read every element in the list in the worst case (if the item is not there). This algorithm (read everything in the list) grows linearly with the size of the list. If you double the size of the list, the algorithm takes twice as long to complete (simplification, but don't worry). A quantum algorithm for this problem, called [Grover's Algorithm](http://en.wikipedia.org/wiki/Grover's_algorithm) solves this problem in N^(1/2) time. This means that if you increase the size of the list by a factor of X, the algorithm takes X^(1/2) times as long to complete. This is *fundamentally* faster than a classical computer. There is provably no classical algorithm that could solve this particular problem in N^(1/2) time. 

What is really interesting is that we don't actually know how much faster quantum machines are than classical machines. We know they offer at least a polynomial speedup but what we *really* want is an exponential speedup. If we had that then all of a sudden a huge number of relevant problems become feasible to solve. Currently, the fastest known classical algorithm for factoring integers takes exponential time to complete but we have a polynomial quantum algorithm for integer factorization (Shor's Algorithm). If we could prove that classical integer factorization *must* take exponential time then we would be able to say that quantum machines can give an exponential speedup. Unfortunately, this proof is at least as hard as P!=NP (since it would imply P!=NP). 

It is not widely believed that quantum computers will completely replace classical computers. They are extremely difficult to program and wouldn't provide much benefit to the kinds of things people do on their PCs. That said, it is very hard to predict the future of computation. Nobody thought that computers would be used for anything other than massive calculations in the 50s, for example. 

Even if quantum computers completely replaced classical computers, the lessons learned in CS are not going anywhere. Things will be different, but the field wont be replaced with something completely new. Cryptography changes dramatically, for example, but most the principles would survive the transition. A field like software verification doesn't change must unless we find some fast quantum SAT solver or something.   Okay. Since a lot of people explain what Quantum Computing is here but mentioned nothing about it as a career, here's my insight. As a computer science major, a career with Quantum Computers is null, I really don't think it exists. Quantum Computers are still in the early stages (as in there's one somewhere in Yale). In order to get a career in Quantum Computing, a CS degree is nice, but what you really need is a PhD in Physics. This is a field for physicists because we're still discovering a lot about the quantum world and it really hasn't spawned its own field yet (like quantum computer science). I guess if Quantum Computers were mass produced then there would be a CS degree for QM computers. But I don't think that will be for at least 10 years (I might be understating). if you're interested in research, there's a heck of a lot of QC research that can be done from the CS side, without any physics background at all! Okay. Since a lot of people explain what Quantum Computing is here but mentioned nothing about it as a career, here's my insight. As a computer science major, a career with Quantum Computers is null, I really don't think it exists. Quantum Computers are still in the early stages (as in there's one somewhere in Yale). In order to get a career in Quantum Computing, a CS degree is nice, but what you really need is a PhD in Physics. This is a field for physicists because we're still discovering a lot about the quantum world and it really hasn't spawned its own field yet (like quantum computer science). I guess if Quantum Computers were mass produced then there would be a CS degree for QM computers. But I don't think that will be for at least 10 years (I might be understating). There is one company off the top of my head that does create machines for quantum computing: [D-Wave Systems](http://www.dwavesys.com/en/dw_homepage.html)

Ignoring their fairly cheesy website, they have solved some pretty advanced optimization problems using their quantum computers and quantum annealing. Most serious QC researchers are, to put it nicely, a little skeptical of D-Wave's claims. Upvoted because I'm curious.  

What do QC researchers specifically think about D-Wave's claims?  I'd definitely be interested in hearing more. There is one company off the top of my head that does create machines for quantum computing: [D-Wave Systems](http://www.dwavesys.com/en/dw_homepage.html)

Ignoring their fairly cheesy website, they have solved some pretty advanced optimization problems using their quantum computers and quantum annealing.   &amp;gt; and when they become widely used or if they replace the old

This seems unlikely. The set of problems in BQP that are not in P are vanishingly small &amp;amp; have few practical applications. If you're interested in comput*ability* and want to explore it that's great, but if you want to be a practitioner ( ie, computer programmer ) you can safely ignore quantum computation for the forseeable future. Plausibility of quantum computing has plenty of ramifications when it comes to cryptography, which is used plenty in the real world. yes, that's one field of many though, and quantum computing making modern crypto obsolete really has nothing to do with quantum computing from 99% of peoples perspective. It just becomes a story of "crypto is broke, so figure something else out" &amp;gt; and when they become widely used or if they replace the old

This seems unlikely. The set of problems in BQP that are not in P are vanishingly small &amp;amp; have few practical applications. If you're interested in comput*ability* and want to explore it that's great, but if you want to be a practitioner ( ie, computer programmer ) you can safely ignore quantum computation for the forseeable future.  &amp;gt; will I have to relearn everything i know about how computers work?

Not all, but a lot. But that's no different then any other change in computing. You program massively parallel systems other then the old sequential ones, your graphic card has to be handled differently then your CPU, you can't program a neural networks the same as your daily GUI app and so on. Quantum computing is just a another tool in your toolbox and it will be great for some problems and useless for others.  This will sound like a silly questions but have quantum computers actually done anything yet? Or is this all theoretical? Yes, but the scope has been very small. For example, the integer factorization problem discussed by /u/UncleMeat: so far, 21 is the largest number factored on a quantum computer.  This is only because Quantum Computers are exponentially more unstable as you add more and more qubits.

There are many examples of very small QC's that have small numbers of qubits, but they are, for obviously reasons, limited in the calculations that they can perform.

According to [this article](http://www.technologyreview.com/view/426586/worlds-largest-quantum-computation-uses-84-qubits/) the worlds largest QC uses 84 qubits &amp;gt;  Quantum Computers are exponentially more unstable as you add more and more qubits

Is there any hope in overcoming this? It seems like a rather fatal snag. If you are literal about the exponentially that is.  [deleted]</snippet></document><document><title>My first research opportunity; lend me your insights o great ones!</title><url>http://www.reddit.com/r/compsci/comments/14y02f/my_first_research_opportunity_lend_me_your/</url><snippet>Hello everyone, I'm a loooong time lurker and I'm happy to say my first post is to compsci! 

Anyway, I'm entering into my junior year if my C.S. track and have was given the opportunity to take graduate class which is basically an introduction to Artificial Intelligence. Additionally, the professor asked me if there is anything I would like to research on the side related to AI and he would give me one-on-one advice as I take on my first research project. 

Needless to say I was pretty excited and I've started taking Udacity's newly reopened AI class as a preparatory step.

Now, prior to starting to school I spent several years in the military as a network/satellite engineer and as a result have strong background in networking theory and security protocols. I enjoy math, recognizing patterns, and designing things.

Alas, I'm not really sure what to research. From everything I've read on here, the biggest thing I've come to understand is _don't study something you won't be interested in for a long time_.  

Keeping in mind that my project must relate to AI do you all have any ideas to help jump start my brain? The sooner I can get a solid idea, the sooner I can get started working!  Hello!  

I think that one of the most exciting parts of research is looking for the problem that you will address.  I can advise looking for open problems in Artificial Intelligence to get a general idea of the research area, and then maybe look for articles, papers, etc to narrow it down to a fun or useful project.  Have you looked into AI with computer vision or robotics?

I don't know your level of experience, work ethic, or available time during the semester.  I think that it will be useful to come up with ideas
of varying difficulty and intensity and then presenting them all to your professor.  He will have a good idea of what can be accomplished in the amount of time that you have available.

It's great that you are taking advantage of the opportunity, good luck! Thanks for the advice! Are there any specific places you would recommend looking for articles or current events in AI? I know the ACM is a good start but I don't have access (or at least I'm not sure how to gain it yet) their articles. Unfortunately, my school's staff is on their Christmas vacation already and are slow to respond to emails.  Thanks for the advice! Are there any specific places you would recommend looking for articles or current events in AI? I know the ACM is a good start but I don't have access (or at least I'm not sure how to gain it yet) their articles. Unfortunately, my school's staff is on their Christmas vacation already and are slow to respond to emails.  Thanks for the advice! Are there any specific places you would recommend looking for articles or current events in AI? I know the ACM is a good start but I don't have access (or at least I'm not sure how to gain it yet) their articles. Unfortunately, my school's staff is on their Christmas vacation already and are slow to respond to emails.       </snippet></document><document><title>Sleeper theorems.</title><url>http://www.johndcook.com/blog/2012/12/10/sleeper-theorems/</url><snippet>  pretty basic, but the intermediate and mean value theorems always seemed to be useful in more places than i expected them to be</snippet></document><document><title>Apply computer science in another field or go to graduate school?</title><url>http://www.reddit.com/r/compsci/comments/14wxxq/apply_computer_science_in_another_field_or_go_to/</url><snippet>I graduated in May of this year and got a job in one of the big web companies a couple months later. I went in knowing almost nothing about web development and learned more than I ever imagined in that small time frame. 

However, I think my interest is starting to wane. I blame my disinterest as being the cause for wanted change because everyone I work with is incredibly intelligent, the company is one of the best, and I've worked through several different projects on multiple platforms.

My favorite classes in college were the ones that involved a lot of math and theory. I would love to actually get to use and work with some of that stuff, but it seems like a graduate degree is required for most of these positions. I don't mind the idea of returning to school, but I still don't exactly know what I would want to research (machine learning, bioinformatics, and computational theory all sound good). 

Another route that I thought might be cool is applying computer science to another field like programming for chemistry, biology, or physics research or something. This may require extra schooling as well, but I think it would be faster to get into and maybe even more interesting. I've been reading a lot of physics books lately, which has kind of sparked in an interest in that area.

Basically, I'm looking for some insight from someone who has been in this position or done something like this.

edit: I just looked at the sidebar and saw this is probably better suited for /r/cscareerquestions. Apologies.  I've been involved with doing CS for scientists in other fields. It is very inspiring and I highly recommend checking out such interdisciplinary approaches (in Europe often called e-Science). I learn something new everyday, about the exotic problems that people struggle with in physics, chemistry, biology, astronomy, etc. As a CS graduate O would be able to see "Ah. That problem you have with sensor data collection is almost like what we do with distributed database transactions, let me get you in touch with professor X". There are of course other aspects, like "my god, you don't have version control? You store multidimensional data in CSV files?? You call that.. Perl?" I've been involved with doing CS for scientists in other fields. It is very inspiring and I highly recommend checking out such interdisciplinary approaches (in Europe often called e-Science). I learn something new everyday, about the exotic problems that people struggle with in physics, chemistry, biology, astronomy, etc. As a CS graduate O would be able to see "Ah. That problem you have with sensor data collection is almost like what we do with distributed database transactions, let me get you in touch with professor X". There are of course other aspects, like "my god, you don't have version control? You store multidimensional data in CSV files?? You call that.. Perl?" That sounds awesome! I'll definitely have to look into these e-Science positions you speak of. Thanks! 

Did you have an advanced degree or another one in the same area as the position? Do you know if these are normally required?

edit: I just read the e-Science wiki page. That definitely sounds like something I would like to do.  I do bioinformatic research at an Ivy League medical school and have both an undergrad and masters in compsci.  I don't really recommend bioinformatics type work unless you have a strong background in biology and statistics.   Digital Physics.  I read an argument for P vs NP based on that. Was an interesting read that got me thinking about discrete vs continuous physics.  Any chance you can point me to the article.. or the jist of it ?   Cognitive Science 

thank me later. Whoa.

edit: I've done a little research into this before, or at least parts of it. Yeah. It's definitely interesting. I just need to do more research on how to get into it. Whoa.

edit: I've done a little research into this before, or at least parts of it. Yeah. It's definitely interesting. I just need to do more research on how to get into it. Indiana University  has a world renowned graduate program.  Im planning on attending after my bachelors in CS. That's where Douglas Hofstadter is/was professor, wasn't it? Yup.  I took a course with him over using intuitive mental maps of cities to solve euclidean geometry Is he as mentally deficient as his book would suggest? Indiana University  has a world renowned graduate program.  Im planning on attending after my bachelors in CS. Cognitive Science 

thank me later. Oh god, thank you for informing me about this, I really want to study this now. i said you would thank me later ;) Well technically I did thank you later :P 

So if I wanted to do this, I'm specifically interested in combining it with linguistics too, would it be good to major in CompSci, minor in linguistics, then get a degree for Cognitive Science? Or should I major in linguistics too? at IU you pick a concentration of study under cog sci, and linguistics is offered as one of them.  There is also neuro science, informatics, and a few others.  Who cares? Whatever you do stay in school because working for other people just sucks. It sucks.. Every fucking day of it sucks. I don't mind working for someone else as long as I'm interested in what I'm working on. I do. Mostly because in my experience (30 years in the slave force) my bosses have been total assholes. Ignorant morons who make decisions based on internal politics, personal power plays, manipulation and pure fucking ego. 

It really sucks when you find that the thing you are interested in and want to work with is presided over by fucking assholes.  Honestly, this just sounds like you're not very good at interviewing and screening out jobs/bosses you don't want. Hah! Honestly it seems like you live in a fantasy world! When a person wants a job he has to bow and scrape and perjure his soul in any way demanded by his perspective employers. 

Only be demonstrating full understanding of the kow-tow and obsequious behavior can a person get through their initial level interviews or even their advanced interviews.

Basically in the US you fight and struggle for a minimum of one year before finding a job in your chosen field. By then you are ready to kiss the ring so to speak. You must live in some other country where laborers actually have power.

I bless your country and wish I lived in it but I don't. I live in the US of A where the rich corporations rule and everyone else can eat a fucking dick. I'm sorry you feel that way. I really am.

I actually recently graduated from Portland State University and now live in Seattle working in a position that I'm glad I'm a good fit for and glad that I found a project to work on that I can really get behind.  It took me 9 months of searching to find this position, but holding out for something that was a good fit from both my perspective and theirs really paid off. Hah! Honestly it seems like you live in a fantasy world! When a person wants a job he has to bow and scrape and perjure his soul in any way demanded by his perspective employers. 

Only be demonstrating full understanding of the kow-tow and obsequious behavior can a person get through their initial level interviews or even their advanced interviews.

Basically in the US you fight and struggle for a minimum of one year before finding a job in your chosen field. By then you are ready to kiss the ring so to speak. You must live in some other country where laborers actually have power.

I bless your country and wish I lived in it but I don't. I live in the US of A where the rich corporations rule and everyone else can eat a fucking dick. I live in the us and have never had to do that to get a job. I currently havr multiplr standing job offers. Hah sounds like you are just bs'ing. If not congrats but your situation is hardly normal. I am not bsing, but im recently out of grad school. I see. Well the world will look different to you in a decade or so. Go forth and find out young man! Plant your feet in the well trodden path. I do. Mostly because in my experience (30 years in the slave force) my bosses have been total assholes. Ignorant morons who make decisions based on internal politics, personal power plays, manipulation and pure fucking ego. 

It really sucks when you find that the thing you are interested in and want to work with is presided over by fucking assholes.  Wow... Fortunately, my experience thus far is nothing like that, which leads me to believe more and more that my experience is probably one of the better ones you can have in industry. Extend your timeline. I see.. Have you tried going off on your own? Hell yeah I ditched IT years ago. And keep in mind I was making 60k a year. I do salvage and recycling now as a personal business. I got some fingers in other shit too occasionally but mostly I sell salvage. I see how that could be appealing, but I don't want to completely ditch the field yet. I chose my degree because I found it interesting, not because of the money. I just got lucky.  I found it interesting too. But my bosses and their calumny destroyed that very quickly after I achieved my position.

Honestly? I admit it. I loved solving computer problems and I liked helping people. So my position running the help desk for several academic departments was awesome in that way. However more and more my bosses destroyed my ability to render aid and solve problems for my constituents. 

Just to explain it in the baldest terms my bosses took all of the money that each academic department at my university used to buy new computers. They took that money without a well thought out plan for how to spend it or make sure that each department got what it needed.

The resulting chaos and bullshit took over every single function of my job and most of my co-workers. Not only that but while all this chaos was going on instead of trying to fix it my idiot manager had most his staff written technical documentation which his english major ass got to redink and have us technical people redo 40 fucking times to his exacting but retarded specifications. Who cares? Whatever you do stay in school because working for other people just sucks. It sucks.. Every fucking day of it sucks. Luckily, you're in the right field, stick it out for a few years longer and don't let them affect your thoughts, be a Thinker, Concentrate INTJ style on the task at hand whatever it may be, though you're probably a Feeler. You're an efficient robot, ser. Stick it out and at some point you can work remotely.

Source: same feel I don't really understand what you're suggesting, though I'm interesting, since I'm an INTJ. Are you saying that working remotely is the solution to working for other people? Because then, you're still working for other people.

I realize there is probably sarcasm in your comment, but I can't tell exactly where. No sarcasm. As an INTJ, it would be hard (typically) for you to understand. Male INFJs have a hard time ignoring inter-office politics. We must adapt. What about the ENFP? Luckily, you're in the right field, stick it out for a few years longer and don't let them affect your thoughts, be a Thinker, Concentrate INTJ style on the task at hand whatever it may be, though you're probably a Feeler. You're an efficient robot, ser. Stick it out and at some point you can work remotely.

Source: same feel I don't know about all this remote work stuff. There is something strange here. The money saved by having workers work from home is tremendous. You need less bosses, less work space, no need to bother with climate controls or providing safe working conditions. And yet... And yet it seems most people are still forced to make the commute each day. Even though this costs our society a ridiculous amount of resources and it also makes people pretty much crazy telecommuting has not become the norm.

Why you ask? Because it undermines the power of the bosses. It undermines the entire corporate power structure to have people working from home. That corporate power structure is the model upon which our society has become based. It flies in the face of the ancient values of our society but who cares about ancient values when money is to be made and all we gotta do is suck a fat authoritarian cock to get it? So a big reason we commute to work, at least with my team, is because it's easier to get help with what you're working on when the people you're working with are in the same room. Getting help remotely is possible and online conferences can be quite nice, which is why we can and do work from home on occasion. The problem with working from home, at least from my perspective, is lower productivity, but that could, again, be blamed on my interest in the work. I do consent to and understand that reason. However it would not be a good idea to advise a person entering this field that telecommuting can easily become his life because that is not true. Who cares? Whatever you do stay in school because working for other people just sucks. It sucks.. Every fucking day of it sucks. Pro tip: you will always work for someone else. That isn't true. I run my own business currently and I choose when and how I work. I choose what rules and what guidelines I follow. Nobody else can choose them for me and if someone tries such a thing I simply end the contract and walk away. That only works to a certain degree. I'm sure you'll be the first to admit that you work your ass off. I do. But I only work on my own terms and if things go in such a way that it feels like I'm working on terms I don't like well I really do walk. 

I'm not trying to be a millionaire. If I can feed my family and still hold my head up high that is enough for me. In fact that is the very basic minimum that I fucking demand from this universe. You have my respect. I tip my cap to anyone willing to make a name for themselves. Pro tip: you will always work for someone else.</snippet></document><document><title>What books on logic, set theory, graph theory, etc. are recommended for Computer Science students?</title><url>http://www.reddit.com/r/compsci/comments/14waic/what_books_on_logic_set_theory_graph_theory_etc/</url><snippet>
I would like to get a book or other resource that focuses on an introductory level of these topics with a focus on (formal) proofs for them (without skipping steps).

I have some courseware that is fairly decent from a class I expect to be taking again. I would like to have some better materials. 

Most of the materials I can find about this type of topic that have proofs are very casual in their nature written in plain language. While this can be okay for initial intuition, it is not ideal when I need to know a more step by step approach with justifications.  Perhaps try /r/csbooks?

Otherwise, [here's an overall discrete mathematics text that I just finished using and focuses on puzzles, it's pretty good](http://www.amazon.com/Discrete-Mathematics-Mathematical-Reasoning-Patterns/dp/0471476021/ref=pd_sim_b_1/192-0980967-4442829), [the very first game theory book, by the guy who invented the damn field](http://www.amazon.com/Economic-Behavior-Commemorative-Princeton-Editions/dp/0691130612), [here's a nice list of graph theory things](http://mathoverflow.net/questions/12161/what-are-some-good-beginner-graph-theory-texts).

If you want anything else on proofs, I have an old trig book around (ahem... I may have forgotten to give it back to my high school eight years ago) that really showed me how to prove things mathematically for the first time. What kind of proofs are you talking about here? Proof by induction, mathematical proof through example, what? The books /u/cloudaday listed, separated with linebreaks (even with the commas, it sorta looked like 1 big long link to me)

[here's an overall discrete mathematics text that I just finished using and focuses on puzzles, it's pretty good](http://www.amazon.com/Discrete-Mathematics-Mathematical-Reasoning-Patterns/dp/0471476021/ref=pd_sim_b_1/192-0980967-4442829)

[the very first game theory book, by the guy who invented the damn field](http://www.amazon.com/Economic-Behavior-Commemorative-Princeton-Editions/dp/0691130612)

[here's a nice list of graph theory things](http://mathoverflow.net/questions/12161/what-are-some-good-beginner-graph-theory-texts) Sorry... I was just doing that quickly. Should probably have gone full-on bulleted list. Perhaps try /r/csbooks?

Otherwise, [here's an overall discrete mathematics text that I just finished using and focuses on puzzles, it's pretty good](http://www.amazon.com/Discrete-Mathematics-Mathematical-Reasoning-Patterns/dp/0471476021/ref=pd_sim_b_1/192-0980967-4442829), [the very first game theory book, by the guy who invented the damn field](http://www.amazon.com/Economic-Behavior-Commemorative-Princeton-Editions/dp/0691130612), [here's a nice list of graph theory things](http://mathoverflow.net/questions/12161/what-are-some-good-beginner-graph-theory-texts).

If you want anything else on proofs, I have an old trig book around (ahem... I may have forgotten to give it back to my high school eight years ago) that really showed me how to prove things mathematically for the first time. What kind of proofs are you talking about here? Proof by induction, mathematical proof through example, what? All I can say is formal proofs including proof by induction. 

Luckily I can show examples,

Some Graph theory and sets: http://cs.uwindsor.ca/~peter/60-231/Soln5.pdf

More ordered logic: http://cs.uwindsor.ca/~peter/60-231/Soln1.pdf

Otherwise, the list is here: http://cs.uwindsor.ca/~peter/60-231/t5.html

I am sorry that I cannot give you more specifics about this, but my terminology may be on the weak side.
  Sipser's "Introduction to the Theory of Computation" Sipser and [Hopcroft](http://www.amazon.com/Introduction-Automata-Languages-Computation-Edition/dp/0321455363) are two texts that I am happy having been required to buy for coursework.    For graph theory, I'd suggest D.B. West's [Introduction to Graph Theory](http://www.amazon.com/Introduction-Graph-Theory-2nd-Edition/dp/0130144002). The material is proof-based and it comes with a good amount of examples and exercises.      </snippet></document><document><title>Independent Study - Looking for tutorials/suggestions on image recognition</title><url>http://www.reddit.com/r/compsci/comments/14sedx/independent_study_looking_for/</url><snippet>I have the opportunity to do an independent study with my professor and I think I'd like to do something with image recognition for example identifying a license plate number programmatically from an image. Do any of you kind folks have any experience with this, links to tutorials or advice?


Edit: Looks like I'll try using OpenCV - Tutorials and resources welcome &amp;amp; thank you for your help.     A couple OpenCV resources:
http://www.shervinemami.info/introToOpenCV.html
http://www.pages.drexel.edu/~nk752/tutorials.html Thanks, looks like the suggestions point toward utilizing OpenCV. 
 
Resources will be helpful.      I'd recommend getting a clear goal/assignment before looking into specific solutions as there are a lot of different types of image recognition. If you want to match-recognize license plates you have to ask yourself what kind of images you will be processing.. How clear will these images be, is the license plate always in the same location etc etc. Do you want to do some sort of preprocessing or not. It all depends on the assignment and resources available :)
I've done some image recognition work with matching logo's from compagny's on webpages. Due to the nature of webpage I knew localization wouldn't be a problem but the type of pictures/colors could be. 
So knowing a more specific assignment would help focusing on type of research you should conduct.</snippet></document><document><title>One possible answer to overcome inevitable frequent faults in giant exaflops systems: oblivion  </title><url>http://ascr-discovery.science.doe.gov/exascale/exa_fault1.shtml</url><snippet>  This certainly isn't restricted to HPC at all.  The future of computer science is full of fuck-ups.  Research into amorphous computing relies upon doing computational work across unreliable networks of meager computational nodes which might fail or disappear at any given time.  Intel has been working on accepting faulty processors for a long time, since accepting a 0.01% failure rate can provide greater than a 10x speed improvement, making duplication of work to cover the errors a good investment.

Wherever you look, the future is full of systems built from pieces that break all the time.   </snippet></document><document><title>Public lecture on P vs NP by Michael Sipser (2006)</title><url>http://www.youtube.com/watch?v=msp2y_Y5MLE</url><snippet>  Intro to the Theory of Computation is such a good textbook. I'm excited to watch this lecture. I'm procrastinating studying from that textbook right now.  Intro to the Theory of Computation is such a good textbook. I'm excited to watch this lecture. Intro to the Theory of Computation is such a good textbook. I'm excited to watch this lecture.   </snippet></document><document><title>CVC4: A new, re-designed SMT (Satisfiability Modulo Theories) solver released!</title><url>http://cvc4.cs.nyu.edu/web/</url><snippet>  This webpage shows a laudable effort to communicate to the outside world by using positive marketing language. It's great that academic open-source software also gets this kind of publicity.

However, it is also a bit meager on the technical details. I'm glad to know that "Before using any designs from CVC3, [you] have thoroughly scrutinized, vetted, and updated them." and that you have taken "advantage of recent engineering and algorithmic advances", but could we get a bit more meat on which changes have actually been made, and which engineering and algorithmic advances have been taken advantage of? Any references to additional details, more detailed changelogs or publications would be helpful. Yes, they should cite relevant publications.  Came across a report from earlier this year with some details:
http://smtcomp.sourceforge.net/2012/reports/cvc4.pdf Very helpful, thanks. Choice quote:

&amp;gt; SMT solvers are currently an area of considerable research interest. Barcelogic [6], CVC3 [5], MathSat [10] OpenSMT [11], Yices [16], and Z3 [14] are
examples of modern, currently-maintained, popular SMT solvers. OpenSMT and
CVC3 are open-source, and CVC3, Yices, and Z3 are the only ones to support
all of the defined SMT-LIB logics, including quantifiers.
&amp;gt;
&amp;gt; [..]
&amp;gt;
&amp;gt; CVC4&#8217;s superior performance (compared to CVC3) is apparent by comparing
their performance in SMT-COMP 2012 [8]. CVC4 outperformed CVC3 in every
category. CVC4 performs competitively with other solvers and was the winner
in the QF UFLRA division.

.

(A benchmark result visualization would be even more convincing, but "performs competitively" is what we have for now.) This webpage shows a laudable effort to communicate to the outside world by using positive marketing language. It's great that academic open-source software also gets this kind of publicity.

However, it is also a bit meager on the technical details. I'm glad to know that "Before using any designs from CVC3, [you] have thoroughly scrutinized, vetted, and updated them." and that you have taken "advantage of recent engineering and algorithmic advances", but could we get a bit more meat on which changes have actually been made, and which engineering and algorithmic advances have been taken advantage of? Any references to additional details, more detailed changelogs or publications would be helpful. </snippet></document><document><title>Anyone have the math to check out this P != NP proof?</title><url>https://junfukuyama.wordpress.com/</url><snippet>  I always default back to this article by Scott Aaronson (MIT): http://www.scottaaronson.com/blog/?p=458  Here is my 30-second debunking.

Download paper. Seems to be a circuit lower bound? CTRL-F "[natural proof](http://en.wikipedia.org/wiki/Natural_proof)". No results.

Almost certainly wrong.

edit: I swear Scott Aaronson had a blog post where he suggested this method, but the link to his blog that brainminer posted doesn't have it.  Perhaps a different post... The Wikipedia page for "natural proof" says...

&amp;gt;no such proof can possibly be used to solve the P vs. NP problem Right, which means that any proof that uses a lower bound on a circuit to prove that P != NP had better have a good story as to why their proof is not "natural". Keep in mind that this assumes that the conjecture about existence of pseudorandom functions. In other words, if the proof is correct, the conjecture have also been refuted. Here is my 30-second debunking.

Download paper. Seems to be a circuit lower bound? CTRL-F "[natural proof](http://en.wikipedia.org/wiki/Natural_proof)". No results.

Almost certainly wrong.

edit: I swear Scott Aaronson had a blog post where he suggested this method, but the link to his blog that brainminer posted doesn't have it.  Perhaps a different post...   Proof by obfuscation. There's no way I'll spend more than 30 seconds reading something that long that is not peer reviewed.  I don't see any obfuscation, and that's fucking rude to claim.

I don't grok the math at this level, but the proof itself is written with clear and precise structure with the typical conventions.

Papers don't go immediately from a blank sheet to a journal. It is entirely typical for authors to circulate papers for informal review first, then publish as a technical report for wider circulation and then eventually submit to a journal or conference for formal review.

Unlike the russian paper that attracted media attention, Dr Fukuyama is only calling this a "possible proof" rather than declaring he's changed the world and holding press conferences. It seems quite clear to me he's acting in good faith and is not simply a crank.

All that said, I expect to see a flaw found in this because I believe it's very unlikely that P != NP.

But I also don't see any reason to pull out the torches and pitchforks. The TCS community will dig through this in due time.

Edit: I misspoke above, I meant it's unlikely that P == NP. I'll let my error stand so as to not make the corrections below confusing. &amp;gt; All that said, I expect to see a flaw found in this because I believe it's very unlikely that P != NP

Are you sure you don't mean that you think it's unlikely that P = NP?

All of the CS professors I have heard talk about this have expressed the opinion that almost everyone thinks P != NP. My understanding is that a proof that P = NP would essentially be an algorithm for solving NP-complete problems in polynomial time. Solving SAT in polynomial time seems pretty far-fetched to me. &amp;gt;My understanding is that a proof that P = NP would essentially be an algorithm for solving NP-complete problems in polynomial time.

It's entirely possible to have a proof that P=NP that is not constructive.  That is, one could show that there exists a polynomial time solution for all problems in NP without necessarily determining what that solution is.  

It happens all the time in mathematics, and it's a big letdown when you realize that you've proven that there _is_ an answer, but not how to find that answer...  &amp;gt;My understanding is that a proof that P = NP would essentially be an algorithm for solving NP-complete problems in polynomial time.

It's entirely possible to have a proof that P=NP that is not constructive.  That is, one could show that there exists a polynomial time solution for all problems in NP without necessarily determining what that solution is.  

It happens all the time in mathematics, and it's a big letdown when you realize that you've proven that there _is_ an answer, but not how to find that answer...  If P = NP, then we already have a polynomial-time algorithm: [Levin's Algorithm](http://en.wikipedia.org/wiki/P_%3D_NP#Polynomial-time_algorithms). Yep. It has an enormous multiplicative constant though. I don't see any obfuscation, and that's fucking rude to claim.

I don't grok the math at this level, but the proof itself is written with clear and precise structure with the typical conventions.

Papers don't go immediately from a blank sheet to a journal. It is entirely typical for authors to circulate papers for informal review first, then publish as a technical report for wider circulation and then eventually submit to a journal or conference for formal review.

Unlike the russian paper that attracted media attention, Dr Fukuyama is only calling this a "possible proof" rather than declaring he's changed the world and holding press conferences. It seems quite clear to me he's acting in good faith and is not simply a crank.

All that said, I expect to see a flaw found in this because I believe it's very unlikely that P != NP.

But I also don't see any reason to pull out the torches and pitchforks. The TCS community will dig through this in due time.

Edit: I misspoke above, I meant it's unlikely that P == NP. I'll let my error stand so as to not make the corrections below confusing. if you wouldn't mind, why do you think it's unlikely that P!=NP? if you wouldn't mind, why do you think it's unlikely that P!=NP? Because no one has ever found even the slightest evidence of P = NP. Each and every single person to try and show that NPC problems are solvable in poly time/space, have failed. (NPC are the problems that, if you can solve them, there is a (poly time/space) reduction to all other NP problems) We trivially know that all problems in P are also in NP, but so far, we have not been able to show even a single NPC problem is solvable in poly time/space.

So because of the lack of results from otherwise very researched area, most people think it's unlikely. It would also completely change our definitions of what is feasible to compute. You just argued that it's unlikely that *P = NP*, but the post above claimed (probably mistakenly) that it's unlikely that *P != NP*. I don't see any obfuscation, and that's fucking rude to claim.

I don't grok the math at this level, but the proof itself is written with clear and precise structure with the typical conventions.

Papers don't go immediately from a blank sheet to a journal. It is entirely typical for authors to circulate papers for informal review first, then publish as a technical report for wider circulation and then eventually submit to a journal or conference for formal review.

Unlike the russian paper that attracted media attention, Dr Fukuyama is only calling this a "possible proof" rather than declaring he's changed the world and holding press conferences. It seems quite clear to me he's acting in good faith and is not simply a crank.

All that said, I expect to see a flaw found in this because I believe it's very unlikely that P != NP.

But I also don't see any reason to pull out the torches and pitchforks. The TCS community will dig through this in due time.

Edit: I misspoke above, I meant it's unlikely that P == NP. I'll let my error stand so as to not make the corrections below confusing. &amp;gt;Papers don't go immediately from a blank sheet to a journal. It is entirely typical for authors to circulate papers for informal review first, then publish as a technical report for wider circulation and then eventually submit to a journal or conference for formal review.

This isn't particularly true. Maybe for very important results this happens, but this is not what happens for the majority of papers. Most people write their own papers with their coauthors *maybe* have a few colleagues look over it and then submit it in a journal. So I don't think it's rude at all to say that you're not going to read this paper. 

I think what cutty_sark is saying here is actually pretty valid, everyone and their mother seems to think they've proved P != NP. If this guy can't guy this published then what's the point. 

There are also tons of other reasons to not take this paper seriously. For instance: it never separates P and PSpace.  My criticism of cutty_sark boils down to the view that shame is not a useful behavior in formal investigation.

Vaguely speaking, the more mistaken someone is, the more easily that should be to demonstrate. On the other hand, the most pivotal works of human intellect have directly thwarted conventional opinion at their time. I read this combination as saying we should be liberal and charitable in what we consider, but faithful that unusual claims will be defeated without resorting to rhetorical force in the manner of a bully.

As a concrete example: I think Penrose is wrong concerning his theory of mind. But I think the word is far better for him having it. I see no reason to convince others that he should be shamed for offered it to the rest of us.

Why does PSpace necessarily matter for a P =? NP proof? I understand that you might expect a result to also say something about PSpace, but I don't see how you can turn that into a rejection predicate.

Also I think most participants in this discussion are assuming Fukuyama has weaker credentials than them, which is unlikely to be true. That doesn't mean he's right, but it does mean a certain respect should be paid to his efforts, particularly if it's clear he's acting in good faith. My criticism of cutty_sark boils down to the view that shame is not a useful behavior in formal investigation.

Vaguely speaking, the more mistaken someone is, the more easily that should be to demonstrate. On the other hand, the most pivotal works of human intellect have directly thwarted conventional opinion at their time. I read this combination as saying we should be liberal and charitable in what we consider, but faithful that unusual claims will be defeated without resorting to rhetorical force in the manner of a bully.

As a concrete example: I think Penrose is wrong concerning his theory of mind. But I think the word is far better for him having it. I see no reason to convince others that he should be shamed for offered it to the rest of us.

Why does PSpace necessarily matter for a P =? NP proof? I understand that you might expect a result to also say something about PSpace, but I don't see how you can turn that into a rejection predicate.

Also I think most participants in this discussion are assuming Fukuyama has weaker credentials than them, which is unlikely to be true. That doesn't mean he's right, but it does mean a certain respect should be paid to his efforts, particularly if it's clear he's acting in good faith. I don't see any obfuscation, and that's fucking rude to claim.

I don't grok the math at this level, but the proof itself is written with clear and precise structure with the typical conventions.

Papers don't go immediately from a blank sheet to a journal. It is entirely typical for authors to circulate papers for informal review first, then publish as a technical report for wider circulation and then eventually submit to a journal or conference for formal review.

Unlike the russian paper that attracted media attention, Dr Fukuyama is only calling this a "possible proof" rather than declaring he's changed the world and holding press conferences. It seems quite clear to me he's acting in good faith and is not simply a crank.

All that said, I expect to see a flaw found in this because I believe it's very unlikely that P != NP.

But I also don't see any reason to pull out the torches and pitchforks. The TCS community will dig through this in due time.

Edit: I misspoke above, I meant it's unlikely that P == NP. I'll let my error stand so as to not make the corrections below confusing. Proof by obfuscation. There's no way I'll spend more than 30 seconds reading something that long that is not peer reviewed.  If everyone had this philosophy there would be no peers :-P Why? When you submit it to a journal/conference it will be peer reviewed. It's a job some people do. [deleted] They peer review things that are *submitted for* peer review (like to a journal or conference), which the OP's article has not been. I think perhaps /u/qrios' point was that it *has* been submitted for peer review (or at least, for review) where the readers of /r/compsci are the "peers." Not a journal or conference, but still a form of review. I think perhaps /u/qrios' point was that it *has* been submitted for peer review (or at least, for review) where the readers of /r/compsci are the "peers." Not a journal or conference, but still a form of review. [deleted] It still doesn't make any sense.

Peer reviewers in a professional committee: It's their job. They are there because they want to review non reviewed stuff.

A commenter on reddit: Just a guy with opinions.

P = NP problem is notoriously open to many layers of quackery akin to "I discovered a perpetual machine" claims with many layers of obfuscation in the physics world. It is one of the biggest open problems we collectively have, notoriously difficult and a valid proof will arguably make one famous for the rest of eternity. In the best case, verification of such a proof would take weeks if not months of labor from top-of-their-game people from diverse perspectives to be of any value. If the author is serious about it, I'd expect it to be submitted for peer review and it will be viable for folks like us to comment on the progress of the review. He posted his 1000000$ worth proof into a blog instead. Expecting someone to "check the math" for such a work quickly is futile. That's why expecting a work of this magnitude to pass even the preliminary phases of an official review process is more than reasonable. It still doesn't make any sense.

Peer reviewers in a professional committee: It's their job. They are there because they want to review non reviewed stuff.

A commenter on reddit: Just a guy with opinions.

P = NP problem is notoriously open to many layers of quackery akin to "I discovered a perpetual machine" claims with many layers of obfuscation in the physics world. It is one of the biggest open problems we collectively have, notoriously difficult and a valid proof will arguably make one famous for the rest of eternity. In the best case, verification of such a proof would take weeks if not months of labor from top-of-their-game people from diverse perspectives to be of any value. If the author is serious about it, I'd expect it to be submitted for peer review and it will be viable for folks like us to comment on the progress of the review. He posted his 1000000$ worth proof into a blog instead. Expecting someone to "check the math" for such a work quickly is futile. That's why expecting a work of this magnitude to pass even the preliminary phases of an official review process is more than reasonable. [deleted] Proof by obfuscation. There's no way I'll spend more than 30 seconds reading something that long that is not peer reviewed.  Who exactly are these peers then? (Curiosity, no irony) Who exactly are these peers then? (Curiosity, no irony) Who exactly are these peers then? (Curiosity, no irony) Who exactly are these peers then? (Curiosity, no irony) Peer review is the process a scientific paper goes through in order to get the contents verified. It is quite interesting. Try googling it.  I think he was making a joke that if everyone said "Nobody else has reviewed this, I'm not reviewing it" then nothing would ever be reviewed. Proof by obfuscation. There's no way I'll spend more than 30 seconds reading something that long that is not peer reviewed.  Proof by obfuscation. There's no way I'll spend more than 30 seconds reading something that long that is not peer reviewed.  Proof by obfuscation. There's no way I'll spend more than 30 seconds reading something that long that is not peer reviewed.   Hey crabsock, are you Mr. Fukuyama himself, or just posting on his behalf?

If you're him, and if you'd like, I can forward your proof along to some of my peers and professors to look at. I just finished my Theory of Computation class today, so I'm not advanced/learned enough to check your proof, but I know people who might.  There are two questions:

One is the correctness of the P&#8800;NP proof, which can be difficult to verify depending on how lazy and time-wasting-averse you are, and on the quality of the work.

Second is how much of a crank the author is.

The second question can be answered using some very simple heuristics. For example, the ratio of pages to citations in the proof. In this case, the most important result in computer science that will likely *ever* be proved - an answer to a question that has spawned entire subfields of computer science and mathematics - takes 59 pages and refers only to 12 papers.

tl;dr: of course it's wrong. wrt. the referencing, *because* it is such a fundamental problem, there doesn't necessarily need to be many references.  There isn't as big a literature "story" to present. That is insane. There is an enormous body of work relating to P vs NP. It's not like we just have no idea whatsoever. The idea that a proof would be completely independent of those results is so unlikely that it's not a possibility any serious person in CS would consider for a moment.    [deleted] Guys. He's coping with schizophrenia. It's rude to downvote something he has little control over. No, it would be rude for us to *insult* him about something he has little control over, but indicating that we think that his comment should not be given space here is simply preventing this forum from being spammed by off-topic posts, and anyway it's not like letting his rants stay visible would  help his condition. That's true. I'm not sure whether he notices the social connotation of downvoting. But the correct thing to do, I think, is to report, not downvote. A sensible mod who would prefer that things remain on topic would delete it straightaway.

By the way, this doesn't have to do with much, but since this *is* /r/compsci (nonono theoretical computer scientists, no eye-rolling), SparrowOS actually wrote a killer OS, named Sparrow OS, that I suggest you go check out. He may not be a conversationalist, but damn, he's excellent at programming. Guys. He's coping with schizophrenia. It's rude to downvote something he has little control over.</snippet></document><document><title>Is there a systems equivalent of Hennessy and Patterson?</title><url>http://www.reddit.com/r/compsci/comments/14mrc7/is_there_a_systems_equivalent_of_hennessy_and/</url><snippet>I'm looking for a text that outlines the concepts, tradeoffs and history of modern OS mechanisms, specifically focusing on historical research and its influence on modern OS structure.  Quantitative comparisons between different approaches would be ideal. Is there anything out there that provides this sort of information?      </snippet></document><document><title>Aspiring Computer Scientist, some questions inside.</title><url>http://www.reddit.com/r/compsci/comments/14m0j2/aspiring_computer_scientist_some_questions_inside/</url><snippet>Sorry this will be a long winded post, just like to get some background in as well as my questions. TL;DR at bottom if needed! 

Hey Reddit! I would love to go on to university to study Computer Science. I am a 16 year old male, so just got into college a couple of months ago to begin my A-Levels (I chose Maths, Physics and Chemistry - My college does not offer Further Maths), and am pretty confident I will get some good grades when I finish, hoping for A's in the Physics and Maths and at least a B in Chemistry. I have always wanted to work on a technology related job, and love the idea of programming (just the idea so far). I downloaded python a couple of months ago, and while I have not had as much time to use it as I would have liked these past months, due to personal issues, they are all set aside now so I once again have tonnes of free time. I suppose my main questions are (In the TL;DR).

Tl;DR

Is Python a good program to start with? 
If not, what is? 
What is the difference between the different Computer Science courses at university? 
What else can I do to get me ahead and give me an insight into the world of computer science?
  Computer science is about computers as much as astronomy is about telescopes.

Keep this in mind and everything will be fine :)

PS: python is good *programming language*, but so is every other.

Also check the sidebar, this probably isn't the right subreddit :P &amp;gt; Computer science is about computers as much as astronomy is about telescopes.

Surgeons need a good understanding of knives. Does he have to know about a molecular arrangement of iron atoms on the edge? I don't think so. On most topics, I trust those computer scientists who know how computers/large programs/the world works more than those that don't.

BTW

&amp;gt; python is good programming language, but so is every other.

Thanks for the insight. Pick one at random, is it?  I think you missed where OP called Python a 'program', so Fuco1337 emphasised that it was a programming language. Computer science is about computers as much as astronomy is about telescopes.

Keep this in mind and everything will be fine :)

PS: python is good *programming language*, but so is every other.

Also check the sidebar, this probably isn't the right subreddit :P Oops, probably should of went over to askcomputerscience, ironically I didn't go to cscareerquestions as I figured I would of had to have actually studied it to be looking for a career! After 30 minutes of lurking before I posted this, I have seen that quote so many times! I understand what it is saying, but any expansion on that at all? Thanks! It means that if you want to go to university to learn programming, you can do that at home for free, because they don't teach programming at university. [A smart man](http://norvig.com/21-days.html) once said it takes 10000 hours to get good at a thing. That's more then all the classes of your 5 year program combined.

You want to go to uni if you want, mostly, learn theory (=math) and meet people :P I somewhat disagree. If you want a career in programming, it makes far more sense to learn about computer science (and in doing so, practice programming) in the structured environment provided by a university than to learn it on your own at home.

Learning about the theory will help you in your pursuits later on, even if you don't intend to focus much time on theory after you're done with school. Autodidact professional programmer here. Most of the *programming* of computers has very little to do with the *science* of computers. The "structured environment provided by a university" may be a good learning environment for some things but it also has very little to do with the realities of professional programming. Meanwhile, the curricula necessarily elide a very large amount of on-the-job knowledge you will eventually acquire the hard way. Of course, perhaps OP's goal is actually to become a computer *scientist* rather than a computer *programmer*.

Not saying that you can't learn a lot about programming in compsci courses. But it's also true that there's a lot you *won't* learn. Six months spent reading a few programming books, experimenting, and trying to contribute to some low-hanging open source projects might well be time better spent than in a compsci major.

Also, OP would probably find /r/programming more relevant to his interests. Personally, I spent about three years essentially teaching myself to program throughout high school (although technically I was in classes, there wasn't a lot of direction).

Within two semesters of undergraduate studies in computer science, the advantage that those three years gave me was negligible compared to other students in the CS program that started without prior experience.

Perhaps I'm just bad at self-teaching, but I know other people with similar backgrounds that experienced the same thing.

Personally, I've found the pursuit of my degree to have been helpful in both making me a better programmer, and in opening up opportunities that might not otherwise have been available, even though I do intend to stay away from academia. This. CS is a good base to have for software engineering (what I assume you refer to as programming).

You can't program useful, new software without a firm understanding of algorithms and data structures. You can contribute to easy copy-paste boilerplate code, but that's just not very interesting... You can certainly learn those concepts by yourself, but most people aren't great autodidacts. Algorithms and data structures are overrated. I've amassed a fair amount of knowledge in these subjects over the years but rarely find that I need it. A basic knowledge is often sufficient. An understanding of the abstraction mechanisms your language provides (whether object oriented, functional, concurrent, or etc) is far more important.

And you're creating quite the false dichotomy here. You can definitely contribute meaningful code to useful, new software that isn't boilerplate code and also doesn't involve a deep understanding of algorithms and data structures.

CS is a good base to have for "software engineering" (which is a terrible term), but the real work of programming is far more about communication with other people (collaboration, managing expectations, understanding user requirements, etc) than about computer science. Most of that isn't the problem solving, it's the business aspect. Even if that's an important part of the career, learning to communicate is not learning to program. I somewhat disagree. If you want a career in programming, it makes far more sense to learn about computer science (and in doing so, practice programming) in the structured environment provided by a university than to learn it on your own at home.

Learning about the theory will help you in your pursuits later on, even if you don't intend to focus much time on theory after you're done with school. Oops, probably should of went over to askcomputerscience, ironically I didn't go to cscareerquestions as I figured I would of had to have actually studied it to be looking for a career! After 30 minutes of lurking before I posted this, I have seen that quote so many times! I understand what it is saying, but any expansion on that at all? Thanks! Computer science is about computers as much as astronomy is about telescopes.

Keep this in mind and everything will be fine :)

PS: python is good *programming language*, but so is every other.

Also check the sidebar, this probably isn't the right subreddit :P This. I think computer science would be more accurately labeled Computing science. Computer science is about computers as much as astronomy is about telescopes.

Keep this in mind and everything will be fine :)

PS: python is good *programming language*, but so is every other.

Also check the sidebar, this probably isn't the right subreddit :P Computer science is about computers as much as astronomy is about telescopes.

Keep this in mind and everything will be fine :)

PS: python is good *programming language*, but so is every other.

Also check the sidebar, this probably isn't the right subreddit :P Computer science is about computers as much as astronomy is about telescopes.

Keep this in mind and everything will be fine :)

PS: python is good *programming language*, but so is every other.

Also check the sidebar, this probably isn't the right subreddit :P  I'll throw in two cents as a current CS student at Purdue University.

First, what is computer science? 

Well, computer science isn't all programming. In fact, I wouldn't even say half of computer science is programming. 

The first semester of any University with a good CS program will be an introduction to programming. Here at Purdue, CS students start with Java. After that class, students are on their own for learning any language that pops up during your studies. 

Think of programming as a shovel and CS as a gardening class. The very first thing you do is learn how to use the shovel. After you learn how to use it, everything else you do is merely something else that just happens to use that shovel. 

Programming is just a tool to use. 

What can you do to get yourself ahead? 

Hmm... To get ahead at the *beginning* of any CS track, I would advise you know a high level language front to back; or at least the standard libraries and methods for that language.

A good language to choose is Java, as it seems to be what a lot of Universities are using for their students. 

Understand logic flow (conditional and loop statements), object oriented programming (classes, etc), and rudimentary data IO (reading standard input, reading files, etc). 

I don't have any off the cuff, but I'm sure some other Redditors will have some good intro to Java tutorials. Yeah sorry, I was not trying to insinuate that computer science was all programming, my original post looks pretty bad for that actually. Now you have mentioned Java, I am sure my Dad had to learn the basics for his job, so when he gets back I'll ask him if he still has the books around so I can look into that. 

I know of conditional and loop statements and object oriented programming, but I would be lying if I said I knew what you meant by rudimentary data IO, would you please expand? Thanks Yeah sorry, I was not trying to insinuate that computer science was all programming, my original post looks pretty bad for that actually. Now you have mentioned Java, I am sure my Dad had to learn the basics for his job, so when he gets back I'll ask him if he still has the books around so I can look into that. 

I know of conditional and loop statements and object oriented programming, but I would be lying if I said I knew what you meant by rudimentary data IO, would you please expand? Thanks  I only recently started to program in college, so I don't know if what I'm saying is completely accurate. Someone else can probably give you a better answer.

Answering your tldr:

I've heard that python is a good language to start with and to learn the basics.
I started with Java and it's going well. I'd say it's also a good start.

The difference between science courses is really what focus you want to have, whether it's networking or security or whatever. There's also the survey of computer science which gives a little of everything, including the basic knowledge of how a computer works.

As for the third thing, I can't really say. Reading, I guess.  &amp;gt; Is Python a good program to start with? If not, what is?

I'm going to quote Joel Spolsky here as I whole heartedly agree with him  
&amp;gt; For various reasons too complicated to go into here, I believe that you have to start programming at a level that is as close to the machine as reasonable. I think that this book, universally known as K&amp;amp;R, is THE book anyone who wants to be a programmer must learn first. Pick it up and work through it in detail. If you love every minute of it, you can be a programmer. If you find this old-school text programming stuff boring, or the pointer stuff drives you crazy, trust me, you're not going to like programming very much. If you need to be seduced into programming or if you don't have the patience to figure out what all those crazy asterisks mean, you're going to be happier doing something else. Really. But if you can make it through this book by yourself, you've got what it takes to be a top gun programmer, and you've got a terrific foundation for everything else you're going to learn.

edit: hah, just noticed that it didn't actually say the language in the quote.  He was talking about C and the book was "The C Programming Language", universally known as K&amp;amp;R. He wants to be a computer scientist, not a programmer. C is a must-know language anyway, and I agree he should learn C later on. But right now, he wants to know if CS is right for him and python is better for that purpose. &amp;gt; Is Python a good program to start with? If not, what is?

I'm going to quote Joel Spolsky here as I whole heartedly agree with him  
&amp;gt; For various reasons too complicated to go into here, I believe that you have to start programming at a level that is as close to the machine as reasonable. I think that this book, universally known as K&amp;amp;R, is THE book anyone who wants to be a programmer must learn first. Pick it up and work through it in detail. If you love every minute of it, you can be a programmer. If you find this old-school text programming stuff boring, or the pointer stuff drives you crazy, trust me, you're not going to like programming very much. If you need to be seduced into programming or if you don't have the patience to figure out what all those crazy asterisks mean, you're going to be happier doing something else. Really. But if you can make it through this book by yourself, you've got what it takes to be a top gun programmer, and you've got a terrific foundation for everything else you're going to learn.

edit: hah, just noticed that it didn't actually say the language in the quote.  He was talking about C and the book was "The C Programming Language", universally known as K&amp;amp;R. I don't really agree with starting in C... To me it's kind of like learning how an internal combustion engine works before you learn how to drive. I'd much rather let people drive, and if they decide they like cars based on that, then teach them how the insides work. &amp;gt; To me it's kind of like learning how an internal combustion engine works before you learn how to drive.

I see it as learning what all the pieces on of a car does before becoming an expert mechanic. I think in your car analogy a mechanic is a compiler-writer not a general programmer, which is more like a driver. (And note that most mechanics are drivers, and plenty of drivers know a little bit about fixing some kinds of car trouble.) &amp;gt; Is Python a good program to start with? If not, what is?

I'm going to quote Joel Spolsky here as I whole heartedly agree with him  
&amp;gt; For various reasons too complicated to go into here, I believe that you have to start programming at a level that is as close to the machine as reasonable. I think that this book, universally known as K&amp;amp;R, is THE book anyone who wants to be a programmer must learn first. Pick it up and work through it in detail. If you love every minute of it, you can be a programmer. If you find this old-school text programming stuff boring, or the pointer stuff drives you crazy, trust me, you're not going to like programming very much. If you need to be seduced into programming or if you don't have the patience to figure out what all those crazy asterisks mean, you're going to be happier doing something else. Really. But if you can make it through this book by yourself, you've got what it takes to be a top gun programmer, and you've got a terrific foundation for everything else you're going to learn.

edit: hah, just noticed that it didn't actually say the language in the quote.  He was talking about C and the book was "The C Programming Language", universally known as K&amp;amp;R. I must admit, so far I have only done simple stuff on Python, such as hello world or made a crappy calculator (Although if you try to divide by zero, it all goes wrong!). I share this as it seems as though the book you are talking about contains tasks, which is what I love to do and how I learn, by getting in there and solving problems. Thanks for the book name too! A new book came out called "Head First C", I've looked through it and I think this is the ideal way for a new (and motivated) programmer to start out.

If you need to feel like you are doing something, then go for Python. But you will go further, faster, with C. This depends a lot on what you mean by further. You can learn a lot of interesting concepts (and implement useful things with them) in Python that are not possible in C. The most important thing OP can do right now if he is interested in programming is program a lot. A lot of beginners get stuck in analysis paralysis and never learn to program as a result. Everything you do in Python can be done in C (since Python is implemented in C). It is just that those things are a lot easier in Python.

The value of C is that it helps to remove the mystery of what is between you and the hardware. Removing that mystery helps you realize how those abstract concepts are made concrete in the computer. I must admit, so far I have only done simple stuff on Python, such as hello world or made a crappy calculator (Although if you try to divide by zero, it all goes wrong!). I share this as it seems as though the book you are talking about contains tasks, which is what I love to do and how I learn, by getting in there and solving problems. Thanks for the book name too! Just remember the point isn't that C is the language you should stick with it's that C does everything short of dealing with registers and learning it will allow you to be intimately familiar with what other languages are doing when you do say, an echo statement in php.

On top of that it will help you understand memory management, buffer overflows and many other things which more modern languages try to obfuscate from you.  C will create a good foundation for you and if it's not the language you want to stick with it will allow you to easily transition into another language. &amp;gt; Is Python a good program to start with? If not, what is?

I'm going to quote Joel Spolsky here as I whole heartedly agree with him  
&amp;gt; For various reasons too complicated to go into here, I believe that you have to start programming at a level that is as close to the machine as reasonable. I think that this book, universally known as K&amp;amp;R, is THE book anyone who wants to be a programmer must learn first. Pick it up and work through it in detail. If you love every minute of it, you can be a programmer. If you find this old-school text programming stuff boring, or the pointer stuff drives you crazy, trust me, you're not going to like programming very much. If you need to be seduced into programming or if you don't have the patience to figure out what all those crazy asterisks mean, you're going to be happier doing something else. Really. But if you can make it through this book by yourself, you've got what it takes to be a top gun programmer, and you've got a terrific foundation for everything else you're going to learn.

edit: hah, just noticed that it didn't actually say the language in the quote.  He was talking about C and the book was "The C Programming Language", universally known as K&amp;amp;R. I've been programming for over 14 years now, haskell, lisp, java, prolog, C++, python, perl... whatever you want.

The one thing that I *HATE* the most are pointers and C and K&amp;amp;R.

"Learning C" and "down to the machine" is simply pretencious bullshit. Old C programmer here, so I'm biased, but most software you rely on is written in C/C++ (Ridiculously pretentious claim, but Windows/Linux/OSX, most compilers,  smartphones, routers and cable boxes meet the criteria). 

People have been claiming 'C is old-fashioned' for at least the last 30 years (really, I was there), but it hasn't gone away.  That should indicate something of its value.  I think C is great for a lot of things, but not necessary for most things.

While it is good to know that stuff, os's and compilers were writen so that you dont need to know entirely how they work to be able to use them. It's certainly possible to do a lot without having to descend in to the internals of a compiler or an OS.  But, some kinds of things can only be built (or fixed) by descending in to (or creating) the internals of a compiler or an OS.  Knowing when and why and how it's necessary to do so isn't just pretense.  

 I've been programming for over 14 years now, haskell, lisp, java, prolog, C++, python, perl... whatever you want.

The one thing that I *HATE* the most are pointers and C and K&amp;amp;R.

"Learning C" and "down to the machine" is simply pretencious bullshit. I learned assembler before it was cool! .. Well, seriously how can you say that learning pointers is pretentious bullshit in r/compsi? Are you a troll? How does understanding pointers further your knowledge of anything compsci?

I'll answer it for you: it doesn't lol!

Also, I'm not a troll. I just hate people who defend C because it's "obscure" and the only thing they know and they want to feel better about it. It's like haskell people but those are actually nice :)

If you recommend C and pointers and "down to machine" to someone who wants to learn to program you're insane and that's fact end of story. ... uh, C is obscure?  C is, by most measures, the most used programming language by a significant margin, and if you include its descendant C++ it's *definitely* the most used.

You personally not having a clue about something does not mean it is "obscure". I think he means it's similar to people who think using vim or emacs makes you a "hacker", hate anything to do with GUI's, and think code completion is cheating. It makes them look like pretentious snobs.(kinda like hipsters) C is an important language, but some people think it's god's gift to programming and anything else is "unpure" ... uh, C is obscure?  C is, by most measures, the most used programming language by a significant margin, and if you include its descendant C++ it's *definitely* the most used.

You personally not having a clue about something does not mean it is "obscure". I can't add more than "cool story bro". Sorry. 

You people are like... a freshman once came to me and asked how to solve [insert simple problem]. I told him to spend 6 months studying Galois theory and then come back. Your comparing C to Galois Theory?

Yeah, you're either off your rocker or a troll. And why is it such a stretch? I've been programming for over 14 years now, haskell, lisp, java, prolog, C++, python, perl... whatever you want.

The one thing that I *HATE* the most are pointers and C and K&amp;amp;R.

"Learning C" and "down to the machine" is simply pretencious bullshit. I can see you have zero respect for the machine from which you likely derive your livelihood. Understanding "down to the machine", at least to a degree, is fundamental to being an effective programmer in C. There's a reason it's still in use too.

Things you don't like != pretentious bullshit  A: Hi I'm new, what should I do?

B: Learn assembler

Me: B, you're a fucking idiot.

That's my opinion :) I can see you have zero respect for the machine from which you likely derive your livelihood. Understanding "down to the machine", at least to a degree, is fundamental to being an effective programmer in C. There's a reason it's still in use too.

Things you don't like != pretentious bullshit  I've been programming for over 14 years now, haskell, lisp, java, prolog, C++, python, perl... whatever you want.

The one thing that I *HATE* the most are pointers and C and K&amp;amp;R.

"Learning C" and "down to the machine" is simply pretencious bullshit. " "down to the machine" is simply [pretentious] bullshit."

very true. my main gripe with C is that it's library has some of the worst named functions ever written (strtoul). I understand the times the language was created in, but come on....function overloading would be cool too... on the other hand, C does just about what every other language does, so use whatever makes your job easier . I'm partial to Java/C++. (Ada isn't terrible either)

 &amp;gt; Is Python a good program to start with? If not, what is?

I'm going to quote Joel Spolsky here as I whole heartedly agree with him  
&amp;gt; For various reasons too complicated to go into here, I believe that you have to start programming at a level that is as close to the machine as reasonable. I think that this book, universally known as K&amp;amp;R, is THE book anyone who wants to be a programmer must learn first. Pick it up and work through it in detail. If you love every minute of it, you can be a programmer. If you find this old-school text programming stuff boring, or the pointer stuff drives you crazy, trust me, you're not going to like programming very much. If you need to be seduced into programming or if you don't have the patience to figure out what all those crazy asterisks mean, you're going to be happier doing something else. Really. But if you can make it through this book by yourself, you've got what it takes to be a top gun programmer, and you've got a terrific foundation for everything else you're going to learn.

edit: hah, just noticed that it didn't actually say the language in the quote.  He was talking about C and the book was "The C Programming Language", universally known as K&amp;amp;R.  Note sure how helpful this would be, but as its on my mind right now, I'll post it anyway. 

In the UK there is an annual competition based around solving computer science problems. It may well be way over your head, but if after reading a few past papers, even if you don't really have a clue how to go about solving them, the *idea* of solving them is of interest, then that's a good sign. If you can learn a bit of python and then try solving some, that's even better. I'll be sitting one of these papers in a week or so, but I'm a year ahead of you and have been doing programming for much longer. None of that is really important, but I think its a good idea to enjoy this kind of thing, even if you don't have the syntax/programming knowledge to do it.

Anyway, everything's available at http://www.olympiad.org.uk/, possibly one of the worst websites ever.  Note sure how helpful this would be, but as its on my mind right now, I'll post it anyway. 

In the UK there is an annual competition based around solving computer science problems. It may well be way over your head, but if after reading a few past papers, even if you don't really have a clue how to go about solving them, the *idea* of solving them is of interest, then that's a good sign. If you can learn a bit of python and then try solving some, that's even better. I'll be sitting one of these papers in a week or so, but I'm a year ahead of you and have been doing programming for much longer. None of that is really important, but I think its a good idea to enjoy this kind of thing, even if you don't have the syntax/programming knowledge to do it.

Anyway, everything's available at http://www.olympiad.org.uk/, possibly one of the worst websites ever.     [deleted] What is the Comp Sci career market like in the UK? Is it easy to get started with a job in computer science after graduating from university? (I live in London btw)   Python can be a good starter, but if you start making more complicated programs make sure you 

* educate yourself about mutable vs immutable objects

* try not to pull your hair out when importing modules from different directories

* don't ever use a mutable object (such as an array) as a default function argument value.

With respect to classes. Take discrete math and a theory course and take it seriously. These classes are often difficult, but very valuable to the aspiring computer scientist.

Good luck

EDIT: edited for formatting issues. &amp;gt; don't ever use a mutable object (such as an array) as a default function argument value.

Could you expand on this? Python can be a good starter, but if you start making more complicated programs make sure you 

* educate yourself about mutable vs immutable objects

* try not to pull your hair out when importing modules from different directories

* don't ever use a mutable object (such as an array) as a default function argument value.

With respect to classes. Take discrete math and a theory course and take it seriously. These classes are often difficult, but very valuable to the aspiring computer scientist.

Good luck

EDIT: edited for formatting issues. I'll keep that in mind once I actually get to the more complicated programs! Thanks for the info, can you expand on taking discrete math? Do you mean now, as some sort of night class or something, alongside my A levels? 

(The Math I am doing right now is just core 1-4 statistics 1 and mechanics 1)    Plenty of good insight here so i'll be brief. Python is a fine place to start. But remember, variety is the spice of life.  Try to expose yourself to as many different languages as possible.       Am I the only one who sees an issue with someone going into a CS degree program without any previous programming experience? You don't need previous programming experience going into an undergraduate degree in CS.

In most places, at least. Am I the only one who sees an issue with someone going into a CS degree program without any previous programming experience? I had zero programming or CS experience before my first CS class in university, using Scheme. Now I'm a PhD student in CSci =). Definitely not necessary to be a master programmer when entering a CS program. That's cool. Scheme first huh? Interesting. Look into SICP. It is a 1st programming course/book that is all in Scheme. That's cool. Scheme first huh? Interesting. Am I the only one who sees an issue with someone going into a CS degree program without any previous programming experience? I have slightly under 2 years to gain experience, which is why I am asking rather than later

Edit: Bad grammar Am I the only one who sees an issue with someone going into a CS degree program without any previous programming experience? Am I the only one who sees an issue with someone going into a CS degree program without any previous programming experience?    Python is increasing in popularity, both as a "glue" (or even primary) language for scientific work, and as the language used for introductory courses.

There are many opinions about the best language to start with, but Python is definitely a good choice.  There are modules for so many things it makes doing lots of high level stuff simple, and the paradigms used in Python are ones common to most popular languages.

If you like puzzles, perhaps try [The Python Challenge](http://www.pythonchallenge.com/), though not CS it's a fun way to get your programming feet wet.

That said, I am partial to the [SICP](http://mitpress.mit.edu/sicp/) way of introduction to computer science, as used until recently by MIT.  If you want to work through a book designed specifically to introduce programming to those completely unfamiliar with it, I very highly recommend this one.  Though Scheme is not nearly as widely used as Python, its extreme simplicity makes it great for learning.</snippet></document><document><title>Computer Science stackexchange, currently in beta a great resource but could use some more activity. Check it out!</title><url>http://cs.stackexchange.com/</url><snippet>  There is also http://cstheory.stackexchange.com/ for "research-level" questions in Computer Science.  Isnt stack exchange a place were people who suck at programming go to ask for someone else to do their work for them?

Edit: seems crappy programmers have fragile ego's Isnt stack exchange a place were people who suck at programming go to ask for someone else to do their work for them?

Edit: seems crappy programmers have fragile ego's Isnt stack exchange a place were people who suck at programming go to ask for someone else to do their work for them?

Edit: seems crappy programmers have fragile ego's Isnt stack exchange a place were people who suck at programming go to ask for someone else to do their work for them?

Edit: seems crappy programmers have fragile ego's You are thinking of Stack Overflow I think.</snippet></document><document><title>Question about hashing large strings of data.</title><url>http://www.reddit.com/r/compsci/comments/14lbaq/question_about_hashing_large_strings_of_data/</url><snippet>I'm currently working on a project where I'll be hashing unique keys and then hashing them to be stored as values in cookies. This will be used for authenticating user information so the hashed value must be unique. I'll be using sha512 and for it's purpose I'm confident the keys should be unique. **However it did make me think.**

What if (just for fun) I wanted to hash an entire book, from front to back? Expanding on that what if I wanted to create a hash for each book ever written?

Does the size of the string you input into the hash function create a greater chance of duplicate keys?

This is probably a stupid question, but it's one that got me thinking..  &amp;gt;hashed value must be unique

While there is no real worry that sha512 will ever produce unique values, it still makes me a little nervous when they *absolutely positively must* be unique.  You could still hash ~(every atom in the universe, at every second in time since the big bang) and still not have a credible chance of getting an accidental hash collision, so this is not a reasonable concern, but I still feel the need to voice it.

&amp;gt;Does the size of the string you input into the hash function create a greater chance of duplicate keys?

No, not really. (Ignoring nitpicks,) each letter you add to the input completely changes the outputted hash, and doesn't change the chances of a hash collision at all.  It's not like making your input bigger is going to lock down any of the output bits:

    SHA512("The quick brown fox jumps over the lazy dog")

`0x07e547d9586f6a73&#8203;f73fbac0435ed769&#8203;51218fb7d0c8d788&#8203;a309d785436bbb64&#8203;2e93a252a954f239&#8203;12547d1e8a3b5e&#8203;d6e1bfd709782123&#8203;3fa0538f3db854fee6`

    SHA512("The quick brown fox jumps over the lazy dog.")

`0x91ea1245f20d46ae&#8203;9a037a989f54f1f7&#8203;90f0a47607eeb8a1&#8203;4d12890cea77a1bb&#8203;c6c7ed9cf205e67b&#8203;7f2b8fd4c7dfd3a7&#8203;a8617e45f3c463d4&#8203;81c7e586c39ac1ed&#8203;` &amp;gt; it still makes me a little nervous when they absolutely positively must be unique.

For my project I'm planning on running a query before inserting it into the database to be absolutely sure, unless primary keys are automatically checked for duplicates (for some reason I feel like they aren't though.)

I guess I'm just having a hard time grasping the idea that a string could be vastly longer than the hashed string (an entire novel) and still be a unique key.

At the same I suppose that combination of letters is just a single key, no matter how long it will always just be one key.

Thanks for the reply! Do not worry about collision, the chances are so small it is not worth it. Do not worry about collision, the chances are so small it is not worth it. It's being used as a remember me cookie, so duplicate keys can't happen, that's why I'll just do a check before inserting it into the db.  &amp;gt; Expanding on that what if I wanted to create a hash for each book ever written?

FYI, that's mainly how works synchronization suite like dropbox. Each file is hashed then "deduped" both server side and client side. It leads to lightspeed synchronization

 - what an user has uploaded once will never be reuploaded again, even if randomly renamed or moved

 - no other user will ever need to upload a copy of this file! The deduped instance is automagically shared across users.

That's also how work distributed version managers today.
  </snippet></document><document><title>Can I make every recursive function tail recursive?</title><url>http://www.reddit.com/r/compsci/comments/14jpr4/can_i_make_every_recursive_function_tail_recursive/</url><snippet>I just discovered tail recursion and how it can bring a big performance boost. I tried some academic functions like Fibonacci and so on, but when I try to implement tail recursion in a real life scenario I kinda fail. I would like to to make the naive MSPaint  pixel fill function tail recursive

	function recfill(x, y, oldcolor, newcolor) {
		if(getPixel(x,y) === oldcolor) {
			setPixel(x,y,newcolor); 

			recfill(x-1,y, oldcolor, newcolor);
			recfill(x+1,y, oldcolor, newcolor);
			recfill(x,y-1, oldcolor, newcolor);
			recfill(x,y+1, oldcolor, newcolor);
		}
	}


but there is a obvious problem. Tail recursion only works when the recursive part is at the end of the function but I have not one but four recursive calls. Does it make sense to make this function tail recursive?    You can make every recursive function tail recursive, but then you'd be allocating (stack) space on the heap. Some functions just require storage. More importantly, this specific problem is a perfect example of bad recursion. This code really should simply be iterated through to get maximum performance for minimum cost. More importantly, this specific problem is a perfect example of bad recursion. This code really should simply be iterated through to get maximum performance for minimum cost. The question is how...  The question is how...  While that's true, people too often don't ask "why" when they're using recursion. It is a wonderful tool when it is used right, and induction is very powerful.

That being said, examples like this one and Fibonacci are often taught as examples of recursive functions, but in reality they really degrade performance. the recfill function is a simple example which looks quite elegant as a recursive function, but javascript actually has a very small stack and I often get stack exceptions when executing this function for big pictures.  For the record, JavaScript, not being designed as a functional language, does not guarantee tail recursion will be optimized, and in fact I just tested a tail recursive function on two browsers and neither of them optimized it. None of the browsers support tail recursion because Javascript allows you to inspect the stack with `arguments` and `arguments.callee` The browsers could, in theory, do control-flow analysis to determine that arguments.callee would never be invoked, and then do tail call in those particular cases. None of the browsers support tail recursion because Javascript allows you to inspect the stack with `arguments` and `arguments.callee` Actually, `.caller` only allows a single caller for a given function, so you can't really stack walk in the face of recursion-- so browsers *could* do it.  However, considering that this would be the only feature to *require* any kind of optimization/analysis of code (and current optimizers completely punt when you do things like eval), and would break backwards compatibility, I don't see this ever being guaranteed - at least without a syntax addition to mark a tail recursive call. For the record, JavaScript, not being designed as a functional language, does not guarantee tail recursion will be optimized, and in fact I just tested a tail recursive function on two browsers and neither of them optimized it. While that's true, people too often don't ask "why" when they're using recursion. It is a wonderful tool when it is used right, and induction is very powerful.

That being said, examples like this one and Fibonacci are often taught as examples of recursive functions, but in reality they really degrade performance. More importantly, this specific problem is a perfect example of bad recursion. This code really should simply be iterated through to get maximum performance for minimum cost. I believe that this is the exact algorithm used by MSPaint and the like.  Side note: You fell into the common trap of getting an infinite loop if `oldcolor == newcolor`. I call this function within a function which prechecks this condition  Is that a leaky abstraction? I call this function within a function which prechecks this condition    No, you can't (trivially) make every recursive function tail recursive&#8212;the function you've just described is an example. Tail recursion requires that the recursive call be the *last* operation in the calling function, which is of course impossible if there are multiple recursive calls.

You can make the transformation in a more complex manner by making an explicit "call" stack part of the function's state. This is only true in inexpressive languages. In a functional enough
programming languages, turning a function in continuation-passing style
(CPS) is a trivial way to automatically get the tail-recursive
version.

His function naively ported to [OCaml](http://en.wikipedia.org/wiki/OCaml):

    let recfill x y oldcolor newcolor =
      let rec loop x y =
        if getpixel x y = oldcolor then begin
          setpixel x y newcolor;
          loop (x-1) y;
          loop (x+1) y;
          loop x (y-1);
          loop x (y+1);
        end
      in
      loop x y


The equivalent tail-recursive version. The idea is to "accumulate" in an additional argument (here punningly named `return`, which is not a keyword) what needs to be done "after the recursive call" (therefore turning the call into a tail-call).

    let ($) f x = f x
    
    let recfill x y oldcolor newcolor =
      let rec loop x y return =
        if getpixel x y = oldcolor then begin
          setpixel x y newcolor;
          loop (x-1) y $ fun () -&amp;gt;
          loop (x+1) y $ fun () -&amp;gt;
          loop x (y-1) $ fun () -&amp;gt;
          loop x (y+1) $ fun () -&amp;gt;
          return ()
        end
        else return ()
      in
      loop x y (fun () -&amp;gt; ())

Of course there may be better way to code this algorithm that are more efficient. But just making the algorithm tail-recursive (with the understanding that the space used by the recursive calls doesn't magically go away) is in fact trivial.
 [deleted] This is only true in inexpressive languages. In a functional enough
programming languages, turning a function in continuation-passing style
(CPS) is a trivial way to automatically get the tail-recursive
version.

His function naively ported to [OCaml](http://en.wikipedia.org/wiki/OCaml):

    let recfill x y oldcolor newcolor =
      let rec loop x y =
        if getpixel x y = oldcolor then begin
          setpixel x y newcolor;
          loop (x-1) y;
          loop (x+1) y;
          loop x (y-1);
          loop x (y+1);
        end
      in
      loop x y


The equivalent tail-recursive version. The idea is to "accumulate" in an additional argument (here punningly named `return`, which is not a keyword) what needs to be done "after the recursive call" (therefore turning the call into a tail-call).

    let ($) f x = f x
    
    let recfill x y oldcolor newcolor =
      let rec loop x y return =
        if getpixel x y = oldcolor then begin
          setpixel x y newcolor;
          loop (x-1) y $ fun () -&amp;gt;
          loop (x+1) y $ fun () -&amp;gt;
          loop x (y-1) $ fun () -&amp;gt;
          loop x (y+1) $ fun () -&amp;gt;
          return ()
        end
        else return ()
      in
      loop x y (fun () -&amp;gt; ())

Of course there may be better way to code this algorithm that are more efficient. But just making the algorithm tail-recursive (with the understanding that the space used by the recursive calls doesn't magically go away) is in fact trivial.
 This is only true in inexpressive languages. In a functional enough
programming languages, turning a function in continuation-passing style
(CPS) is a trivial way to automatically get the tail-recursive
version.

His function naively ported to [OCaml](http://en.wikipedia.org/wiki/OCaml):

    let recfill x y oldcolor newcolor =
      let rec loop x y =
        if getpixel x y = oldcolor then begin
          setpixel x y newcolor;
          loop (x-1) y;
          loop (x+1) y;
          loop x (y-1);
          loop x (y+1);
        end
      in
      loop x y


The equivalent tail-recursive version. The idea is to "accumulate" in an additional argument (here punningly named `return`, which is not a keyword) what needs to be done "after the recursive call" (therefore turning the call into a tail-call).

    let ($) f x = f x
    
    let recfill x y oldcolor newcolor =
      let rec loop x y return =
        if getpixel x y = oldcolor then begin
          setpixel x y newcolor;
          loop (x-1) y $ fun () -&amp;gt;
          loop (x+1) y $ fun () -&amp;gt;
          loop x (y-1) $ fun () -&amp;gt;
          loop x (y+1) $ fun () -&amp;gt;
          return ()
        end
        else return ()
      in
      loop x y (fun () -&amp;gt; ())

Of course there may be better way to code this algorithm that are more efficient. But just making the algorithm tail-recursive (with the understanding that the space used by the recursive calls doesn't magically go away) is in fact trivial.
 &amp;gt;`let rec loop x y return =`

what is `rec`? No, you can't (trivially) make every recursive function tail recursive&#8212;the function you've just described is an example. Tail recursion requires that the recursive call be the *last* operation in the calling function, which is of course impossible if there are multiple recursive calls.

You can make the transformation in a more complex manner by making an explicit "call" stack part of the function's state. I'm not sure if is you're trying to say.

Using CPS, every recursive function is tail recursive. Usually it isn't worth it though. Yeah, but I wouldn't describe the transformation to CPS as "trivial", and it's not possible in every language since it requires first-class functions. Not trivial no, but not difficult either if the language support first class functions (as you say). However, the state that is passed is effectively the same as you would need in a conventional call stack. 

I benchmarked a nontrivial function with CPS in Scheme it was slower and used far more memory, than the non CPS version. So either I did it wrong (doubt, my former professor took a look and it didn't look obviously wrong). What scheme were you using? Not every scheme implementation takes advantage of the same opportunities to make optimizations -- I'm not certain it'd make the difference, but I'd be pleased to learn that it was. I'm not certain (it's a while ago) and not near my computer. The main problem was the heavy memory usage - the execution time was not asymptotical worse, just slower.    This code won't even work. It will infinitely recur over the 2d plane of your rectangle, and will probably overflow the stack in milliseconds. But yes, in theory you can. This code won't even work. It will infinitely recur over the 2d plane of your rectangle, and will probably overflow the stack in milliseconds. But yes, in theory you can.  That method is so grossly inefficient, I don't even... after it calls at x+1, that call is going to call back to x-1, which is going to fail 100% of the time. Ditto on the y-1s. Full on half of the recursive calls are completely unneeded. You *really* shouldn't be using recursion for a method like this, it is drastically less efficient than a simple loop.  How does your simple loop handle the case where you need to flood fill something with a spiral shape? This is just a graph traversal. It can easily be written as a loop or tail  recursive function. Simple depth first or breadth first traversals come to mind. Yes, but if you don't use a hack like the scanline algorithm does, your graph traversal algorithms will very probably de tests that are equivalent to the original code calling for x+1 and x-1 Sure, it would be slow, but at least it terminates. Also, scanline can be expressed recursively too. That method is so grossly inefficient, I don't even... after it calls at x+1, that call is going to call back to x-1, which is going to fail 100% of the time. Ditto on the y-1s. Full on half of the recursive calls are completely unneeded. You *really* shouldn't be using recursion for a method like this, it is drastically less efficient than a simple loop. </snippet></document><document><title>References to sliding window based localisation algorithms in computer vision</title><url>http://www.reddit.com/r/compsci/comments/14ji4j/references_to_sliding_window_based_localisation/</url><snippet>Could some one point me to some reference papers and texts on sliding window based localisation alogorithms  To do what? Almost every paper that does localisation or detection makes use of sliding windows.

Pick three random papers from any CVPR in the last 10 years, one of them will probably use sliding windows for localisation. You have to narrow  it down a bit. i meant is there a source to the idea... a paper where the idea is presented for the first time?</snippet></document><document><title>Can you explain the Bell&#8211;LaPadula model to someone who doesn't know anything about compsci or computer security?</title><url>http://www.reddit.com/r/compsci/comments/14ipbp/can_you_explain_the_belllapadula_model_to_someone/</url><snippet>And to make positively sure I can understand the concept, can you explain the way you would to a ten year old?

Thanks! :)  Think of how the government classifies documents in movies.  There's usually a "Secret" level that most of the government has access to, a "Top Secret" level that even fewer people have access to, maybe a "Double-Z Top Secret" level that is for the President's eyes only, and then a "Regular" level for documents that don't have to be classified.  Bell-LaPadula implements three rules on a classification system like this.

The first rule says that you can't read documents if you don't have a high enough classification level.  So if you have a "Secret" clearance, then you can read "Secret" and "Regular" documents, but you can't read "Top Secret" or "Double-Z Top Secret" documents.  Simple enough, and works exactly how you'd expect.  The simple description of this rule is "no read-up."

The second rule says that you can't write documents lower than your classification level.  Interestingly, you can write documents higher than your classification level, but you still can't read them.  So again if you have a "Secret" classification level, you can write to "Secret," "Top Secret," and "Double-Z Top Secret", but you can't write to "Regular".  Why does this make sense?  Imagine you have a bunch of secrets in your head... so many secrets that you don't remember what level each of them should be classified at.  That means that if you could write to the "Regular" level, you might accidentally reveal a "Secret" level secret to a file that anyone can read.  At the same time, if you're a secret agent in the field and need to tell the President something, but make sure that no one else can read it, you'd need to have write permission to the "Double-Z Top Secret" level.  You still can't read the other documents that the President can, but you can certainly write to them.  The simple description of this rule is "no write-down."

The third rule just adds an extra level of detail to access.  It allows for much more fine-grained access, based on who you are and what you're trying to access.  For example, a researcher with the Department of Education might have a "Top Secret" clearance for access to some documents, but this researcher shouldn't be allowed to access every single "Top Secret" document, like nuclear missile locations - only the ones relevant to education.  So the third rule allows for what's called an "Access Control Matrix," so that you can look up a user row, look up a file column, find the cell in the matrix where they meet, and look up the precise access which that user should have to that file.  Can the user delete the file?  Write to the file?  Read from the file?  Is the user the owner of the file?  That's what the third rule provides. Think of how the government classifies documents in movies.  There's usually a "Secret" level that most of the government has access to, a "Top Secret" level that even fewer people have access to, maybe a "Double-Z Top Secret" level that is for the President's eyes only, and then a "Regular" level for documents that don't have to be classified.  Bell-LaPadula implements three rules on a classification system like this.

The first rule says that you can't read documents if you don't have a high enough classification level.  So if you have a "Secret" clearance, then you can read "Secret" and "Regular" documents, but you can't read "Top Secret" or "Double-Z Top Secret" documents.  Simple enough, and works exactly how you'd expect.  The simple description of this rule is "no read-up."

The second rule says that you can't write documents lower than your classification level.  Interestingly, you can write documents higher than your classification level, but you still can't read them.  So again if you have a "Secret" classification level, you can write to "Secret," "Top Secret," and "Double-Z Top Secret", but you can't write to "Regular".  Why does this make sense?  Imagine you have a bunch of secrets in your head... so many secrets that you don't remember what level each of them should be classified at.  That means that if you could write to the "Regular" level, you might accidentally reveal a "Secret" level secret to a file that anyone can read.  At the same time, if you're a secret agent in the field and need to tell the President something, but make sure that no one else can read it, you'd need to have write permission to the "Double-Z Top Secret" level.  You still can't read the other documents that the President can, but you can certainly write to them.  The simple description of this rule is "no write-down."

The third rule just adds an extra level of detail to access.  It allows for much more fine-grained access, based on who you are and what you're trying to access.  For example, a researcher with the Department of Education might have a "Top Secret" clearance for access to some documents, but this researcher shouldn't be allowed to access every single "Top Secret" document, like nuclear missile locations - only the ones relevant to education.  So the third rule allows for what's called an "Access Control Matrix," so that you can look up a user row, look up a file column, find the cell in the matrix where they meet, and look up the precise access which that user should have to that file.  Can the user delete the file?  Write to the file?  Read from the file?  Is the user the owner of the file?  That's what the third rule provides. Awesome. "Bell" is my dad and I never knew until now what the model meant. Thanks! Think of how the government classifies documents in movies.  There's usually a "Secret" level that most of the government has access to, a "Top Secret" level that even fewer people have access to, maybe a "Double-Z Top Secret" level that is for the President's eyes only, and then a "Regular" level for documents that don't have to be classified.  Bell-LaPadula implements three rules on a classification system like this.

The first rule says that you can't read documents if you don't have a high enough classification level.  So if you have a "Secret" clearance, then you can read "Secret" and "Regular" documents, but you can't read "Top Secret" or "Double-Z Top Secret" documents.  Simple enough, and works exactly how you'd expect.  The simple description of this rule is "no read-up."

The second rule says that you can't write documents lower than your classification level.  Interestingly, you can write documents higher than your classification level, but you still can't read them.  So again if you have a "Secret" classification level, you can write to "Secret," "Top Secret," and "Double-Z Top Secret", but you can't write to "Regular".  Why does this make sense?  Imagine you have a bunch of secrets in your head... so many secrets that you don't remember what level each of them should be classified at.  That means that if you could write to the "Regular" level, you might accidentally reveal a "Secret" level secret to a file that anyone can read.  At the same time, if you're a secret agent in the field and need to tell the President something, but make sure that no one else can read it, you'd need to have write permission to the "Double-Z Top Secret" level.  You still can't read the other documents that the President can, but you can certainly write to them.  The simple description of this rule is "no write-down."

The third rule just adds an extra level of detail to access.  It allows for much more fine-grained access, based on who you are and what you're trying to access.  For example, a researcher with the Department of Education might have a "Top Secret" clearance for access to some documents, but this researcher shouldn't be allowed to access every single "Top Secret" document, like nuclear missile locations - only the ones relevant to education.  So the third rule allows for what's called an "Access Control Matrix," so that you can look up a user row, look up a file column, find the cell in the matrix where they meet, and look up the precise access which that user should have to that file.  Can the user delete the file?  Write to the file?  Read from the file?  Is the user the owner of the file?  That's what the third rule provides. That's a very good description, and there is just one thing to add: in order for the second rule (no write down) to be effectively usable, it's always possible to lower down your level of security (that's the definition of the BLP model for the Multics system). So, if you're Top-Secret, you can always log-on as a Secret user, thus allowing you to write to Secret document. But in that case, you lose the possibility to read Top-Secret documents. 

The first definition of the BLP model ("a mathematical model"), defines the second rule as "you cannot write lower than what you are currently reading", or, in other words "if you read o1 and write to o2, then the level of o2 must greater or equal than that of o1". Although it's mathematically more elegant, it's not very efficient, which is why they implement it in Multics using the current level of security, which can be lowered down by the user. 

Just wanted to mention this, because it's often forgotten, which causes some people to think that BLP is more restrictive than it actually is.    </snippet></document><document><title>[Q] Does anyone know any good books for learning Boolean Algebra? </title><url>http://www.reddit.com/r/compsci/comments/14idsq/q_does_anyone_know_any_good_books_for_learning/</url><snippet>Not 100% sure if this is in the right subreddit, but any help would be appreciated. Thanks!

:Edit: 
Just wanted to thank everyone for their replies, they helped a lot! Turns out I was just a little confused!

Seeing as we are talking about books, I would appreciate any "Sequential Logic" books?   A book on discrete mathematics? 1 1+0 &#955;f.&#955;x. f x  You're looking for a discrete mathematics book, my class last semester used [How to Prove It: A Structured Approach](http://amzn.com/0521675995). It's not a bad book, I haven't read any others on discrete though so I have nothing to compare it to. The reviews on Amazon seem good though.   I would say that you are thinking of discrete mathematics, an example of an appropriate undergraduate compsci coursebook would be [this](http://www.amazon.co.uk/Discrete-Mathematics-Computer-Scientists-International/dp/0201360616/ref=sr_1_1?ie=UTF8&amp;amp;qid=1355006163&amp;amp;sr=8-1).

As always for university/college textbooks, shop around for price and remember that there are usually textbooks by different authors that offer virtually the same information.

Also [here](http://en.wikiversity.org/wiki/Introductory_Discrete_Mathematics_for_Computer_Science) is a [wikiversity](http://en.wikiversity.org) course which I think will be relevant.

Hope that helps :)      What do you mean by Boolean Algebra? You are probably completely misunderstanding what it is :P

Anyway, pick up an book on Lattice Theory, it should be explained there. </snippet></document><document><title>Could someone please explain the difference between Call-by-value and Applicative order evaluation strategies?</title><url>http://www.reddit.com/r/compsci/comments/14hxyd/could_someone_please_explain_the_difference/</url><snippet>Lots of websites I have looked at treat them as the same thing but in Wikipedia it says "Unlike call-by-value, applicative order evaluation reduces terms within a function body as much as possible before the function is applied.". I would be really grateful if someone could explain what this means. Thanks.  That quote you have is wrong - where is it from?  The [Wikipedia page on lambda calculus](https://en.wikipedia.org/wiki/Lambda_calculus) to distinguish applicative vs normal order and call-by-* strategies pretty clearly.

Short answer: they work out to the same thing, though the two terms are usually used to highlight different features of the strategies. Quotes for the lazy:

&amp;gt; **Applicative order**  
&amp;gt; The rightmost, innermost redex is always reduced first. Intuitively this means a function's arguments are always reduced before the function itself. Applicative order always attempts to apply functions to normal forms, even when this is not possible.
Most programming languages (including Lisp, ML and imperative languages like C and Java) are described as "strict", meaning that functions applied to non-normalising arguments are non-normalising. This is done essentially using applicative order, call by value reduction (see below), but usually called "eager evaluation".  
&amp;gt; **Call by value**  
&amp;gt; Only the outermost redexes are reduced: a redex is reduced only when its right hand side has reduced to a value (variable or lambda abstraction).

Naively, those don't sound the same to me. It sounds to me like some applicative-order strategies are allowed to reduce under binders, which is a pretty unusual thing and certainly not allowed in call-by-value semantics.  If I understand it correctly: Applicative order evaluation only cares what a parameter is when/if it is used; otherwise, the parameter is unevaluated. Call by value evaluation depends on all parameters being evaluated before the function can be evaluated, regardless of whether or not they are used.


The important difference here is that applicative order is more robust than call by value. Suppose one of the parameters to a function is x/y, where y = 0. Under call by value, this would generate an exception (or whatever the language's equivalent is). Under applicative order, if that parameter is never needed then it won't be a problem. By the [Church Rosser theorem](http://en.wikipedia.org/wiki/Church%E2%80%93Rosser_theorem), if any two evaluation schemes terminate (in a functional environment, no side effects), then they evaluate to the same value. So if both applicative order and normal order work, then they result in the same answer.  IIRC they are the same thing. AOE does not reduce the function, unlike what Wikipedia says.

The only thing in my mind that rewrites the function is normal order reduction, which is never used in practice, exactly because that requires reducing the function. I think whoever wrote that sentence had normal order reduction in mind. AOE evaluates under lambdas.

CbV: `(&#955;f.&#955;g.&#955;x.f(gx))(&#955;x.x)(&#955;x.x) =&amp;gt; (&#955;g.&#955;x.(&#955;x.x)(gx))(&#955;x.x) =&amp;gt; (&#955;x.(&#955;x.x)((&#955;x.x)x))`

AOE: `(&#955;f.&#955;g.&#955;x.f(gx))(&#955;x.x)(&#955;x.x) =&amp;gt; (&#955;g.&#955;x.(&#955;x.x)(gx))(&#955;x.x) =&amp;gt; (&#955;g.&#955;x.gx)(&#955;x.x) =&amp;gt; (&#955;x.(&#955;x.x)x) =&amp;gt; (&#955;x.x)` I really should learn lambda calculus... :(
 AOE evaluates under lambdas.

CbV: `(&#955;f.&#955;g.&#955;x.f(gx))(&#955;x.x)(&#955;x.x) =&amp;gt; (&#955;g.&#955;x.(&#955;x.x)(gx))(&#955;x.x) =&amp;gt; (&#955;x.(&#955;x.x)((&#955;x.x)x))`

AOE: `(&#955;f.&#955;g.&#955;x.f(gx))(&#955;x.x)(&#955;x.x) =&amp;gt; (&#955;g.&#955;x.(&#955;x.x)(gx))(&#955;x.x) =&amp;gt; (&#955;g.&#955;x.gx)(&#955;x.x) =&amp;gt; (&#955;x.(&#955;x.x)x) =&amp;gt; (&#955;x.x)` May I ask who defined applicative order evaluation as such? My prof apparently believes applicative order does not go into lambdas.

Edit: Wait, my bad. Your AOE is correct. But I don't think your CbV is correct. CbV is AOE that doesn't evaluate under lambdas. (maybe it looks wrong because the arguments are already fully evaluated?)  </snippet></document><document><title>Nice overview of various interpolation methods</title><url>http://paulbourke.net/miscellaneous/interpolation/</url><snippet>  The sample data set used for the 2D examples isn't very good. The slope changes sign between every pair of points. There should be at least one example of continuously decreasing or increasing points in there.   It's interesting that some of the best-looking interpolation functions don't use trigonometry at all (meaning they will be much faster in this case). I wonder if that is just a byproduct of their chosen sample set.  What makes trigonometry so slow? As far as I understand they're just retrieving cached data from a table, not doing Taylor series. A table for accurate interpolation would be humongous. </snippet></document><document><title>Knuth-Morris-Pratts failure function?</title><url>http://www.reddit.com/r/compsci/comments/14hkw9/knuthmorrispratts_failure_function/</url><snippet>Hey redditors I'm going to thank you guys ahead of time, this has been bothering me for a long time now, and with no prevail to understanding the failure function. I'm looking at two examples;

j    | 0 1 2 3 4 5
p[j]| a b a c a b
f(j) | 0 0 1 0 1 2

and

j    | 0 1 2 3 4 5
p[j]| a b a b a c
f(j) | 0 0 1 2 3 0

Now for some reason I can't understand the numbers for f(j), therefore try your best please to put in the most simplest English words you can think of to teach a toddler like brain of mine.

Thank you for who ever responds or gives any thoughts! Much appreciated. </snippet></document><document><title>History of Computer Science</title><url>http://www.reddit.com/r/compsci/comments/14fllv/history_of_computer_science/</url><snippet>Hi, **I've recently become interested in Computer Science History**. In particular, I am interested in *teaching* Computer Science History. I've been doing some preliminary research, and there doesn't seem to be much cohesive material on the subject. In fact, I can't seem to find any Universities that offer established courses on the matter, or any online curriculums covering the entire history of CS. I can find a number of "brief" overviews of the history of CS, but I'm looking for something that aims to do more than just list a few key events.

To be clear, I don't mean the history of computers, which sounds like it would focus on hardware. I mean the history of Computer Science, covering important events like Shannon's formalization of Information Theory, the AI Winter, the evolution of the Linux Operating System... a very broad category of events, but all focused on the history of topics of interest to this subreddit.

I'm interested in resources on this subject. **In particular, I'd love to see courses, whether collegiate or online, that intend to cover the history of CS**. I'd also be interested in finding good books (printed is okay, but preferably online) and articles. 

If there is no existing curriculum, I have been considering developing one of my own. **If you have thoughts and opinions on what a "History of Computer Science" course should look like, I'd be very interested in hearing them too.** In my mind, the curriculum should focus on historical patterns (especially motivations and consequences), the culture of Computer Science, and recent and projected trends. I'm not sure how interested the CS community is in this sort of subject, but it seems like there'd be a lot of benefit in seeing where CS came from and where it's going.  The Information by James Gleick Cool! Thanks!  The history of the academic theory side of computer science is one thing, but you also said:

&amp;gt; In my mind, the curriculum should focus on historical patterns (especially motivations and consequences), the culture of Computer Science

In these regards, definitely read "What the Dormouse Said - How the Sixties Counterculture Shaped the Personal Computer Industry", by [John Markoff](http://en.wikipedia.org/wiki/John_Markoff) (NY Times journalist). Edit:  [Amazon](http://www.amazon.com/What-Dormouse-Said-Counterculture-Personal/dp/0143036769), [Wikipedia](http://en.wikipedia.org/wiki/What_the_Dormouse_Said) -- rather short entry

As the description on amazon says,

&amp;gt; Very few people outside the computing scene, however, have connected the dots before Markoff's lively account. He shows how almost every feature of today's home computers, from the graphical interface to the mouse control, can be traced to two Stanford research facilities that were completely immersed in the counterculture.

It ties together threads concerning landmark topics like Doug Englebart, Xerox PARC, etc. in many ways that no other history does, and helps show patterns that are only latent in other histories.

There's also a lot of culture lurking in The New Hacker's Dictionary, especially the older material from its origins as The Jargon File at the MIT AI Lab in the 1970s. Edit: [Amazon](http://www.amazon.com/New-Hackers-Dictionary-3rd/dp/0262680920), [Wikipedia "Jargon File"](http://en.wikipedia.org/wiki/Jargon_File)

Ted Nelson (famous for proselytizing hypertext decades before the Web) wrote a book that was a strong influence on many industry pioneers, "Computer Lib / Dream Machines" (1st ed. 1975), that talked about a large number of interesting things people were doing with computers, like hypertext, but also computer languages designed for doing computer graphics (like Grass and Logo), CDC mainframe Plato multiuser systems with graphical games(in the 70s) and most particularly, about the interesting projects at Xerox PARC -- many, many people first heard about all that from this book, before the Macintosh made PARC technology super famous.

Edit: Wikipedia entry: http://en.wikipedia.org/wiki/Computer_Lib_/_Dream_Machines

I have a whole stack of computer/computer science history books, mostly non-cultural and much drier and more academic than the above, but they're not at hand this second. I'm always struck by stories of computer science researchers and theorists who discover ideas that cannot be implemented or don't become useful until there's appropriate hardware/infrastructure.

The ability to think far ahead, and imagine "If I had a machine capable of doing X, we could then utilize algorithm Y to determine Z".  This would be amazing. Could you post the info you find? Absolutely. If I can't find anything to satisfy what I'm looking for, I've been considering developing this curriculum as a PhD project. Of course, that necessitates a lot of checks to make sure that it A) doesn't already exist, or B) isn't in development, and C) has interest from the community, both at large and specifically in Education. I know such a course exists in Mathematics, at least in my university, which I've wanted to take in a long time.

I think a history of CS course, would have to go to the roots of computability, or what it means to compute, and what can be computed.

I would encourage you to read [The Annotated Turing](http://www.amazon.com/Annotated-Turing-Through-Historic-Computability/dp/0470229055) which discusses the Church-Turing thesis (particularly from Turing's paper), and all sorts of interesting facts on computation. I would also cover Lambda calculus. Then continue from there, in the realm of computability and complexity theory.

If such a course were to exist, I would hope, that it would incorporate actual problems, which were conjectured/solved in history, and the results widespread.

I want to also mention checking out:

* [Godel, Escher, Bach](http://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567)
* [Logicomix](http://www.logicomix.com/en/)

Also, [this guy](http://www.youtube.com/watch?v=2bLCjMA0YlE) does an amazing lecture on Turing and some of the history surrounding him. "I know such a course exists in Mathematics" - one of the most interesting Mathematical books I've ever read was a History of Mathematics (unfortunately, I'm not at home so can't give more details). It both gave context to the topics, and a real sense of the process of discovery and influence of ideas. Compelling. Absolutely. If I can't find anything to satisfy what I'm looking for, I've been considering developing this curriculum as a PhD project. Of course, that necessitates a lot of checks to make sure that it A) doesn't already exist, or B) isn't in development, and C) has interest from the community, both at large and specifically in Education.  I'm not sure there's enough history to find "historical patterns".  The influence of individual thinkers is far more significant.  It's hard to understate the importance of individuals like Church, Turing, Von Neumann, Dijkstra, Simon, etc.
  I think it's the wrong scope. History of computer languages or interfaces or algorithms or machinery would be great. History of computer "science" doesn't work that well.

I'm well versed in interfaces and languages.  I don't know say, the predominant graph algorithms in the 50s though (lots of graph stuff happened in the 60s) and I don't know if there were anything like hash tables prior to the late 50's work on them; so algorithm history I'm weak on.  There was a lot of drama around AVL trees, I know that.

Anyway, here's some big ones you should cover:

 * memex (as we may think)
 * engelbart (mother of all demos)
 * dynabook (keys)
 * mccarthy (lisp)
 * babbage (many)
 * turing (many)
 * hopper
 * djikstra
 * von nuemann
 * shannon 
 * chomsky
 * sutherland
 * diffie/hellman
 * lucifer/des 
 * boole

Talk about 

 * the shift of programmers from female to male
 * the decline of analog and rise of digital computers
 * the old att phone system and the hacking of the 70s surrounding it
 * time-share systems and the culture enshrined by them
 * the following computers: [ABC](http://en.wikipedia.org/wiki/Atanasoff%E2%80%93Berry_Computer), Z3, [Colossus](http://en.wikipedia.org/wiki/Colossus_computer), [enigma](http://en.wikipedia.org/wiki/Enigma_machine), [Difference machine](http://en.wikipedia.org/wiki/Difference_machine), 1890 census, and if you want, [SAGE](http://en.wikipedia.org/wiki/Semi-Automatic_Ground_Environment) ... make allusions to Dr. Strangelove if you want.

Also please mention how as a community, computer scientists are really accepting people; with gay and female pioneers far before it was the norm.

I'm incredibly proud of my industry for that. Great list. I know about the PARC Dynabook, but what is the "keys" you put after it? All I can think of is "Kay" (Alan).

&amp;gt; the shift of programmers from female to male...computer scientists are really accepting people; with gay and female pioneers far before it was the norm.

Agreed, but historically it was a mixed bag. Look at poor Turing.

But certainly it's been true in more modern history, e.g. in Unix history, generally speaking the computing community was accepting before it was "in", and had nothing but respect for gay couple [Eric Allman](http://en.wikipedia.org/wiki/Eric_Allman) and [Kirk McKusick](http://en.wikipedia.org/wiki/Marshall_Kirk_McKusick) famously made contributions to BSD Unix like [Sendmail](http://en.wikipedia.org/wiki/Sendmail) and [the Berkeley Fast Filesystem](http://en.wikipedia.org/wiki/Berkeley_Fast_File_System), respectively -- for instance.

And although Lovelace and Hopper are hugely important, and although female "computers" were pivotal in World War II, I'm not sure we can say that early programming was dominated by females; is that really true? I think it's the wrong scope. History of computer languages or interfaces or algorithms or machinery would be great. History of computer "science" doesn't work that well.

I'm well versed in interfaces and languages.  I don't know say, the predominant graph algorithms in the 50s though (lots of graph stuff happened in the 60s) and I don't know if there were anything like hash tables prior to the late 50's work on them; so algorithm history I'm weak on.  There was a lot of drama around AVL trees, I know that.

Anyway, here's some big ones you should cover:

 * memex (as we may think)
 * engelbart (mother of all demos)
 * dynabook (keys)
 * mccarthy (lisp)
 * babbage (many)
 * turing (many)
 * hopper
 * djikstra
 * von nuemann
 * shannon 
 * chomsky
 * sutherland
 * diffie/hellman
 * lucifer/des 
 * boole

Talk about 

 * the shift of programmers from female to male
 * the decline of analog and rise of digital computers
 * the old att phone system and the hacking of the 70s surrounding it
 * time-share systems and the culture enshrined by them
 * the following computers: [ABC](http://en.wikipedia.org/wiki/Atanasoff%E2%80%93Berry_Computer), Z3, [Colossus](http://en.wikipedia.org/wiki/Colossus_computer), [enigma](http://en.wikipedia.org/wiki/Enigma_machine), [Difference machine](http://en.wikipedia.org/wiki/Difference_machine), 1890 census, and if you want, [SAGE](http://en.wikipedia.org/wiki/Semi-Automatic_Ground_Environment) ... make allusions to Dr. Strangelove if you want.

Also please mention how as a community, computer scientists are really accepting people; with gay and female pioneers far before it was the norm.

I'm incredibly proud of my industry for that.  One thing that seems to get overlooked a lot -- I've been listening to the audiobook of "Turing's Cathedral", which is excellent -- is the amazing work done by Konrad Zuse. 

There are excellent books about Babbage -- can't leave him out.

For a fun cruise through the practical matter (sort of a gross anatomy course) I haven't seen anything better than "Code" by Charles Petzold....   One thing we really take for granted today, but which took time to develop, was the basic concept of stored programs -- nor was that simply yet another sole invention by von Neumann.

See e.g. "Stored Program Concept - The Origin of the Stored Program Concept",
Allan G. Bromley, 1985. Scanned copy online: http://sydney.edu.au/engineering/it/research/tr/tr274.pdf

One important historical thread concerns Object Oriented Programming, since it was integral to the PARC effort as well as of major languages from the 1980s through today.

It's well-known that Alan Kay coined the term, and that [Simula 67](http://en.wikipedia.org/wiki/Simula) was the first OO language, and that Ivan Sutherland's phenomenal 1960 [Sketchpad](http://en.wikipedia.org/wiki/Sketchpad) pioneered OO programming and GUIs and influenced Doug Englebart, leading to [The Mother Of All Demos](http://en.wikipedia.org/wiki/The_Mother_of_All_Demos) -- all tightly interwoven history.

Much less well-known to non-specialists is that the Lisp thread of history ties directly in to that.

Alan Kay introduced [messaging to Smalltalk/OO programming](http://lists.squeakfoundation.org/pipermail/squeak-dev/1998-October/017019.html) and considered it an essential part of OO that most other OO systems lack -- and in any case is an important part of Smalltalk -- Kay got this from Lisp research including the Lisp-based PLANNER language and [Carl Hewitt's](http://en.wikipedia.org/wiki/Carl_Hewitt) research on the originally Lisp-based [Actor](http://en.wikipedia.org/wiki/Actor_model) model of concurrent computation.

See http://en.wikipedia.org/wiki/History_of_the_Actor_model#Smalltalk

(Messaging semantics remained in the Smalltalk language definition, but for [efficiency reasons](http://c2.com/cgi/wiki?MessagePassing) was modeled by function calls in the implementation beginning with Smalltalk 80)

Related: [Viewing Control Structures as Patterns of Passing Messages](http://www.erights.org/history/actors.html), Carl Hewitt, 1976.

["The Early History Of Smalltalk"](http://worrydream.com/EarlyHistoryOfSmalltalk/), Alan C. Kay is a good read for understanding the general historical context.

Regarding the key point "Kay got this from Lisp research", I'm having trouble locating the essential quote that especially clarifies Kay's insight, even beyond what he cites in "Early History", about control structure implementation unification; I'll post later if/when I can find it.

That leaves this post somewhat less helpful than I'd like, but I want to save the above before my browser/computer crashes just to spite me. If you mention Simula you should really mention the guys behind it - Ole-Johan Dahl and Kristen Nygaard. (I had the privilege to have them as professors when I took my computer science degree)  I too am interested in the History of Computer Science, and have done a little bit of study on this matter, so I'll help you with what I have. 

However, I'm afraid I'm going to have to ask you to be patient until tomorrow, because I'm not at home right now and I don't really have time to search on the 'net for all the resources.

Also, please don't expect anything mind-blowing, it's just a few documentaries that I've watched, articles I've read and my own thoughts on how it should be taught. Not a problem, happy to get anything I can. I expect this project to take a long time, especially since I don't have any kind of background in history. Please share what you can, when you can :)                I wrote a blog post a while back, this was essentially it.  I would suggest focusing on Turing and his history.  I feel he is at the center of it all.  Knuth for the pure computer science algorithms.

Here are some interesting dates:

  *  1903 - Alonzo Church was born in Washington, D.C. (USA)
  *  1928 - The Entscheidungsproblem decision problem was proposed by David Hilbert 
  *  1936 - Church publishes "An Unsolvable Problem of Elementary Number Theory", Church's Thesis [1]. It is a paper on untyped lambda calculus. American Journal of Mathematics, Volume 58, No. 2. (Apr., 1936)
  *  1936 - Alan Turning publishes a paper on an abstract machine , On Computable Numbers, with an   *  Application to the Entscheidungsproblem' Proceedings of the London Mathematical Society, Series 2, 42 (1936-37). He proposed the concept of the stored-program. 
  *  1936 - 1938 - Alan Turing studies under Alonzo Church
  *  1937 - John von Neumann recommends Alan Turing for Fellowship at Princeton.
  *  1938 - Alan Turing receives Ph.D from Princeton
  *  1946 - Alan Turing presents a paper on the stored-program computer (Automatic Computing Engine).
  *  1937+ - John von Neumann gains knowledge from Alan Turing's papers but Turing was not directly related to the development of ENIAC.
  *  1943 - 1946 - Creation of ENIAC (Electronic Numerical Integrator And Computer). Note: ENIAC was not a stored-program computer.
  *  1944 - John von Neumann became involved with ENIAC 
  *  1945 - John von Neumann publishes paper on Electronic Discreet Variable Computer (EDVAC)
  *  1948 - Manchester Mark I developed at Manchester University, first stored-program computer
  *  1949-1960 - Early stored computers were created, some of the based on von Neumann architecture.
  *  1938 - Donald Knuth was born
  *  1957 - Donald Knuth had access to a computer. "I saw my first computer in 1957, which is pretty late in the history game as far as computers are concerned. There were about 2000 programmers in the entire world"
  *  1963 - Donald Knuth began work on the Art of Computer Programming.
  *  1973 - C programming language appeared.

Note: I presented milestones but some of these events were not directly related. Damn you and your lack of mention of Kleene and his FSM! And your mention of the Atanasoff-Berry Computer the first Digital computer built in the early 1940s. So Kleene did work with Church.  Hmm, maybe when the guy does research on Church, Kleene will come up for him.  </snippet></document><document><title>A poll for students and others involved in the Compsci world</title><url>http://www.reddit.com/r/compsci/comments/14bw4g/a_poll_for_students_and_others_involved_in_the/</url><snippet>Been having discussions with friends about a few things about Computer Science and I got tired of making guesses and assumptions and set out to collect hard data. This is a questionnaire about some simple CS concepts. I'll probably take the data I get and make some kind of infographic and put it up on the internet for us to all see. I'm particularly interested in what kinds of fields people are interested in. 
  
Here's the link: [Google Docs Form](https://docs.google.com/spreadsheet/viewform?formkey=dGF4MGJRSGktWkhFNC1VTU9sRTBWZ3c6MQ) 
   
Thanks in advance!  
  
(btw, none of the information will be used for any purposes other than raw curiosity i.e. no commercial/academic interests are involved here. Also, I don't see any personal information in the results so don't worry about that.)  

I'll have a follow up post eventually with the results.  

**Results are in! Look below!** More will come tomorrow as the results of the poll keep coming in. Feel free to keep taking the poll. We've done some basic analysis. Enjoy!  

**More Results** This link contains 1071 responses in csv format. I'll play around with exploring the data set tomorrow probably. I've just finished my final final of the semester and need to defragment. [Link](http://pastelink.me/dl/e3e6ce)  
 

Edit: Brain is fried from studying/end of semester activities. Spelling and grammar reflects that. Made minor changes.    You may want to specify whether "hardest/easiest class" refers only to computer science or to any class. I have gotten both kinds of responses. I'm OK with this. It's interesting to see the responses that are paired with people who say "English". I see your point but I think I'll avoid changing it after I've had it that way for so long. It's bad to temper with an experiment in the middle. It can contaminate data.  i stuck with CS-related classes...i chose linear algebra for hardest (was a toss up between that and models of computation for me...). Linear isn't necessarily needed for CS major (it was required where I went, but if you aren't doing 3d graphics, it's not that useful it seems like). I use Linear Algebra routinely in my Machine Learning class and I wish I had a deeper background in it. Here you can take DiffEQ or Linear Algebra. I took DiffEQ when I was Aero, I never ended up taking Linear Algebra.  Hello. Would it be ok if I x-posted this to r/girlsgonewired please? I'm sure it would be of interest to our sub and you'd get a few responses there :)  I definitely had to read this comment 3 times to realized it was girlsgonewiRed not wiLd. I was so confused. Cool subreddit! I just subscribed :) Hello. Would it be ok if I x-posted this to r/girlsgonewired please? I'm sure it would be of interest to our sub and you'd get a few responses there :)  Absolutely! Share it with whomever you'd like! This goes for others who are interested as well.  

Thanks for thinking about it. When I post the results here, you can x-post those as well. :) Hello. Would it be ok if I x-posted this to r/girlsgonewired please? I'm sure it would be of interest to our sub and you'd get a few responses there :)   Too busy tonight to process the data. I'm sure we'll get more as the night rolls on and tomorrow comes around.  

In the mean time, there are more than 600 responses which will let people get some idea of the opinions.  

As I promised, here are the results. [paste.me link](http://pastelink.me/dl/434675)  

I'll be around tomorrow around 7PM EST to mess with the rest of the results. Some nice SQL Injection there boys

R summary(cspoll):

    What.year.in.school.is.this.for.you.
    1 : 62    2 :118    3 :149    4 :166    5 : 70    6+: 62

    What level of degree are you pursuing
    BS: 404     BS - Graduated: 69     MS: 82     MS - Graduated: 23
    PhD: 38     PhD - Graduated: 11

    Job Plans
    Academia: 54
    Job: 420 (lol)
    Both: 150
    Unemployed: 3

    Motivation
    Money: 64
    Passion: 276
    Trade-Off: 287

Other fields require disambiguation to look at.

Looks like one guy put 'C' for everything
     This isn't entirely complete but so many of you put obscure linux versions that it's more work than I'm willing to do right now. At most these numbers are each missing about 30 points, which means that these numbers are fairly reflective of the population.   
Linux: 268  
Windows: 213  
OSx: 104  
 I suspect the best way to do this, is does the answer contain "linux", "osx"  or "windows".

Obviously you'd want case insensitivity + "os x" as well. The proper iSpelling is OS X, anything else is heresy, and shouldn't be counted. I suspect the best way to do this, is does the answer contain "linux", "osx"  or "windows".

Obviously you'd want case insensitivity + "os x" as well. Some nice SQL Injection there boys

R summary(cspoll):

    What.year.in.school.is.this.for.you.
    1 : 62    2 :118    3 :149    4 :166    5 : 70    6+: 62

    What level of degree are you pursuing
    BS: 404     BS - Graduated: 69     MS: 82     MS - Graduated: 23
    PhD: 38     PhD - Graduated: 11

    Job Plans
    Academia: 54
    Job: 420 (lol)
    Both: 150
    Unemployed: 3

    Motivation
    Money: 64
    Passion: 276
    Trade-Off: 287

Other fields require disambiguation to look at.

Looks like one guy put 'C' for everything
     Some nice SQL Injection there boys

R summary(cspoll):

    What.year.in.school.is.this.for.you.
    1 : 62    2 :118    3 :149    4 :166    5 : 70    6+: 62

    What level of degree are you pursuing
    BS: 404     BS - Graduated: 69     MS: 82     MS - Graduated: 23
    PhD: 38     PhD - Graduated: 11

    Job Plans
    Academia: 54
    Job: 420 (lol)
    Both: 150
    Unemployed: 3

    Motivation
    Money: 64
    Passion: 276
    Trade-Off: 287

Other fields require disambiguation to look at.

Looks like one guy put 'C' for everything
      i have a BA in computer science though! Blasphemy! Get out! ;-) i highly recommend it, much more freedom My school doesn't offer it, actually. Ours is probably modeled closer to your BA than your BS program though. We take very few/no Engineering courses. Our Computer Engineering program is probably our version of that.  

I didn't know the BS/BA thing was so common. I had only heard of it happening at University of Michigan. i was told that it would basically make no difference to employers/grad schools so the choice was obvious  What about those of us who dropped out to work as developers full time instead? What about those of us who dropped out to work as developers full time instead?  Awesome survey. I did it. How far are you in your degree?  Final year. How about you? About to enter my 3rd semester. Did 5 years in the Marines so got a late start.  I did 3 years in Aerospace Engineering before I switched to CS, so I can sort of relate on the late start part. I kind of like it though. Being older than everyone else generally means the professor remembers me and the experience I've gotten from other areas of my life help me do better with all my CS stuff. Plus, I can legally reach the [Balmer Peak](http://xkcd.com/323/)! I did 3 years in Aerospace Engineering before I switched to CS, so I can sort of relate on the late start part. I kind of like it though. Being older than everyone else generally means the professor remembers me and the experience I've gotten from other areas of my life help me do better with all my CS stuff. Plus, I can legally reach the [Balmer Peak](http://xkcd.com/323/)! Haha awesome. I like being a little bit older too. It does sometimes help the concentration.  Haha, I said nothing about concentration. I have a final in three hours and I'm on Reddit, after all. :)  Just want to add a quick note to my survey. When I ask what your favorite OS/Language is, it's just what you have the most fun with, or even just admire its qualities. I'm not trying to ask what is best, because, obviously, there is a different tool for every purpose. :)  

Edit for brain fart um i put brainfuck. im not sure if that's even a real language. sorry op Don't forget the [Whitespace Language](http://compsoc.dur.ac.uk/whitespace/index.php). It's all spaces, tabs, and newlines. All other characters are considered comments. um i put brainfuck. im not sure if that's even a real language. sorry op It is a real language. I believe that it is an implementation of a Turing Machine. Lots of carrots. You're not the only one. :) lol I just had a test about the Turing machine!  
 Deterministic TM's Non-Deterministic TM's Multi-Tape TM's my head is so full of TM's that I dream in states and transition tables. Deterministic TM's Non-Deterministic TM's Multi-Tape TM's my head is so full of TM's that I dream in states and transition tables. I have that class at 8:30 in the morning...so brutal. I also have to take the final at 8:30 in the morning......PRAY FOR ME lol I just had a test about the Turing machine!  
 Just want to add a quick note to my survey. When I ask what your favorite OS/Language is, it's just what you have the most fun with, or even just admire its qualities. I'm not trying to ask what is best, because, obviously, there is a different tool for every purpose. :)  

Edit for brain fart I reluctantly put Windows as it's the O/S that I have the most expertise in, but I could've chosen AmigaOS as it has so many happy memories for me. I could do shit with it as I haven't even seen it running in about 15 years. I've always believed you should do what makes you happy. Thinking about it, though, banging your head against Amiga probably isn't going to make you happy. Not in the short term at least. haha. Just want to add a quick note to my survey. When I ask what your favorite OS/Language is, it's just what you have the most fun with, or even just admire its qualities. I'm not trying to ask what is best, because, obviously, there is a different tool for every purpose. :)  

Edit for brain fart  Upvoted for justice. haha, I can live without the imaginary internet points. I've really loved some of the free response inputs I've gotten though! Thanks for your time :) Free internet points and perhaps more visibility kind sir!

Where are you in school btw? This is my senior year. I've (thankfully) already secured a job for after I graduate. Just need to wrap up my classes next semester!  only for US students I see. I had originally intended it to just be for my university's Facebook board, but decided to post it on Reddit to get a wider variety of answers. Do you have any specifics that prevents non-US students from answering? I'm not too familiar with the differences, to be honest. I'm sorry if you feel left out. I still filled it in! It's not that bad, actually, I just saw the years. The years are a little messed up, university lasts for 6 years for you guys?! Or is this counting post-graduate study too? In the UK it's 3 years for a BSc, 1 year on top for a masters, and a PhD takes anywhere between 3-4 years (typically 4). I just did that to force people to use numbers versus the freshman, sophomore, etc. titles. BS is generally 4 years here. MS in 1, with no thesis. PhD can take any number of years though. I've personally been in school for 5 years thanks to a last minute change of major to CS from Aerospace Engineering. I'll never look back! :P I had the same issue from Computer Engineering to Computer Science so I feel you man Super seniors FTW! I had the same issue from Computer Engineering to Computer Science so I feel you man Super seniors FTW! I just did that to force people to use numbers versus the freshman, sophomore, etc. titles. BS is generally 4 years here. MS in 1, with no thesis. PhD can take any number of years though. I've personally been in school for 5 years thanks to a last minute change of major to CS from Aerospace Engineering. I'll never look back! :P I still filled it in! It's not that bad, actually, I just saw the years. The years are a little messed up, university lasts for 6 years for you guys?! Or is this counting post-graduate study too? In the UK it's 3 years for a BSc, 1 year on top for a masters, and a PhD takes anywhere between 3-4 years (typically 4). I still filled it in! It's not that bad, actually, I just saw the years. The years are a little messed up, university lasts for 6 years for you guys?! Or is this counting post-graduate study too? In the UK it's 3 years for a BSc, 1 year on top for a masters, and a PhD takes anywhere between 3-4 years (typically 4). I still filled it in! It's not that bad, actually, I just saw the years. The years are a little messed up, university lasts for 6 years for you guys?! Or is this counting post-graduate study too? In the UK it's 3 years for a BSc, 1 year on top for a masters, and a PhD takes anywhere between 3-4 years (typically 4). I had originally intended it to just be for my university's Facebook board, but decided to post it on Reddit to get a wider variety of answers. Do you have any specifics that prevents non-US students from answering? I'm not too familiar with the differences, to be honest. I'm sorry if you feel left out. We at least have no "course numbers" that would be useful to you (something like 667.345A), and I have no idea what sophomore, etc., are  Can you/will you publish your results? I am interested in seeing what you came up with. I'll absolutely publish the results. It's for science after all!

Edit: I have a final tonight at 7EST. I'll probably try and just summarize some of the data tonight. All my finals will be finished tomorrow and I'll publish the whole data set as well as a more in depth analysis of things that interest me. Well, good look on the finals! And, thanks!
 I always look good, but I appreciate the sentiment! :P And to think, I put "Technical Writing" as my easiest class! Reminds me of the time I misspelled the word "intelligent" on a resume. Well, look good for the final! I know I will try to look good for mine, because I am certainly too dumb to pass the test anyway. :O I'll absolutely publish the results. It's for science after all!

Edit: I have a final tonight at 7EST. I'll probably try and just summarize some of the data tonight. All my finals will be finished tomorrow and I'll publish the whole data set as well as a more in depth analysis of things that interest me. Hey, If you still have the results can you please upload them again? I missed them the first time around and now the link has expired! Thanks Ask and you shall receive!  

https://docs.google.com/spreadsheet/ccc?key=0AomAyP6kHMjrdGF4MGJRSGktWkhFNC1VTU9sRTBWZ3c  You attend UCF? Wasn't that the university that gutted its CS research program in favor of teaching only? I don't know the whole history of the program. Everyone would have their shade to add to it, I'm sure. Right now there is a plethora of Graduate and PhD positions available here. We literally have a whole building full of researchers and the research park that partners with UCF is a world leader in Virtual Reality research. I've never heard of anyone criticize UCF's graduate program.  

You may be thinking of UF (our northern cousin) who cut a lot of CS programs, but I believe they kept their CompEng stuff going.  

Source: I don't know what I'm talking about, why are you listening to me? When I was there a few years ago it was the teaching program that blew.  It was a running "joke" among TAs that the department didn't give a rats ass about the students.  Worst thing I found about the program was the tendency to adjust grades to fit the test scores of the class instead of adjusting teaching style or the tests themselves.  The tests tended to remain nearly identical semester to semester and it mostly came down to luck if you signed up when the class was getting B's with a 50% average test score or a 70% average test score. I don't know the whole history of the program. Everyone would have their shade to add to it, I'm sure. Right now there is a plethora of Graduate and PhD positions available here. We literally have a whole building full of researchers and the research park that partners with UCF is a world leader in Virtual Reality research. I've never heard of anyone criticize UCF's graduate program.  

You may be thinking of UF (our northern cousin) who cut a lot of CS programs, but I believe they kept their CompEng stuff going.  

Source: I don't know what I'm talking about, why are you listening to me?  I don't want to nitpick, but I'm going to nitpick. Why is it green on black in a terrible font? It was very difficult for me to read and I had to struggle to complete the survey. Sorry. You're probably right. I just saw the description "CONSOLE" for the theme and thought that it fit the purpose. :)  

I'll change it to something more legible. Thanks! Changed it to a white background. Kept the consolas font for giggles though. Thanks! Sorry to complain, as I enjoyed taking the survey. I am just bad at pointing things out like that in a polite way.
edit: oh no, now it's light green on white! Changed it to a white background. Kept the consolas font for giggles though. Please don't; it's still hard to read and really ugly. First rule of UX is to make it as easy as possible for your target user/customer to do what you ask. Consolas is a fixed-width font without and not thick enough --- in short, *not* suited for the Web. I'd suggest you revert to the standard Google spreadsheets font. Sometimes, it takes an army of great designers to build the simplest products that make you think have no thought behind them. 

I don't know where you are in your CS education, but if I may give some unsolicited advice: focusing on the user is something we computer scientists almost always forget to do, but it is something that most kids right out of HS building websites/apps know well. I recommend the book 'The Non-designer's Design Book' for a good overview of general design concepts. [Don't Make me Think](http://www.amazon.com/gp/product/0321344758?ie=UTF8&amp;amp;tag=uxbydesign-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=0321344758) is a good book on general UX concepts.  I changed it because you spent enough time explaining the concept of UX to know it really bothered you. I know it didn't look great, but it still tickled me. I swear I'd never used a fixed width font in any real application. :) Thanks! Being able to accept critical advice is a quality that is in short supply these days :)  Nice survey! It will be interesting to see the results. I kind of wish you had put at least an email address field so that we could be notified when you publish.  Hope you'll get enough data, I'm really interested in the results.  i want to see those free response answers I'll give a sneak peak. One of them is "Penis". Lots of anger in some. But the majority are well thought out and polite. That's the internet for you. :)  

They'll be out a little bit later. Only one? i want to see those free response answers  You might want to let the user indicate whether they are a professional returning to school or freshly graduated from HS. 
 This is an interesting concept. I may do follow up polls based off of some hypothesis I generate looking at the data I generate from the current poll. The poll itself came about to settle some disputes about CompSci that me and some friends had. My response to our circular debate was to gather hard data. This is an interesting concept. I may do follow up polls based off of some hypothesis I generate looking at the data I generate from the current poll. The poll itself came about to settle some disputes about CompSci that me and some friends had. My response to our circular debate was to gather hard data. Mind if I ask, what was the debate about? What the hardest class was. We have another experiment we want to try involving the hardest and easiest courses between a few disciplines and seeing what the reactions are from people attending a different discipline's classes. I think we'll be too lazy to do it, but we couldn't even agree on the hardest class. I'd be also interested in the hardest class, but also in the composition of the majors between institutions, and between the courses/classes themselves.  I'm getting a BA, where's my degree option? Please assume a BA is equivalent to a BS in this case. I could have more generically said a Bachelor's but, as I stated in a previous post, I originally wrote this for only my University CS program and lazily wrote BS instead of bachelor's.  I'm getting a BA, where's my degree option? I'm getting a BA, where's my degree option? &amp;gt; Please only answer this if you're a CS major. &amp;gt; Please only answer this if you're a CS major. I am getting a BA in CS. You don't know what you're talking about. MSc here. Nothing but gentle ribbing. Calm down. MSc here. Nothing but gentle ribbing. Calm down.    Of all the possible places that you could be attending, you're at UCF. That's awesome, me too!! I am at UCF. But, hopefully, not for long. Hi ho graduation! What advice would you have for me? I finish intro to C tomorrow. Can you give me some advice? Was CS difficult, study habits, anything and everything I could know? Thanks a lot LOL </snippet></document><document><title>Spin Transformations of Discrete Surfaces - Conference Presentation</title><url>http://www.youtube.com/watch?v=UQC_emOPVK8</url><snippet> </snippet></document><document><title>Isn't the incomputability of Kolmogorov complexity a non-practical theorem?</title><url>http://www.reddit.com/r/compsci/comments/14broq/isnt_the_incomputability_of_kolmogorov_complexity/</url><snippet>I was reading about [the Incomputability of Kolmogorov complexity](http://en.wikipedia.org/wiki/Kolmogorov_complexity#Incomputability_of_Kolmogorov_complexity), but I could not understand how the proof was relevant to our world.

The problem I saw was that say you had a function Halts(x) that takes a program and returns true if it halts (ignoring the halting problem for now) then it becomes computable. We create this program that loops through all possible programs until if finds one that halts on the input string, then returns the size of that program.

    KolmogorovComplexity(string s)
        for i = 1 to infinity:
            for each Program P of size i:
                if Halts(P):
                    if s equals P():
                        return sizeof(P)

This would contradict the proof, since the proof states we can find a smaller program. But that's impossible, we already tried all programs that are smaller. In fact we already tried running it through the contradiction proof, but since it is referring to itself it created an infinite loop and therefore doesn't halt.

Of course, this relies on us ignoring the halting problem, but since the halting problem doesn't exist for finite memory (like in our physical world) then isn't it a lie to say we can't make an algorithm that can find the smallest compression of a string?

**tl;dr: shouldn't it be possible to create an algorithm that returns the smallest string compression possible for a given amount of memory?**

Edit: I realized there were a crucial flaw with my thinking, [kahirsch gave a good explanation of why it still isn't going to be practical.](http://www.reddit.com/r/compsci/comments/14broq/isnt_the_incomputability_of_kolmogorov_complexity/c7bnghz)  Yes, if you limit the memory to a finite amount, then the computers are no longer general purpose and you can theoretically solve the halting problem.

In practice, it's no help at all since it would take billions of years to enumerate even very short programs.
 And whether it will actually take billions of years for even short programs or can be short-circuited to be polynomial time is the P != NP conjecture. I don't believe so. That would only be the case if every program halted in polynomial time (if it halted). In reality, there probably wouldn't be a poly-time verification routine for arbitrary programs.  Yes, I overstated my case.  But the basic idea is that given a Turing machine and finite memory, is there an input that halts in finite time.  Verification is clearly polynomial time (just simulate) but finding such a program is most likely difficult.

The more general case, finding if the Turing machine halts on _all_ inputs with finite memory conditions is actually in CoNP.  This distinction is important and it makes these two conditions different, but you can see how the questions are very similar. &amp;gt; But the basic idea is that given a Turing machine and finite memory, is there an input that halts in finite time. Verification is clearly polynomial time (just simulate) but finding such a program is most likely difficult.

I don't see it. Either the memory size allowed is part of the input in which case finding something that halts is an EXPSPACE-complete problem (so definitely not NP) or the amount of memory you're given is fixed in which case running time is trivially near-linear (the time it takes to simulate a Turing machine). Some classification in the W hierarchy comes to mind.

Same issue applies to the second paragraph. And whether it will actually take billions of years for even short programs or can be short-circuited to be polynomial time is the P != NP conjecture.  You already have a function that returns true when a program halts.
Just run the program, and when it halts, you return true. If it halts, you'll return true.

The problem isn't there, it's in returning false when it doesn't halt. That's the non computable part. "if Halts(P)" couldn't be evaluated, because there will be some program P, such that you cannot return false from Halt(P), even when P does not halt.

If you are assuming finite memory (i.e. finite states and finite input), sure, but there may be strings for which the shortest program that outputs it is larger than your hardcoded memory bound, and so your program will not be able to find their Kolmogorov complexity. If there is a program that fits in your memory and calculates s in the shortest way possible, and if you have a way to decide for each input if it halts or not, then sure, you will find said program.

If you're going for practicality by assuming finite memory, however, you can't go the route of "finite memory avoids halting problem", because that algorithm which, for each program up to a given size, tells you if it halts or not, is ridiculously impractical (and I'm not even sure if the machine of finite memory itself can compute such a table, even when you bound the input size or state size). &amp;gt;you can't go the route of "finite memory avoids halting problem", because that algorithm which, for each program up to a given size, tells you if it halts or not, is ridiculously impractical

Has it been proven to be slow? That would of course mean even if you *can* find the smallest compression, it still will be as impractical as dismissing the original incomputability of Kolmogorov theorem. &amp;gt; Has it been proven to be slow? 

If you're talking about running a program until it halts or it repeats a state, yes this is impractically slow.

Here's a C program which can fit in a tiny number of bytes:

    unsigned short a[12];
    #define ARRAYSIZE (sizeof(a)/sizeof(*a))

    #define N 2

    int
    main(void)
    {
      unsigned int i;
      do {
        for (i=0; i &amp;lt; ARRAYSIZE &amp;amp;&amp;amp; ++a[i] == 0; i++)
            ;
      } while (i &amp;lt; N);
      return 0;
    }

That takes about 16 seconds to run on the computer I'm typing this on.

Change the 2 to a 6 and it will run for about 9 trillion years. But it never repeats a state and it halts!

EDIT: recalculated after I realized that I used 'short' with 16 bits, not 'char' with 8 bits.

 You already have a function that returns true when a program halts.
Just run the program, and when it halts, you return true. If it halts, you'll return true.

The problem isn't there, it's in returning false when it doesn't halt. That's the non computable part. "if Halts(P)" couldn't be evaluated, because there will be some program P, such that you cannot return false from Halt(P), even when P does not halt.

If you are assuming finite memory (i.e. finite states and finite input), sure, but there may be strings for which the shortest program that outputs it is larger than your hardcoded memory bound, and so your program will not be able to find their Kolmogorov complexity. If there is a program that fits in your memory and calculates s in the shortest way possible, and if you have a way to decide for each input if it halts or not, then sure, you will find said program.

If you're going for practicality by assuming finite memory, however, you can't go the route of "finite memory avoids halting problem", because that algorithm which, for each program up to a given size, tells you if it halts or not, is ridiculously impractical (and I'm not even sure if the machine of finite memory itself can compute such a table, even when you bound the input size or state size).    </snippet></document><document><title>A tutorial on checksums and CRCs.</title><url>http://www.ece.cmu.edu/%7Ekoopman/pubs/KoopmanCRCWebinar9May2012.pdf</url><snippet>  Anybody know how they brute force CRCs? [Some anime fansubs](http://rorisubs.com/2012/11/21/why-is-the-file-locked/) like to do this, but I have no idea how. </snippet></document><document><title>Good books on the structure/semantics of programming languages.</title><url>http://www.reddit.com/r/compsci/comments/149m5b/good_books_on_the_structuresemantics_of/</url><snippet>I'm writing an essay and I'm struggling finding decent books for research/references. The only good book I have found so far is Structure and Interpretation of Computer Programs by Abelson and Sussman. 

If you have any recommendations I would be really grateful, thanks.  [Types and Programing Languages](http://www.cis.upenn.edu/~bcpierce/tapl/), by Benjamin Pierce, explores programming language semantics through development of the lambda calculus. I'm reading this right now, and it's an excellent introduction to approaching languages through operational semantics. Also, I think the way it works up from a simple arithmetic language and then the untyped lambda calculus would be well-suited to someone presently using SICP.

One note, though. This book is particularly about type systems and typed languages. So, if that's not the author's interest or focus right now, TaPL might not be suitable. I'm reading this right now, and it's an excellent introduction to approaching languages through operational semantics. Also, I think the way it works up from a simple arithmetic language and then the untyped lambda calculus would be well-suited to someone presently using SICP.

One note, though. This book is particularly about type systems and typed languages. So, if that's not the author's interest or focus right now, TaPL might not be suitable. From what I remember of the book, typing followed naturally from the way they define the operational semantic's step relation *(^(the theoretical behavior of a language on some abstract computing machine))* as an inductive set of inference rules *(^(constraints that a single program-point reduction most satisfy in order to satisfy the step relation, which intuitively matches the right hand side of the relation to the left hand side in such a way that the abstract machine, after taking one step, will lead to one of the satisfying right hand side expressions))*. Instead of only have expression and value terms *(^(in pierce style, values are terms irreducible by the stepping relation, [e.g. the machine cannot make any more progress in execution], that can qualify as the output/return value of a program, loosely defined; usually useful in creating other derivations on top of our language and in proofs of soundness of certain properties, whereby if we can conclude some property holds, then we should expect that our program will not get stuck at a non-value term))*, we now tack on a type term and introduce another form of "[operational] semantic" (it's helpful to think of them as similar at the start) that specifies the reduction between expression terms and type terms.

For example, the following is an excerpt from chapter 23 that talks about parametric polymorphism in types (think of generics in Java or templates in C++) by presenting a variant of the simply typed lambda calculus (known as System F) extended with the concept of type-abstractions and applications:

http://dl.dropbox.com/u/25316665/systemf.pdf

Overall, it's a deep but well written (and for the most part, easily read) text on the subject. Now, typing is quite heavy in the PL field because most program property verification problems can be reduced to one of either proving the well-typedness of certain terms in your language or one of type construction. 

Other suggestions/resources:

* The Formal Semantics of Programming Languages by Glynn Winskel, MIT Press, 1993.
* http://www.cs.cmu.edu/~rwh/plbook/book.pdf - Practical Foundations for Programming Languages by Robert Harper.
* http://www.cs.brown.edu/~sk/Publications/Books/ProgLangs - Programming Languages: Applications and Interpretation by Shriram Krishnamurthy.
* http://www.cs.cornell.edu/Courses/cs6110/2011sp/lectures/lecture37.pdf - Summary of Category Theory and a quick introduction to monads, complement to http://homepages.inf.ed.ac.uk/wadler/papers/marktoberdorf/baastad.pdf
* http://www.cs.cmu.edu/~fp/courses/15816-s12/schedule.html - introduction to linear logic 

Cool extensions (these are mostly all citable articles, a lot of which came from POPL)
* http://cs.nyu.edu/~soule/tr2010-924.pdf
* http://www.cs.cornell.edu/~kozen/papers/New.pdf
* http://dl.acm.org/citation.cfm?id=2103721
* http://dl.dropbox.com/u/25316665/p125-cook.pdf
* http://cqctworld.org/docs/cqct.pdf - along with http://cqctworld.org/ A lot of people use PLAI but to me it's too simplistic a text. It's just about implementing interpreters in racket, it's not a good text for programming language theory, IMO.

TAPL and PFPL are great.       On what level do you write the essay and how much experience do you have? I once wrote [a series of comments detailing different types of programming](http://www.reddit.com/r/learnprogramming/comments/utih5/types_of_programming/c4yfztm) that might be of help for getting into the subject.   </snippet></document><document><title>Uniqueness and Reference Immutability for Safe Parallelism [pdf from research.microsoft.com]</title><url>http://research.microsoft.com/pubs/170528/msr-tr-2012-79.pdf</url><snippet /></document><document><title>Feature Selection for Ranking</title><url>http://research.microsoft.com/en-us/people/tyliu/fsr.pdf</url><snippet /></document><document><title>Computational Neuroscientists from University of Waterloo AMA</title><url>http://www.reddit.com/r/IAmA/comments/147gqm/we_are_the_computational_neuroscientists_behind/</url><snippet>  Hey guys, the original post is a link to the AMA    </snippet></document></searchresult>
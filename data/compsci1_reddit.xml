<searchresult><compsci /><document><title>I'm a SoftEng with CS background intent on drawing manga of a new genre: CSPunk. I appreciate fellow redditors' ideas.</title><url>http://www.reddit.com/r/compsci/comments/17rz2p/im_a_softeng_with_cs_background_intent_on_drawing/</url><snippet>I'm a software engineer and I work on compiler optimizations. On the side, I draw manga. I'm very interested in Cyberpunk (my favorite being GitS) and I'm inspired to further specialize the genre into one we all CS fellows would appreciate: CSPunk/CompsciPunk. The motivation is two-fold: I want to create something new, and I want to get people interested in Computer Science.


Now, with CS being such a specialized field, some may doubt the feasibility of it gaining mainstream attention. With this post, I hope to draw some ideas from the crowd, and see if it's possible to craft a believable fictional world with CS as its theme.


I have some [concept art](http://imgur.com/a/O6puQ). I imagine a near future where people can plug their brain into the Internet, directly manipulating virtual objects using their mind. The heroine is a data parsing/manipulation specialist, and I intend to dramatize and visualize exploits/information manipulation using actual CS knowledge. I hope to write a story that is grounded on true CS knowledge (nothing like the hacker movie), but is also comprehensible to a layman.


Examples of ideas I'm looking for: heroine infiltrates a process running in a server's userland, identifies the structure of the floating data (she gathers it's an acyclic graph of some sort), constructs a parser on the fly, and extracts information from it. Unbeknownst to her, the graph is really cyclic, and so the intelligence she obtains from the graph turns out to be wrong, and as a result she loses a comrade (who has made use of the intel).


She may also be a counter-intelligence agent, saving the world from a nuclear missile by means of a series of clever bitwise operations on positional data feeding to a ballistics computation process.
I'm unfortunately not nearly smart enough to come up with dramatized exploits like this. If this post sparks some ideas in you, feel free to share!
Also, I'm aware that this post is rather different from a regular /r/compsci post, but I also do not find this post violating any rules. If it does then please point me to the right direction.
Thank you! All inputs appreciated! If something comes out of this (hopefully a webcomic), I'll make sure to credit your name.


TL;DR have ideas for a dramatized CS exploit suitable for a manga medium? Share!

**edit** thank you all so much for the advice! I'm in the absorbing (the resources you all suggested) and planning (the pilot) stage. I'll keep people updated!  I liked the puzzle-adventure ideas in Diamond Age (Neal Stephenson).  If you haven't read it, there were a series of automated castles that were controlled by different uh... compsci metaphors?  Like there was a message-passing land where people swapped books based on what was written in the books, and there was a Turing-machine land controlled by chains...

Side note: Have you read [Maoyuu Maou Yuusha](http://mangafox.me/manga/maoyuu_maou_yuusha_kono_watashi_no_mono_tonare_yuusha_yo_kotowaru/)?  It's the only "educational" manga I've ever read but I thought (at least this version) was pretty good.

Aaalso, the "Surprise twist: the graph was actually CYCLIC" made me laugh hard, thanks! Oh wow thanks! I've never heard of those. Maoyuu Maou Yuusha seems pretty cool. I'll also make sure to check out Diamond Age. Message-passing and turing land sound like some of the things I'm looking for (a bit metaphorical, but I'm starting to get some ideas :) ), but I appreciate something more literal as well. On the subject of Neal Stephenson, you could use Snow Crash as a 'CSPunk' bible. You might also check out William Gibson, specifically Neuromancer. Thank you. These are all really valuable suggestions :) I'll look them up on amazon. I think I'll start with Isaac Asimov, and then move on to William Gibson and Neal Stephenson. There's a LOT of asimov.. I'd suggest reversing that order, lol I just got Robot Visions! It comes with commentary from Asimov himself. I'll check out Gibson and Stephenson when I'm done reading that :D I just got Robot Visions! It comes with commentary from Asimov himself. I'll check out Gibson and Stephenson when I'm done reading that :D Thank you. These are all really valuable suggestions :) I'll look them up on amazon. I think I'll start with Isaac Asimov, and then move on to William Gibson and Neal Stephenson. Thank you. These are all really valuable suggestions :) I'll look them up on amazon. I think I'll start with Isaac Asimov, and then move on to William Gibson and Neal Stephenson. On the subject of Neal Stephenson, you could use Snow Crash as a 'CSPunk' bible. You might also check out William Gibson, specifically Neuromancer. I liked the puzzle-adventure ideas in Diamond Age (Neal Stephenson).  If you haven't read it, there were a series of automated castles that were controlled by different uh... compsci metaphors?  Like there was a message-passing land where people swapped books based on what was written in the books, and there was a Turing-machine land controlled by chains...

Side note: Have you read [Maoyuu Maou Yuusha](http://mangafox.me/manga/maoyuu_maou_yuusha_kono_watashi_no_mono_tonare_yuusha_yo_kotowaru/)?  It's the only "educational" manga I've ever read but I thought (at least this version) was pretty good.

Aaalso, the "Surprise twist: the graph was actually CYCLIC" made me laugh hard, thanks! &amp;gt; It's the only "educational" manga I've ever read

There are a bunch of HS/college level textbooks in comic form in the "...for Beginners" series. [An Example.](http://amzn.com/1934389021) I think this idea can be done better, though. The comic art in those I've read are okay, but not excellent. It would be a challenge to find artists who are truly passionate about each and every subject, though. Much easier to find someone who has requisite skills and just wants to be paid.  That is true, I could have paid someone else to do this, but I can draw to some extent, so it's kind of pointless. I admit I'm not very good at story telling, so I see this as a challenge for myself :)

Thanks for the suggestion! I'll take a look at the for Beginners comic series.   Though I find the idea almost ridiculously awesome, it seems incredibly niche and very difficult to properly dramatize. Though you might be a hobbyist mangaka, are you any good with writing fiction? That is, are you good at crafting plot, dialogue, character development, lore exposition, and all those things which make a story good? More importantly, do you have ideas on how to apply that to computer science concepts? I personally have written and directed a short before, so I think I'm okay in the dialogue department. Writing fiction on a scale like this is something I've never attempted before...so right now I'm trying to gather ideas and hopefully make a plot out of it. As for character development, that's something I do pay attention to as well, however I definitely could use some advice :) Are you a writer? If you're planning to make some money out of this, I'd say invest in making it into a strong graphic novel and perhaps getting it published. Since hard-core sci-fi tends to draw very few people, I can't see a long lasting published series - unless it's a webcomic. Still, keep at it. For visual inspiration, get some books on geometry. Even if it's for laypeople.
Also hit up http://dw-wp.com/ and check out their 2 how-to books, especially the second one. I think a webcomic would be an awesome media for this idea if the OP doesn't plan on publishing the manga. Publishing is the goal, but I'll settle for webcomic for now :) If you're planning to make some money out of this, I'd say invest in making it into a strong graphic novel and perhaps getting it published. Since hard-core sci-fi tends to draw very few people, I can't see a long lasting published series - unless it's a webcomic. Still, keep at it. For visual inspiration, get some books on geometry. Even if it's for laypeople.
Also hit up http://dw-wp.com/ and check out their 2 how-to books, especially the second one. wow dw-wp is pretty great! I've made comics for a while but there's still lots to learn. No not thinking of making money yet. But yes I'm thinking of graphic novel as a medium as well. I'm not particularly good with words, so for what I lack in writing skills, I'll make up in drawing. I say practice on paneling. A lot of people mess up on panel composition and area of focus, making the reading experience cumbersome/awkward. Also if you do plan on publishing, you need to think about the page format. Will it be like a shonen jump page format? Traditional american american format? UK? This will be important during the printing process. I personally have written and directed a short before, so I think I'm okay in the dialogue department. Writing fiction on a scale like this is something I've never attempted before...so right now I'm trying to gather ideas and hopefully make a plot out of it. As for character development, that's something I do pay attention to as well, however I definitely could use some advice :) Are you a writer? Ha! I haven't even consumed enough media to consider myself an avid reader, movie watcher, or anime nut. In fact, I'm probably less familiar with the classics of any particular art form than the average Redditor. I don't even read manga, really, as I prefer anime for my Japanese-original entertainment.

That being said, I think I've seen, read, and played enough "fuck yeah let's make some [insert media here]" to be wary of indie endeavors that aren't well-founded. Not to say that it can't be done though! There are Kickstarter success stories, and Katawa Shoujo arose from a 4chan thread, not to mention myriad fanfictions and indie games which became absurdly famous.

As aforementioned, the hardest bit will be applying that to computer science, since you'll have to strike a precarious balance between some hardcore logic/math/abstraction and thriller/drama/action. Personally, I think it would be best to go all-out, with a really detailed setting, something with some simple, intuitive mechanics behind the comp-sci-fi parts.

I'm a bit personally biased, since I prefer quite heavy character development and a universe which has something at least resembling consistent mechanics. Lots of people, however, (I would disdainfully refer to them as the lowest common denominator) would much prefer some stereotypical romance, happy-forever-after endings, and lots of boobs (OK, there's nothing inherently wrong with that, but...).

What definitely bothers me most is absolutely irrational characters... If you've ever gone through TV Tropes, or read HPMoR, you'll understand to some extent. Although I think everybody, to some extent, is meant to hate idiot balls, I myself might be a little extreme in that regard, especially after reading a story specifically crafted to have a cast of abnormally intelligent (realistically so) characters.

Well, I haven't really given you any specific ideas or advice, I suppose, but that's my rant for the day.  I like the idea, but you're not going to draw many women into the field by drawing the female characters in such an oversexualized style.  Who's your target audience? People who are interested in cyberpunk, or computer science, or just psychology/sociology/political/philosophical meanderings in general. I'm heavily influenced by GitS, but I also hope to avoid being a complete copy of GitS, story structure and theme wise. I'd highly recommend reading [this](http://www.amazon.com/Comics-Sequential-Art-Principles-Instructional/dp/0393331261/ref=sr_1_1?ie=UTF8&amp;amp;qid=1359858433&amp;amp;sr=8-1&amp;amp;keywords=comics+and+sequential+art) before jumping in the deep end.  Especially if you want your comic to be successful. Thanks! Do you draw comics?  I like the view of programming (not CS in general) in Vernor Vinge's SF books. For example, there are situations where so much code has piled up in a system over the centuries that they have "archeologist" whose job is to understand the layering of the thing. Once people find an old data archive and inadvertently release a very very bad AI by executing some of its code; another time, one of the low-rank programmer in a ship actually knows some of the low-level support layer of the code that was written (... by him) hundreds of year before, and use that to spy on the other members of the ship.

I'm not saying you should reuse those precise idea, but I like the balance between talking about tech (in a way that domain experts won't find ridiculous) and not going into the technical detail. Code layering in the book is not an educational device (look, you should have used more design patterns!), it's used to describe a situation that even non-CS-people can relate to ("ok he's found this trick that others don't know about"). I like the view of programming (not CS in general) in Vernor Vinge's SF books. For example, there are situations where so much code has piled up in a system over the centuries that they have "archeologist" whose job is to understand the layering of the thing. Once people find an old data archive and inadvertently release a very very bad AI by executing some of its code; another time, one of the low-rank programmer in a ship actually knows some of the low-level support layer of the code that was written (... by him) hundreds of year before, and use that to spy on the other members of the ship.

I'm not saying you should reuse those precise idea, but I like the balance between talking about tech (in a way that domain experts won't find ridiculous) and not going into the technical detail. Code layering in the book is not an educational device (look, you should have used more design patterns!), it's used to describe a situation that even non-CS-people can relate to ("ok he's found this trick that others don't know about"). I like the code archaeologist one. It's not just layered, it's how the system evolves in a progressively complex manner by piling up layers and increasing levels of abstraction, even if done in an unplanned fashion (basically programmers being lazy and reusing code/subsystems). It really makes you wonder if our reality works the same way (not in *it's turtles all the way down* sense, but that the fabric of our reality is built atop another possibly wholly different substrate). I think I can also draw some parallels with the way the brain evolves (perhaps a multitude of neural circuits tuned for very specific purposes (e.g. look for bright yellow thing, or kill anything that's crawly)...perhaps a plotline involving an A.I with an exposition along the same line.

Sorry I digressed, everyone's ideas here lead me to think of many things :) but yeah the balance is a fine line to walk on. I like the view of programming (not CS in general) in Vernor Vinge's SF books. For example, there are situations where so much code has piled up in a system over the centuries that they have "archeologist" whose job is to understand the layering of the thing. Once people find an old data archive and inadvertently release a very very bad AI by executing some of its code; another time, one of the low-rank programmer in a ship actually knows some of the low-level support layer of the code that was written (... by him) hundreds of year before, and use that to spy on the other members of the ship.

I'm not saying you should reuse those precise idea, but I like the balance between talking about tech (in a way that domain experts won't find ridiculous) and not going into the technical detail. Code layering in the book is not an educational device (look, you should have used more design patterns!), it's used to describe a situation that even non-CS-people can relate to ("ok he's found this trick that others don't know about").  There needs to be someone with data structure based powers/puns. 
  It sounds like an interesting idea with a lot of juicy details built up behind it. I suggest you scale your expectations and start with a manga built on simpler concepts and themes. You'll burn yourself out quickly enough trying to go for your magnum opus from the get go. Thanks. That's a solid advice :) I have to "pace" my expectations, so to speak.  is it going to be heavy on mathematics? proof theory, complexity theory? information theory? that sort of thing? Look up blood music. An old sci-fi that's aged well. Also read up Gardner Dozois' Year's Best Science Fiction Anthology Series. Check out Turing's Apple in the 26th volume. You will find good inspirational material.
You might want to mention/discuss/use axioms, intractable, and undecidable problems. is it going to be heavy on mathematics? proof theory, complexity theory? information theory? that sort of thing? Look up blood music. An old sci-fi that's aged well. Also read up Gardner Dozois' Year's Best Science Fiction Anthology Series. Check out Turing's Apple in the 26th volume. You will find good inspirational material.
You might want to mention/discuss/use axioms, intractable, and undecidable problems.  Write about when this becomes productized: http://edition.cnn.com/2012/10/12/tech/human-brain-computer/index.html?hpt=hp_t3 Yep I have plans for that! In fact I have some ethical conundrums lined up that I think can be made into some interesting stories. Not that you're beholden to Internet strangers, but I'd be interested to hear how it turns out. Developer with CS background as well, and I started writing with that link as my base concept. Alas, turns out I was more interested in coding on my writing tool than I was the writing itself. Let me know if you're interested in a Git-based, Markdown friendly novelwriting tool. It's like writing on a typewriter, but for the otaku-neckbeard-hacker crowd :) I've never really written a novel, but doesn't hurt to have a good tool! Show me the link! I'm currently using notepad to flesh out the dialogues and it's a pain in the ass.  On cyberpunk manga(i take it you mean Japanese comic), i recommend you read from Tsutomu Nihei or specifically, "BLAME!"
If you like the style you can follow Dorohedoro, which author worked with Tsutomu Nihei and share similar style, a fantasy but no cyberpunk.
Imho, not so much amazing from these manga but they have their visual style or worldview. If you want idea i suggest you read book, or keep enriching your knowledge on related field.

My opinion is that don't start off from certain genre or theme which looks cool, or hope to make it cool with enough effort. Imho, this is the wrong way to make the manga or story meaningful. A meaningful story should be sketched from some idea which is already innovative or unique, or better, innovative and inexpressible outside manga. For example, "The Matrix" was such a great hit,it is not solely depends on action and CGI but it proposes some problems which make people skeptical about their surrounding, that is, something which is interesting enough already. GitS is great for not only the worldview but it actually proposes some problems we may have in the near future. This is what hooks readers' mind and eagerness on reading/sharing, it also makes the manga really meaningful.

Aside from any suggestion, how do you measure the feasibility to produce a manga which is worth the effort?  There are plenty good manga takes 5 or 10 years of hard work, that is, there are full-time mangaka making very good stuff, profitable from these, how do you convince yourself that the manga you draw has some value afterall?
Since I've read quite a lot manga, i concludes that it is inevitable that a good manga requires super-hard work. On cyberpunk, it is particularly the case. What readers are expecting(or probably what you like at the first place) is the *visual art* and/or worldview. That is, a great deal of laborious effort on drawing is kind of expected,imo. It is not something like a slice of life or drama can compare. I am not sure what you are focusing on but if cyberpunk is merely a setting, instead of the main storyline or such. My questions above shouldn't be much of a concern.  This actually reminds me of [Tom Clancy's Net Force Explorers](https://en.wikipedia.org/wiki/Tom_Clancy's_Net_Force_Explorers) that I read many years ago. Note: The wikipedia description is rather sparse but the story is set in a world not unlike what you're suggesting and the protagonists deal with cyber criminals in a similar way as well (probably not as technical as what you have in mind though).  What a coincidence. I am working on a potential novel of a similar nature trying to use computer science concepts and theories in a metaphorical and allegorical manner, and thinking of calling my genre "virtualpunk!"

I really like the concepts you have come up with so far, and I too will definitely look into the novels other people here have suggested. Good luck, and when you get this published and go through with it, I look forward to reading it! Hey high five! I look forward to reading yours as well! I'm going for a more literal route, simply because as far as I know, no one has done it to the extent I'm looking for.

Also, virtualpunk sounds awesome :D  Just posting because it's kind of (but only kind of) relevant: http://www.amazon.com/Manga-Guide-Databases-Mana-Takahashi/dp/1593271905 Just posting because it's kind of (but only kind of) relevant: http://www.amazon.com/Manga-Guide-Databases-Mana-Takahashi/dp/1593271905  I'll admit: when I saw your art, I was like "Holy crap! He can draw!" and your drawing style is very good. 

I would also consider that a lot of programs written by programmers aren't that complicated, but there's those that are Thank you! I'm still learning. I think if a program is complicated, it's because the programmer has failed to do proper abstraction. But of course some programs are group effort (making them highly complex), so to understand one in its entirety necessarily can't be a solo effort either. This is giving me ideas (on creating plot involving teamwork). Thanks! Thank you! I'm still learning. I think if a program is complicated, it's because the programmer has failed to do proper abstraction. But of course some programs are group effort (making them highly complex), so to understand one in its entirety necessarily can't be a solo effort either. This is giving me ideas (on creating plot involving teamwork). Thanks! Another thing I thought of: there are varying levels of cooperation between separate teams. Sometimes the frontend/backend/testing teams work like a well-oiled machine, but other times, there's politics involved. 

"Sure! I would love to help out!" vs "Why are you bothering us?" and even "Are you guys sure you know what you are doing?" Thank you! I'm still learning. I think if a program is complicated, it's because the programmer has failed to do proper abstraction. But of course some programs are group effort (making them highly complex), so to understand one in its entirety necessarily can't be a solo effort either. This is giving me ideas (on creating plot involving teamwork). Thanks!   It's really outdated at this point, and obviously for kids, but Reboot was a pretty awesome show that had some fun CS puns in it. Not even close to what you're thinking but I went back and rewatched some episodes now that I know CS and it's awesome lol   I would certainly read whatever you produced, but I have one request.  At some point, you HAVE TO mention a specific papers title.  The concepts within it will mostly likely be fruitful for your story, so I doubt it will be too much trouble, but the title is just too excellent to be as obscure as it is.  This is the paper:

[The Geometry of Innocent Flesh on the Bone](http://cseweb.ucsd.edu/~hovav/papers/s07.html) </snippet></document><document><title>There is not enough energy in the universe to bogo sort 66 elements?.. double check me.</title><url>http://www.reddit.com/r/compsci/comments/17rcet/there_is_not_enough_energy_in_the_universe_to/</url><snippet>Estimated total mass energy in the observable universe: 4 x 10^69 Joules [source](http://en.wikipedia.org/wiki/Orders_of_magnitude_%28energy%29)  
minumum amount of energy needed to flip a bit  according to [Landauer's principle](http://en.wikipedia.org/wiki/Landauer%27s_principle) , assuming that our machine is submerged in a pool of liquid Helium (4 Kelvin) that never boils.    
Landauer limit:  kT  &#183; ln 2  
    Where&#8230;  
    k is the Boltzmann constant, ~1.38&#215;10&#8722;23 J/K  
    T is the absolute temperature of the circuit in kelvins  
so..  
energy required to flip a bit at 4K:  
(1.38 x 10^-23)  x 4 x ln2 = ~ 3.82617 x 10^-23  Joules  
Total bit flips with all energy in universe:  
4 x 10^69 J / 3.82617 x 10^-23 J = 1.045 x 10^92  
66! = 4.5 x 10^92  ,so even if each operation only required 1 bit flip, still not possible.  
  
EDIT: This assumes the average case of bogo sort.  In theory, the elements may already be sorted, or may become sorted after the first shuffle, or may never reached a sorted state.   You're misunderstanding Landauer's principle. It doesn't necessarily cost any energy to flip can bit, only to erase one. That is, performing a not gate can be done at arbitrarily low cost, but initializing your memory into the starting state has a cost per bit proportional to the temperature.  I'm in agreement with OP on this one. Bogosort is awful *precisely because* at every step you erase all information you learned from previous steps. Randomly permuting all the elements is equivalent to erasing log_2(N!) bits of information. Well, I suppose it depends on your method of generating random numbers, but really Landauer's principle is about the energy costs of removing entropy from a system. Bogosort can be seen as *adding* entropy at every step, and thus shouldn't have any cost at all.   You're misunderstanding Landauer's principle. It doesn't necessarily cost any energy to flip can bit, only to erase one. That is, performing a not gate can be done at arbitrarily low cost, but initializing your memory into the starting state has a cost per bit proportional to the temperature.  okay, how about this:  
for(int i=0;i&amp;lt;10;i++){  
       if(i &amp;gt; 3){ // do something }  
}  
doesnt i's binary representation change like this:  
0...0001  (1 flip)  
0...0010  (2 flips)    
0...0011    (1 flip)  
0...0100  (3 flips)  
0...0101    (1 flip)  
that is, is not i erased at each iteration?  I see how the i &amp;gt; 3  check does not erase anything   okay, how about this:  
for(int i=0;i&amp;lt;10;i++){  
       if(i &amp;gt; 3){ // do something }  
}  
doesnt i's binary representation change like this:  
0...0001  (1 flip)  
0...0010  (2 flips)    
0...0011    (1 flip)  
0...0100  (3 flips)  
0...0101    (1 flip)  
that is, is not i erased at each iteration?  I see how the i &amp;gt; 3  check does not erase anything   You're misunderstanding Landauer's principle. It doesn't necessarily cost any energy to flip can bit, only to erase one. That is, performing a not gate can be done at arbitrarily low cost, but initializing your memory into the starting state has a cost per bit proportional to the temperature.  I recently read Feynmans Lectures on Computation, and in it he claims that computation can be done (if I understand him correctly) for arbitrarily close to zero energy cost if it is done symmetrically (though it may be slow and you need to spend energy to do it faster).  How does this jive with Landauer's principle? Current computers use irreversible gates like the AND and OR gates, which take 2 bits and produce 1 bit, consequently 'erasing' the remaining bit. This runs into Landauer's limit. If you perform your computation [reversibly](http://en.wikipedia.org/wiki/Reversible_computing), i.e don't erase any information, you can avoid all energy costs during computation. However, you do have to pay costs at the beginning to throw out whatever junk is sitting in your memory from the last computation, and that's unavoidable, although the cost bounded by the size of the memory you wish to use. Ahh, ok, that is how I understood it, I just had not heard the term 'Landauer's limit' before.  Feynmans lectures on computation has really been the only thing I've read that deals with concepts like typing computation to actual physics.  By any chance do you know of any other good resources for learning about such things for someone who is far more informed on the computation side of the house rather than the physics side?  I don't think you understand bogosort, or you're talking about the expected number of shuffles needed. Given the random nature, any list can be sorted with one single sort. It is also possible, but extremely unlikely, for a list with 2 elements to never get sorted. I don't think you understand bogosort, or you're talking about the expected number of shuffles needed. Given the random nature, any list can be sorted with one single sort. It is also possible, but extremely unlikely, for a list with 2 elements to never get sorted. *Never* get sorted? I think you overstate your case. not really

&amp;gt; It is also ***possible, but extremely unlikely***, for a list with 2 elements to never get sorted.

and it is possible, nothing in the laws of physics prevents it from happening, it is just such a tiny tiny tiny possibility.

Note that it assumes that the time left to sort in, is limited, as fairly well established by thermodynamics. If we indeed had infinite time to sort in, it would converge to a limit of 0, which is to say it is impossible.  Wait, am I missing something obvious?  Isn't 1.045 x 10^92 &amp;gt; 4.5^92?  I mean, forgetting about that 1.045, it seems clear that 10^92 &amp;gt; 4.5^92, so the number of bit flips in the universe should be bigger than the number required to bogo sort 66 items.   In fact, if Wolfram Alpha is to be believed, then you should be able to bogosort 66 items (1.045 x 10^92) / (4.5^92) = 8.3 x 10^31 times, if they only took one bit flip, or sort them once if it took 8.3 x 10^31 bit flips.   This really depends on your interpretation of bogosort.  IIRC to qualify as an algorithm it has to guarantee termination.

Repeated shuffling of the entire input is not guarantee to terminate.  To make an actual algorithm you would do the following:


    1. Make every possible permutation of the input array O(NN!)
    2. Find the permutation that is in the correct order O(N^2N!)

Guaranteed to terminate, but will take a large amount of time and memory.  Of course this just means you need a larger N.  

[edit: I realized this assumes that you want a stable sort, which is not really relevant to pure numbers.  But if we assume on average there are 0 repeats anyway it's not a problem and this holds out.] I would say that bogosort is guaranteed to terminate in finite time if you shuffle the input 'correctly' (i.e. repeatedly sample from a [usually uniform] distribution over all possible permutations, where each permutation has a non-zero probability).

The probability of terminating after n samples certainly goes to 1 if n is allowed to become arbitrarily large. There will always be a finite n after which you reach the termination criterium, even if that n is *huge* and depends on the particular run (even for similar input). That's what I said: if you generate all permutations and then enumerate them searching for the first sorted one you have guaranteed termination, and thus an algorithm.

If you interpret boho sort as being

    1. temp = input
    2. while (!InOrder(temp)) temp = shuffle(temp)
    3. return temp

the shuffle is random, so strictly speaking it might _never_ produce a sorted list, and so this would not be an algorithm.
 &amp;gt; That's what I said: if you generate all permutations and then enumerate them searching for the first sorted one you have guaranteed termination, and thus an algorithm.

I said: if you (randomly) *sample* from a ('good') distribution of permutations, then you are guaranteed to terminate. You don't need to generate them in order and check them one by one.

All you need is that your distribution of permutations is 'good', in the sense that every possible permutation will have a non-zero chance of getting generated. (Strictly speaking, it is sufficient that only the correctly sorted list has a non-zero probability of getting generated.)

Any textbook shuffling algorithm will satisfy this criterium (as pretty much all will have a uniform distribution over possible permutations, because that's usually what you want to have anyway)

&amp;gt; the shuffle is random, *so strictly speaking it might never produce a sorted list*, and so this would not be an algorithm.

It is not because something is highly unlikely that it will never happen. If you repeatedly flip a coin, I *guarantee* you that you will generate a series of 100 heads in a row after a *finite* number of attempts. Sure, it might take you many times the age of the universe to actuall see it happen, but it will *eventually* happen (becase the probability of it occuring is 1/2^100, which is *tiny*, but strictly larger than zero). &amp;gt; It is not because something is highly unlikely that it will never happen. If you repeatedly flip a coin, I guarantee you that you will generate a series of 100 heads in a row after a finite number of attempts. Sure, it might take you many times the age of the universe to actuall see it happen, but it will eventually happen (becase the probability of it occuring is 1/2100, which is tiny, but strictly larger than zero).

This is the gambler's fallacy, it's presuming that the shuffling operations are dependent - an equivalent piece of logic that occurs in many people's day-to-day lives is picking lottery ticket numbers: "I always pick the same numbers because they have to come up eventually".

The reality is that because the probability of your ordered set being generated is independent (if the shuffling operations are not independent, and are guaranteed to eventually produce all possible orders, you are simply taking a round about way of getting to the "generate all the permutations" algorithm).

But that said, you argue that eventually you will generate a series of 100 heads in a row after a _finite_ number of attempts.  This is simply not correct.

Every throw is independent, so the probability of getting 100 heads in a row is 1/2^100 this is tiny.  It is a probability that is greater than 0.  That is still not a guarantee that it will definitely happen, to be a guarantee that it will happen doesn't mean that you have a non-zero probability of it occurring.  What you need is the probability of it occurring to be _1_.

So how do you get a probability of 1?  Well that's just a matter of throwing enough coins, if you throw 100 coins the probability is 1/2^100.
If you throw 1000 coins your odds are better, but still not 1.  The only way to get to p(100 heads in a row) = 1.0 is by throwing an infinite number of coins.  So no, you are not guaranteed that it will eventually happen, only that it probably will. This really depends on your interpretation of bogosort.  IIRC to qualify as an algorithm it has to guarantee termination.

Repeated shuffling of the entire input is not guarantee to terminate.  To make an actual algorithm you would do the following:


    1. Make every possible permutation of the input array O(NN!)
    2. Find the permutation that is in the correct order O(N^2N!)

Guaranteed to terminate, but will take a large amount of time and memory.  Of course this just means you need a larger N.  

[edit: I realized this assumes that you want a stable sort, which is not really relevant to pure numbers.  But if we assume on average there are 0 repeats anyway it's not a problem and this holds out.] Algorithms do not guarantee termination.  See the halting problem.  The total energy in the universe is zero.

Edit: You could work a similar problem and calculate the increase in entropy during the bogo sort, then show that the universe would not be able to finish the sort due to its high-entropy state. The total energy in the universe is zero.

Edit: You could work a similar problem and calculate the increase in entropy during the bogo sort, then show that the universe would not be able to finish the sort due to its high-entropy state. What do you mean when you say the total energy in the universe is zero? I know all energy in a closed system stays constant, but I've never heard it should be a constant zero. What do you mean when you say the total energy in the universe is zero? I know all energy in a closed system stays constant, but I've never heard it should be a constant zero. It seems that we have zero total energy in the entire universe...just interestingly arranged. Where is the negative energy located?  The OP has mislabeled the sort he wants to use.

He claims "bogo sort" but is actually yalking about "Bogobogo sort" :

An algorithm that was designed **not to succeed before the *heat death of the universe*.** on any sizable list. It works by implementing the Bogo sort on the first two elements in the list. If they are in order, then it Bogo sorts the first three elements, and so on, increasing by one until the entire list is sorted. Should the list not be in order at any point, the sort starts over with the first two elements.

http://en.wikipedia.org/wiki/Bogosort#Related_algorithms The OP has mislabeled the sort he wants to use.

He claims "bogo sort" but is actually yalking about "Bogobogo sort" :

An algorithm that was designed **not to succeed before the *heat death of the universe*.** on any sizable list. It works by implementing the Bogo sort on the first two elements in the list. If they are in order, then it Bogo sorts the first three elements, and so on, increasing by one until the entire list is sorted. Should the list not be in order at any point, the sort starts over with the first two elements.

http://en.wikipedia.org/wiki/Bogosort#Related_algorithms   I've bogosorted 100 elements before.

It took a long ass-time, but I've done it.  You've misunderstood Landauer's principle.

(Yeah, given the random nature of bogosort, it's possible for it to sort very quickly.  It's also possible that it will never sort.) &amp;gt;  I've bogosorted 100 elements before.

A 3 GHz machine can perform ~ 1.57 x 10^15 clock cycles per year.  For simplicity, lets say that each shuffle and check operation takes 1 clock cycle.  

100! = 9.3 x 10^157, so your chances of sorting within a year would be something like 1 in 1.68 x 10^143  
The chances of winning the Mega million lottery is about 1 in 200 million.  If you can repeat your luck, you could buy a [single ticket every week for 4 months](http://www.wolframalpha.com/input/?i=%281%2F200000000%29%5E16) and win each time

 I've bogosorted 100 elements before.

It took a long ass-time, but I've done it.  You've misunderstood Landauer's principle.

(Yeah, given the random nature of bogosort, it's possible for it to sort very quickly.  It's also possible that it will never sort.)</snippet></document><document><title>Does the Halting Problem apply to Turing Machines with finite tape?</title><url>http://www.reddit.com/r/compsci/comments/17ruf7/does_the_halting_problem_apply_to_turing_machines/</url><snippet>By definition a Turing Machine has infinite tape, but in practice no existing computer has infinite memory.

Would it be possible to define the maximum amount of time a program of length n and input m would run?  A Turing Machine with a tape of length N, an alphabet of size S and X states can be simulated by a DFA with S^N \* X states.

*edit* moor-GAYZ is quite right If the tape size is some fixed k, you are correct. But if you allow the tape size to be finite but determined as a linear function of the input size, then you get a [Linear Bounded Automaton](http://en.wikipedia.org/wiki/Linear_bounded_automaton), which can accept the [context-sensitive languages](http://en.wikipedia.org/wiki/Context-sensitive_language). And for which halting is still decidable. A Turing Machine with a tape of length N, an alphabet of size S and X states can be simulated by a DFA with S^N \* X states.

*edit* moor-GAYZ is quite right S^(N) \* X Oops, good catch - GP edited

(hysterical note: I originally wrote S\*N\*X) Hysterical or historical?  A *configuration* is a combination of the current state (selected from a finite set of possible states) and the current tape contents and tape head position. Each configuration leads to at most one successor configuration.

If the tape is finite, then there's a finite number of possible configurations. Say this number is N. Then the machine can run for at most N many steps before it either halts or returns to a previous configuration.

If a machine ever returns to a configuration it has been in before, it will follow that cycle infinitely many more times. So to decide the halting problem for these finite machines, all you need to do is run it for N+1 many steps. If it hasn't halted by then, it never will.

Trying to detect duplicate configurations for normal Turing machines doesn't solve the halting problem in general. This is because the trickiest cases of non-halting behavior aren't loops at all, but continue to grow the tape in ways that are not predictable by any single rule or system. &amp;gt; This is because the trickiest cases of non-halting behavior aren't loops at all, but continue to grow the tape in ways that are not predictable by any single rule or system.

Right, but if we limit the tape (just like in practice, computer memory is limited), then can we solve the halting problem?  I guess we'd end up with a new state which isn't halting or non-halting but running out of tape. I'm not sure that this argument translates to real-world computers as one would expect. Turing machines have the special property that tape state A always yields tape state B. But that property doesn't hold for my laptop's memory: a `time()` call depends on the system clock, which is *not* part of my laptop's memory, so state A could yield state B at time T1, but yield state C at time T2.

So, if a program takes any decision that's not based exclusively on memory state, the finite-tape Turing analogy doesn't hold. The passage of time makes your laptop's state change. It's still part of your computer's state, it's just a ridiculously huge and complicated state. You can even think of a networked computer the same way&#8212;all the remote resources it accesses are still part of the state of the computation. It's still all Turing computation. I didn't know the model included outside inputs after the computation had already started. Well, in terms of Turing machines there is no concept of "outside inputs." If you want to model a program that uses time as a changing input as a Turing machine, the passage of time would have to itself be encoded on the tape somehow. Well, in terms of Turing machines there is no concept of "outside inputs." If you want to model a program that uses time as a changing input as a Turing machine, the passage of time would have to itself be encoded on the tape somehow. I'm not sure that this argument translates to real-world computers as one would expect. Turing machines have the special property that tape state A always yields tape state B. But that property doesn't hold for my laptop's memory: a `time()` call depends on the system clock, which is *not* part of my laptop's memory, so state A could yield state B at time T1, but yield state C at time T2.

So, if a program takes any decision that's not based exclusively on memory state, the finite-tape Turing analogy doesn't hold. &amp;gt; This is because the trickiest cases of non-halting behavior aren't loops at all, but continue to grow the tape in ways that are not predictable by any single rule or system.

Right, but if we limit the tape (just like in practice, computer memory is limited), then can we solve the halting problem?  I guess we'd end up with a new state which isn't halting or non-halting but running out of tape. &amp;gt; This is because the trickiest cases of non-halting behavior aren't loops at all, but continue to grow the tape in ways that are not predictable by any single rule or system.

Right, but if we limit the tape (just like in practice, computer memory is limited), then can we solve the halting problem?  I guess we'd end up with a new state which isn't halting or non-halting but running out of tape.  No.  

There are a maximum number of states, let's call it N.  
Before you have switched state N+1 times, one of two things will happen:  
1. The machine will halt.  
2. The state at the current iteration *t* will be equal to the state at iteration *t-d*, where *d* is some finite number no larger than *t*. If this happens then the machine will loop with a period of *d* and never halt.

So you can solve the halting problem by simply running the machine for a finite length of time.  This finite time could still be physically untenable for a tape of practical length. (Translation, you'd have to wait a freaking long time.)  Well, you *can* run a program with infinite memory, if you're ready to have your programs run quite slowly. You just need to freeze computation when it risks exceeding the memory capacity, and manually hotplug more memory before continuing the program.

You may run into trouble when the addressing space of your 64 bits architecture will be exceeded; porting your OS to a larger pointer size, and live-rewriting the program to encode these larger memory operations in terms of what the hardware support, will make for a longer interruption than usual. It may even be beneficial to directly move to an unbounded-pointer addressing technique, just like "bignums" currently make numbers unbounded.</snippet></document><document><title>If you like Hamming Code you'll love Turbo Code</title><url>http://en.wikipedia.org/wiki/Turbo_code</url><snippet>  </snippet></document><document><title>Can bytecode interpreters make use of SIMD instructions?</title><url>http://www.reddit.com/r/compsci/comments/17q4je/can_bytecode_interpreters_make_use_of_simd/</url><snippet>The job a bytecode interpreter (decode, dispatch, repeat) seems fundamentally at odds with the uniformity required to get speedups from vector operations. Nonetheless, does anyone here know of any clever language implementations which make use of packed register instructions? Or even more radically, has anyone managed to shoehorn a dynamic language runtime onto a GPU? I don't mean compiling specialized fragments of a program to a GPU, but actually running an interpreter on top of wide-vector hardware. 

edit: Sorry, I don't think my question was clear. I'm not asking about just-in-time translators/compilers which target SIMD instruction sets (like the JVM or DAISY). I'm also not asking whether there are any interpreted languages which use precompiled vectorized library functions (such as the array operators of APL/J/K/etc...). 

A dispatch loop for a dynamic language interpreter might look something like this:

    for each instruction in the program:
        switch(instruction.opcode) { 
            case ADD_INT:
                x = stack.pop()
                y = stack.pop()
                stack.push(x+y)
            case MULT_INT:
                ...
        }

Of course, the bytecode format might be dynamically typed (and thus use a generic ADD w/ type tags on the value), or the intepreter might be [directly threaded](http://blog.mozilla.org/dmandelin/2008/06/03/squirrelfish/) instead of using a switch, or it might [use registers](http://markfaction.wordpress.com/2012/07/15/stack-based-vs-register-based-virtual-machine-architecture-and-the-dalvik-vm/) instead of a stack, or a million other design variants. But the basic pattern remains the same. It seems extremely unlikely that this sort of code is amenable to parallelization. However, few things are impossible--- so I'm asking if someone has found a clever way to use SIMD instructions to speed up a program that behaves like the one I sketched out above.

     The interpreter side of it isn't my area, so let me know if this is totally preposterous.  

If you looked at your interpreter as some higher-level version of a superscalar processor architecture, I think you could use SIMD instructions as a time-efficient implementation of the execution part of that architecture.  It would have the same issues with underused throughput if the instruction stream doesn't have exactly the structure of parallelism that the hardware design is expecting.  The Pentium 1 was like this: up to 2 instructions per clock, in order, so you only take advantage of that 2-wide execution if instructions happen to come in just the way the processor can use them.

If, let's say, you had multiple interpreter threads looking ahead in the instruction stream for non-overlapping instructions, your back end could look for batches of independent in-flight instructions that could fill up SIMD operations.  This would make your interpreter an emulated implementation of an out-of-order superscalar architecture.

Whether that would be any faster than a more naive way of doing it is a wide open question.

Would that do what you're asking? As an alternative to having multiple threads, I guess you could also fetch k instructions at once and then  have some very large (probably auto-generated) chunk of logic for when subsets of those are combinable. However, in either case, it's really unclear if it would be a performance win due to the packing/unpacking cost of going in and out of SIMD registers. Also, don't superscalar architectures allow unrelated operations to issue at once? If I have two ALUs then I can run an ADD and MULT simultaneously. With SIMD instructions, however, I would be looking to combine multiple instances of the same opcode. 

Nonetheless, it's definitely the sort of thing I'm interested in. Thanks!   The standard JVM will generate SSE3 instructions (though not particularly well). The next release of LLVM (which is also a JIT) can generate SSE3 from instructions as well by default. 
 I'm guessing you mean that the JVM's runtime compiler generates SSE instructions for compiled traces-- but I'm curious more about the use of SSE instructions in the implementation of the underlying interpreter.        On Android, things such as video decoding is written in a language such as C or C++ or ARM assembly with SIMD instructions (NEON specifically) and compiled into native code.  Both first-party and third-party apps take advantage of this.</snippet></document><document><title>Computer Science PhD trends: Taulbee Survey for 2011-12</title><url>http://blog.vivekhaldar.com/post/42007984829/computer-science-phd-trends</url><snippet>  I'm a first-year PhD student who's hoping to one day get a job as a teaching-track faculty.

According to this document, my odds are 1 in 50.

Awesome. Reread the data.

7% started with tenure-track faculty positions.  Meaning their first job was such.

How many were seeking a tenure-track faculty position as their first gig?  I know that's my path.  But what about everyone else?   


This guy is putting a very pessimistic, debbie-downer spin on this.

 Exactly. According to his source, 16.8% of new PhDs got a postdoc position, and with other positions listed in academia (researcher and teaching faculty), the total new PhDs entering into academic careers comes to about 31%. I'd say that's not particularly discouraging.  The actual report has a lot of interesting statistics. I'll try to do a more full summary with more stats later- for now, I'm on my phone.

* only 11.7% of undergraduate CS degrees and 11.8% of undergrad CE degrees are awarded to women. But twice as high a proportion, 22.3%, of Ph.D. students and graduates are women. In CE, the drop out rate for women from Ph.D. programs is significantly lower than for men, for CS the proportion is identical, and in I, there is a slightly higher drop out rate for women. So don't listen to anyone who claims the gender gap is due to a difference in competence or women wanting to have families instead.

* The 11.7/11.8% is really a problem. It seems that, once in the field, women do just as well or better than men, and they are arguably more likely to stick with it and finish grad school. So why are there so few undergrads? There has been a great deal written about how the culture of many CS programs, and the attitudes of many of the students, drive women away. At the very least it is worth investigating and making an effort to correct this. You can't just chalk it up to "women just inherently don't like science/math/logic/gizmos/whatever" - the gap in CS/CE is substantially worse than in other STEM fields.

* the proportion of tenure track positions, both full and associate, held by women is is less than in non tenure track positions, but the numbers for new hires are more balanced. This could mean hiring practices are getting better, or it could mean that women are not getting promoted or selected for advancement once they get to tenure track positions. The actual report has a lot of interesting statistics. I'll try to do a more full summary with more stats later- for now, I'm on my phone.

* only 11.7% of undergraduate CS degrees and 11.8% of undergrad CE degrees are awarded to women. But twice as high a proportion, 22.3%, of Ph.D. students and graduates are women. In CE, the drop out rate for women from Ph.D. programs is significantly lower than for men, for CS the proportion is identical, and in I, there is a slightly higher drop out rate for women. So don't listen to anyone who claims the gender gap is due to a difference in competence or women wanting to have families instead.

* The 11.7/11.8% is really a problem. It seems that, once in the field, women do just as well or better than men, and they are arguably more likely to stick with it and finish grad school. So why are there so few undergrads? There has been a great deal written about how the culture of many CS programs, and the attitudes of many of the students, drive women away. At the very least it is worth investigating and making an effort to correct this. You can't just chalk it up to "women just inherently don't like science/math/logic/gizmos/whatever" - the gap in CS/CE is substantially worse than in other STEM fields.

* the proportion of tenure track positions, both full and associate, held by women is is less than in non tenure track positions, but the numbers for new hires are more balanced. This could mean hiring practices are getting better, or it could mean that women are not getting promoted or selected for advancement once they get to tenure track positions. Thanks for digging up those statistics.

I'm just a newbie to CS, so I don't have much to say. But from the little I've seen in my school, it seems the issue has become self-perpetuating: Women aren't staying in the major because there aren't any women in the major. My intro course had a dozen girls. The next semester there were five of us. Two years ahead of me there are only two. (But weirdly enough, the grad student population is half women, almost all of them Chinese.) The 'CS is for men' is an American thing; in many other cultures it is gender neutral, which is why when you have a big foreign population, like in grad school, it is more balanced. &amp;gt; The 'CS is for men' is an American thing

It's the same thing in Germany. Out of 200 CS students in my year, less than 5 are female. The 'CS is for men' is an American thing; in many other cultures it is gender neutral, which is why when you have a big foreign population, like in grad school, it is more balanced.  I find it a bit annoying that you have to scroll down the bottom of the post to make sure that, yes, it is only about research in the US&#185;. Hello, this is not the only country doing CS research !

The last point on aliens is very good, though. I've heard bright indian interns talk about how they would rather try to find a company than a PhD programme, because they would get more help for their visas.

&#185;: actually the CRA statistics range over Canada as well, but it has a rather small number of CS depts compared to the US. &amp;gt; I find it a bit annoying that you have to scroll down the bottom of the post to make sure that, yes, it is only about research in the US&#185;. Hello, this is not the only country doing CS research !

Then do your own damn research, write a blog article about it, and then post it here! It is *fine* to have good-quality, detailed statistics about the US PhD trends. I would like people to be clear about the fact that they are *US PhD trends* (or Noth-America PhD trends), and not just "PhD trends".

And this apply equally to the title picked on reddit than on the blog post itself. This would allow, people interested in PhD trends but less so in North-America-only PhD trends to evaluate their interest in the link.  Sorry it doesn't meet your every expectation, but you're not the intended audience. You totally miss the point, and you insist. The phrasing of the title is wrong. I would not mind if it was a tabloid, but I do mind if it is an article written by a PhD and clearly adressed to other current or future PhDs. It's a blog post.  This isn't some peer reviewed article, it isn't something that has past under the purview of some editor, it's a personal statement.  And you are getting upset that he didn't think about YOU when he wrote it.  I am very surprised by the breakdown of the PhD students. There are 119 CS students in theory and algorithms. Surpassing almost all other topics. </snippet></document><document><title>Creating a Simple CFD Program.</title><url>http://www.reddit.com/r/compsci/comments/17oaxy/creating_a_simple_cfd_program/</url><snippet>I was hoping to get some input here on my project that a friend and I are undertaking. 

We've participated in some challenges over the years that deal heavily with aeronautics and optimizing the aerodynamics of aircraft and vehicles. In dealing with these, we often have to use large software packages to perform fluid dynamics on our designs. Our idea is to create a tool written in Python and/or Java in order to simply import an STL, mesh it, and export the lift/drag forces.

Here are the steps that it would have to undertake:

1. Opening GUI
1. Enter volume of fluid
1. Input density of fluid (i.e. air, water, etc.)
1. Import STL
1. Mesh model (given a mesh fine-ness/resolution)
1. Determine velocity of the air and other variables (if necessary - temperature, fir example)
1. SOLVE
1. Export colored image/animation of test (like [this](http://www.padtinc.com/images/cfd-fluent-flow-through-valve.jpg))
1. Export spreadsheet of values of drag, lift, etc.

Now, this is obviously no simple task. We'd like to use a simple programming language mostly to test its boundaries and see if it is possible to do so. As far as libraries go for Python/Java in order to mesh the object, I have looked into MeshPy and others but it seems like it may be advantageous to write out own. Are there any other libraries that may be worth looking into?

Let's also assume that we have a pretty firm grasp on the basics of CFD and what it means/tells us about the object.

**tl;dr - Making a CFD program in Python/Java, need advice on where to start**   1. Repeat this question in /r/CFD, /r/numerical and /r/aerospace.
2. Pick a method to implement. If you're using a loose enough definition for CFD, I'd consider a panel method. 
3. Offload the meshing to another tool and generate some simple meshes. 2-D you can do by hand, 3-d you might look at something like OpenVSP.
4. Write the 2-d solver.
5. Write the 3-d solver.
6. Test the solvers on the meshes you generated separately.
7. Now go back and write your mesh generator.
8. Repeat 2-8 for any other solvers you want to write.
9. Ignore people bitching about your language choices.
10. Keep us posted on your progress. :)

Python libraries that might be useful: the numerical stuff like SciPy. Some sort of GUI toolkit, I don't do so well with those, visualization tools like matplotlib, Chaco, Mayavi. I posted to /r/aerospace and /r/programming but /r/CFD unfortunately looks rather dead.. Thanks for the tips though!

I was thinking of starting by making a 2D airfoil solve (like XFOIL) but in Python and to not make the same mistakes JavaFoil did. From there, I'd like to extrapolate that into 3D CFD. I'm pretty well versed in OpenVSP (had to use it in this last challenge), and I'm curious as to how I could maybe call on OpenVSP to mesh an object, like:

Create STL -&amp;gt; Open in CFD program -&amp;gt; OpenVSP Mesh -&amp;gt; Back to CFD for testing 

</snippet></document><document><title>Cache oblivious algorithms</title><url>http://iainkfraser.blogspot.co.uk/2013/01/cache-money-hoes-attached-code.html</url><snippet> </snippet></document><document><title>Is it possible to prove that a chess implementation is "correct"?</title><url>http://www.reddit.com/r/compsci/comments/17nfws/is_it_possible_to_prove_that_a_chess/</url><snippet>I'm not talking an implementation of an engine, just a simple game of chess where you can enter moves for both sides. By correct I mean the program will reject all invalid moves and will always evaluate the final position correctly (Draw or Win for some side). I understand there are languages like Coq for doing stuff like that but would need some guidance to understand how this works better.   [Here's a great starting point on chess verification](http://corp.galois.com/blog/2011/5/26/formally-verified-chess-endgames.html)

You will be able to prove a given 'game' (aka a full list of moves in some unambiguous chess notation) is legal; but it depends what you mean by implementation.

Given an initial board state and then a move, evaluating whether that move is legal is doable; as each piece has a finite set of possible moves it can make. (Castling, promotion, and en-Passat are a pain, but again there is a finite formula for possible moves for a given piece, so if your 'game' tells you a piece did X, you can check the rules against the previous game state).

[Endgame conditions](http://en.wikipedia.org/wiki/Chess#End_of_the_game) are also a finite set, but I believe you need at least a 50 move memory to discount the 50 move draw condition.

So I suppose if you can prove that you correctly handle every piece's move possibilities and every given endgame condition then you should be able to prove your implementation correct in theory, as there's no 'ambiguity' in chess from a rules standpoint.



 &amp;gt; You will be able to prove a given 'game' (aka a full list of moves in some unambiguous chess notation) is legal; but it depends what you mean by implementation.

By implementation I mean the following: Given a starting position the player can input moves (for both sides) and the program will return the new position after the move on the board has been played or  reject the invalid move. The program can also take a position and evaluate it as a checkmate or a stalemate (We can ignore drawn endgames and the rest as unknown for example). 

&amp;gt; Given an initial board state and then a move, evaluating whether that move is legal is doable; as each piece has a finite set of possible moves it can make. (Castling, promotion, and en-Passat are a pain, but again there is a finite formula for possible moves for a given piece, so if your 'game' tells you a piece did X, you can check the rules against the previous game state).

But my point is how do I find out if my implementation of the possible moves is valid? I would like to prove that it is in fact correct. I was building an engine of my own, but stopped halfway through. I don't know if you want a formal proof, but I planned to run my engine through one of those muli-thousand recorded games database and then planned some more manual comparative testing with other chess engines to get those trickier parts correct. Note that just for checking correctness of moves (king in check complictes things) or producing PGN/FIDE compliant notation (the way ambiguity is resolved) you'll need full move generator, meaning there is no 'lightweight' approach to it.  Unfortunately that's not what I'm looking for. Your method can only disprove an implementation. Seems like what I'm looking for isn't easy at all unfortunately :/ What did you have in mind? I thought that if you wanted to have it verified in something like Coq you wouldn't be asking questions here.  &amp;gt; I thought that if you wanted to have it verified in something like Coq you wouldn't be asking questions here. 

Could you kindly point me to a better place to ask this question? I'll retract, mabye compsci was ok place to ask, I would've added 'formally prove' somewhere in the title just to attract people that know a bit about that subject and not other people that think of other ways of verification. Other places where you could ask might be /r/coq if you're familiar with that or stackoverflow.com. 

EDIT: If I may ask, why do you want to make a formal proof? It seems to me that a formal verification, extremely cumbersome and laborius that it often is, is only well justified with systems that need to be absolutely free of any bugs in situations where lives are in danger or where larger sums of money are involved, chess software usually isn't associated with either group. Unless you weren't introduced to the subject then you'll have to process a graduate class of material. The way I would do it is I would ask on /r/coq if it was appropriate for the case, then proceed with reading the manual (which I'm led to believe is excelent), maybe take an online class as an intro before that - but as I said I would make better investment of my time if I added other features to my program i.e. I'd only attempt formal verification if I was well versed in the subject beforehand.  Yes, it's possible, although it will be much more difficult than just writing the code. Basically, you have three tasks:

1) Write the actual code, in a programming language/system that also supports steps 2 and 3 below

2) Write down the specification: what it means for your program to be correct, in your case when a board/move is valid

3) Prove that the program written in (1) behaves as described in (2).

Basically, it means that you write down the rules of chess *twice*, once in step (1), once in step (2), and prove that they are equal in step (3). Usually, the specification is much simpler to read/write than the code, and can be checked manually for correctness (e.g. be compared with the chess rules).

Integrated systems (programming language + proof system) are [Coq](http://coq.inria.fr/), [Isabelle](http://www.cl.cam.ac.uk/research/hvg/Isabelle/), [Agda](http://wiki.portal.chalmers.se/agda/pmwiki.php) .... Tools for existing programming languages are [Frama-C](http://frama-c.com/) for C, [SPARK](http://www.adacore.com/sparkpro/) for Ada, and others.   </snippet></document><document><title>Computer scientists find new shortcuts for Traveling Salesman problem.</title><url>http://www.wired.com/wiredscience/2013/01/traveling-salesman-problem/all/</url><snippet>  "...showed that the traveling salesman problem is &#8220;NP-hard,&#8221; which means that it has no efficient algorithm...unless a famous conjecture called P=NP is true."

I really dislike this pop-sci presentation of P=NP. It conflates efficient in a technically defined way ("in P") with the more pragmatic efficient ("fast enough").

If false, there could still be some efficient (in the practical sense) algorithm for TSP, like n^(log log log n).

See also: http://rjlipton.wordpress.com/2009/07/03/is-pnp-an-ill-posed-problem/ How would you explain it in two sentences so that the layman would have a general idea of what you're talking about and why it matters? He's essentially saying that even though we can't figure out if Travelling Salesman is a part of a set of easier to solve problems, we can still find faster solutions to it than we know of today. It's important because its a problem that is applicable to a lot of problem domains, such as routing internet traffic. I understood what he was saying.  He was complaining about their treatment of P=NP.  I was asking if he could provide a better concise explanation of P=NP that would be appropriate for Wired's target audience. I don't claim to be a writer so what I come up with won't be good. The words "worst case"  should probably appear in there somewhere, though. I understood what he was saying.  He was complaining about their treatment of P=NP.  I was asking if he could provide a better concise explanation of P=NP that would be appropriate for Wired's target audience. How would you explain it in two sentences so that the layman would have a general idea of what you're talking about and why it matters? "...showed that the traveling salesman problem is &#8220;NP-hard,&#8221; which means that it has no efficient algorithm...unless a famous conjecture called P=NP is true."

I really dislike this pop-sci presentation of P=NP. It conflates efficient in a technically defined way ("in P") with the more pragmatic efficient ("fast enough").

If false, there could still be some efficient (in the practical sense) algorithm for TSP, like n^(log log log n).

See also: http://rjlipton.wordpress.com/2009/07/03/is-pnp-an-ill-posed-problem/ "...showed that the traveling salesman problem is &#8220;NP-hard,&#8221; which means that it has no efficient algorithm...unless a famous conjecture called P=NP is true."

I really dislike this pop-sci presentation of P=NP. It conflates efficient in a technically defined way ("in P") with the more pragmatic efficient ("fast enough").

If false, there could still be some efficient (in the practical sense) algorithm for TSP, like n^(log log log n).

See also: http://rjlipton.wordpress.com/2009/07/03/is-pnp-an-ill-posed-problem/ &amp;gt;It conflates efficient in a technically defined way ("in P") with the more pragmatic efficient ("fast enough").

I've seen this conflation in a number of papers, though. I see it pretty much everywhere in AI: in P is tractable, NP-hard is intractable. Really?  All the AI work I've done was with heuristic approaches to things in NP (or harder classes), since those are the really interesting problems.  &amp;gt; Nevertheless, Saberi said, solving the traveling salesman approximation problem in its full generality will probably require an infusion of new ideas.

Unless I'm misreading what he's saying here, it's certainly not possible (unless P=NP). It's been proven (and it's a fairly simple proof) that it's n-inapproximable. Right?

Perhaps the article misunderstood what Saberi said and scribed it unfaithfully. &amp;gt; Nevertheless, Saberi said, solving the traveling salesman approximation problem in its full generality will probably require an infusion of new ideas.

Unless I'm misreading what he's saying here, it's certainly not possible (unless P=NP). It's been proven (and it's a fairly simple proof) that it's n-inapproximable. Right?

Perhaps the article misunderstood what Saberi said and scribed it unfaithfully. Solving the TSP in full efficiently would require P=NP; solving the TSP approximation would not. For instance, the existence of a poly-time algorithm which comes within 0.00001% of the true answer, or one which computes the true answer in 99.99999% of cases do not require P=NP.

Obviously these would be huge breakthroughs. I believe I remember seeing proof that the general TSP is n-inapproximable; i.e. there is no polynomial time algorithm that can calculate it within a constant factor.

The proof is because if you can approximate TSP to within *any* constant factor, you can solve the Hamiltonian cycle problem. Are you sure the proof applies to probabilistic algorithms?  I wonder how the UPS and FedEx routing algorithms fair against this... At least UPS' route efficiency system is a known highly guarded trade secret. UPS and FedEx can use spanning trees. They don't have to hit every city with a single truck. Yes, but each leaf represents the territory of a single truck which must solve a travelling salesman problem for its deliveries for that day. They've just added the additional problem of efficiently dividing up which deliveries should be handled by each truck on top of this other problem. Computationally not any less trivial, but by throwing resources at redundancy they can make inefficient solutions simply less noticeable. It's going to get more nuanced than that once you factor in how well the drivers know their own routes vs. how they would perform on an arbitrarily assigned new route.  Plus UPS can still perfectly solve for the routes they want.  It's expensive, sure, but it's not like it's cryptography cracking expensive. UPS and FedEx can use spanning trees. They don't have to hit every city with a single truck.   What Andrew Wiles quote is paraphrased?

He's the Fermat's Last Theorem guy, but I couldn't find anything similar by him on "quotes" pages, e.g. [his wikiquote page](http://en.wikiquote.org/wiki/Andrew_Wiles)

There is a kinda related [quote maybe by Darwin](http://en.wikiquote.org/wiki/Charles_Darwin):

&amp;gt; A mathematician is a blind man in a dark room looking for a black cat which isn't there. I found it [here](http://www.pbs.org/wgbh/nova/physics/andrew-wiles-fermat.html). I think I have seen him say it on video too, maybe in the documentary?

&amp;gt;Perhaps I can best describe my experience of doing mathematics in terms of a journey through a dark unexplored mansion. You enter the first room of the mansion and it's completely dark. You stumble around bumping into the furniture, but gradually you learn where each piece of furniture is. Finally, after six months or so, you find the light switch, you turn it on, and suddenly it's all illuminated. You can see exactly where you were. Then you move into the next room and spend another six months in the dark. So each of these breakthroughs, while sometimes they're momentary, sometimes over a period of a day or two, they are the culmination of&#8212;and couldn't exist without&#8212;the many months of stumbling around in the dark that proceed them. What Andrew Wiles quote is paraphrased?

He's the Fermat's Last Theorem guy, but I couldn't find anything similar by him on "quotes" pages, e.g. [his wikiquote page](http://en.wikiquote.org/wiki/Andrew_Wiles)

There is a kinda related [quote maybe by Darwin](http://en.wikiquote.org/wiki/Charles_Darwin):

&amp;gt; A mathematician is a blind man in a dark room looking for a black cat which isn't there. It was in the Horizon documentary, right at the beginning around 1 minute in. It's an interesting doc, well worth watching. http://www.youtube.com/watch?v=7FnXgprKgSE  Learnt about the traveling salesman problem in my discrete mathematics class last semester.  Not too sure what implication this has, but an interesting application of it nonetheless! The way I see it, it doesn't have any implications apart from being able to approximate a metric TSP with a smaller error. It's provable that no such approximation algorithm for the general TSP can exist unless P=NP, in which case no such algorithm is needed.

However, the difference between a 50% error and a 40% error is quite substantial for sufficiently large graphs. It's a good result.

As it stands, the TSP is one of the hardest problems in computer science and finding an efficient algorithm (for the corresponding decision problem) would mean that P=NP, which would be a *major* breakthrough. However, proving that no such algorithm can exist would be a major breakthrough as well. At the moment we haven't really got a clue which is the case, although most people in the field suspect the latter. I think saying "we haven't really got a clue" is a bit unfair to the state of research. As far as I know, all (yes, ALL) the experts in complexity are certain P != NP; see, for example, [Scott Aaronson's reasons to to believe as such](http://www.scottaaronson.com/blog/?p=122) (admittedly some are more technical than others).
 &amp;gt; As far as I know, all (yes, ALL) the experts in complexity are certain P != NP;

A [poll](http://www.cs.umd.edu/~gasarch/papers/poll.pdf) found that 61% thought that P!=NP and 9% thougt P=NP. I think saying "we haven't really got a clue" is a bit unfair to the state of research. As far as I know, all (yes, ALL) the experts in complexity are certain P != NP; see, for example, [Scott Aaronson's reasons to to believe as such](http://www.scottaaronson.com/blog/?p=122) (admittedly some are more technical than others).
 Learnt about the traveling salesman problem in my discrete mathematics class last semester.  Not too sure what implication this has, but an interesting application of it nonetheless! Learnt about the traveling salesman problem in my discrete mathematics class last semester.  Not too sure what implication this has, but an interesting application of it nonetheless! Examples of TSP mentioned in the comments and in the article:

 1. Routing network traffic
 1. Routing UPS trucks
 1. Guiding computer-controlled manufacturing devices (possibly CNC mills, laser cutters, 3d printers, etc)


&amp;gt; [The TSP has several applications even in its purest formulation, such as planning, logistics, and the manufacture of microchips. Slightly modified, it appears as a sub-problem in many areas, such as DNA sequencing.](http://en.wikipedia.org/wiki/Travelling_salesman_problem)  </snippet></document><document><title>Writing Loop Invariants in Whiley [VIDEO]</title><url>http://whiley.org/2013/01/29/understanding-loop-invariants-in-whiley/</url><snippet /></document><document><title>Can MEMS-based navigation systems handle activities like walking- jogging- running-in-place?</title><url>http://www.reddit.com/r/compsci/comments/17jyx6/can_memsbased_navigation_systems_handle/</url><snippet>I noticed that these activities can sometimes produce the same accelerometer signals as, say, walking normally. Are there already ways to work around these problems?  Inertial navigation, especially with MEMS sensors, does not work without some other sort of correction. The sensors are not accurate enough, they are prone to measurement biases, and the numerical integration used in those types of schemes are generally not all that accurate. 

In aircraft navigators, for example, inertial sensors are only used to keep track of the vehicle state in between the GPS and magnetometer readings used to correct the inertial position/velocity and attitude estimates. The idea being that GPS and magnetometers are very slow, but very accurate over time, and inertial sensors are very fast but inaccurate over time, so you fuse them together to get the best of both.

So MEMS based navigation systems can't even handle regular walking well without some other sensors, so if you throw running in place at them they will perform even more poorly.

In an idealized system, however, it would have no problem. In order to remain pretty much in place, any accelerations in one direction would have to be offset in the opposite direction in order for you to remain in place. In an actual system, increased variance in the measurements will certainly degrade performance. The way you handle that is by either getting better sensors (decent ones will be around a few hundred bucks, fairly accurate ones a few grand, the kind accurate enough for military aircraft will be hundreds of thousands) or redundant measurements.  </snippet></document><document><title>Creating a visual representation of software performance</title><url>http://www.reddit.com/r/compsci/comments/17jtiz/creating_a_visual_representation_of_software/</url><snippet>I am trying to figure out the best way to represent software system performance. This visualization will be a tool to debug performance problems. How should such a tool look like?



My current idea is based on [chrome's debug tool](http://3.bp.blogspot.com/-xkkPZXpqatA/UKZlMLDQr9I/AAAAAAAAAvA/52-CWOHoQCw/s1600/Google+Chrome+Developer+Tools+-+Timeline+Panel.png). Each task/thread has it's own line and he length of the bar represents its duration. The bar would have visible segments representing execution time and wait time. Seeing all tasks on one page let's you see whats being executed concurrently. I would also add an overlay of system memory usage over time.



Does anyone have experience with performance modeling?    Look up some of the user manuals and help pages for the old Apple developer tool called Shark.

They did it right. I miss Shark...    as a programmer i think you should just show the Big O notation and expect people to figure it out :D edit: sorry for pissing everyone off, was just trying to be funny and or trolly as per ReinH

also, Conky is a great customizable script for linux that can be used as a resource monitor! Check it out, its really sexy. Can you write me a program to analyze a block of code and report the Big-O for it.  Thanks. Can you write me a program to analyze a block of code and report the Big-O for it.  Thanks. Any reasonably competent programmer should be able to figure that out. Presumably, your block of code was written by a programmer. . .If they weren't reasonably competent, fire 'em and give it to one that is so they can figure it out. Now, figure it out. :-&amp;gt; 

But seriously, there's enough profile apps out there that cover the needs here. Use those as the basis for the "visual" data. Once you have that data, you can start looking at all sorts of different ways to represent it visually and I think that different situations will be best represented by different methods. Maybe a part of it should be a line graph that compares inputs to execution times while another part is just a stacked bar graph that shows system time versus cpu time for different threads. Any reasonably competent programmer should be able to figure that out. Presumably, your block of code was written by a programmer. . .If they weren't reasonably competent, fire 'em and give it to one that is so they can figure it out. Now, figure it out. :-&amp;gt; 

But seriously, there's enough profile apps out there that cover the needs here. Use those as the basis for the "visual" data. Once you have that data, you can start looking at all sorts of different ways to represent it visually and I think that different situations will be best represented by different methods. Maybe a part of it should be a line graph that compares inputs to execution times while another part is just a stacked bar graph that shows system time versus cpu time for different threads. I don't think it's that easy. Proving the complexity of a bit of code can be quite complex. Sure, for the most part it's simply counting but some recursive algorithms can be quite tricky. Automating the task is non-trivial I'd say. We can't even *prove* that a program will halt at all, let alone when it will halt. It is literally impossible to do this, no matter how competent the programmer.

What we *can* do is use tools to record the number of operations of an algorithm and compare that to the number of its argument and then use that data to try to find some correlation via curve fitting and other statistical techniques. You know what we call that? *Profiling.*

Edit: and apparently, [this exists](http://www.reddit.com/r/compsci/comments/17jtiz/creating_a_visual_representation_of_software/c86bxup). Cool. Profiling != Complexity analysis

At least we can prove that we can't prove. as a programmer i think you should just show the Big O notation and expect people to figure it out :D edit: sorry for pissing everyone off, was just trying to be funny and or trolly as per ReinH

also, Conky is a great customizable script for linux that can be used as a resource monitor! Check it out, its really sexy. That's a very naive assumption. Lots of parts of a computer's architecture contribute to performance problems, as do OS issues. Pretty sure this is a troll. Nope, just a computer engineer with 7 years industry experience who knows about computer architecture, that's all.

There is a lot more to performance than just Big-O, once you leave the theoretical space (classroom) and move into the practical space (actually run code on a computer).

A lot.

EDIT: oh, you meant the other guy? 

Whoops!! I agree with everything you just said. And yes, I meant the other guy. :)</snippet></document><document><title>100,000 punch cards later, Flossie lives again</title><url>http://www.mirror.co.uk/news/technology-science/technology/flossie-oldest-working-british-computer-1388601</url><snippet>  &amp;gt;Despite its 25 square foot size, Flossie&#8217;s 16,000 transistors and 4,000 logic boards only result in a puny 2kb memory and 1mhz processing speed &#8211; all of which can now fit on a couple of modern-day 10mm silicon chips.

A single 5mm chip can do that today.

A little ATMega chip that you'd find on an Arduino hobby board is 16 times faster, and has similar memory. http://en.wikipedia.org/wiki/Arduino

Even smaller - the Texas Instruments eZ430-F2013 - a programmer card, and detachable "computer on a chip" bits...
http://www.conrad.de/medias/global/ce/1000_1999/1700/1700/1702/170285_ZB_00_FB.EPS_1000.jpg

It's 16MHz, runs at 1.8 volts! , and has 2KB + 256B Flash Memory, 256 bytes RAM, and has 5 I/O lines.

Yep, that's all on the end of a USB plug!     
I love using them! (PDF of tec specs: http://www.ti.com/lit/ds/slas491i/slas491i.pdf ) What do you use them for? &amp;gt;Despite its 25 square foot size, Flossie&#8217;s 16,000 transistors and 4,000 logic boards only result in a puny 2kb memory and 1mhz processing speed &#8211; all of which can now fit on a couple of modern-day 10mm silicon chips.

A single 5mm chip can do that today.

A little ATMega chip that you'd find on an Arduino hobby board is 16 times faster, and has similar memory. http://en.wikipedia.org/wiki/Arduino

Even smaller - the Texas Instruments eZ430-F2013 - a programmer card, and detachable "computer on a chip" bits...
http://www.conrad.de/medias/global/ce/1000_1999/1700/1700/1702/170285_ZB_00_FB.EPS_1000.jpg

It's 16MHz, runs at 1.8 volts! , and has 2KB + 256B Flash Memory, 256 bytes RAM, and has 5 I/O lines.

Yep, that's all on the end of a USB plug!     
I love using them! (PDF of tec specs: http://www.ti.com/lit/ds/slas491i/slas491i.pdf ) Is it just me, or is the USB connector really bulky by comparison?   How many bytes do you fit on a punch card? Filling 2kB of memory should not take that much. Was the punch cards loaded depending on which routine was being executed? (ie. master programme executed by mainframe operators loading routine A, then routine B, etc)   It would seem the major impediment to keeping a machine like this in working order is that the moving parts and consumable goods are just not available.  I would guess you do a lot of restoring the manufacturing base existing at the time just to demonstrate the thing can run.

Whether that is truly an issue or not, this sure brought a smile to my face.  </snippet></document><document><title>A couple questions for people who work in the Computer Science field.</title><url>http://www.reddit.com/r/compsci/comments/17g1d4/a_couple_questions_for_people_who_work_in_the/</url><snippet>Hey guys, I hope this is the right place for this.

I'm currently a Sophomore in college and I have been assigned an interview essay that pertains to a particular major. I picked Computer Science and I have to interview someone who works in the field I picked. If someone who has a job in the Comp Sci field could take the time to answer a few of these questions, that would be awesome.

1. How many of the things you learned while getting your degree do you use in the job you have now?
2. Was finding a job after graduation difficult? Or is there always a demand for Computer Science students?
3. In the job you have, what do you do on a daily basis?
4. As someone who is majoring in Computer Science, any advice on things you can do while still in school to help out future job prospects?


Thanks to anyone who takes the time to answer these.  Comp Sci is a broad field. You might want to focus on a discipline... That being said, I'm a web developer (Software Engineer) with a CS degree.

1. From my classes, I learned Java (I'm a Java developer). Everything else I use, day-to-day, I learned while in school (but on my own time). The most useful thing I learned at school was how to learn (as opposed to literal material). Knowing how to learn CS topics is something I employ everyday.

2. Finding a job isn't hard if you're willing to start at the bottom. I actually took a job unrelated to my degree. In that job, I offered to "automate" some of the grunt work (using Java). I used that experience as a backbone for applying to my current position.

3. Bug fixes and implement new source.

4. Find a focused field that interests you and research it on your own time. Most universities will give you a taste of a subject. But the real learning comes from your passion and desire to know more about a topic. I love(d) web development. So I bought books and read websites. I made websites and built a portfolio/experience (which is something that employers will ask about). A discipline.. interesting, I'll keep that in mind when I'm writing this. This is exactly the information I was looking for.. thanks! Even expanding on the discipline sides of things, people who use the same language and have the same titles will often do profoundly different things. For example, I am also a software engineer, but I work on real-time systems (C++); to a finer grain, I work on a lower-level component that feeds into customer facing applications that are often written in Java. If customer apps require less meaty processing, then they may be written in an interpreted language like perl or javascript. 

Also, I should probably mention that I don't do Windows development. I have heard that it requires a different bread of engineer, but really couldn't say either way.
 A discipline.. interesting, I'll keep that in mind when I'm writing this. This is exactly the information I was looking for.. thanks! Comp Sci is a broad field. You might want to focus on a discipline... That being said, I'm a web developer (Software Engineer) with a CS degree.

1. From my classes, I learned Java (I'm a Java developer). Everything else I use, day-to-day, I learned while in school (but on my own time). The most useful thing I learned at school was how to learn (as opposed to literal material). Knowing how to learn CS topics is something I employ everyday.

2. Finding a job isn't hard if you're willing to start at the bottom. I actually took a job unrelated to my degree. In that job, I offered to "automate" some of the grunt work (using Java). I used that experience as a backbone for applying to my current position.

3. Bug fixes and implement new source.

4. Find a focused field that interests you and research it on your own time. Most universities will give you a taste of a subject. But the real learning comes from your passion and desire to know more about a topic. I love(d) web development. So I bought books and read websites. I made websites and built a portfolio/experience (which is something that employers will ask about). Whats wrong with web development? You don't enjoy it anymore? Comp Sci is a broad field. You might want to focus on a discipline... That being said, I'm a web developer (Software Engineer) with a CS degree.

1. From my classes, I learned Java (I'm a Java developer). Everything else I use, day-to-day, I learned while in school (but on my own time). The most useful thing I learned at school was how to learn (as opposed to literal material). Knowing how to learn CS topics is something I employ everyday.

2. Finding a job isn't hard if you're willing to start at the bottom. I actually took a job unrelated to my degree. In that job, I offered to "automate" some of the grunt work (using Java). I used that experience as a backbone for applying to my current position.

3. Bug fixes and implement new source.

4. Find a focused field that interests you and research it on your own time. Most universities will give you a taste of a subject. But the real learning comes from your passion and desire to know more about a topic. I love(d) web development. So I bought books and read websites. I made websites and built a portfolio/experience (which is something that employers will ask about). &amp;gt; I actually took a job unrelated to my degree. In that job, I offered to "automate"  some of the grunt work (using Java). I used that experience as a backbone for applying to my current position.

I was just curious was kind of job unrelated to CS involves programming in java? Can you elaborate a little more?  1. Everything (Not counting stuff like Macro Economics, and other similar "soft" skills). Programming? VB.net? Check. PHP? check. SQL? check. AS400? Check.

2. Nope. I settled for a lower paying job, with good experience potential... and don't regret a moment.

3. Data Analyst (Many databases, reports, etc), Programmer (not much, but a couple small programs. Helping add functionality to others programs more than my own), network admin script writing.

4. Experience, Experience, Experience. Make stuff, do things, practice, learn. You want to stand out? Have a game written, a website up or something in the "Successful Project" category - especially that relates to your desired field.  I got a Computer Science 'Traditional' degree, that was heavy in programming, math and algorithm work.  I work as an application developer for a large company and use ABAP and SAP.

1. I use the programming skills I used every day.  It's a different programming language and a different environment, but the logic is the same.  Even things like database design and performance come into play.  It's hard to assign a percentage to 'things you learned in classes that you use every day', but it would be high if I could.

2. I accepted a job offer during the fall semester of my senior year.  I never had problems finding internships or jobs during school.  CS students are in demand, but you probably won't turn any heads walking into an interview and only referring to coursework when answering questions.  Technology is a great field to get into, so I would say there is a decent demand for CS grads.

3. I change/create programs to meet requests in specifications.  It could be something simple like adding something to an output screen, or adding huge functionality to a project.  My job revolves around data, so most of the time I make programs that handle/manipulate large amounts of data.

4.  Try different things to find out what you like.  I'm guessing that since you're majoring in CS, you enjoy playing around programming and technology in general.  Keep doing that.  Make a website from scratch that will display a different cat depending on where a user places their cursor.  Make a script that will tell the date on which the least recent file in a directory was edited.  Have fun with it and document it.  

"So madscientest, tell me about a time that you used programming to make a certain task easier."  

"Well, I started making some scripts as a sophomore to learn more about scripting, and realized that I could automate how I organized my schoolwork.  I saved all my files starting with 2-letter keywords Math:'MA', Programming:'Pg' etc when I was working on computers at school.  When I copied my whole profile folder to my personal computer, I could just run my script and it would put the files into the right folder based on the first 2 letters."

Dumb example, you get the idea.  I just graduated with my CS/EE degrees and had a job lined up 6 months before I even graduated.

1. The basics of programming are what I use the most, my company doesn't even use Java (my main language) but I get by just fine because I can pick it up easily.
2. At our job fairs there is literally more jobs than students. I also help recruit, for a main employer, and we literally are desperate for more programmers. I feel safe saying computer science (not even engineering in general) has the most job opportunities post school.
3. I'm a PM, so I architect code and user experiences. I do code but it's not the main focus of my job.
4. INTERNSHIPS! Holy crap I cannot stress that enough. I had 4 job offers to choose from during my senior year. 3 of which were from the companies I had interned at.

Good luck :) Where do you find internships? See if your school's department of computer science knows anything. If not, it doesn't hurt to ask companies directly if they offer such. The worst they can do is say no which is what they are already saying by not asking. Where do you find internships?  1.  Nothing in articular, but everything.  The degree helped me develop a lot of "common sense" for programming.  I make mistakes, I mess things up, but I'm immeasurably better than I would otherwise be.  I was introduced to things I would never have willingly studied, like higher math, computer engineering/organization, low-level OS stuff, assembly, that ended up being tremendously useful.  Having fleshy squishy teachers was awesome, even if they weren't all very good teachers.

2.  Easy, but I had a good relationship with a small specialized company and they answered when I went knocking.  I was very fortunate.  (This is basically an unhelpful answer.)

3.  Code.  A little bit of support ticket handling.

4.  Have good projects.  Good-sized projects.  

I figure a lot of people in this subreddit will disagree (and I'm probably wrong), but I would recommend writing a large general-purpose library/framework.  CrapThatYouNeed.dylib or whatever.  And make it big and bloated and general, you know, shove networking code in there and filesystem code and hardcore logging utilities and any other crap you can think of.  

Why?  Because I think it mirrors what most applications end up being -- a lot of differently purposed code all smushed together -- but you don't have to worry about the actual app idea and how that will change and evolve over time.  A "general purpose library" is sufficiently well-defined that I think it can grow to an arbitrary size without suffering from the ailments that normally trouble applications -- second system effect, etc.  

Maybe.  I dunno.  That's what I'd advise, but keep in mind that I'm an idiot. &amp;gt; I was introduced to things I would never have willingly studied, like higher math, computer engineering/organization, low-level OS stuff, **assembly**, that ended up being tremendously useful.

Can you give examples for assembly? Just had my test in *computer architecture* today and it feels like "yeah good to know about it but will it ever be directly applied?" &amp;gt; I was introduced to things I would never have willingly studied, like higher math, computer engineering/organization, low-level OS stuff, **assembly**, that ended up being tremendously useful.

Can you give examples for assembly? Just had my test in *computer architecture* today and it feels like "yeah good to know about it but will it ever be directly applied?"   Speaking as someone who has hired lots of programmers (and I am a programmer myself):

When hiring someone, I don't care much about their degree, nor do I care about what computer languages they learned in classes. Some of the best programmers I've ever hired were physics majors!

The most important thing to learn in college is how to learn. Programming languages change every few years.  If you understand programming at a deep level, it is easy to pick up the next popular programming language quickly.

Learn about design.  Not just program design, but also graphic design and interaction design. College is a good place to learn that, and it is difficult to learn on the job.

Learn how to communicate, both written and spoken. You will have to do a lot of that. Publish articles, even online. Give talks at conferences. Learn how to network.

Do lots of programming, on projects you find interesting. Do some internships. Extra points if you contribute to open source software. Good programmers love to program. Programming big projects is different and much harder than small assignments (like the ones they assign in classes).

If you are going to do anything related to graphics, learn math (especially trigonometry). User interfaces often involve graphics. Learning hard sciences (like math and physics) also helps you learn how to learn.

Don't worry too much about preparing for a job.  If you love programming, you will always have a job.  Note: I have a computer engineering degree

1. C/C++, UML, algorithms, data structures, calculus

2. I did not have any trouble finding work

3. Designing, design reviews, requirements document reviews, status meetings, management meetings, and on occasion I get to program

4. Do lots of coding. Find time to do your own projects. Take an active roll in learning.  Join user groups.  Show your future employers that you have the right attitude and you are passionate about what you do. I HATE UML...maybe it was the professor that I had.  Is it really that necessary?  I found it to be so tedious and dull.  Maybe the project 'summarized' in UML wasn't of big enough scope to benefit from UML. I HATE UML...maybe it was the professor that I had.  Is it really that necessary?  I found it to be so tedious and dull.  Maybe the project 'summarized' in UML wasn't of big enough scope to benefit from UML. If you have a job in a field requiring complex data design, UML is all important. I had a job in financial software and our UML diagrams were INSANE (not because stupid design, but necessary features/requirements). That's what I was thinking.  I imagine the benefits are most noticeable in complex designs that include tons of people.   I HATE UML...maybe it was the professor that I had.  Is it really that necessary?  I found it to be so tedious and dull.  Maybe the project 'summarized' in UML wasn't of big enough scope to benefit from UML. Note: I have a computer engineering degree

1. C/C++, UML, algorithms, data structures, calculus

2. I did not have any trouble finding work

3. Designing, design reviews, requirements document reviews, status meetings, management meetings, and on occasion I get to program

4. Do lots of coding. Find time to do your own projects. Take an active roll in learning.  Join user groups.  Show your future employers that you have the right attitude and you are passionate about what you do.   1. Learned programming, databases, data structures. That's about it. Everything else will help me grad school, however.
2. It took me 9 months, but the job I got is fantastic with great pay. However, I had no internships and a lowish GPA.
3. Software development: Coding, design, unit testing, provide support to other teams that call our programs. Also troubleshoot and provide code fixes for problems that are wrong in the live environment.
4. Intern and achieve fluency in multiple languages. I've heard contributing to open source projects you can show to employers are wonderful to have. Do u mind me asking what your GPA was (mine isn't great but i didnt think jobs cared so much). But I guess after college thats all they have to go on?                        </snippet></document><document><title>Good CS TED talks?</title><url>http://www.reddit.com/r/compsci/comments/17bfg7/good_cs_ted_talks/</url><snippet>My university is doing a computer science week (whohooo!) and one of the things we are trying to do is pick a day to show a ton of videos to help teach people about different aspects of computer sciences. Does anyone have a link to, or remember a good TED talk about CS?  </snippet></document><document><title>The landscape of just-in-time compilers for numerical Python</title><url>http://www.phi-node.com/2013/01/a-journey-through-parakeet.html</url><snippet>   "Python is an extremely popular language for number crunching and data analysis."

Now that seems to be stretching it a little... numpy, scipy, matplotlib &amp;amp; co are nice, and I've seen some people use it, but I don't think it comes *anywhere* close in popularity to others like Matlab, C/C++ and fortran... "Python is an extremely popular language for number crunching and data analysis."

Now that seems to be stretching it a little... numpy, scipy, matplotlib &amp;amp; co are nice, and I've seen some people use it, but I don't think it comes *anywhere* close in popularity to others like Matlab, C/C++ and fortran... I think it strongly depends on what community you're coming from. Many of the machine learning folks I know are jumping ship from Matlab to Python (though some of the deep learning folks seem to like Lua more). Every physics lab I know (admittedly that's only three of them) uses Python to pull together simulation/experimental results and do analysis on them. And, of course, the use of C, C++, and Fortran doesn't really exclude Python. Out of curiosity, what's your background?  </snippet></document><document><title>Coeffects: typing context-dependent computations using comonads</title><url>http://www.cl.cam.ac.uk/~tp322/drafts/coeffects.html</url><snippet>  I'm currently working on an extension for the GHC compiler as part of my degree to allow the use of comonads in a similar syntax to how monads are currently used in Haskell (unsurprisingly called codo notation, corresponding with do notation), under the supervision of Dominic Orchard (one of the authors of this paper).

As such, I was interested to see this come up. I'm impressed at the name. I don't think I'd've been able to resist calling it "undo notation", "don't notation", or something similarly silly. I'm impressed at the name. I don't think I'd've been able to resist calling it "undo notation", "don't notation", or something similarly silly.  Can someone explain like I'm a C++ programmer what this means? Context-dependent things all have a similar structure which we can leverage to write and reason about programs that are generic to the kind of context we're dealing with in a more structured way. We call the stuff with the same structure that contexts have "comonads". I still have no idea what you're talking about. So by context dependent do you mean you pass all the stuff about the context that the structure uses as some sort of parameter to your functions?

If you have a function that changes a global variable, doesn't that still change the context of other functions?

What does "generic to the kind of context we're dealing with" mean, and how can something be that as well as context dependant?

I'm not sure if what I'm asking is possible, but I'd think it would be possible to explain this in terms of C/C++; I mean using a map function is a functional programming idea, but you can pretty much implement one in C/C++ using function pointers.  The idea is that computations (functions) execute in some *context*. The context may be global variables that they use, resources like databases or other information (functions that are only available on certain kinds of CPUs or platforms).

In a normal code, you just use the context - you access a variable, call a database, invoke a system call. However, if you run the program on a platform where something is not available, you get a runtime error.

The idea of *coeffects* is that we want to make this dependence explicit - so when calling a function, you will know what it requires (what it accesses) - it will be part of the type (just like the type of arguments is present in the signature).

Comonads are just a mathematical structure that models how context propagates through a program (how variables and resources can be passed around) - when you make two function calls, the context is passed to both of them, when you define a nested function, it can access the outer context (etc.) Using comonads, we can figure out what are the rules for the propagation of the contextual information, which tells us how the correct types (or context annotations that are automatically inferred for your code) should look. I still have no idea what you're talking about. So by context dependent do you mean you pass all the stuff about the context that the structure uses as some sort of parameter to your functions?

If you have a function that changes a global variable, doesn't that still change the context of other functions?

What does "generic to the kind of context we're dealing with" mean, and how can something be that as well as context dependant?

I'm not sure if what I'm asking is possible, but I'd think it would be possible to explain this in terms of C/C++; I mean using a map function is a functional programming idea, but you can pretty much implement one in C/C++ using function pointers.  Can someone explain like I'm a C++ programmer what this means?</snippet></document><document><title>Incremental algorithm for finding "islands" in a graph?</title><url>http://www.reddit.com/r/compsci/comments/177kyt/incremental_algorithm_for_finding_islands_in_a/</url><snippet>I don't know a lot of graph theory, so please excuse my terminology, but I often run into a specific problem. First I should probably explain my terminology.

I call a set of nodes an island in a graph if there is a path from every node to every other node, *and* there is no superset which also fulfills the previous requirement. That it, an island is "the" biggest set such that there is a path between every pair of nodes.

Now, I need an algorithm which can find the set of islands in a graph, in faster than linear time, but it may be incremental. That is, given a previous set of islands, a previous graph and information describing an edge that has been added/removed or a node that has been added/removed, and it has to figure out a new set of islands in faster than linear time.

Is that possible?  You're looking for a "dynamic data structure" for "connected components".

[Here's some references](http://cstheory.stackexchange.com/questions/2548/is-there-an-online-algorithm-to-keep-track-of-components-in-a-changing-undirecte). You're looking for a "dynamic data structure" for "connected components".

[Here's some references](http://cstheory.stackexchange.com/questions/2548/is-there-an-online-algorithm-to-keep-track-of-components-in-a-changing-undirecte).  Is there any particular reason this needs to run in better than O(n)?  

At some point, it will have had to analyze each and every node for membership in an island.  (This may happen during  the n insertions, but there are still going to be O(n) insertions)


  I think "clique" (pronounced like click) is the usual term for this, except that (IIRC) it doesn't have the requirement that it isn't a subset of a larger clique.

Take a look at these:

https://en.wikipedia.org/wiki/Clique_(graph_theory)

https://en.wikipedia.org/wiki/Clique_problem

Unfortunately, the clique problem is NP complete. I don't know about the incremental aspect of your question, but the initial search is going to be in exponential time. Cliques require that the nodes are directly connected. The correct terminology for islands is apparently "connected components". Also, it's quite easy to solve my problem in linear time (but that is too slow).  Cliques require that the nodes are directly connected. The correct terminology for islands is apparently "connected components". Also, it's quite easy to solve my problem in linear time (but that is too slow).   http://en.wikipedia.org/wiki/Disjoint-set_data_structure ? Interesting, but does that allow for deletion of edges? Apparently not, but if you google for "disjoint set deletion" you can find more stuff, [for example](http://stackoverflow.com/questions/12689651/disjoint-set-data-structure-supporting-deletion), mentioning an O(log N) data structure (disjoint sets are O(1), amortized). Also, now you know that your "islands" are usually called "connected components", so maybe try googling for that in particular.

And then there's [this](https://docs.google.com/viewer?a=v&amp;amp;q=cache:lB3_2byuZXgJ:www2.imm.dtu.dk/~inge/uf.pdf+&amp;amp;hl=en&amp;amp;pid=bl&amp;amp;srcid=ADGEESg4dw2hDqejx-71XDDxJeIB-w-3yfWpl0_XqHbcwOJJe71Y4QtMiHJ6R9K3OizqzyanlBfAahuuvVSwSiyBVt5FkvJVKISU82vPp5kLcE_L2LTdEb2BrqynXpYElX6u2RrJomWn&amp;amp;sig=AHIEtbRDFQSRLUmCtb7ibKRa0AUsr13hdw) but I haven't read it.  </snippet></document><document><title>Computer files accurately encoded on DNA [x-post from r/science]</title><url>http://www.telegraph.co.uk/science/science-news/9821895/Computer-files-stored-accurately-on-DNA-in-new-breakthrough.html</url><snippet>  Smells like singularity. [deleted] Is Moore's law applicable to this? [deleted] Ahhhh. So what law is applicable for storage space? God's Law? [deleted] &amp;gt;"Within a decade, they expect the technique to have become cheap enough that DNA storage could become cost-efficient for the public to store lifelong keepsakes like wedding videos." 
- Right in the article. [deleted] Smells like singularity. What happens if you pirate something into your DNA..... What happens if you pirate something into your DNA.....  "The advantage of using DNA over hard drives is that it does not require a constant supply of electricity, while "no-power" archiving materials such as magnetic tape degrade within a decade." What? Hard drive's don't need to be on all the time. I shuttered at this comment, because I once had a comp sci prof teach this too. "The advantage of using DNA over hard drives is that it does not require a constant supply of electricity, while "no-power" archiving materials such as magnetic tape degrade within a decade." What? Hard drive's don't need to be on all the time.   I wonder if they paid for the license for MLK's "I have a dream". </snippet></document><document><title>Minimizing nondeterministic finite state automata</title><url>http://www.reddit.com/r/compsci/comments/176j3a/minimizing_nondeterministic_finite_state_automata/</url><snippet>There is a lot of literature on minimizing the number of states in a deterministic finite state automata, however I have been having trouble finding results for optimizing *non*deterministic finite state automata --- and everything that I can find cheats by determinizing the automata first and then applying a standard algorithm for minimizing the resulting deterministic automaton, but this produces suboptimal results as in general the optimal deterministic automaton is larger than optimal nondeterministic automaton.

Are there results out there for nondeterministic finite state automata out there that I have been missing?  Honest question here: why would you want to minimize a nondeterministic automata? What does "minimal" even mean for NDAs? (Is "minimal number of nodes" really a meaningful concept for nondeterministic automata?) I want to minimize a nondeterministic automaton because in general it will have fewer states than the equivalent minimal deterministic automaton.  To see an example of this, consider the language (AA|BA|BC|CC).  Because there are three possible choice for the first character of the input string, a deterministic automaton requires at least six states:  a start state, an end state, a dead state, and a state for each of the A, B, and C first input character inputs.  By contrast, a nondeterministic automaton needs only four states:  a start state, an end state, a state that has edges from the start for A and B and an edge to the end for A, and a state that has edges from the start for B and C and an edge to the end for C.  The reason the nondeterministic automaton is smaller (in addition to not needed a dead state) is because it can factor based on the second character, which is more efficient then factoring based on the first character.

It is quite easy to define the notion of a minimal nondeterministic automaton.  Let X be the set of all nondeterministic automata recognizing some language L.  Each automaton in X has a finite number of states, bounded below by zero.  Ergo, at least one automaton has a number of states which is the least of any in X.  We shall define that to be the *minimal* nondeterministic automaton. I'm sure your heart is in the right place, but I'm going to continue to push back a bit, as I don't think you've really tried to think about the point I'm trying to make with my questions. I agree that we can define "minimal nondeterministic automaton" in the way you describe, so let's do so for the rest of this chat.

Now, my question is, once you have a minimal NDA, what will you do with it? If your answer is "run it", then I think you'll find that in almost all cases, running it amounts to determinizing it and running the deterministic automaton. (As an aside: below you talk about "tracking a state vector". I've got sad news for you. The traditional method of determinizing an NDA is to make each state of the DA be a state vector from the NDA. Congratulations, you just invented exactly the process usually used: determinizing, then running.) Since it's not at all clear (and probably not true) that a determinized minimal NDA is a minimal DA, you might as well skip the NDA-minimizing step and skip straight to determinizing the DA.

So, I ask again: Is "minimal number of nodes" really a meaningful concept for NDAs? What are you going to do with the automata that makes this sense of "minimal" a reasonable one? I agree that if one wanted one could construct a DA where every state refers to a set of states in the original NDA.  However, obviously this can produce an exponential blow-up in the number of states, and it isn't necessary.  Given a set of states that the NDA could be in, you can compute in polynomial time the next set of states that it could be in for a given input.  Thus the running time and the memory required is polynomial in the number of states in the NDA, whereas if you converted it to a DA you might get an exponential blowup.

So in short, expanding the DA into exponential number of states is far more expensive then tracking the set of states at each step of the input word.

Furthermore, my automata is weighted and has no epsilons, which means that I can compute the value in the power series by multiplying a chain of matrices together (one for each input symbol), which again is far cheaper then first exponentially blowing up the number of states.

So in short, I have no idea where you are getting the idea that the only meaningful way to run an NDA is to first determinize it.  Also, for the record, I did try to think about the point that you were trying to make and I decided that I disagree, so please in the future don't infer from my disagreement that I haven't thought about what you said. I agree that if one wanted one could construct a DA where every state refers to a set of states in the original NDA.  However, obviously this can produce an exponential blow-up in the number of states, and it isn't necessary.  Given a set of states that the NDA could be in, you can compute in polynomial time the next set of states that it could be in for a given input.  Thus the running time and the memory required is polynomial in the number of states in the NDA, whereas if you converted it to a DA you might get an exponential blowup.

So in short, expanding the DA into exponential number of states is far more expensive then tracking the set of states at each step of the input word.

Furthermore, my automata is weighted and has no epsilons, which means that I can compute the value in the power series by multiplying a chain of matrices together (one for each input symbol), which again is far cheaper then first exponentially blowing up the number of states.

So in short, I have no idea where you are getting the idea that the only meaningful way to run an NDA is to first determinize it.  Also, for the record, I did try to think about the point that you were trying to make and I decided that I disagree, so please in the future don't infer from my disagreement that I haven't thought about what you said.  Look up bisimulation quotienting. Nondeterministc automata *can* be minimised but the algorithm may not be very efficient.  Look up bisimulation quotienting. Nondeterministc automata *can* be minimised but the algorithm may not be very efficient.   http://www.amazon.com/Introduction-Automata-Languages-Computation-Edition/dp/0201441241

Chapter 2 has a great explanation for your problem.  Give me an example of the one you want to minimise.
 Actually I don't think you can prove its minimal without determinizing. Give me an example of the one you want to minimise.
 I want to design an algorithm that takes as input a nondeterministic weighted automaton with complex weights and produces as output the minimal automaton that computes the same power series.  My input has the additional structure that it is a DAG except that some nodes might have an edge pointing to itself (i.e., the only possible cycles are node self-cycles).

I have found algorithms for minimizing weighted automata but they only work if the automaton is deterministic, which in general will increase the number of states over the minimal nondeterministic automaton.  This is unfortunate because the nondeterminism is essentially for free as all it means is that you have to track the state *vector* that the automaton is in rather than the state. What is the difference between a state vector and a state, theoretically speaking? What is the difference between a state vector and a state, practically speaking?  Check out Markov Chains  Check out Markov Chains  How do Markov Chains help? It's a finite state machine with probabilities leading to each state. By definition that's nondeterministic. From that you can have a matrix representing the machine and then see whether or not its irreducible or not.  It's called the study of Stochastic Processes You are correct that Markov Chains are nondeterministic.  You are incorrect in assuming that all nondeterministic FSAs are Markov Chains, and that the ability to minimize an arbitrary Markov Chain allows you to minimize any FSA as well.</snippet></document><document><title>Habitual selfish agents and rationality &#171; Theory, Evolution, and Games Group</title><url>http://egtheory.wordpress.com/2013/01/23/habitual-rationality/</url><snippet /></document><document><title>Will Machines Ever Master Translation?</title><url>http://spectrum.ieee.org/podcast/robotics/artificial-intelligence/will-machines-ever-master-translation</url><snippet>    *Many* languages are far more vague than English. Japanese, e.g., almost always leaves out pronouns (+ has no verb/subject agreement), and has no future tense.  So given a simple Japanese one-word sentence:

&amp;gt;  &#12383;&#12409;&#12427;&#65311;

you have a choice of:

* (Will/Do) (you/I/he/she/they/it) eat?

Sure, a statistical method can be found to essentially guess the most common usage--but there's no way short of AI that it'll be able to tell that a child asking his mother what will happen when a diver throws a fish in the shark-tank is asking if the Shark is going to eat.

So is it easy to go from English-&amp;gt;Japanese then?  No, because there are plenty of other concepts that are more specific in Japanese. E.g., they have two words:

* &#12381;&#12371; : over there by you
* &#12354;&#12381;&#12371; : over there away from both of us

So a simple English sentence "I put it over there" cannot be translated into Japanese without also implying that the object either definitely is, or is not, next to the listener.

They have two words for cold:

* &#12373;&#12416;&#12356; : to be cold (oneself)
* &#12388;&#12417;&#12383;&#12356; : to be cold to the touch (also cold-hearted)

So "Mary is cold" cannot be translated from English to Japanese without essentially guessing. And this basically apply to any language. Synonymes, idioms and words with multiple senses are a bitch to get right. Plus things like anaphora resolution and correct pronoun substitution. In english, every object is "it", not so in russian where "river" is "she" -&amp;gt; Did you see it (the river)? must be translated as "Did you see her (the river)?"

So yea, these is just another example of additional difficulty. Of course this example sentence can be dealt with with a dictionary, but if the "it" refers to something from 3 sentences back, it's not so simple anymore :P Right, so the problem is that in Russian, if someone says the equivalent of "She killed my father", it might have to be translated to "The river killed my father" without an obvious and near reference to the word "river" (and so a dictionary isn't really the issue).  Is that right? Kind of, yea. Imagine following piece of text:

&amp;gt; Peter was sick. His brother came in and gave him soup. He ate it.

Now, soup in russian is "sup" (both english and russian come from french soupe). In russian, both brother and soup are "he".

So if I give you the russian sentence

&amp;gt; [...] gave him soup. He ate HIM (the soup).

Obviously, with a simple search&amp;amp;replace for pronouns, this could very well mean Peter ate his brother :P

(**note**: russian acually distinguish between animate and inanimate masculine, so this example would be solvable. I can't recall any russian feminine food :D (**edit: also see vytah's comment below**)

**Edit**: Vodka works :D

&amp;gt; [...] His sister came in and gave him a shot of Vodka. He threw HER out of the window. -- sister or the Vodka? Computer can't really tell without knowledge that usually peopel don't throw other people out the window.

)

Now this is a bit contrived example, but this happens all the time when you translate from language with gramatical genders to language without gramatical gender.

Your sentence "She killed my father" is also a good example, but usually the pronouns are dropped, so they would only say "ubila mojego otca" (where ubila is FEMININE past tense of kill). It can mean both "the river killed him" or just as well "my sister/mother/girlfriend/..." (all of those are obviously feminine gender). Without context you can't really tell if it was a person or something else. Kind of, yea. Imagine following piece of text:

&amp;gt; Peter was sick. His brother came in and gave him soup. He ate it.

Now, soup in russian is "sup" (both english and russian come from french soupe). In russian, both brother and soup are "he".

So if I give you the russian sentence

&amp;gt; [...] gave him soup. He ate HIM (the soup).

Obviously, with a simple search&amp;amp;replace for pronouns, this could very well mean Peter ate his brother :P

(**note**: russian acually distinguish between animate and inanimate masculine, so this example would be solvable. I can't recall any russian feminine food :D (**edit: also see vytah's comment below**)

**Edit**: Vodka works :D

&amp;gt; [...] His sister came in and gave him a shot of Vodka. He threw HER out of the window. -- sister or the Vodka? Computer can't really tell without knowledge that usually peopel don't throw other people out the window.

)

Now this is a bit contrived example, but this happens all the time when you translate from language with gramatical genders to language without gramatical gender.

Your sentence "She killed my father" is also a good example, but usually the pronouns are dropped, so they would only say "ubila mojego otca" (where ubila is FEMININE past tense of kill). It can mean both "the river killed him" or just as well "my sister/mother/girlfriend/..." (all of those are obviously feminine gender). Without context you can't really tell if it was a person or something else. &amp;gt;russian acually distinguish between animate and inanimate masculine, so this example would be solvable. I can't recall any russian feminine food

Not in case of pronouns. &#8220;He ate **him**&#8221; would be translated to &#8220;&#1054;&#1085; **&#1077;&#1075;&#1086;** &#1089;&#1098;&#1077;&#1083;&#8221; regardless of whether &#8220;him&#8221; refers to the soup or the brother. Hah, that's true. Although, not in case of all pronouns. For example "tot/togo".

"Introspective" analysis always fails me :P (plus this happens when one edits the post multiple times). 

Thanks for the correction. You're right, there are pronouns other than personal ones.

Even more silliness happens in Polish, I'll just list accusatives of personal pronouns, because I don't feel creative enough to think of example sentences:

* singular masculine: jego/niego
* singular feminine: j&#261;/ni&#261;
* singular neuter: **je/nie**
* plural masculine personal: ich/nich
* all other plurals: **je/nie** Wait, but jego/niego isn't the animate/inanimate distinction, or is it? To my understanding animates simply use the genitive form for accusative. Which is based on a fact that old slavic used genitive form for accusative, later the inanimate split from that.

It should be just a matter of preposition or stress (stressed/unstressed pronouns), no? (also with the 'go' variant)

To give an example (slovak) : Zaplatil som za neho (I've paid for him). Jeho som videl / Videl som ho (I've seen him)

(not giving polish example because I don't speak much polish, but it is really transparent to me. Eventually, I'd like to learn to speak it too).

Plus the ich/nich thing exists in Russian too, no? Yup, I just listed both forms, free one and the one that comes after a preposition (totally forgetting about the unstressed variant *go*).

N- prefix after a preposition is probably present in all Slavic languages.

What I wanted to say is that meaning of word *je* (even if the algorithm is able to rule out its third meaning, which is a verb) is heavily context-dependent.

Example: *Kaczki wyl&#261;dowa&#322;y na jeziorze. Spojrza&#322;em na* ***nie***. (Ducks landed on a lake. I looked at **them/it**.) &#8211; ambiguous even for a native Polish speaker.
      </snippet></document><document><title>Forget Table (via /r/prograrticles)</title><url>http://word.bitly.com/post/41284219720</url><snippet /></document><document><title>Depth- and Breadth-First Search</title><url>http://jeremykun.com/2013/01/22/depth-and-breadth-first-search/</url><snippet>  The more powerful chess computers of the past 4 years have blown away their predecessors precisely because they balance depth-searching vs breadth-searching depending on the needs of the position they are analyzing.  When moves are forced they dive in deeply, when there are lots of choices they taste everything on the buffet. 
 I think the best chess computers use minimax using alphaBeta-pruning and iterative deepening. That is at least what I would have done. Can you provide a bit more explanation on this alternation between BFS and DFS tactic?  The more powerful chess computers of the past 4 years have blown away their predecessors precisely because they balance depth-searching vs breadth-searching depending on the needs of the position they are analyzing.  When moves are forced they dive in deeply, when there are lots of choices they taste everything on the buffet. 
</snippet></document><document><title>Jonathan Edwards: Down the rabbit hole of types</title><url>http://alarmingdevelopment.org/?p=724</url><snippet /></document><document><title>Map fusion is great, what else is there?</title><url>http://www.reddit.com/r/compsci/comments/171cc5/map_fusion_is_great_what_else_is_there/</url><snippet>Are there other compiler optimizations which can have dramatic impact on performance and can also be expressed in terms of simple/local rewrite rules?   You may want to ask in r/Haskell, there's a lot of knowledge about these things there, like the build/folder optimisation for example You may want to ask in r/Haskell, there's a lot of knowledge about these things there, like the build/folder optimisation for example  </snippet></document><document><title>Are there any algorithms for large scale, unsupervised pattern finding in text files?</title><url>http://www.reddit.com/r/compsci/comments/171f1b/are_there_any_algorithms_for_large_scale/</url><snippet>I am digging a lot more in to text processing. One frequent problem that comes up is trying to find patterns in raw text. So I would like to know if there are algorithms that deal with this problem.

Edit: To elaborate, I have several gigabytes of newspaper titles that I am trying to parse through for any common patterns between one set of titles and another set of titles. I need a broad based method that can identify any text structure patterns. There is no specific question other than what patterns and trends are there.

What adds complexity is some are foreign languages, so is there a possibility of finding patterns in one langiage to the next?   Still not really sure what you are trying to achieve but there are three basic levels of linguistic knowledge: lexical (word-form), syntactic (grammar) and semantic (meaning). The easiest to start with would be lexical. You could look at word-frequency, collocations, ngrams and such. You could also use unsupervised techniques such as clustering and LDAs to find groupings between the texts.

Are you looking to interpret these patterns directly or are youj looking for features to be used in other models?  You might look into Latent Semantic Indexing. It uses Singular Value Decomposition on the high dimensional space of the words used in your titles and compresses them down to a lower dimensional space.  You can then use this to group similar titles together.  Other clustering algorithms might also help you discover patterns in the data.

You can also look frequent itemset mining; trying to discover recurring phrases across your corpus of text.

 Thanks, looking in to svd for text right now. Which might be some other clustering algorithms? Kmeans?  I suspect some of the tools used in cryptography could aid in this problem. It depends on what sort of patterns you are looking for, but there are many algorithms for common characters, common words, and other things of that nature that can be used on plain text as well as encrypted content. Without more detail on what sort of pattern you are looking for, we can't be of much help.    You'll have to be more specific about the type of patterns you are looking for. Grammar? Word prediction? Patterns over time? I am lookijg for some specifics like frequency, terms after one another, etc. I can hard code those, but I am looking for something more open ended that can find any patterns I would not normally notice.  I am lookijg for some specifics like frequency, terms after one another, etc. I can hard code those, but I am looking for something more open ended that can find any patterns I would not normally notice. </snippet></document><document><title>2-prover 1-round games</title><url>http://www.reddit.com/r/compsci/comments/16zgde/2prover_1round_games/</url><snippet>I'd like to know how these kind of games could be expressed in the form of a bipartite graph. as shown [here](http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=6&amp;amp;ved=0CEgQFjAF&amp;amp;url=http%3A%2F%2Fwww.cs.nyu.edu%2F~khot%2Fpapers%2Fkhot-csp.ps&amp;amp;ei=WCn9UPOqKcv5sgbj7YDwAQ&amp;amp;usg=AFQjCNH5va0vPjU_bTHxl7iHLPr1RtlXAw&amp;amp;bvm=bv.41248874,d.Yms&amp;amp;cad=rja) in section 2.3


I didn't understand why maximizing the satisfied weight of the graph is the same as solving the game. 


Thanks.</snippet></document><document><title>C and C++ Aren&#8217;t Future Proof</title><url>http://blog.regehr.org/archives/880</url><snippet>   I'm confused. Wouldn't this be an argument for saying that current C/C++ *compilers* aren't future proof? It seems like this has nothing to do with the languages' standards, but more with GCC and Clang. The problem is that almost all C and C++ programs use undefined operations that are treated consistently by compilers, so programmers rely on those behaviors. The compiler makers are under no obligation to continue treating those operations consistently, and they often don't, which breaks existing code that worked just fine before. So then shouldn't programs not rely on those undefined behaviors? This seems to me like it is more of a problem with how people write code than the compilers themselves. People should write code that doesn't exhibit undefined behaviour, but they usually aren't careful enough to do so.

This is not likely to change. If someone uses undefined behavior and that behavior later changes, thats their fault, not the standard's. pointing the finger doesnt make the broken code go away Nor does it make it any less their responsibility. The standard is not responsible for broken code. Nor does it make it any less their responsibility. The standard is not responsible for broken code. If someone uses undefined behavior and that behavior later changes, thats their fault, not the standard's. If someone uses undefined behavior and that behavior later changes, thats their fault, not the standard's. An individual failure is the fault of the programmer responsible: a pattern of many individual failures is a fault in the design of the standard. I can guarantee that there are hundreds of missing semicolon errors each day. Does this mean that this is a fault in the standard? I can guarantee that there are hundreds of missing semicolon errors each day. Does this mean that this is a fault in the standard? If syntax errors were a serious failure, would you argue that it was not? If someone uses undefined behavior and that behavior later changes, thats their fault, not the standard's. C has a huge amount of stuff left up to the whims of implementers, though. Now, that's exactly what makes it a good language for relatively low-level code that works on a bunch of targets, but it's also a legitimate target for critique of the language. Undefined behaviour is nowhere near being the same as implementation-defined behaviour. Undefined behaviour is nowhere near being the same as implementation-defined behaviour. So then shouldn't programs not rely on those undefined behaviors? This seems to me like it is more of a problem with how people write code than the compilers themselves. Indeed, that is one of the four options for moving forward that are listed at the end of the article:

&amp;gt;1. We ditch the C and C++, and port our systems code to Objective Ruby or Haskell++ or whatever.
&amp;gt;2. Developers take undefined behavior more seriously, and proactively eliminate these bugs not only in new code, but in all of the legacy code.
&amp;gt;3. The C/C++ standards bodies and/or the compiler writers decide that correctness is more important than performance and start assigning semantics to certain classes of undefined operations.
&amp;gt;4. The undefined behavior time bombs keep going off, causing minor to medium-grade pain for decades to come. &amp;gt; We ditch the C and C++, and port our systems code to Objective Ruby or Haskell++ or whatever.

Will never happen, when a company has invested in people who know C/C++, they won't allow them to rewrite everything in a new language because of "a silly bug in the compiler".

&amp;gt; Developers take undefined behavior more seriously, and proactively eliminate these bugs not only in new code, but in all of the legacy code.

There's no immediate benefit for doing so and worse, there's no penalty for not doing so. Besides, nobody wants to touch Module X, which was written in the 90's by some kind of genius who has since left the company.

&amp;gt; The C/C++ standards bodies and/or the compiler writers decide that correctness is more important than performance and start assigning semantics to certain classes of undefined operations.

Correctness doesn't ship products, performance does.

&amp;gt; The undefined behavior time bombs keep going off, causing minor to medium-grade pain for decades to come.

Pretty much this. &amp;gt; We ditch the C and C++, and port our systems code to Objective Ruby or Haskell++ or whatever.

Will never happen, when a company has invested in people who know C/C++, they won't allow them to rewrite everything in a new language because of "a silly bug in the compiler".

&amp;gt; Developers take undefined behavior more seriously, and proactively eliminate these bugs not only in new code, but in all of the legacy code.

There's no immediate benefit for doing so and worse, there's no penalty for not doing so. Besides, nobody wants to touch Module X, which was written in the 90's by some kind of genius who has since left the company.

&amp;gt; The C/C++ standards bodies and/or the compiler writers decide that correctness is more important than performance and start assigning semantics to certain classes of undefined operations.

Correctness doesn't ship products, performance does.

&amp;gt; The undefined behavior time bombs keep going off, causing minor to medium-grade pain for decades to come.

Pretty much this. &amp;gt; We ditch the C and C++, and port our systems code to Objective Ruby or Haskell++ or whatever.

Will never happen, when a company has invested in people who know C/C++, they won't allow them to rewrite everything in a new language because of "a silly bug in the compiler".

&amp;gt; Developers take undefined behavior more seriously, and proactively eliminate these bugs not only in new code, but in all of the legacy code.

There's no immediate benefit for doing so and worse, there's no penalty for not doing so. Besides, nobody wants to touch Module X, which was written in the 90's by some kind of genius who has since left the company.

&amp;gt; The C/C++ standards bodies and/or the compiler writers decide that correctness is more important than performance and start assigning semantics to certain classes of undefined operations.

Correctness doesn't ship products, performance does.

&amp;gt; The undefined behavior time bombs keep going off, causing minor to medium-grade pain for decades to come.

Pretty much this. &amp;gt; Will never happen, when a company has invested in people who know C/C++, they won't allow them to rewrite everything in a new language because of "a silly bug in the compiler".

Will never happen because those languages are not suitable for writing systems code.

Actually, if the author would check some kernel code he might get a heart attack, because those very things what he is complaining about are what enables those languages to write operating systems. So then shouldn't programs not rely on those undefined behaviors? This seems to me like it is more of a problem with how people write code than the compilers themselves. I think the author's claiming that it's a problem with the standards lumping too many things under "undefined behavior" when they could have perfectly well-defined behavior, since even well-written, bug-free code turns out to have a lot of undefined behavior. I think the author's claiming that it's a problem with the standards lumping too many things under "undefined behavior" when they could have perfectly well-defined behavior, since even well-written, bug-free code turns out to have a lot of undefined behavior. So then shouldn't programs not rely on those undefined behaviors? This seems to me like it is more of a problem with how people write code than the compilers themselves. Quite a few common things are either undefined or implementation defined.

Example:  Dereferencing a null pointer.  We usually expect it to segfault, but how many C or C++ programmers know that it's actually undefined behavior?

Pointers in general, really.  They're fun, but they're a minefield of undefined behavior.  Consider:

    char a[10] = { '\0' };
    cout &amp;lt;&amp;lt; *(a + 12);

What happens?  Logically, we'd expect the value of memory location &amp;lt;address of a&amp;gt; + 12 to be printed, unless it happens to be an invalid address, then I suppose we should segfault.  In reality?  Sorry, undefined behavior. Quite a few common things are either undefined or implementation defined.

Example:  Dereferencing a null pointer.  We usually expect it to segfault, but how many C or C++ programmers know that it's actually undefined behavior?

Pointers in general, really.  They're fun, but they're a minefield of undefined behavior.  Consider:

    char a[10] = { '\0' };
    cout &amp;lt;&amp;lt; *(a + 12);

What happens?  Logically, we'd expect the value of memory location &amp;lt;address of a&amp;gt; + 12 to be printed, unless it happens to be an invalid address, then I suppose we should segfault.  In reality?  Sorry, undefined behavior. C and C++ are often lauded in discussions on the pedagogical value of various languages specifically because of pointer shenanigans. A lot of people think that students don't *really* understand pointers without having experienced C/C++ debugging. Not being an ass: can you (or someone) point me towards an argument that this isn't true? Quite a few common things are either undefined or implementation defined.

Example:  Dereferencing a null pointer.  We usually expect it to segfault, but how many C or C++ programmers know that it's actually undefined behavior?

Pointers in general, really.  They're fun, but they're a minefield of undefined behavior.  Consider:

    char a[10] = { '\0' };
    cout &amp;lt;&amp;lt; *(a + 12);

What happens?  Logically, we'd expect the value of memory location &amp;lt;address of a&amp;gt; + 12 to be printed, unless it happens to be an invalid address, then I suppose we should segfault.  In reality?  Sorry, undefined behavior. &amp;gt;Example: Dereferencing a null pointer. We usually expect it to segfault, but how many C or C++ programmers know that it's actually undefined behavior?

Dereferencing `NULL` is a bug any way you slice it, so this is only an issue in debug builds, and I don't care *what* kind of compiler you're making, if I pass `-g -O0`, I expect `*NULL` to segfault, crash, or otherwise terminate.  Do whatever crazy bullshit you want when optimizing, but if you've been explicitly instructed *not* to optimize, then you *shouldn't be optimizing*. &amp;gt;if I pass -g -O0, I expect *NULL to segfault, crash, or otherwise terminate.

You pretty much just confirmed my point.  You have no guarantee that any of the above will happen regardless of the build type. I *know* that.  But if the compiler is dumb enough to perform optimizations when *I specifically told it not to*, I'm switching compilers.  And if it doesn't optimize, then the code will literally try to access address 0 and crash. I'm not sure you understand what I'm saying, because it has nothing to do with optimization.

1. While a compile time constant value of 0 can be used to represent a null pointer, there is no requirement that the *run-time* value of a null pointer is 0, therefore,
2. There is no guarantee that dereferencing a null pointer results in an attempt to access address 0, therefore,
3. There is no guarantee whatsoever that dereferencing a null point will result in a crash.

It all goes back to what I said originally - there is a lot of behavior in C and C++ that coders *assume* is standard and well defined, when in reality it is undefined behavior that just happens to have similar implementations in most of the common compilers. Having `NULL` translate to a *legal* (within the process's space) address is completely insane, however, so it's incredibly unlikely, in practice.  It might be nonzero for some bizarre reason, but it's not going to be *legal* unless the compiler designers are smoking crack. The problem is that almost all C and C++ programs use undefined operations that are treated consistently by compilers, so programmers rely on those behaviors. The compiler makers are under no obligation to continue treating those operations consistently, and they often don't, which breaks existing code that worked just fine before. I understand the concept, but not what the author applies it to. How is creating an invalid pointer an undefined operation in C? Does the standard allow the compiler to treat the pointers in a different way than other data types, say, signed int? The problem is that almost all C and C++ programs use undefined operations that are treated consistently by compilers, so programmers rely on those behaviors. The compiler makers are under no obligation to continue treating those operations consistently, and they often don't, which breaks existing code that worked just fine before. Well then, almost all C and C++ programmers aren't future proof. If it's undefined in the standard, don't use it. How hard can it be to adhere to this really simple rule? I think the author's point is that even pieces of software that are generally regarded as well-written use undefined behavior all the time, so it's unrealistic to expect anyone to avoid it. I'm confused. Wouldn't this be an argument for saying that current C/C++ *compilers* aren't future proof? It seems like this has nothing to do with the languages' standards, but more with GCC and Clang.  &amp;gt; We ditch the C and C++, and port our systems code to Objective Ruby or Haskell++ or whatever.

Yeah, let's pick something obscure to use instead. &amp;gt; We ditch the C and C++, and port our systems code to Objective Ruby or Haskell++ or whatever.

Yeah, let's pick something obscure to use instead.  &amp;gt; many people expect that creating an invalid pointer (but, again, not dereferencing it) is harmless. 

I'd be interested in seeing what part of the standard forbids creating null pointers.  Just to elaborate, NULL pointers are valid pointer values.  Dereferencing a NULL pointer is undefined behavior, yes, but it is still a valid pointer value.

However, an uninitialized pointer will have an invalid pointer value, so will a pointer that has been deallocated using free.  As such the following is undefined behavior:

    int* x;
    int* y;
    x = y;  // Undefined behavior

This is also undefined behavior:

    void f(int* p) {}

    int* x;
    f(x); // Undefined behavior


Undefined behavior really means the compiler is free to do whatever it wants.  For an example of something unintuitive consider the following code:

    bool x;
    if(x) {
      printf("true");
    }
    if(!x) {
      printf("false");
    }

You might think that since x is a bool, even if it's uninitialized it can only be either true or false, but actually GCC can treat it as being both simultaneously or it can be neither simultaneously!  Depending on the compiler version and optimization flags, GCC will produce code that prints both true AND false, in effect it gets rid of the if statement's check.  Or it can produce an executable that prints neither true or false, in effect it optimizes out the entire if statement and its body.  Or it can produce an executable to prints one or the other.  Technically it can do whatever it wants given that performing a read operation on an uninitialized variable is undefined behavior. I don't believe a freed pointer would be invalid. Nothing forces you to use malloc/free. If you manage your memory yourself you just need sbrk. &amp;gt; many people expect that creating an invalid pointer (but, again, not dereferencing it) is harmless. 

I'd be interested in seeing what part of the standard forbids creating null pointers.  He's not talking about null pointers.  Null pointers are valid and initialized pointer with well-defined semantics (though, clearly, the can't be dreferenced).  What he's talking about is creating a pointer that wasn't obtained or initialized in a valid way (e.g. creating a new pointer with initializing to the address of a valid variable or to a null pointer). &amp;gt; Null pointers are valid and initialized pointer with well-defined semantics (though, clearly, the can't be dreferenced)

You could say that about any pointer. "This pointer is valid and has well defined semantics.... as long as you never use it." No, actually; that's the point he's making in the article.  Creating an invalid pointer (i.e. a pointer that doesn't point to a valid variable address or isn't a null pointer) *does not* have well defined semantics . . . it's undefined behavior, and the compiler may do whatever it pleases with the implicated code, *even if you never use the pointer*. What does the standard permit to do with pointers that is any different than, say, integers? I'm not intricately familiar with the C/C++ standards, but author's claim sounds to me like "the standard doesn't say anything about what happens when I assign 42 to a variable, so that behaviour is undefined, and is dangerous." What does the standard permit to do with pointers that is any different than, say, integers? I'm not intricately familiar with the C/C++ standards, but author's claim sounds to me like "the standard doesn't say anything about what happens when I assign 42 to a variable, so that behaviour is undefined, and is dangerous." What does the standard permit to do with pointers that is any different than, say, integers? I'm not intricately familiar with the C/C++ standards, but author's claim sounds to me like "the standard doesn't say anything about what happens when I assign 42 to a variable, so that behaviour is undefined, and is dangerous." What does the standard permit to do with pointers that is any different than, say, integers? I'm not intricately familiar with the C/C++ standards, but author's claim sounds to me like "the standard doesn't say anything about what happens when I assign 42 to a variable, so that behaviour is undefined, and is dangerous."    Suggesting we rewrite all our code is pretty ridiculous. In fact it's beyond ridiculous.

Nobody would ever fix a wheel because it might hit a rock 200 miles down the road. If you have code that works why would you rewrite it because of something that "might happen". 

Especially to suggest migrating it to languages so far from C. Why not one of the many Managed C like languages. Option 1 was *clearly* a joke on the author's part.  'Objective Ruby' and 'haskell++' aren't even real languages, and even their names sound like they're meant to evoke the notion of the oft promised but never delivered panacea languages.</snippet></document><document><title>Lightning strike recorded at over 7000 FPS. It's like a natural recursive pathfinding algorithm.</title><url>http://imgur.com/Fe8Lp</url><snippet>  High quality video:

http://www.youtube.com/watch?v=_1mB5rM8WHU That shows the negative "stepped leader", but not the positive [upward streamers](https://www.youtube.com/watch?v=6MUYsIjTKvk), which are eerie. It also shows an interesting extra path at the top.

Even more interesting, but much more rare, is [upward lightning](https://www.youtube.com/watch?v=RDDfkKEa2ls). High quality video:

http://www.youtube.com/watch?v=_1mB5rM8WHU [deleted] It's still a lot better than a crappy gif. [deleted] [deleted]  We keep comparing lightning to graph search algorithms, and there seems to be a disagreement over whether it is breadth-first or depth-first.  It's like BFS in the sense that it does several branches in parallel, but it's like DFS in that it doesn't go in all directions at the same time (it only branches sometimes, and when it does, it usually branches into only two paths).  It is also like gradient descent, except a) it doesn't have a discrete step size (except it actually does!) and b) it traverses several paths in parallel.

It seems more like [beam search](http://en.wikipedia.org/wiki/Beam_search) to me, where the beam width is analogous to the branching factor of the lightning.  But even this analogy isn't perfect, so maybe we should invent a new search algorithm called *lightning search*. I'd say it's parallel depth-first, with no limit on how many branches can be traversed at once. We keep comparing lightning to graph search algorithms, and there seems to be a disagreement over whether it is breadth-first or depth-first.  It's like BFS in the sense that it does several branches in parallel, but it's like DFS in that it doesn't go in all directions at the same time (it only branches sometimes, and when it does, it usually branches into only two paths).  It is also like gradient descent, except a) it doesn't have a discrete step size (except it actually does!) and b) it traverses several paths in parallel.

It seems more like [beam search](http://en.wikipedia.org/wiki/Beam_search) to me, where the beam width is analogous to the branching factor of the lightning.  But even this analogy isn't perfect, so maybe we should invent a new search algorithm called *lightning search*.  I believe the lightning strikes travels via [path of least resistance](http://en.wikipedia.org/wiki/Path_of_least_resistance). This is almost true, but not quite as the final path isn't the "best" solution, but rather the first solution found.

In more detail, each feeler segment sent out changes the state of the air (creating ionized paths), which will be followed by the main strike. Since lightning basically just wanders about always following the highest vector of electric potential until it hits the ground, the path it follows isn't optimal and doesn't improve. So it's more like a gradient descent search. I wouldn't call it a gradient descent search as that implies a  "step size" of some sort. I'd instead call it a generic greedy algorithm (of which gradient descent is a subset) that occasionally forks. You could probably model it as a breadth-first search with some conditions where you eliminate potential paths, or as a depth-first search with some conditions where you fork and follow multiple potential paths. Til lightning is bfs. Mind blown It's sort of a hybrid dfs-bfs (because it doesn't have to do them in sequence but can operate each branch of the tree in parallel). fucking quantic multicore The power requirements are ridiculous though.  This is almost true, but not quite as the final path isn't the "best" solution, but rather the first solution found.

In more detail, each feeler segment sent out changes the state of the air (creating ionized paths), which will be followed by the main strike. Since lightning basically just wanders about always following the highest vector of electric potential until it hits the ground, the path it follows isn't optimal and doesn't improve. So it's more like a gradient descent search. This is almost true, but not quite as the final path isn't the "best" solution, but rather the first solution found.

In more detail, each feeler segment sent out changes the state of the air (creating ionized paths), which will be followed by the main strike. Since lightning basically just wanders about always following the highest vector of electric potential until it hits the ground, the path it follows isn't optimal and doesn't improve. So it's more like a gradient descent search. This is almost true, but not quite as the final path isn't the "best" solution, but rather the first solution found.

In more detail, each feeler segment sent out changes the state of the air (creating ionized paths), which will be followed by the main strike. Since lightning basically just wanders about always following the highest vector of electric potential until it hits the ground, the path it follows isn't optimal and doesn't improve. So it's more like a gradient descent search. This is almost true, but not quite as the final path isn't the "best" solution, but rather the first solution found.

In more detail, each feeler segment sent out changes the state of the air (creating ionized paths), which will be followed by the main strike. Since lightning basically just wanders about always following the highest vector of electric potential until it hits the ground, the path it follows isn't optimal and doesn't improve. So it's more like a gradient descent search. I believe the lightning strikes travels via [path of least resistance](http://en.wikipedia.org/wiki/Path_of_least_resistance). So Breadthe first, then it flashes the solution to show off at the end. the etudes are all weighted by resistance so I think it's more like djikstras I believe the lightning strikes travels via [path of least resistance](http://en.wikipedia.org/wiki/Path_of_least_resistance).   No one seems to be guessing the algorithm this actually looks like. [Rapdily exploring random trees](http://en.wikipedia.org/wiki/Rapidly_exploring_random_tree).  If you think about it...everything that moves is like a natural recursive path finding algorithm. All the way down to the subatomic level! I've seen this gif so many times, but thinking about it like this blew my mind. O.o     What would happen if we removed the ground just as the searcher lightening parts came out to touch it? We'd probably get electrocuted.    Strange fact: During the "backstroke" (the big huge strike at the end), the electrons are actually still traveling from cloud to earth. The reason the electricity appears to travel in the reverse direction is because electricity is *motion* of electrons, not the electrons themselves. When the backstroke begins, the electrons closest to earth start moving first, followed closely by the electrons behind them, and so forth; thus, the *motion* of electrons travels from earth to cloud, much like the motion of traffic travels from front to back when a traffic light turns green.  It's a breadth first search. Most recursive pathfinding algorithms are depth first - It's a lot easier to do breadth first with a stack and a loop instead of recursion. No it isn't, if it were breadth-first it would be spreading out evenly in a "cloud" in all directions. If you must compare it to an algorithm, it's more like a depth-first search with some occasional parallel branching, somewhat weighted by electrical resistance. No it isn't, if it were breadth-first it would be spreading out evenly in a "cloud" in all directions. If you must compare it to an algorithm, it's more like a depth-first search with some occasional parallel branching, somewhat weighted by electrical resistance. It's a breadth first search. Most recursive pathfinding algorithms are depth first - It's a lot easier to do breadth first with a stack and a loop instead of recursion. BFS with a stack? A FIFO data structure, whatever you want to call it.  I suppose it's normally called a queue. A FIFO data structure, whatever you want to call it.  I suppose it's normally called a queue.           [deleted]</snippet></document><document><title>Exploring TLA+ with two-phase commit</title><url>http://brooker.co.za/blog/2013/01/20/two-phase.html</url><snippet /></document><document><title>Proof of equivalence between Turing-computability and URM-computability</title><url>http://www.reddit.com/r/compsci/comments/16xfef/proof_of_equivalence_between_turingcomputability/</url><snippet>Can anyone please push me in the direction of somewhere that can walk me through the proof of equivalence between Turing-computability and URM-computability?

Or if such a thing doesn't exist, could someone provide me with a link to a proof? I've tried searching by can't seem to find anything. 

Thank you!  I can't give you anything precise, because to be honest I've never heard of a URM before and I'm no longer fluent enough in computability proofs to give something short and compelling.  But here's some hand-wavey ideas anyway.

Looking at the definitions on [ProofWiki](http://www.proofwiki.org/wiki/Definition:URM_Program), it seems there are two differences between a URM and a TM:  1 - the URM has registers instead of a tape;  2 - the URMs registers use an infinite alphabet (the natural numbers).  Oh, one more:  3 - the URM's registers are disjoint from its program.

It seems pretty straightforward to simulate a URM in a TM.  1 is trivial: you can use an unbounded tape to represent an unbounded number of registers.  We can deal with that and 3 by observing that a 1-tape TM can simulate a 2-tape TM and separate the program and registers onto two virtual tapes - that's a common enough trick.  For 2, ProofWiki hints at the solution in the way it demonstrates that a UTM program must use a finite number of registers ... we know that a terminating UTM program does so in a finite number of steps, ergo the maximum number stored in any register is finite, ergo we can write it to a Turing tape.

Simulating a TM in a URM shouldn't be much more difficult, but if I try I'm sure to miss some subtleties.  "Church-Turing thesis!" *(waves hands and sprinkles fairy dust)*.

I'd like to see someone else do it though.
 </snippet></document><document><title>Grammar: The language of languages (BNF, EBNF, ABNF and more)</title><url>http://matt.might.net/articles/grammars-bnf-ebnf/</url><snippet>  you could say that BNF and the others are the lanugage of grammars, and therefore the language of languages of languages

now could we go one level higher and make some language to specify the various types of forms? well, BNF can describe itself and a wide-variety of other potential notations for grammars. and of course, EBNF can do the same and more. whether EBNF can describe all possible languages that describe grammars, I have no idea.  </snippet></document><document><title>Can someone explain "Web Services" to me in layman's terms?</title><url>http://www.reddit.com/r/compsci/comments/16wbtk/can_someone_explain_web_services_to_me_in_laymans/</url><snippet>I have an assignment for one of my classes when I need to do a 15 minute presentation on Web Services.
I don't completely understand it and unfortunately searching that term on Google tends to give me more about web hosting than what is considered web services.

I know it has to do with using XML to share information between different programs. It then transfers the data (XML data?) using SOAP.  After that I'm lost. 
Is this only used for websites? Are there any alternatives to this?

Thank you.

EDIT:
Thanks guys, there are a lot of great ideas and helpful information in here.
For clarification the class is about IT and is a business class.  It is the only IT class needed for the accounting program.  It is also only two weeks old, so I don't have a lot of the background knowledge now that I will at the end of the class.

Here's what I understand so far.  A webservice is something that will receive a request from one computer, it will then find the appropriate response, and send the response.
Whether this is me sending my e-mail/password login information, which then gets verified/denied by webservice.  This means that I don't need to host the information on my computer/servers.  This is especially important with online purchases and use of credit cards.  They can send the requests for verification of credit card information without ever seeing the details.  

Webservice is used for domain names too and is in nearly every aspect of our internet.  Apps would be big users of this as well.  It seems like Webservice is required for the internet and impossible to operate without it.

I don't think there is a plausible alternative to web service.  The topic itself is too broad to provide an alternative, but within web services there are SOAP and REST messages.

Is XML like HTML? It is a code based language? Or is XML just the ability to see things as they are written (see the actual code as opposed to the results)?


EDIT 2:
I'm done my assignment now. I want to say thank you to every for your help.  Although I'm far from an expert in this, I understand it enough to know the gist of what is going on.
Your examples and simplified explanations really helped.
Thank you very much.
I'm new to reddit so I don't know how to give karma, but I'd love you give you guys some.    At a really high level web services are about when you "get things" from some other computer over a network by passing is a message (an HTTP request, or something like that). Right now we are in the age of the internet so I guess you can imagine why this sort of decoupling would be useful :)

After that, people started adding extra features and protocols to their web services to handle common problems:

1.  How do you locate a service? (having a fixed URL isn't very flexible)

2.  How do you ensure the returned data is what you expect? (if your messages are XML there are many ways to verify the messages against schemas)

3.  And so on...

My advice would be getting a better idea of what in particular they want the presentation to be about. "Web services" by itself is just too broad. At a really high level web services are about when you "get things" from some other computer over a network by passing is a message (an HTTP request, or something like that). Right now we are in the age of the internet so I guess you can imagine why this sort of decoupling would be useful :)

After that, people started adding extra features and protocols to their web services to handle common problems:

1.  How do you locate a service? (having a fixed URL isn't very flexible)

2.  How do you ensure the returned data is what you expect? (if your messages are XML there are many ways to verify the messages against schemas)

3.  And so on...

My advice would be getting a better idea of what in particular they want the presentation to be about. "Web services" by itself is just too broad.    A Web Service is an interface that accepts input over the web to perform some action. They're basically around to help two different systems talk to each other using the HTTP protocol. Before web services if you wanted to talk to each other you'd have to pass files back and forth (this is still done quite a bit today unfortunately).

The only requirement for something to be a web service is that it works over HTTP. It could be a SOAP, RPC-XML or as you see more often now REST based web services.

You could have learned all of this if you opened up wikipedia instead of googling. Shocking incompetence on your part! I did open up Wiki and read a lot of stuff... guess what, I'm not an extremely technical person.  The reason I'm taking this class is because I need it for my accounting designation.
Why an accountant needs to know about Web Services, I have no clue.
If I wanted to be a computer engineer/web designer, I would have gone that route, but that's not my goal.
I'm sorry I don't know what XML, SOAP, UDDI or WSDL is.
I'm also not asking for you guys to do my assignment.  I wanted it in layman's terms so I could figure stuff out on my own, once I get the basic understanding.

Shocking of you to make these assumptions about me. Oh sorry you're not in comp sci and you need to know about web services? It's so you can sit in a meeting in 4 years and scream you need web services! I did open up Wiki and read a lot of stuff... guess what, I'm not an extremely technical person.  The reason I'm taking this class is because I need it for my accounting designation.
Why an accountant needs to know about Web Services, I have no clue.
If I wanted to be a computer engineer/web designer, I would have gone that route, but that's not my goal.
I'm sorry I don't know what XML, SOAP, UDDI or WSDL is.
I'm also not asking for you guys to do my assignment.  I wanted it in layman's terms so I could figure stuff out on my own, once I get the basic understanding.

Shocking of you to make these assumptions about me.             </snippet></document><document><title>Recursive Hash Function</title><url>http://69.195.124.79/~albertri/wordpress/2013/01/19/recursive-hash-function/</url><snippet>  Why did you give a link to an IP address? haha.. it's because i haven't had a domain name. I am currently in the process of buying of a domain name.  Thanks for sharing your blog with us. =)

I believe the solution you give for question 1 has some typos: line 3, the second x_2 should be x_2'; lines 4-5, you want to peel off the outer H_0's; line 6, the H_1 should be H_0.

After this, I think the proof is still incomplete, since the final line will only indicate a collision if y is not equal to y'. It may be the case that x is not equal to x' but still y = y', in which case there must be a collision among at least one of the pairs H_0(x_1), H_0(x_1'), or H_0(x_2), H_0(x_2'). Thanks for sharing your blog with us. =)

I believe the solution you give for question 1 has some typos: line 3, the second x_2 should be x_2'; lines 4-5, you want to peel off the outer H_0's; line 6, the H_1 should be H_0.

After this, I think the proof is still incomplete, since the final line will only indicate a collision if y is not equal to y'. It may be the case that x is not equal to x' but still y = y', in which case there must be a collision among at least one of the pairs H_0(x_1), H_0(x_1'), or H_0(x_2), H_0(x_2'). I have made a revision as well. Could you please take a look again? :D</snippet></document><document><title>Solving the journal problem... a project for the taking</title><url>http://www.reddit.com/r/compsci/comments/16tw6c/solving_the_journal_problem_a_project_for_the/</url><snippet>Hi, all,

Just want to post this in case anyone is looking to take on a big, worthwhile coding project. Nothing groundbreaking by itself, but we fused ideas from other places in a combination we haven't seen proposed elsewhere.

------------------------------------------------

The academic publishing industry is archaic at best, unethical at worst. Read commentary [here](http://michaelnielsen.org/polymath1/index.php?title=Journal_publishing_reform)

Below is our solution to this problem. It would be a huge undertaking to create it and we don&#8217;t have the requisite skills/time to do so. We just want to float it on the web so someone else might pick it up and start the process of making it a reality.

Crowd-sourced ratings systems, on a large enough scale, can be very effective (you're on reddit). We envision a publishing system where scientists upload novel research to a cloud-based platform in somewhat traditional manuscript form, including introduction, methods, results, conclusions, limitations, etc. Peers will evaluate these manuscripts by the same criteria that are used today. Diverging from today, manuscripts will be linked to underlying data supporting said manuscripts. The value of the manuscript will be decided by the peer community, rather than by 3-5 anonymous &#8220;experts&#8221; requested by the manuscript&#8217;s author or selected by the journal&#8217;s editor. Non-anonymous researchers will evaluate the manuscript for quality/importance, leaving a &#8220;score&#8221; and supporting discussion for posterity (comments that will also be given a score by reviewers). Manuscript, based on an algorithm, will be given a comprehensive score that will be dynamic (more about algorithm below). Good manuscripts will make the &#8220;front page&#8221;, their visibility will increase as they&#8217;re targeted to &#8220;high value&#8221; lists, and poor quality manuscripts will fall to the bottom. However, all manuscripts will remain visible/searchable, allowing an idea ahead of its time to receive proper recognition with age. Authors will be listed as now, and each author will be linked to a historical research profile. Each manuscript/comment score is factored into a historical user score, which will serve to weight the importance of comments left on other manuscripts/comments. 

In the end, manuscript submission is completely transparent, has supporting data, includes relevant critical commentary traditionally seen only by authors, eliminates the &#8220;third reviewer&#8221; phenomenon, rewards authors with novel ideas without regard to institution/financial/ backing/geography, and overcomes problems documented at the polymath project. It would have completely open access. It would eliminate submitter/reader fees, freeing information to all. It would have other perks, but you get the idea.

We envision this system laid out in a three-tier system with a &#8220;dashboard&#8221; type user interface. The &#8220;bottom&#8221; level of this system would include all data sets. Datasets would include relevant epidemiological characteristics. Datasets would be linked to citations of data within articles that&#8217;d identify which pieces of data, equations, and interpretations were used therein. Checking for accurate interpretation of data by peers would be simple and limits could be built into citation software prohibiting inaccurate data utilization. All new manuscripts would be tied to data in a dataset, though the dataset used might not be new. An independent user might identify something novel in a dataset not identified by the original dataset poster, and he/she could create a separate manuscript from it, giving full credit to the original submitter with data-links. Original posters would be rewarded each time a dataset was used successfully via historical user scores mentioned throughout this article (encouraging submission of data for community processing). This system would reward transparency and thoroughness at original submission.

The &#8220;middle&#8221; level of the system would include manuscripts and their associated comments. We envision this level being a split screen, allowing users to read the new manuscript, while older manuscripts cited in the new manuscript would come directly into view on the second screen (this could be customizable to allow several manuscripts and data, multiple manuscripts, and outside sources to be viewed simultaneously for cross-referencing). Users could read new manuscripts, give them a rating, and leave comments with citations in the form of discussion here. Comments could address issues with new manuscripts, or the data underlying new manuscripts, directly. Consequently, rather than discrete manuscripts, new information would be disseminated as a novel dialogue, allowing important commentary on new ideas to be included in the historical record (ensuring accountability for new information, providing context, and facilitating quicker synthesis of new ideas with older ideas). As above, good manuscripts/comments would float to the top via a strong score, rewarding the submitter and the scientific community.

The &#8220;top&#8221; level of this system would be an information synthesis similar to Wikipedia. This level would allow users to cite and synthesize information from manuscripts and comments (as well as, initially, from outside sources) in discrete articles &#8220;about&#8221; scientific topics. This level eliminates the need for independent external publishing sources, as all discrete information at this level could be linked directly to relevant manuscripts/comments discussing it, making &#8220;truth&#8221; easily linked to primary sources. Again, users would be credited by the user algorithm for their contributions here.

Underlying this process is an algorithm accounting for the spectrum of user contributions. A user&#8217;s individual score would account for all of his historical contributions to the project, including scores for manuscripts provided, when/where/how often each manuscript is cited by other users, constructive comments left, and datasets submitted. Credentials (appointments, higher education, etc), similarly, could be accounted for in the individual user score. Value associated with comments he/she leaves on other manuscripts would take historical user score into account, weighting his/her comments fairly for historical contributions. Similarly, new manuscripts submitted by a user would receive higher/lower baseline ratings based on historical contribution. We envision this score being included on academic curricula vitae, replacing absurdly long lists of publications/abstracts that give no indication of the quality of individual manuscripts.

The beauty of this system is that it allows complete transparency while offering each user a historical rating assigned by his peers. It would include translation software, eliminating linguistic barriers. It would allow new literature to be targeted directly to user preferences, allowing users to customize lists of manuscripts they would like to read and review by key words, discipline, user score underlying a new manuscript, &#8220;hot&#8217; manuscripts receiving a lot of attention from readers/commenters, country/state/neighborhood of origin, etc. It would facilitate streamlined and effective literature searches, allowing users to search for key words across the entire body of scientific knowledge, including comments and datasets. It would force improvement of data quality by making data subject to inspection along with commentary on that data. It would facilitate high quality meta-analyses by allowing users to compile original data for processing, rather than processed data from "peer-reviewed" articles. It would eliminate siloed departmental research by opening commentary to all disciplines and encouraging interdisciplinary collaborations. And much, much more...

This system could be supported financially by grants, government funding or other charitable contributions. Alternatively, we envision this system having the potential to support itself (and be quite lucrative) through advertising revenue. Because a user&#8217;s entire historical research archive would be posted, his/her search and review history would be available, his biographical sketch posted, his discipline noted, advertisers would have a wealth of information for targeting. For example, a device manufacturer selling pipette tips would know that user X recently posted a dataset in which 30,000 pipette tips were used. That company would also know that user X searched for manuscripts from users completing similar research. Consequently, it would be clear that user X purchased many, many pipette tips and advertising could reflect this.

As I said, this is not a comprehensive characterization of the project. However, it offers a good place for someone to start.

If anyone wants to take it on, we&#8217;d love to help.   I would be very hesitant to include ads. This needs to be seen as a serious Scientific endeavor to get traction.   You addressed many of my questions in your post, but I do have one specific concern.  If a paper is submitted which includes ideas that are very popular and provides support for the popular views on a topic but is based on an argument which has a small but fatal flaw, how would it be handled?  The paper would likely get a very high score from mostly everyone, and an objection naming the flaw would likely be downvoted.  On the one hand, I am comfortable with relying in part of the good will of the participants.  We are currently relying on the good will of participants in the journal system with far less accountability and far graver consequences.  On the other hand, we are faced with the simple truth that a single flaw in an argument invalidates the entirety of the argument that relies upon it.  For instance it has recently come to light in the past few years that there are large numbers of accepted papers that rely upon statistically invalid analyses.  Those papers advocate popular positions, and present ideas which very well might be correct, but they do so in a way that is invalid.  

Would there be a way to flag a paper as "popular but invalid"?  Or would there be means for submitters to submit updated versions of their paper?  I see no reason why we need to cling to the limitations of paper, and one of the major limitations of paper is that when something is printed and distributed, it is fixed.  Online we aren't limited like that.  This obviously creates complications in terms of citation.  What if your paper cites something that cites something that is revealed as flawed?  In the paper world, it is a true mess and very difficult to address.  In the digital world, we could design things to enable handling this.  I think rather than presuming that all papers included in the system are generally legitimate and reliable, we'd need to keep in mind that things change over time, things are revealed as unreliable or incorrect, and those changes ripple through the system.

I really want to see a project like this happen, though, I think it would benefit humanity immensely.  A few academic friends and I had basically the same idea a few months back, but we never followed through with it. It's nice to know that there are other people who also think this is a great idea.

Why we didn't do it: we have no means of making it popular, we are just some CS students. Coding it is not the problem, hosting it neither until it gets famous and our machines break done ;)

So yeah, we are still interested in doing it. A few academic friends and I had basically the same idea a few months back, but we never followed through with it. It's nice to know that there are other people who also think this is a great idea.

Why we didn't do it: we have no means of making it popular, we are just some CS students. Coding it is not the problem, hosting it neither until it gets famous and our machines break done ;)

So yeah, we are still interested in doing it.   This sounds like a horrible idea. Popularity should not be the criteria for publishing/vetting scientific research. It already is. that makes it okay No, that's not what I meant.  I meant that though this system does not entirely remove the influence of popularity, the fact that it solves many of the other problems present in the current system is important.  Presuming that the current journal system protects us from popularity being the criteria for publishing/vetting research is an incorrect idea that a lot of people rely on.  There's really no way for us to avoid it, when it comes right down to it.     </snippet></document><document><title>Floating point quirks: how summing a series backwards gave 4 orders of magnitude more accuracy</title><url>http://www.phailed.me/2013/01/floating-point-quirks/</url><snippet>  That's why you should pay attention in numerical methods class.  The same problem happens when you subtract two numbers that are very close together. We need to pay attention to the relative magnitude of the result too.</snippet></document><document><title>"I will consider the job finished when no manufacturer anywhere makes a PC with a reset button."</title><url>http://www.cs.vu.nl/~ast/reliable-os/</url><snippet>  Written by somebody who's evidently never incorrectly programmed a DMA device.  ... Which, given who's doing the writing, is rather surprising. Written by somebody who's evidently never incorrectly programmed a DMA device.  ... Which, given who's doing the writing, is rather surprising.  [deleted] &amp;gt;Nobody has the chance to accidentally install 52 different Toolbars to their car.

*yet* http://imgur.com/H6YdO [deleted] As someone writing software for cars, I can say that we've actually built in automatic resets. Lots of watchdog timers and every couple of days a pre planned reset. 

I think the big difference is that the user can't fuck much up by installing stuff. &amp;gt; and every couple of days a pre planned reset

Wait, so what if I drive for days straight, without shutting my engine off for refills? :O
 Your car would blow up!  Don't you read the signs at the gas station? Your car would blow up!  Don't you read the signs at the gas station? [deleted] That's not really his premise. Two sentences later he says:

&amp;gt; but users just want them both to work and don't want lectures why they should expect cars to work and computers not to work.

Your comment is said "lecture".

&amp;gt;I want to build an operating system whose mean time to failure is much longer than the lifetime of the computer so the average user never experiences a crash.

Now that's his premise. Of course it's hard, but that is why he's so passionate about developing OS design to achieve that degree of reliability and robustness someday. The objective is a design that can manage all that complexity, not one that avoids it. A system that remains standing in spite of stupid users (that's why it's self-healing). &amp;gt; Your comment is said "lecture".

I will stop lecturing people about the differences between single purpose devices and computers, when they are happy using only the OS supplied programs, without any additional software.

No flash, no firefox, no thunderbird, no outlook, excel, libre office, word, acrobat reader, VLC, media player classic, games, or any other software that you can't select during initial installation. &amp;gt; Your comment is said "lecture".

I will stop lecturing people about the differences between single purpose devices and computers, when they are happy using only the OS supplied programs, without any additional software.

No flash, no firefox, no thunderbird, no outlook, excel, libre office, word, acrobat reader, VLC, media player classic, games, or any other software that you can't select during initial installation. If it were impossible to have a sufficiently reliable full-fledged computer, we wouldn't have them masquerading as single-purpose devices as we do today. Making them as reliable as the single-purpose devices of old has been accomplished. A whole industry of smartphones and tablet devices also disagrees with you. They get browsers, games, media players and all that good stuff with spectacular stability.  If it were impossible to have a sufficiently reliable full-fledged computer, we wouldn't have them masquerading as single-purpose devices as we do today. Making them as reliable as the single-purpose devices of old has been accomplished. A whole industry of smartphones and tablet devices also disagrees with you. They get browsers, games, media players and all that good stuff with spectacular stability.  To me walled garden device are a big step towards these single purpose devices. Yes you can still install custom software, but this software has been pre-screened and approved. - And yet most of these devices still have a way to reset them. If it were impossible to have a sufficiently reliable full-fledged computer, we wouldn't have them masquerading as single-purpose devices as we do today. Making them as reliable as the single-purpose devices of old has been accomplished. A whole industry of smartphones and tablet devices also disagrees with you. They get browsers, games, media players and all that good stuff with spectacular stability.  That's not really his premise. Two sentences later he says:

&amp;gt; but users just want them both to work and don't want lectures why they should expect cars to work and computers not to work.

Your comment is said "lecture".

&amp;gt;I want to build an operating system whose mean time to failure is much longer than the lifetime of the computer so the average user never experiences a crash.

Now that's his premise. Of course it's hard, but that is why he's so passionate about developing OS design to achieve that degree of reliability and robustness someday. The objective is a design that can manage all that complexity, not one that avoids it. A system that remains standing in spite of stupid users (that's why it's self-healing). [deleted] [deleted] [deleted] Maybe if you were a world leading computer scientist someone would care what you thought on the matter.  [deleted] TheGoomba [-1] -1 points 2 hours ago (1|2) [deleted] &amp;gt; 1) like I give a fuck

You obviously give a fuck. You are crying like a stuck pig. 

&amp;gt; 4) you aren't shit either, so don't pretend you are

I am not claiming I'm right because of my personal, and limited experience.  [deleted] [deleted] [deleted] I don't understand. User space programs, the things that users interract with and install, should never cause a system halt that would require the press of a restart button.

Having an operating system that doesn't allows user space programs to take it (or other programs on it) down, to the point of needing a force reboot, is the goal.

Having user space programs, written by bad programmers, not crash themselves is not the goal. If you run bad code that can't handle what you throw at it, it will crash. I remember reading a while ago that at one point, Microsoft was working on a highly secure OS that would only run programs if they could be statically proven not to segfault, and thus did not actually need proper memory protection. You're talking about the [Singularity](http://en.wikipedia.org/wiki/Singularity_(operating_system\)) operating system with its "software-isolated processes". Singularity is also a microkernel operating system and was mentioned in OP's article. [deleted] [deleted] [deleted] [deleted] [deleted]   That essay is almost 7 years old. and therefore ... ?

Are PCs still being sold with reset buttons? When is the last time you've actually used yours? Yesterday. It turns out that present-day machines become nearly unusable when you accidentally attempt to create 50,000 threads. /r/NoShitSherlock Yes shit, Watson

[/r/Haskell](http://www.reddit.com/r/Haskell)

[/r/Erlang](http://www.reddit.com/r/Erlang)
 When is the last time you've actually used yours? Twice in the last week when my computer locked up, probably due to crappy razer mouse drivers. 

When AMD fucked up their drivers I had to use it at least twice a day for two months.

When I was new to GPU computing I probably used it at least 10 times a day because I was locking up my GPU Would a microkernal have fixed these issues?  This essay is about microkernals not computer failures in general.

What would have fixed your crashes is not a microkernal but rather user-mode drivers which you can have with a monolithic kernel. It would fix the driver issues.  It would fix the driver issues.  Would a microkernal have fixed these issues?  This essay is about microkernals not computer failures in general.

What would have fixed your crashes is not a microkernal but rather user-mode drivers which you can have with a monolithic kernel. When is the last time you've actually used yours? Last night.  The program being used was a web browser.  Lock ups can and do happen all the time. Strange that your web browser was able to lock up your entire OS.  But these days failures due to hardware and drivers are more common than software failures in the OS kernel (this essay was about microkernals - so a crash from something a microkernal wouldn't have fixed is irrelevant).  

Its impossible to know the cause of yours last night without a full core dump but its probably a hardware failure. Well, it's a lockup, who knows what actually locked up?  All I know is that it happened the precise instant I went to click on a web page control object.  It might even have been an upvote here on reddit.

It's equally impossible to support your suggestion.
 Fortunately for me research suggests it was much more likely to be a non-kernal issue such as a driver or faulty hardware.

For reference:
http://downloadsquad.switched.com/2008/03/28/29-of-windows-vista-crashes-caused-by-nvidia-drivers/

http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.6918&amp;amp;rep=rep1&amp;amp;type=pdf Makes no sense to say a crash because of a driver isn't a kernel issue. It's got everything to do with the kernel. The fact that a driver can bring down a whole monolithic kernel is the central argument in favor of microkernels.  Well more specifically it isn't a monolithic kernel issue, a micro kernel would have the same problem.  The solution for both is user mode drivers, so if it fails due to a driver crash it is not evidence against monolithic kernels.

EDIT: Its been a while since I've researched microkernels.  What about them prevents a faulty driver from crashing the whole kernel?  Does each interface exist within its own separate address space?  Even so a DMA device could bypass that by having the device spew garbage randomly into memory. When is the last time you've actually used yours? When is the last time you've actually used yours? Do you think an essay (any essay) is not relevant because it's 7 years old? The age of the essay certainly affects its relevance.  7 years ago it was frequent for my computer to crash - today it is not.  The fact that we didn't need microkernals to get there acts as a counterpoint to the essay.  The essay speaks to a problem that no longer exists. Computer crashes are not as frequent, but they still happen. 

I still think the essay is relevant and worth reading for historical content at least.

Let the votes fall where they may. Oh I agree its interesting for its historical context.  I will argue with you about stability - an OS crash for me has been very rare.  These days a crash is usually the result of faulty hardware or bits getting flipped in RAM (one day ECC will be standard for desktops).  A microkernal will not help you with that.

I have Linux servers with years of up time on them.  A kernel crash is a very rare thing these days though I can't say it never happens. The age of the essay certainly affects its relevance.  7 years ago it was frequent for my computer to crash - today it is not.  The fact that we didn't need microkernals to get there acts as a counterpoint to the essay.  The essay speaks to a problem that no longer exists. &amp;gt; The fact that we didn't need microkernals to get there acts as a counterpoint to the essay. The essay speaks to a problem that no longer exists.

The problem is that a monolithic kernel will be brought down if any component driver not only fails, but fails catastrophically enough to harm other parts of the system. This still happens. It happens less because drivers have become more mature in both Windows and Linux. For example, fglrx 12.11 beta 7 on Ubuntu 12.10 would fail catastrophically and cause a kernel panic for a few weeks.

The argument is still valid. A microkernel can monitor its peers and revive them if they go down. A monolithic kernel has to try to internally contain damage if some part misbehaves. A much better solution is user-mode (or more generally) contained drivers.  Which is really a hybrid solution.  This is the path most real world kernels have taken.

You get the performance benefits in the core kernel which has now become very stable, and the stability benefits in the most volatile parts of the kernel which is the drivers. The age of the essay certainly affects its relevance.  7 years ago it was frequent for my computer to crash - today it is not.  The fact that we didn't need microkernals to get there acts as a counterpoint to the essay.  The essay speaks to a problem that no longer exists. and therefore ... ?

Are PCs still being sold with reset buttons? Yes. and therefore ... ?

Are PCs still being sold with reset buttons? My laptop doesn't have one. The point is that not that it has a physical reset button, but that the OS should never fail to the point that it requires physical restarting by either a reset or power cycling.

Reset on laptops is provided by the power switch in most cases but sometimes the battery needs to be removed to fully reset the device.

Talk to sysadmins about this. Some laptop problems can not be fixed unless the battery is physically removed.

I've had this problem with USB host controller chips and Windows. If Windows detects "babble error" on the USB line, it will tell the USB host controller to ignore the USB port. Port stops working, and won't work again until you fully reset the whole motherboard. Linux doesn't seem to do this.  CTRL-F .... "halting problem" ... 

&amp;gt; Phrase not found.

Alright, Andy, you're a smart guy. Do you know what the scope of "the job" is that needs to be done in order to obviate the reset button? CTRL-F .... "halting problem" ... 

&amp;gt; Phrase not found.

Alright, Andy, you're a smart guy. Do you know what the scope of "the job" is that needs to be done in order to obviate the reset button? Yes, he is a smart guy. Making a system that never crashes means solving the halting problem, and can't be done. That's why he's only talking about designing systems that probably won't crash for a kinda long time.  I'm sure Tannenbaum's a much smarter person than I; and his work is laudable at the very least. But my issue with that statement, which the submitter took as the title for this post, is that PC's and the computer in a car or stereo aren't just sorta-different; a PC is a **general purpose** computer; its hardware is expected to interface with other manufacturers', and to run any software for any purpose. I've had linux computers with uptime over 700 days; NetBSD and Solaris boxes with more than double that; hell, even my Windows 7 laptop has only blue-screened once in 6 months, and been rebooted about 10 or 15 times. I'd say we're in quite better shape all around than we were in 2006. But we should expect our computers to crash to the extent that we are able to get them to run any set of instructions we please.

I really don't want to get into this whole micro- vs macro- kernel debate that's as old as computing and finds no consensus among the field's top minds. But, I will point out that those who have architected their systems around uncertainty have found a lot more popularity than those who lament that the world doesn't conform itself to their perfect design. Yes, he is a smart guy. Making a system that never crashes means solving the halting problem, and can't be done. That's why he's only talking about designing systems that probably won't crash for a kinda long time.  &amp;gt;Making a system that never crashes means solving the halting problem, and can't be done.

Can you elaborate on that? I agree that its impossible to make a general purpose system where no programs, ones supplied by the user included, ever misbehave. However, this does not mean its impossible to make a kernel such that the whole system never needs a reset. Despite the halting problem it is possible to prove correctness (e.g. that a piece of software always makes progress) for specific instances of software. [Here is one project developing automated methods for doing just that.](http://research.microsoft.com/en-us/um/cambridge/projects/terminator/) CTRL-F .... "halting problem" ... 

&amp;gt; Phrase not found.

Alright, Andy, you're a smart guy. Do you know what the scope of "the job" is that needs to be done in order to obviate the reset button?  </snippet></document><document><title>Converting regular expressions to finite-state automaton</title><url>https://www.youtube.com/watch?v=GwsU2LPs85U</url><snippet>   </snippet></document><document><title>Just started a degree program for working adults - B.S. in Computer Science Technology.  I've learned there are upcoming changes to the program.  Help please?</title><url>http://www.reddit.com/r/compsci/comments/16r82j/just_started_a_degree_program_for_working_adults/</url><snippet>First off, [here is the page for the degree program; The School for Professional Studies at Saint Louis University](http://www.slu.edu/school-for-professional-studies-home/majors-and-programs/bachelors-degree-programs/bs-in-computer-science-technology).  The School for Professional Studies is aimed at working adults, and the classes are accelerated so that a course is finished in an 8-week term rather than a whole semester.

I of course am going to pose these questions to the department chair, but I'd love to get some impartial info from Reddit.  

I've learned that, at some point in the future, they are going to be making some changes to this program.  First is a name change - the Computer Science Technology degree is going to become known as a Bachelor's in Computer Information Systems.  

They are also adding new degree tracks - I can't remember them specifically, but there was one for IT security, one for healthcare systems, and two others.  I don't know if they are going to replace the 3 existing tracks, or just be added to them.  

I've spoken with some other new CS students I met at orientation, and we are all in agreement that there are some important questions we want answered.


-Do you think there is a reason that SLU's degree is known as Computer Science Technology, and not simply Computer Science?  Would this mean that the degree is weaker and less-skilled?  If not, is there perhaps still that perception from an employer's side of things?


-We learned that all of us already enrolled in the program will be grandfathered in, and so by the time we earn the degree it will have changed to Computer Information Systems, but we'll all still have the option to have our degrees be in Computer Science Technology.  What do you think is the better choice in this regard?


-The department chair told us at orientation that the reason for these changes to the program is that they are pursuing accreditation, which I guess they don't have currently.  Is this an important distinction (i.e., if they don't end up obtaining accreditation, will my degree be considerably weakened from an employer's perspective)?


-The other students and I agreed that a degree in Computer Information Systems does not sound like it carries the same weight and mastery as one in Computer Science, or Computer Science Technology.  Is this perception just flat-out incorrect, or what?  Logic tells me that if this name change is what the accreditation board wants, then perhaps they know that a degree in CIS is actually more forward-focused and will be something employers want to see.  Can anyone speak on this?


-Any other good info that I should know?  


I don't have any really focused ideas on what I want to use this degree to do - I have a wide range of interests and I know that I can become passionate about almost anything if I know the skills I'll gain will make me valuable in that way.  I guess I'd say that I'm willing to go in whichever direction would give me the most earning power and long-term employability.  I'm 29 now, so I've got to make sure I'll still be a competitive job candidate as I age.  

Those are my concerns, and I'm incredibly thankful for any information that my fellow redditors can provide.  Thank you so much!  And sorry for the wall of text.

**TL;DR Want to make sure my degree will be valued.  Also have questions about the direction I should take.**  &amp;gt;Do you think there is a reason that SLU's degree is known as Computer Science Technology, and not simply Computer Science?

Departmental politics.  Nothing more.  There's probably someone on the committee who feels that the programme isn't enough theory of computability and pure maths to just call it computer science or they want to emphasize the technology contribution in their programme or some other damn thing.  I wouldn't worry about it.  Countless hours have been wasted debating Computer versus Computing science etc.  


&amp;gt;Would this mean that the degree is weaker and less-skilled? If not, is there perhaps still that perception from an employer's side of things?

Unlikely to matter much.  You can look at the syllabi for 3rd year courses and make sure you're getting a standard comp sci equivalent programme and go from there.  


&amp;gt;-We learned that all of us already enrolled in the program will be grandfathered in, and so by the time we earn the degree it will have changed to Computer Information Systems, but we'll all still have the option to have our degrees be in Computer Science Technology. What do you think is the better choice in this regard?

Personally I would choose CS technology, since CIS sounds more like its database focused but it's basically irrelevant in broader marketplace.  I say this with the place I did my CS undergrad being a "CIS" school though.  Grad schools don't care, employers in the local area don't care. 

&amp;gt; (on accreditation) Is this an important distinction 

Not hugely, but it can't hurt.  

&amp;gt; Is this perception just flat-out incorrect

The perception the two are hugely different would be incorrect.  The perception that people would tend to think having the proper phrase "computer science" in it is preferable is true. 

&amp;gt; I'm 29 now, so I've got to make sure I'll still be a competitive job candidate as I age.

You're not hugely disadvantaged then. 



Honestly, you're a bit too focused on the details for your own good.  It sounds weird to say you're asking too many questions when you're talking about 10's of thousands of dollars and thousands of hours of your time.  But at the end of your degree programme you'll show up at an employers door, and they are going to give you some basic proficiency tests in whatever you are trying to do for a job, and go from there.  

Programmes are always changing, new faculty do new things, old faculty try different things, times change, administrations change, someone decides they need to justify their chairmanship somehow.  In the grand scheme of things it's mostly about keeping students and faculty interested, when you go off to the job hunt having a degree with some words related to computers on it and you're golden.  I just went through a long process of researching school for an IT track and I found that there is in fact a reason that it is called comp sci technology. 

This is NOT a comp sci degree. I can almost gaurantee it does not have a heavy math rotation (that includes diff eq, linear algebra, calc 4) or any engineering elements. The reason that they are changing the title of the degree is probably because CIS fits it better. Comp Sci Technology is just a generic Comp Sci, but CIS can stand on its own with some business/management track in there. Comp sci doesn't require calculus or differential equations or anything past a second linear algebra anymore to still be allowed to be called comp sci (by whatever the agency the ACM farms this accreditation off to).  And I'm not sure it even requires linear algebra.  

I think where I am we still require first year calculus, and 2nd year linear algebra but that's it.  And I know a few schools that have dropped the first year calculus requirement.  

Comp sci really boils down to:

* Languages (including functional and logic)
* Algorithms (data structures and complexity classes and theory)
* Programming paradigms (Software engineering)
* Theory of Computation (theoretical computing machines and computability)
* Computer architecture/hardware/organization
* Operating systems (what they do, not Windows Mac Linux)

You need all of the maths relevant to understand those topics, but not much more.  Now, the school in question might not have a full treatment of a couple of those, and well, then you can't get accredited. Can you point me to the school that has a comp sci degree that does not require math beyond first year calc? i would like to confirm this. I didn't say no math, I said not anything beyond first year calc (as in no second year or 3rd year calculus/DE requirement and no non linear algebra or the like). 

You usually need discrete structures and stats.

But, for example

http://www.uoguelph.ca/registrar/calendars/undergraduate/current/c10/c10bcomp-sofs.shtml

or a bigger school, the University of Toronto

http://www.artsandscience.utoronto.ca/ofr/calendar/crs_csc.htm

recommends, or requires selection from one of several maths courses, and some specializations require DE's but you can avoid most of it after first year if you are so inclined.  

As much as I think a lot of students would benefit from linear algebra and DE's, we'd probably half our enrollment if we required those, or we'd have to pick up a lot more foreign students.   Wow that is the first I have seen. That being said it is still much more math than the comp sci tech programs I have seen. Those only required Trig like CIS programs.

These are also Canadian schools and after taking a longer look it seems as though the programs are decidedly different than what is normally offered in America. 

Thank you for the sources though.  I wouldn't personally worry about the degree name. I graduated from my school's "Information Technology and Web Science" program. My school is the first to offer Web Science in a degree, but I still can't explain it. (The professor that taught the course is heavy into research, not a great teacher though.) Anyway, I had no problems getting job interviews, and ended up interviewing at many of the same places as my friends with Computer Science majors. It ultimately comes down to your ability to do the work desired. It didn't seem like interviewers cared much about the degree name.

Edit: At my school, Comp Sci was almost entirely theory based, with only one real class with a major group project. The IT program focused more on group projects and working with real world clients to deliver real projects. I feel that that was far more valuable to my career than any of the theoretical courses I took. For example, I haven't used a single thing from my Intro to Artificial Intelligence, or algorithm analysis classes (the comp sci theory courses), but I've used a lot from my IT courses that are based on the actual project work. So if they offer coursework in that, I think it's ultimately much more valuable.  Good to know, thanks.  The reason your school's program is called Computer Science Technology and why the name of the program is being changed to CIS in order to get accredited is because it is not a full fledged CS program. From reading through the list of courses offered and required your college is just offering a glorified IT degree. Yes you learn some of the basics of computer science but you don't go far enough in for it to be a real computer science degree. Just compare the courses offered at your school with any respected computer science degree program and you will see that it is much more intensive.   &amp;gt; Do you think there is a reason that SLU's degree is known as Computer Science Technology, and not simply Computer Science? Would this mean that the degree is weaker and less-skilled? If not, is there perhaps still that perception from an employer's side of things?

It is not a Computer Science degree because it is not accredited as a Computer Science degree. CS degree programs have certain material they must cover at a certain level in order to qualify for accreditation. 

As for how strong or weak the degree is, that really depends on the employer, how much they value an accredited CS degree, how well you represent yourself, and how good of a reputation your school has. Even amongst schools with accredited CS programs, there is a big difference in how employers view those schools. Some schools have a reputation for being very theory based and students having a tendency to come out of of college not really knowing much about programming. Other schools have a reputation for being more hands on, while some schools have a reputation for just being crap.

And that is just for accredited CS degrees!

&amp;gt; -The department chair told us at orientation that the reason for these changes to the program is that they are pursuing accreditation, which I guess they don't have currently. Is this an important distinction (i.e., if they don't end up obtaining accreditation, will my degree be considerably weakened from an employer's perspective)?

Accreditation means an outside organization has gone through the program and its teaching materials and ensured a certain level of rigor. If that level of rigor is high enough that you want it to represent your skill set, then good! If not, you may want to seek another program. Alternatively, you can always demonstrate your skills to employers through alternative means on top of getting a degree. 

Honestly your degree matters for your first development job. The quality of that first development job is what really matters there on out. (To an extent of course, having "PhD PhD from a top tier university" on ones resume is naturally going to make one look good!)

&amp;gt; The other students and I agreed that a degree in Computer Information Systems does not sound like it carries the same weight and mastery as one in Computer Science, or Computer Science Technology.

IMHO, if I am doing an interview, the order I evaluate a resume is

1. Previous job experience. 
2. Independent projects
3. Name of degree

So yes, if #1 looks like crap and #2 is non-existent, then #3 can come into play.

The basic fact is a Computer Science degree does not prepare one to be a Software Engineer. A CS degree gives one an opportunity to prepare to be Software Engineer. This sort of sucks, basically while learning all the theory you have to then go and pick up the practicals on your own.

But here is the alternative view. If you have a degree that is more focused on using technology, employers are going to have *very* legitimate fears that you may not have the base underlying ability to learn new technologies. Computer Science degrees teach fundamentals that allow for further learning. Degrees orientated towards "this is how you do Javascript" do not necessarily teach that. Of course with any degree it is up to the individual if they are willing to go out and learn on their own. A non-CS degree just kinda-sorta hints at the individual being more focused on technologies rather than fundamentals, but again, you can **easily** disprove that by working on independent projects, preferably ones that don't use any technologies you learned in school, thus demonstrating that you are capable of learning on your own.

&amp;gt; I have a wide range of interests and I know that I can become passionate about almost anything if I know the skills I'll gain will make me valuable in that way. 

To summarize what I said above, a good degree does not teach skills, it teaches you how to learn new skills throughout the rest of your life.

 &amp;gt; If you have a degree that is more focused on using technology, employers are going to have very legitimate fears that you may not have the base underlying ability to learn new technologies.

In my experience, those who do not have the fundamentals provided by computer science theory tend to write poorer quality code.  There is a huge difference between it's done and it's done right.

edit: clarification - I meant to write poorer quality and I don't mean to imply that this is 100% true. &amp;gt; In my experience, those who do not have the fundamentals provided by computer science theory tend to write poorer quality code. There is a huge difference between it's done and it's done right.

This is often true. My current job is a mix of Electrical Engineers, Computer Engineers, and 2 CS guys.

Even amongst trained professionals, there is a HUGE difference in the style of writing code. (For one thing, the CE and EE people are making everything global, and completely ignoring the "class" keyword in C++!)

That said, a CS degree is NOT an assurance that one can write good code. Likewise a CS degree is NOT a prerequisite to writing good code!

Being passionate about the field and spending a lot of time learning on ones own are the two largest requirements!

All in all, I prefer people who are willing to admit their mistakes and who are willing to learn. If someone is willing to learn (and, well, not dumb as a doorknob), then they can be taught how to do things properly. Someone who refuses to improve is going to eventually become useless, no matter what their degree has printed on it.</snippet></document><document><title>Invited to talk to a 3rd grade</title><url>http://www.reddit.com/r/compsci/comments/16ov26/invited_to_talk_to_a_3rd_grade/</url><snippet>Hello /r/compsci

My wife is a 3rd grade teacher and I have a computer science degree... this day was inevitable.  She asked me to speak to her classroom.

Looking for some help with ideas of things to talk about.

This is what I have floating around my head so far....

Basically, I want to tell them that computers are dumb machines but very good at following directions.  Computer scientists compose those directions in the form of programs which end users interact with.
When the users interact with the program by moving a mouse, clicking a button, pressing the keyboard or touchscreen, shaking or tilting their phone.... someone with a degree like mine wrote the instructions behind it that make your computer do what it does.

Then as an example of a programming task I was thinking about introducing them to the Bubble Sort algorithm.
Obviously I'd make the algorithms / recipes analogy so the word algorithm doesn't throw them off.
I'd also explain the many reasons why you'd want things sorted in a computer... for obvious reasons, and for parts of bigger problems.
Surely they could understand finding song they want to play from a sorted list, or knowing who has the highest score, etc.
I would start by explaining the algorithm in English (possibly side by side with pseudocode or Python).
Then I would act it out with pieces of cardboard...re-arranging them on the blackboard.

Perhaps afterwards I'll have 10 kids stand in line facing the class each holding up a number.
I could then have another kid act as the CPU telling the other kids what to do.
I'm not sure about them acting it out... I'll have to ask the wife if she thinks they'd be able to handle it.

I'd finish it off showing them the youtube video of the hungarian folk dancers performing Bubble Sort.

Do you guys think this is a good idea?
Any suggestions or comments are welcome.

Thanks,
~Eric  Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Someday some kid will say things like "store location of peanut butter into memory; ask input device for location of bread;"     MOV %EBX PB
    MOV %EAX BREAD
    ADD %EAX %EBX
    EAT %EAX

(I realize now how little assembly I remember...) Java:
import pbj_sandwich From my experience with Java it would be more like:
import java.noms.sandwich.pbjsandwich;     import com.reddit.service.user.food.*;
    import com.reddit.service.user.naff.*;
    import java.utils.*;

    public class PBJSandwichFactory implements AbstractFactory, AbstractObjectContext {
        public static Object create() {
            Nommable sammich = new SandwichFacadeContextFactory().build();

            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.KNIFE),
                new Ingredient(new IngredientFactory().mix(Ingredients.PEANUT, Ingredients.BUTTER)));
            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.SPOON),
                new Ingredient(new IngredientFactory().flavor(Ingredients.JELLY, Fruits.GRAPE)));

            sammich.wrap().marshall().wrap().unmarshall().unwrap();

            sammich.nom(); // MY sammich! Calling function doesn't get any damn it!

            return sammich;
        }
    }     import com.reddit.service.user.food.*;
    import com.reddit.service.user.naff.*;
    import java.utils.*;

    public class PBJSandwichFactory implements AbstractFactory, AbstractObjectContext {
        public static Object create() {
            Nommable sammich = new SandwichFacadeContextFactory().build();

            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.KNIFE),
                new Ingredient(new IngredientFactory().mix(Ingredients.PEANUT, Ingredients.BUTTER)));
            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.SPOON),
                new Ingredient(new IngredientFactory().flavor(Ingredients.JELLY, Fruits.GRAPE)));

            sammich.wrap().marshall().wrap().unmarshall().unwrap();

            sammich.nom(); // MY sammich! Calling function doesn't get any damn it!

            return sammich;
        }
    } Nothing sums up Java programming better than [AbstractSingletonProxyFactoryBean](http://static.springsource.org/spring/docs/2.5.x/api/org/springframework/aop/framework/AbstractSingletonProxyFactoryBean.html). Nothing sums up Java programming better than [AbstractSingletonProxyFactoryBean](http://static.springsource.org/spring/docs/2.5.x/api/org/springframework/aop/framework/AbstractSingletonProxyFactoryBean.html). "Convenient proxy factory bean superclass for proxy factory beans that create only singletons"

Awesome. Nothing sums up Java programming better than [AbstractSingletonProxyFactoryBean](http://static.springsource.org/spring/docs/2.5.x/api/org/springframework/aop/framework/AbstractSingletonProxyFactoryBean.html).     import com.reddit.service.user.food.*;
    import com.reddit.service.user.naff.*;
    import java.utils.*;

    public class PBJSandwichFactory implements AbstractFactory, AbstractObjectContext {
        public static Object create() {
            Nommable sammich = new SandwichFacadeContextFactory().build();

            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.KNIFE),
                new Ingredient(new IngredientFactory().mix(Ingredients.PEANUT, Ingredients.BUTTER)));
            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.SPOON),
                new Ingredient(new IngredientFactory().flavor(Ingredients.JELLY, Fruits.GRAPE)));

            sammich.wrap().marshall().wrap().unmarshall().unwrap();

            sammich.nom(); // MY sammich! Calling function doesn't get any damn it!

            return sammich;
        }
    } That's not Java. You don't do any exception handling. At least 40% of the code should be exception handling.     import com.reddit.service.user.food.*;
    import com.reddit.service.user.naff.*;
    import java.utils.*;

    public class PBJSandwichFactory implements AbstractFactory, AbstractObjectContext {
        public static Object create() {
            Nommable sammich = new SandwichFacadeContextFactory().build();

            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.KNIFE),
                new Ingredient(new IngredientFactory().mix(Ingredients.PEANUT, Ingredients.BUTTER)));
            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.SPOON),
                new Ingredient(new IngredientFactory().flavor(Ingredients.JELLY, Fruits.GRAPE)));

            sammich.wrap().marshall().wrap().unmarshall().unwrap();

            sammich.nom(); // MY sammich! Calling function doesn't get any damn it!

            return sammich;
        }
    }     import com.reddit.service.user.food.*;
    import com.reddit.service.user.naff.*;
    import java.utils.*;

    public class PBJSandwichFactory implements AbstractFactory, AbstractObjectContext {
        public static Object create() {
            Nommable sammich = new SandwichFacadeContextFactory().build();

            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.KNIFE),
                new Ingredient(new IngredientFactory().mix(Ingredients.PEANUT, Ingredients.BUTTER)));
            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.SPOON),
                new Ingredient(new IngredientFactory().flavor(Ingredients.JELLY, Fruits.GRAPE)));

            sammich.wrap().marshall().wrap().unmarshall().unwrap();

            sammich.nom(); // MY sammich! Calling function doesn't get any damn it!

            return sammich;
        }
    }     import com.reddit.service.user.food.*;
    import com.reddit.service.user.naff.*;
    import java.utils.*;

    public class PBJSandwichFactory implements AbstractFactory, AbstractObjectContext {
        public static Object create() {
            Nommable sammich = new SandwichFacadeContextFactory().build();

            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.KNIFE),
                new Ingredient(new IngredientFactory().mix(Ingredients.PEANUT, Ingredients.BUTTER)));
            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.SPOON),
                new Ingredient(new IngredientFactory().flavor(Ingredients.JELLY, Fruits.GRAPE)));

            sammich.wrap().marshall().wrap().unmarshall().unwrap();

            sammich.nom(); // MY sammich! Calling function doesn't get any damn it!

            return sammich;
        }
    }     import com.reddit.service.user.food.*;
    import com.reddit.service.user.naff.*;
    import java.utils.*;

    public class PBJSandwichFactory implements AbstractFactory, AbstractObjectContext {
        public static Object create() {
            Nommable sammich = new SandwichFacadeContextFactory().build();

            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.KNIFE),
                new Ingredient(new IngredientFactory().mix(Ingredients.PEANUT, Ingredients.BUTTER)));
            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.SPOON),
                new Ingredient(new IngredientFactory().flavor(Ingredients.JELLY, Fruits.GRAPE)));

            sammich.wrap().marshall().wrap().unmarshall().unwrap();

            sammich.nom(); // MY sammich! Calling function doesn't get any damn it!

            return sammich;
        }
    } Ha! Amateur. Let me show you kids how it's done:

    5 CLS
    10 Input "Putting Jam on Bread. Click anykey to proceed"; A$
    20 If A$="" then goto 30, else goto 30
    30 Input "What would you like to put on the bread?"; B$
    40 Print "You want to put ";B$;" on the bread? OK. So you like ";B$;" sandwiches I see."
    50 for T=1 to 1000; T=9000;
    60 Next T
    70 Goto 5 Oh no, you did'nt just use a goto. 
Now i want pasta Oh no, you did'nt just use a goto. 
Now i want pasta Ha! Amateur. Let me show you kids how it's done:

    5 CLS
    10 Input "Putting Jam on Bread. Click anykey to proceed"; A$
    20 If A$="" then goto 30, else goto 30
    30 Input "What would you like to put on the bread?"; B$
    40 Print "You want to put ";B$;" on the bread? OK. So you like ";B$;" sandwiches I see."
    50 for T=1 to 1000; T=9000;
    60 Next T
    70 Goto 5 Ha! Amateur. Let me show you kids how it's done:

    5 CLS
    10 Input "Putting Jam on Bread. Click anykey to proceed"; A$
    20 If A$="" then goto 30, else goto 30
    30 Input "What would you like to put on the bread?"; B$
    40 Print "You want to put ";B$;" on the bread? OK. So you like ";B$;" sandwiches I see."
    50 for T=1 to 1000; T=9000;
    60 Next T
    70 Goto 5 Ha, Amateur.

000101010111010101010100110101010110
1010010101010101010101010101010101001010101010101001010101010100101101010101011001010101010101101010101001111 010101000110100001100101011100100110010100100000011000010111001001100101001000000011000100110000001000000111010001111001011100000110010101110011001000000110111101100110001000000111000001100101011011110111000001101100011001010010000001101001011011100010000001110100011010000110010100100000011101110110111101110010011011000110010000101100001000000111000001100101011011110111000001101100011001010010000001110111011010000110111100100000011101010110111001100100011001010111001001110011011101000110000101101110011001000010000001100010011010010110111001100001011100100111100100101100001000000110000101101110011001000010000001001100011001010111011101101001011100110100101101101111011011000110001000101110 010101000110100001100101011100100110010100100000011000010111001001100101001000000011000100110000001000000111010001111001011100000110010101110011001000000110111101100110001000000111000001100101011011110111000001101100011001010010000001101001011011100010000001110100011010000110010100100000011101110110111101110010011011000110010000101100001000000111000001100101011011110111000001101100011001010010000001110111011010000110111100100000011101010110111001100100011001010111001001110011011101000110000101101110011001000010000001100010011010010110111001100001011100100111100100101100001000000110000101101110011001000010000001001100011001010111011101101001011100110100101101101111011011000110001000101110 010101000110100001100101011100100110010100100000011000010111001001100101001000000011000100110000001000000111010001111001011100000110010101110011001000000110111101100110001000000111000001100101011011110111000001101100011001010010000001101001011011100010000001110100011010000110010100100000011101110110111101110010011011000110010000101100001000000111000001100101011011110111000001101100011001010010000001110111011010000110111100100000011101010110111001100100011001010111001001110011011101000110000101101110011001000010000001100010011010010110111001100001011100100111100100101100001000000110000101101110011001000010000001001100011001010111011101101001011100110100101101101111011011000110001000101110 Ha! Amateur. Let me show you kids how it's done:

    5 CLS
    10 Input "Putting Jam on Bread. Click anykey to proceed"; A$
    20 If A$="" then goto 30, else goto 30
    30 Input "What would you like to put on the bread?"; B$
    40 Print "You want to put ";B$;" on the bread? OK. So you like ";B$;" sandwiches I see."
    50 for T=1 to 1000; T=9000;
    60 Next T
    70 Goto 5     import com.reddit.service.user.food.*;
    import com.reddit.service.user.naff.*;
    import java.utils.*;

    public class PBJSandwichFactory implements AbstractFactory, AbstractObjectContext {
        public static Object create() {
            Nommable sammich = new SandwichFacadeContextFactory().build();

            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.KNIFE),
                new Ingredient(new IngredientFactory().mix(Ingredients.PEANUT, Ingredients.BUTTER)));
            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.SPOON),
                new Ingredient(new IngredientFactory().flavor(Ingredients.JELLY, Fruits.GRAPE)));

            sammich.wrap().marshall().wrap().unmarshall().unwrap();

            sammich.nom(); // MY sammich! Calling function doesn't get any damn it!

            return sammich;
        }
    }     import com.reddit.service.user.food.*;
    import com.reddit.service.user.naff.*;
    import java.utils.*;

    public class PBJSandwichFactory implements AbstractFactory, AbstractObjectContext {
        public static Object create() {
            Nommable sammich = new SandwichFacadeContextFactory().build();

            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.KNIFE),
                new Ingredient(new IngredientFactory().mix(Ingredients.PEANUT, Ingredients.BUTTER)));
            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.SPOON),
                new Ingredient(new IngredientFactory().flavor(Ingredients.JELLY, Fruits.GRAPE)));

            sammich.wrap().marshall().wrap().unmarshall().unwrap();

            sammich.nom(); // MY sammich! Calling function doesn't get any damn it!

            return sammich;
        }
    }     import com.reddit.service.user.food.*;
    import com.reddit.service.user.naff.*;
    import java.utils.*;

    public class PBJSandwichFactory implements AbstractFactory, AbstractObjectContext {
        public static Object create() {
            Nommable sammich = new SandwichFacadeContextFactory().build();

            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.KNIFE),
                new Ingredient(new IngredientFactory().mix(Ingredients.PEANUT, Ingredients.BUTTER)));
            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.SPOON),
                new Ingredient(new IngredientFactory().flavor(Ingredients.JELLY, Fruits.GRAPE)));

            sammich.wrap().marshall().wrap().unmarshall().unwrap();

            sammich.nom(); // MY sammich! Calling function doesn't get any damn it!

            return sammich;
        }
    }     import com.reddit.service.user.food.*;
    import com.reddit.service.user.naff.*;
    import java.utils.*;

    public class PBJSandwichFactory implements AbstractFactory, AbstractObjectContext {
        public static Object create() {
            Nommable sammich = new SandwichFacadeContextFactory().build();

            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.KNIFE),
                new Ingredient(new IngredientFactory().mix(Ingredients.PEANUT, Ingredients.BUTTER)));
            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.SPOON),
                new Ingredient(new IngredientFactory().flavor(Ingredients.JELLY, Fruits.GRAPE)));

            sammich.wrap().marshall().wrap().unmarshall().unwrap();

            sammich.nom(); // MY sammich! Calling function doesn't get any damn it!

            return sammich;
        }
    }     import com.reddit.service.user.food.*;
    import com.reddit.service.user.naff.*;
    import java.utils.*;

    public class PBJSandwichFactory implements AbstractFactory, AbstractObjectContext {
        public static Object create() {
            Nommable sammich = new SandwichFacadeContextFactory().build();

            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.KNIFE),
                new Ingredient(new IngredientFactory().mix(Ingredients.PEANUT, Ingredients.BUTTER)));
            sammich.applyApplicator(new Applicator(ApplicatorDefaults.SPREAD, new Tool(ToolDefaults.SPOON),
                new Ingredient(new IngredientFactory().flavor(Ingredients.JELLY, Fruits.GRAPE)));

            sammich.wrap().marshall().wrap().unmarshall().unwrap();

            sammich.nom(); // MY sammich! Calling function doesn't get any damn it!

            return sammich;
        }
    } From my experience with Java it would be more like:
import java.noms.sandwich.pbjsandwich; From my experience with Java it would be more like:
import java.noms.sandwich.pbjsandwich; From my experience with Java it would be more like:
import java.noms.sandwich.pbjsandwich; Java:
import pbj_sandwich Java:
import pbj_sandwich Java:
import pbj_sandwich     MOV %EBX PB
    MOV %EAX BREAD
    ADD %EAX %EBX
    EAT %EAX

(I realize now how little assembly I remember...)     MOV %EBX PB
    MOV %EAX BREAD
    ADD %EAX %EBX
    EAT %EAX

(I realize now how little assembly I remember...)     HAI
        CAN HAS STDIO?
        GIMMEH BREAD
        GIMMEH JELLY
        GIMMEH PENUTBUTTR
        I HAS A SAMMICH
        VISIBLE SAMMICH
    KTHXBYE     MOV %EBX PB
    MOV %EAX BREAD
    ADD %EAX %EBX
    EAT %EAX

(I realize now how little assembly I remember...)     MOV %EBX PB
    MOV %EAX BREAD
    ADD %EAX %EBX
    EAT %EAX

(I realize now how little assembly I remember...)     MOV %EBX PB
    MOV %EAX BREAD
    ADD %EAX %EBX
    EAT %EAX

(I realize now how little assembly I remember...) Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. You seem like the only person who's ever taught children here.

Source: I've taught children. Well then that wouldn't make him the only one now, would it! You seem like the only person who's ever taught children here.

Source: I've taught children. I hope you don't teach math. You can be more abstract if you're teaching the same kids every day for many days.  However, for a single presentation on a really foreign concept like programming, the pb&amp;amp;j activity is much more appropriate.  It gets to the core of what's difficult / what needs to be figured out with programming.  If you can get that across at a young age, those kids will be much better prepared to deal with those ideas later on.  It's just planting a little seed of an idea.

Edit: I can't funny he's referring to the fact that with OP included in the count and his source being that he's taught children, there are now two people that have taught children, instead of one. So math. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. When I was introduced to programming in high school, we were given this task (I think it's suitable for younger kids, too):

We were instructed to write out how to turn on the TV in the room, starting at the door. We had to give exact directions (take three steps forward, turn right, etc.) and it was a really interesting and fun exercise.

Something like this is, I think, a great introduction to CS that is accessible even for small kids.  Did you also run a breathing loop? That would be part of the operating system, which the programmer/kids wouldn't have access to. Now I'm annoyingly aware of my breathing. Thanks for that.  Dammit.  Now I'm also aware if your breathing.
 but is your tongue resting in its proper position. stop it you can feel your clothes on your skin. Did you also run a breathing loop? Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. A lecturer from UNSW (I'm not sure if I should mention by name) does this with uni kids! Absolutely hilarious, too. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Id rather just use python.

Import food

Food.make_peanut_butter_jelly_sandwich(bread="white", crust=False)

 Wayyyyy too much going on in that method name. Can we make it an argument?     from Food import Sandwich 
    from Ingredients import Peanut_Butter, Grape_Jelly 

    def main():
       Sandwich(bread = "white", crust = False, fillings = [Peanut_Butter, Grape_Jelly]) I'm abandoning my food library for yours.  I'll make a note on the gothib page.     from Food import Sandwich 
    from Ingredients import Peanut_Butter, Grape_Jelly 

    def main():
       Sandwich(bread = "white", crust = False, fillings = [Peanut_Butter, Grape_Jelly]) What's with the capitalized module names?  It's fine for the classes, but I'm gonna go ahead and guess that you're a Java programmer.     from Food import Sandwich 
    from Ingredients import Peanut_Butter, Grape_Jelly 

    def main():
       Sandwich(bread = "white", crust = False, fillings = [Peanut_Butter, Grape_Jelly])     from Food import Sandwich 
    from Ingredients import Peanut_Butter, Grape_Jelly 

    def main():
       Sandwich(bread = "white", crust = False, fillings = [Peanut_Butter, Grape_Jelly]) Grape jelly is for peasants.  Come on, at least go for strawberry.

EDIT: I regret nothing - go sleep in the great hall with the rest of your kind and your purple sugar paste. Id rather just use python.

Import food

Food.make_peanut_butter_jelly_sandwich(bread="white", crust=False)

 Or perl.

use Food;

$sandwich = new Food::Sandwich(bread =&amp;gt; "white", crust =&amp;gt; 0);
 #!/usr/bin/perl

use strict;
use Wife;

$myWife = new Wife;

$sandwich = $myWife-&amp;gt;makeSandwich("pbj");

return ($myWife) unless ($sandwich); Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. If you want to break kids out into groups, here's another exercise:

**LESSON: DRAWING INSTRUCTIONS**

**Phase One: Writing the Program**

Have two kids to a group (three in one group if there's an odd number is fine). Prepare very simple abstract sketches, composed mostly of straight lines and circles, and hand a small stack to each group along with a small stack of blank paper. Put the kids back to back. One kid chooses a card from the sketch pile and attempts to explain to the other how to draw it, keeping the source card hidden.

This is an incredibly difficult task, even with the most simple sketches.

The nice thing about this example, other than it allows people to divide into individual units, is that you don't need a benevolent intervener who's intentionally confusing the directions (like the teacher in the peanut butter example). The kids understand that even the most simple of directions can be confused, which is why it is necessary to have people who can speak to a computer in an incredibly precise language (programmers).

**Phase 2: Testing**

Alternatively, if you've got maybe an older age group, or you want to go further with the analogy, instead of pairing kids together you could hand out a single drawing to each student. On the back of the drawing, the student writes out instructions of how to draw the thing that's on the other side. All of the instructions/drawings are handed in to the teacher, and the teacher goes through a few of them with the class. The teacher reads an instruction aloud to the class, gives the students a few moments to make their own markings, and then the teacher makes their marking on the chalkboard. At the end of it, the teacher asks the students to hold up their drawings so they can see how wildly different they may be from each other. Then the teacher reveals what it is supposed to look like by showing the class the source drawing.

**Phase 3: Debugging**

For extra bonus points, you can "debug" the program. Now that the class has gone through a program and seen how easily it can go wrong, go back through the directions as a class and try to figure out *why* it went wrong. Were there directions that were not clear enough? What were the most common mistakes that students made? How could they rewrite the instructions to clarify them and make it work better next time?

**Phase 4: Maintenance**

If you had enough class time (you would probably need multiple days for this one), after demonstrating the debugging process you can split the class into two groups, with one teacher (or guest speaker) facilitating each group. The groups would work totally separately and would not be able to see the other group. Group A would start with a set of instructions (with the source sketch on the other side). The facilitator would read through the instructions and Group A would all attempt to sketch the drawing. After the drawing is complete, the facilitator reveals the drawing to Group A and Group A then attempts to debug the instructions (as in phase 3), improving them. These improved instructions are then passed back to Group B. The facilitator from Group B reads these improved instructions to Group B who attempts to draw the sketch. When the sketch is finished, Group B's drawings are compared to Group A's. What would be surprising to learn at this stage is not that Group B's sketches are better (the students would already be expecting that), but that, somehow, Group B's sketches still likely were not perfect, even though Group A went through all the trouble to improve the directions. Even though we may think we have written a perfect program, we often are blind to it's faults, which is why we often see bugs in programs even after they have been "completed" and sold to people.

**Discussion**

Ask the class, "Do you think if we went through this process one more time, with Group B improving the instructions and Group A drawing again, that everyone would have a perfect picture? If not, how many times do you think we would have to do this?" If there's no clear answer, you might ask, "Can a program ever really be considered 'finished', then?"

EDIT: I clarified that the sketches must be abstract - it messes up the process if the students can guess what the picture is supposed to look like (an advantage that a computer doesn't have). Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Obligatory XKCD: http://xkcd.com/149/ Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. My dad, who is an electrical engineer, taught programming to my 3rd grade Montessori class of about 6 or 7 kids.  The way he did it was to bring a box of nilla wafers, and we were going to make an algorithm in BASIC where you provide your age and it outputs the number of cookies you get.  

The algorithm was something like (age/2)+1, so we ran the program and he doled out the cookies as it said to each kid until he got to himself.  He started counting a large number of cookies, then asked us how we could make it more fair, so we put a limit on the total number of cookies which gave him a much more reasonable 8 cookies instead of 20.  It wasn't too in depth but we were pretty young and I still remember it to this day. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age. Great example of computers I did with kids:

One kid got to be the "program" and gave instructions on how to make a peanut-butter and jelly sandwich, but they couldn't watch my actions.  I was the computer, and followed the instructions exactly.  Hilarity resulted.  (Not taking lid off peanut bar, not getting bread out of box I had it in).  After a minute, another kid came up and tried.  They got a little further, but still floundered.  "Put the peanut butter on the bread" resulted in the jar sitting on the loaf, etc.  It took several kids, but eventually we got it.

If you've got an assistant, have them write the instructions as they're said, creating the "program."

You can get into explanations, then, but the thing that really stuck was that the kids were each "smarter than a computer."  But that computers were really, really fast at following instructions.  So they needed to use their smarts and the computers speed, and... programming!


Sorting, or even code, might be much for a presentation for that age.  I think it would be best if you could somehow tie in CS with something they already know and like, like iPads or angry birds or something. Maybe explain that a cell phone is a computer just like any other, and actually our cell phones are much more powerful computers than our desktops from 10 years ago. And explain how the people that make the devices and software on the cell phones all studied CS.

&amp;gt; I'd finish it off showing them the youtube video of the hungarian folk dancers performing Bubble Sort.

I think this idea is terrible. If I wanted to turn kids away from computer science for life, this is exactly the video I would show them. Essentially you're taking a subject they don't understand and juxtaposing dorky costumes, music, and dancing (things that they probably won't find appealing and have nothing to do with CS) on to it. It's a very entertaining video; except that it's a few minutes long. No 3rd grader is going to have that attention span to something not cartoons. (Source: I used to be a 3rd grader) My wife's students love funny YouTube videos and dancing.
I downloaded it with Video Download Helper and planned on playing it with VLC where I can speed the video way up.
Basically... just a visualization.  Your current idea seems pretty good but I just thought I'd point out http://csunplugged.org/ and http://www.youtube.com/user/csunplugged for ideas.     incorporate candy in some way, like give each skittle color a different number and have each kid sort a small bag before they eat it. i used to work in child care and candy always seems to help    I love your idea, and phblj's sounds like a really good exercise. But before that I'd suggest to get everyone into the idea without going too much into it. Tell them to write down the steps they took from waking up to getting out the door and go from there.


A plus is that your wife can find out who isn't eating breakfast, whose parents leave before they do, etc etc.


Also if you want to try and explain the speed of computers, bring your own nanostick ala Grace Hopper.                </snippet></document><document><title>Help! Discrete structure resources</title><url>http://www.reddit.com/r/compsci/comments/16qq6q/help_discrete_structure_resources/</url><snippet>I'm currently in a discrete structures 2 class and I'm having a very hard time understanding some concepts. I was wondering if anyone had some helpful resources they could share. 

I'm quickly realizing I'm not a very mathematical person. I would like to change that before it destroys my confidence with school.

edit: My first hw assignment was to create a program to calculate the pascal's triangle and stirling's numbers of the 2nd kind. I finished pascal's triangle and I wrote it in Python. I can't for the life of me figure out how to generate stirling's numbers of the 2nd kind.  Well, what are the concepts you're having trouble understanding?  </snippet></document><document><title>Good books/resources of Information Theory?</title><url>http://www.reddit.com/r/compsci/comments/16pgzp/good_booksresources_of_information_theory/</url><snippet>I noticed that my knowledge in this subject is quite weak. Would you suggest me some books or other resources?     </snippet></document><document><title>How Particle Physics Is Improving Recommendation Engines</title><url>http://www.technologyreview.com/view/509861/how-particle-physics-is-improving-recommendation-engines/</url><snippet>  I like this idea. In essence, they maximize differentiation by limiting the number of people who get recommendations for a product. As a consequence a larger number of products get reviewed and the biases are reduced.

&amp;lt;rant&amp;gt;

In fact the whole social sphere and blogosphere are doing the same work by hand - finding good stuff, pointing to it, curating the content. Reddit also acts as a content collector and curator. It's a huge field.

The problem of content discovery is one of the challenges of this century. There are millions of music recordings. Even if I could listen to them one by one, each one just once, that turns out about 100 recordings per day - and this is an exaggerated figure. It would take 50 to 100 years to cover the catalog of iTunes. 

With books, movies, online articles, wiki entries, scientific papers - it's the same. We reached a point where one could not realistically see more than 1% of the media produced (BBC archives contain 1 million hours of TV and radio). That means 99% of it will forever remain unseen. 

What if the music/book/movie of my life is amongst those unseen 99% and I never get to find it?

 You shouldn't miss what you never had. Be grateful for what you do. :) I like this idea. In essence, they maximize differentiation by limiting the number of people who get recommendations for a product. As a consequence a larger number of products get reviewed and the biases are reduced.

&amp;lt;rant&amp;gt;

In fact the whole social sphere and blogosphere are doing the same work by hand - finding good stuff, pointing to it, curating the content. Reddit also acts as a content collector and curator. It's a huge field.

The problem of content discovery is one of the challenges of this century. There are millions of music recordings. Even if I could listen to them one by one, each one just once, that turns out about 100 recordings per day - and this is an exaggerated figure. It would take 50 to 100 years to cover the catalog of iTunes. 

With books, movies, online articles, wiki entries, scientific papers - it's the same. We reached a point where one could not realistically see more than 1% of the media produced (BBC archives contain 1 million hours of TV and radio). That means 99% of it will forever remain unseen. 

What if the music/book/movie of my life is amongst those unseen 99% and I never get to find it?

  </snippet></document><document><title>Conor McBride: Agda-curious? (ICFP 2012 keynote)</title><url>https://www.youtube.com/watch?v=XGyJ519RY6Y</url><snippet>  The slides are totally unreadable and the audio is bad :P Are the slides up anywhere? The slides are totally unreadable and the audio is bad :P Are the slides up anywhere?</snippet></document><document><title>Understanding and writing an LLVM Compiler Backend - ELC2009</title><url>https://www.youtube.com/watch?v=b32r4erFGNE</url><snippet> </snippet></document><document><title>PTX Back-End: GPU Programming With LLVM [video]</title><url>https://www.youtube.com/watch?v=Ux3F5MKuPjI</url><snippet /></document><document><title>computational power of arithmetic?</title><url>http://www.reddit.com/r/compsci/comments/16o5ue/computational_power_of_arithmetic/</url><snippet>I'm not sure if this has been asked before.  I did a basic search in compsci and couldn't find exactly what I was looking for.  I did find this posted a day ago which is close. http://www.reddit.com/r/compsci/comments/16kcaq/determining_equality_of_numbers_constructed_with/

The basic question is what is the computational power of arithmetic.  Can you encode Turing Machines in arithmetic? (at which point several properties become undecidable)  I know that arithmetic with addition and multiplication is incomplete, but I'm not entirely sure how that relates to undecidablity.  I feel like arithmetic should be equivalent to a Turing Machine, but I don't know how to prove it.

by arithmetic I mean f(x) is a function involving only +,*,-,/,^
example f(x) = 4*x + 44/(x-2)  We know that there are countably many Turing machines, and countably many expressions constructable with arithmetic operations.  Does that help? If the arithmetic expression involves any real number, how can there be countably many? Trivially, isn't there an expression for f(x)=C, for every C in the real numbers, which would be an uncountable number of functions? If the arithmetic expression involves any real number, how can there be countably many? Trivially, isn't there an expression for f(x)=C, for every C in the real numbers, which would be an uncountable number of functions? Just because there are infinitely many doesn't mean they aren't countable.

[Wiki Page on Countable sets](http://en.wikipedia.org/wiki/Countable_set#Definition)   Arithmetic is weaker than Turing machines, in at least one sense.

The [lambda calculus](http://en.wikipedia.org/wiki/Lambda_calculus) is equivalent to Turing machines, and [equivalence of lambda expressions is undecidable.](http://en.wikipedia.org/wiki/Lambda_calculus#Undecidability_of_equivalence)

I'm almost positive equality for arithmetic is decidable.
</snippet></document><document><title>Computational geometry algorithms for machine learning [video]</title><url>https://www.youtube.com/watch?v=Mk4_3KAESwk</url><snippet> </snippet></document></searchresult>
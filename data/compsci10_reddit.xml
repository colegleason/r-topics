<searchresult><compsci /><document><title>Dealing with climate science's big data deluge</title><url>http://www.hpcwire.com/hpcwire/2012-08-15/climate_science_triggers_torrent_of_big_data_challenges.html</url><snippet /></document><document><title>Khan Academy adds computer science courses</title><url>http://www.khanacademy.org/cs</url><snippet>  Looks more like beginning game design/development courses. Maybe I'm missing it but I'm not seeing much with regard to Computer Science theory in this.

Still cool none the less. It's to get people interested in programming through making interactive/playful programs.  Programming is hard and trying to learn how to do it is almost impossible without some form of motivation.  Making cool fractals and small video games does exactly that. I have no problem with that. But I'm already a programmer. And I was hoping to improve my own knowledge. So I was a bit disappointed that there weren't any courses for anything past computer science 101. I'd like to see more algorithms and theory. Stuff I should have learned in college but was too busy just trying to get through it. I have no problem with that. But I'm already a programmer. And I was hoping to improve my own knowledge. So I was a bit disappointed that there weren't any courses for anything past computer science 101. I'd like to see more algorithms and theory. Stuff I should have learned in college but was too busy just trying to get through it. Looks more like beginning game design/development courses. Maybe I'm missing it but I'm not seeing much with regard to Computer Science theory in this.

Still cool none the less. Looks more like beginning game design/development courses. Maybe I'm missing it but I'm not seeing much with regard to Computer Science theory in this.

Still cool none the less.  I went through a few of the lessons, then I remembered that I have to learn calculus so I went and started those lessons. 3 hours later I remembered that I had to head in for a 6:30 meeting this morning.  And then? As a responsible adult I went to bed, albeit a little late and woke up early enough to make it to the meeting. Sorry, no fancy stories for entertainment. Just boring worker talk here.  How was your meeting? Why so early? TELL US ALL ABOUT YOUR LIFE NOW My life: no girlfriend, living alone, no social life except the occassional night out at a friend's place to play RPGs. 

Now, most people would see that as a bad thing. I see it as: I get to spend my own money, don't have to deal with roommates and get to play video games every night, occassionally going out to spend a night RPGing with friends. Life is pretty fucking awesome. Who cares if I have to go to bed early once every few months because the boss wants to have worldwide attendance at his state of the union meetings? My life: no girlfriend, living alone, no social life except the occassional night out at a friend's place to play RPGs. 

Now, most people would see that as a bad thing. I see it as: I get to spend my own money, don't have to deal with roommates and get to play video games every night, occassionally going out to spend a night RPGing with friends. Life is pretty fucking awesome. Who cares if I have to go to bed early once every few months because the boss wants to have worldwide attendance at his state of the union meetings? My life: no girlfriend, living alone, no social life except the occassional night out at a friend's place to play RPGs. 

Now, most people would see that as a bad thing. I see it as: I get to spend my own money, don't have to deal with roommates and get to play video games every night, occassionally going out to spend a night RPGing with friends. Life is pretty fucking awesome. Who cares if I have to go to bed early once every few months because the boss wants to have worldwide attendance at his state of the union meetings? My life: no girlfriend, living alone, no social life except the occassional night out at a friend's place to play RPGs. 

Now, most people would see that as a bad thing. I see it as: I get to spend my own money, don't have to deal with roommates and get to play video games every night, occassionally going out to spend a night RPGing with friends. Life is pretty fucking awesome. Who cares if I have to go to bed early once every few months because the boss wants to have worldwide attendance at his state of the union meetings? [Relevant](http://www.youtube.com/watch?v=nw6WPSUlKb0)  Just to save everybody time, there was already a full expression of butthurt about this not being *real* computer science when it posted to /r/programming, so you don't have to do it here. Is it OK to express here his lack of pedagogy or lesson planning and reliance on a "do this, then do this, then do this" style of teaching?  Khan has had comp sci for a long time. His videos were using Python and they went through the basics (variables, control flow, etc) and then go on to discuss some data structures. It is a really great way to start. This new thing looks really interesting, though. Having a place for people to edit some already-created graphics code in real-time looks like fun. It's all done in javascript so I imagine a new browser (html5 compliant) will be required. </snippet></document><document><title>Do you know of any good books on the application and use of Computer Science in finance?</title><url>http://www.reddit.com/r/compsci/comments/yagsq/do_you_know_of_any_good_books_on_the_application/</url><snippet>Also does anyone know what this specific area of Computer Science/Finance is called?  I've heard it referred to as "Computational Finance" most of the time. People that I know that are in such programs right now refer to it as such.  In my five minutes of Google-fu I've also heard quantitative finance and finance engineering. Is this correct?  /r/algotrading could answer your question.   I am in finance IT, not quite quant level, but closer than most.  Most of quantitative finance is highly mathematical, and implementing advanced mathematical topics correctly and efficiently is a big job.  But the computation is often not that exciting from a theoretical CS point-of-view.    It's a cool branch of applied mathematics and applied CS, though. It really depends on the frequency and scale of trading though. Sufficiently fast or large trading systems become interesting CS problems in and of themselves. And I really mean CS, not just programming. Simple-sounding functionality (ie, synchronization, position management, logging, etc...) become tricky algorithmic challenges if you want sub-millisecond latency or if you're sucking in gigabytes of information a second.  Sounds pretty interesting. Are there professors or departments doing research on these types of things? I'm not sure. I'm doing a CS PhD but not on anything related to finance or real-time distributed systems. I did, however, spend the past two summers working for a smaller proprietary trading shop. We read a lot of machine learning research papers when trying to create trading models (the last few years of ICML and NIPS have been great), but our trading was both slow and small enough that we didn't need much from systems research (we could get away with simple algorithms and off-the-shelf components like zero-mq and protocol buffers).  ah cool. How did you find out about said company? I am in finance IT, not quite quant level, but closer than most.  Most of quantitative finance is highly mathematical, and implementing advanced mathematical topics correctly and efficiently is a big job.  But the computation is often not that exciting from a theoretical CS point-of-view.    It's a cool branch of applied mathematics and applied CS, though. Do the people you know in quant all require PhDs? Or is a bachelors in CS with several finance courses enough? Do the people you know in quant all require PhDs? Or is a bachelors in CS with several finance courses enough?  </snippet></document><document><title>Is there a resource that teaches math through programming?</title><url>http://www.reddit.com/r/compsci/comments/y79zd/is_there_a_resource_that_teaches_math_through/</url><snippet>Some quick background: I am a programmer that works primarily on enterprise systems, and my math knowledge is very weak.  I'd like to learn more about math, but every text I try to read confuses me and I get frustrated quickly.  I was reading a blog post today about "[the myth of superprogrammers](http://simpleprogrammer.com/2012/08/12/the-myth-of-the-super-programmer/)" where the author gave an example of [a math equation and used a simple `for` loop to illustrate what was happening with the math notation](http://f.cl.ly/items/131k1x3N041q113Q0s1I/Screen%20Shot%202012-08-14%20at%208.28.13%20AM.png).  I had an "ah-ha!" moment and realized that this would be an excellent way for me to become familiar with math concepts, by referencing a field with which I am pretty familiar.  I know a lot of programmers already have a math background, but since I'm approaching this from the other direction, I was wondering if there are any texts, websites, or videos out there that take this approach.  Any suggestions would be most welcome!  I actually would like to make khan academy style videos on this very topic. Learning math through programming is a great way to learn, not to mention useful.

I have done this the hard way where I sit through a math textbook and solve everything in a Python shell. This is how I learned probability and stochastics.  Props to you for working through a textbook with python!  Someone recommended [Concrete Mathematics](http://www.amazon.com/Concrete-Mathematics-Foundation-Computer-Science/dp/0201558025/ref=sr_1_1?ie=UTF8&amp;amp;qid=1344954809&amp;amp;sr=8-1&amp;amp;keywords=concrete+mathematics) to me a while back as a good resource for really sharpening my math skills.  Knuth is one of the authors, and while the book does not discuss programming at all, I'm interested to see how, as a programmer himself, he explains mathematics.

A Khan video series would be awesome.  I think there would be a lot of demand for it.  

I think the thing I have the most trouble with is understanding math notation.  For some reason all the symbols really confuse me.  I can learn a new programming language syntax with relative ease, but when I try to learn math syntax, my mind just goes blank.  Not sure why this is an issue. &amp;gt; I think the thing I have the most trouble with is understanding math notation. For some reason all the symbols really confuse me. I can learn a new programming language syntax with relative ease, but when I try to learn math syntax, my mind just goes blank. Not sure why this is an issue.

I just wanted to chime in and say... **me too**...

It really bugs me.  I was planning on being a fighter pilot when I was in high school, so I did all the advanced maths classes, and did pretty well.  Ended up getting into computer science, though.

14 years later, and my god, whatever neural networks I used to have that were capable of maths have atrophied long ago.  I can't remember my times tables to save my life, let alone understand math notation.

Picking up a new programming language or implementing an elegant solution for a really hard technical problem, though?  Piece of cake. I think learning haskell really made it easier for me  to understand how to think about mathematical notation. Haskell and functional programming is (much) closer to the mathematical mindset than imperative and especially object oriented programming. Just going to butt in and say what a great functional language Lisp is. Though I dislike the syntax. Anyone back me up? But the syntax is so simple.  It's just things like 'this and #'this that aren't straightforward. But why (do argument) and not do(argument)? Because the source code in Lisp is written in lists.  Almost everything in Lisp involves lists.  The only thing that doesn't are conses, which are what lists are made out of.  The thing I love about lists for source code is that the only syntactic thing you ever really have to check is the order of the arguments. Props to you for working through a textbook with python!  Someone recommended [Concrete Mathematics](http://www.amazon.com/Concrete-Mathematics-Foundation-Computer-Science/dp/0201558025/ref=sr_1_1?ie=UTF8&amp;amp;qid=1344954809&amp;amp;sr=8-1&amp;amp;keywords=concrete+mathematics) to me a while back as a good resource for really sharpening my math skills.  Knuth is one of the authors, and while the book does not discuss programming at all, I'm interested to see how, as a programmer himself, he explains mathematics.

A Khan video series would be awesome.  I think there would be a lot of demand for it.  

I think the thing I have the most trouble with is understanding math notation.  For some reason all the symbols really confuse me.  I can learn a new programming language syntax with relative ease, but when I try to learn math syntax, my mind just goes blank.  Not sure why this is an issue. Concrete Mathematics is a great book, but it's aimed more at mathematicians and computer scientists rather than programmers. You'll get a lot of problem solving practice through it, but it won't provide much intuition in terms of programs. That said, I think it has a good explanation of all the notation it uses (it's been a while since I've read it though). It's also a fun book because of all the margin notes. I know that I've got the unpopular opinion, but I actually didn't find that book too useful as a CS student when I read the first quarter of it. It tends to explain the utmost basics of a discrete math field, and then jump into advanced discrete calculus without spending any time in the middle. Reading the first volume of his Art of Computer Programming is actually a better math prep in my opinion.  Props to you for working through a textbook with python!  Someone recommended [Concrete Mathematics](http://www.amazon.com/Concrete-Mathematics-Foundation-Computer-Science/dp/0201558025/ref=sr_1_1?ie=UTF8&amp;amp;qid=1344954809&amp;amp;sr=8-1&amp;amp;keywords=concrete+mathematics) to me a while back as a good resource for really sharpening my math skills.  Knuth is one of the authors, and while the book does not discuss programming at all, I'm interested to see how, as a programmer himself, he explains mathematics.

A Khan video series would be awesome.  I think there would be a lot of demand for it.  

I think the thing I have the most trouble with is understanding math notation.  For some reason all the symbols really confuse me.  I can learn a new programming language syntax with relative ease, but when I try to learn math syntax, my mind just goes blank.  Not sure why this is an issue. Interesting book will have to check it out.

There is also this [book](http://www.cs.princeton.edu/courses/archive/spr10/cos433/mathcs.pdf).

My mind also goes blank with math symbols. Not to mention that it is so hard to turn math symbols into something practical for CS or data analysis. [This](http://blog.ted.com/2010/11/15/teaching-kids-real-math-with-computers-conrad-wolfram-on-ted-com/) guy has it right. 

 More up to date version of the book here: http://courses.csail.mit.edu/6.042/spring12/mcs.pdf Props to you for working through a textbook with python!  Someone recommended [Concrete Mathematics](http://www.amazon.com/Concrete-Mathematics-Foundation-Computer-Science/dp/0201558025/ref=sr_1_1?ie=UTF8&amp;amp;qid=1344954809&amp;amp;sr=8-1&amp;amp;keywords=concrete+mathematics) to me a while back as a good resource for really sharpening my math skills.  Knuth is one of the authors, and while the book does not discuss programming at all, I'm interested to see how, as a programmer himself, he explains mathematics.

A Khan video series would be awesome.  I think there would be a lot of demand for it.  

I think the thing I have the most trouble with is understanding math notation.  For some reason all the symbols really confuse me.  I can learn a new programming language syntax with relative ease, but when I try to learn math syntax, my mind just goes blank.  Not sure why this is an issue. Props to you for working through a textbook with python!  Someone recommended [Concrete Mathematics](http://www.amazon.com/Concrete-Mathematics-Foundation-Computer-Science/dp/0201558025/ref=sr_1_1?ie=UTF8&amp;amp;qid=1344954809&amp;amp;sr=8-1&amp;amp;keywords=concrete+mathematics) to me a while back as a good resource for really sharpening my math skills.  Knuth is one of the authors, and while the book does not discuss programming at all, I'm interested to see how, as a programmer himself, he explains mathematics.

A Khan video series would be awesome.  I think there would be a lot of demand for it.  

I think the thing I have the most trouble with is understanding math notation.  For some reason all the symbols really confuse me.  I can learn a new programming language syntax with relative ease, but when I try to learn math syntax, my mind just goes blank.  Not sure why this is an issue. Did you find anything you liked in the end? I'm still interested in trying to make something, just got no response from anyone when asking for a specific sub-topic and/or programming language they preferred.. I actually would like to make khan academy style videos on this very topic. Learning math through programming is a great way to learn, not to mention useful.

I have done this the hard way where I sit through a math textbook and solve everything in a Python shell. This is how I learned probability and stochastics.  I actually would like to make khan academy style videos on this very topic. Learning math through programming is a great way to learn, not to mention useful.

I have done this the hard way where I sit through a math textbook and solve everything in a Python shell. This is how I learned probability and stochastics.  I actually would like to make khan academy style videos on this very topic. Learning math through programming is a great way to learn, not to mention useful.

I have done this the hard way where I sit through a math textbook and solve everything in a Python shell. This is how I learned probability and stochastics.    Not sure if you've come across [Project Euler](http://projecteuler.net/), but you might find it to be useful. The forums might have some discussions of interest. Not sure if you've come across [Project Euler](http://projecteuler.net/), but you might find it to be useful. The forums might have some discussions of interest.  I have been thinking about this issue from the opposite direction for some time (I have reasonable to advanced maths knowledge and some programming), and would like to develop some resources if none exist (haven't seen anything). One of the main things I've been missing is a willing student for real time feedback/tips/suggestions/questions etc. Feel free to message me if you can't find anything. I have been thinking about this issue from the opposite direction for some time (I have reasonable to advanced maths knowledge and some programming), and would like to develop some resources if none exist (haven't seen anything). One of the main things I've been missing is a willing student for real time feedback/tips/suggestions/questions etc. Feel free to message me if you can't find anything. An idea that comes to mind is explaining how a math problem relates to the code *and* the real world.

I've struggled with math (as well as programming) since middle school because none of the math teachers had any practical example of real world use. Coming up with real world examples (at least for the initial explanation) as well as doing it via code can sometimes cloud the issue a bit.

That being said, I've found programming a simple video game which involves a physics engine and some basic 3d graphics (would have to roll many of the libraries yourself to get the full benefit) can involve almost all of the maths you need to learn as well as providing context for almost everything. What I meant by real world example doesn't have be ~~be~~ quite detailed as say [this](http://plus.maths.org/content/101-uses-quadratic-equation-part-ii), but some way of saying, "This is what you would use this equation for in practice". An idea that comes to mind is explaining how a math problem relates to the code *and* the real world.

I've struggled with math (as well as programming) since middle school because none of the math teachers had any practical example of real world use. Yeah, that would be really awesome.  My brother-in-law has his PhD in physics and he says that is what helped him learn advanced maths -- he could always tie it back to the real world in his physics experiments. What's your background code-wise? Have you done much game programming (I'd gather the workings of many of the underlying libraries would be a bit mysterious if linear algebra is a problem)? 

Would you be interested in working on some kind of public resource? I'm thinking a combined game programming/maths thing, doing the libraries and derivations from scratch (maybe some of the maths in optional sections) to provide context for the maths (calculus and linear algebra at least, some of the discrete stuff which is more useful for algorithm design may still be a bit hard to show the use of). My background is enterprise programming (i.e., web, data storage) so... hardly any significant math, really.  I would love to work on a public resource but I don't think I would actually be much help :-( An idea that comes to mind is explaining how a math problem relates to the code *and* the real world.

I've struggled with math (as well as programming) since middle school because none of the math teachers had any practical example of real world use.   Gerald J. Sussman of [SICP](http://en.wikipedia.org/wiki/Structure_and_Interpretation_of_Computer_Programs) fame has been [giving lectures](http://video.google.com/videoplay?docid=-2726904509434151616&amp;amp;hl=en) about this very topic. He applies it typically to physics and engineering.           Crosspost to SomeoneMakeThis?  Have you studied algorithms formally at all? You might find it interesting because it takes programming and uses math to prove properties such as the running time, solution quality of the algorithm in question.

My favourite algorithms book is Algorithm Design by Kleinberg/Tardos. Other people like Introduction to Algorithms by Cormen/Leiserson/Rivest.

FWIW I view Math as a language. Based on the example you gave above, it seems like you just need to get used to reading it. You can understand for-loops, and you've realized summation notation is pretty much the same thing. Just keep reading and you'll get it! A book like Algorithm Design definitely helps as it really walks through the math. No, I have avoided this precisely because most of the texts I've looked at presuppose a decent grasp of mathematical notation/methods.  I will take a look at the books you've recommended, though. No, I have avoided this precisely because most of the texts I've looked at presuppose a decent grasp of mathematical notation/methods.  I will take a look at the books you've recommended, though. As a person with a math degree who only recently took up programming, i can say that by avoiding a formal study of algorithms, you ve avoided the really mathematical parts of computer science.  [Seymour Papert](http://www.papert.org/) was/is big on that with his idea of [constructionism](http://en.wikipedia.org/wiki/Constructionism_(learning_theory\)). Your link is broken. Put a \ before the penultimate ).      I think I saw a hacker news post where a guy was giving a (TED?) talk about how we should be teaching math with programming, not pen and paper algorthims. So many advantages when you can see the results in real time.

I am terribly sorry I don't have a link, but more people than just yourself think it's a fine idea. I'm sure Kahn Academy would be the best folks to petition for something like this if it doesn't exist already.      The article claims:

&amp;gt; The equation is the same as this code:
&amp;gt; 
&amp;gt;     var total = 0;
&amp;gt;     for(int i = n; i &amp;lt;= m; i++)
&amp;gt;     {
&amp;gt;         total += f(i)
&amp;gt;     }

Technically the article is wrong. The equation is the same as this code:

    sum(n,m,f) === f(n) + f(n+1) + ... + f(m)
               === ( f(n) + sum(n+1,m,f(x)) ) if n&amp;lt;=m else 0

We have a mathematical statement of equivalence, not a procedure to calculate something. It just so happens we can calculate it as:
    
    def calcSum(n,m,f):
        total = 0
        for i in range(n,m+1):
            total += f(i)
        return total

    sum(n,m,f) == calcSum(n,m,f)
    </snippet></document><document><title>No Pathway (Graph Theory Gotye cover by Udacity's Algorithms professor, Dr. Michael Littman)</title><url>http://www.youtube.com/watch?v=stdG3BGmhqo</url><snippet>   if only his class taught you how to do this! It does teach you how to graph-search...unless you mean the vizualization, then yeah, that'd be awesome.</snippet></document><document><title>John Resig - Redefining the Introduction to Computer Science</title><url>http://ejohn.org/blog/introducing-khan-cs/</url><snippet>   I don't see the part where they teach fundamental computer science concepts.

I see some engineering and practical programming introduction, which is good in itself, but it's not necessarily computer science.  I made the mistake of trying to stick with p5 for too long. Let this be a warning.

Processing is great, but it has thought me a lot of bad assumptions about how things should work.

By design, it's not very easy to build big things in processing. I figured out patterns to do it, but it's weird. I was actually working around p5's design patterns. They chose to keep them as obfuscated as possible, so a beginning programmer wouldn't have to worry about them.

It should be stressed that p5 can only be used to teach program flow, arithmetics and maybe some OO stuff (and for fun!), not for in depth computer science.

P5 doesn't claim to do that, but a lot of tutorials fail to point this out and don't redirect the user to advanced resources. I don't know if I agree with that, necessarily. A lot of CS stuff can be visualized: see, for instance, this simulation of a Turing Machine: http://www.khanacademy.org/cs/turing-machine/938201372

It's true that more advanced material can be more difficult to visualize, or require more overhead to do so, but it's certainly possible.

That's not to say there aren't concerns about style that could arise (indeed, they can arise from any largely self-guided study), just that it's certainly possible to do more advanced things with it in reasonable ways. Yeah, but I mean

    var mousePressed = function() {
        if (addOneMachine.state !== "stop"){
            addOneMachine = nextStep(addOneMachine);
        } else{
            drawTape(addOneMachine, 0);
        }
        table();
    };

The method that intercepts mouse clicks performs a state action and decides what to draw.

That's pretty weird, that will become messy with a big system. There's no way around it unless you look at the core code. You need to build up patterns to get around this. Oh, ok. I think I see part of what you're saying. If there were multiple different areas I wanted to get clicks in, it'd be more difficult.

I'm not sure I entirely follow what you mean about the state check being weird, though. Could you elaborate? It's not just a state check, the next line is a state change.

Anyways, this method shouldn't do more than alert observers of a click action, and pass on the event object. But there's the problem, there's no event object, it's factored out. You need to rebuild it to make an observer pattern. 

Again, I'm not bashing Processing for what it is. I still use it often for fun artsy projects. I think that's the main intention. &amp;gt; Anyways, this method shouldn't do more than alert observers of a click action, and pass on the event object.

Why not? (There is a boolean you can use in processing, but it's slightly problematic in that it's too sensitive--holding down the mouse for under a second will send several mouse trigger events.) &amp;gt; Why not?

Low coupling, high cohesion, information expert, delegation... All things needed for MVC-like patterns and modular systems are very hard to do on top of p5.

&amp;gt; There is a boolean you can use in processing

That's part of the problem. It's mutable state in an upper layer class. It's great for small projects where a God Class isn't a problem, but you can't build a good system on this. It encourages the blending of abstraction and representation.  Yeah, but I mean

    var mousePressed = function() {
        if (addOneMachine.state !== "stop"){
            addOneMachine = nextStep(addOneMachine);
        } else{
            drawTape(addOneMachine, 0);
        }
        table();
    };

The method that intercepts mouse clicks performs a state action and decides what to draw.

That's pretty weird, that will become messy with a big system. There's no way around it unless you look at the core code. You need to build up patterns to get around this. That's actually why processing is a library on top of Java. You can interface to it and hopefully get around those patterns.</snippet></document><document><title>Nokia Research Center's Best Entries For Mobility Data Challenge 2012 </title><url>http://research.nokia.com/page/12362</url><snippet /></document><document><title>Interested in Machine Translation</title><url>http://www.reddit.com/r/compsci/comments/y8az4/interested_in_machine_translation/</url><snippet>I recently had the idea that Machine Translation is something I could be interested in. I'm in the process of learning German and a few days ago I started thinking about online translators. I, as of right now, have no idea how they really work, but I had the vague idea of programming one with knowledge on the structure of the language to aid in the translation of phrases. I began my google searching. I quickly found [an overwhelming wealth of information](http://www.mt-archive.info/). I've started looking around, but this doesn't feel like a place I want to start. There is too much information here for me to properly process it. I will keep it in mind though. So, if any on can reccommend a source for some more general information to get me started, that'd be a help. Or really just any discussion on the subject is welcome.

As a frame of reference, I'm a senior in uni, going for a degree in compsci. I'm semi familiar with algorithms, but little to know knowledge of properly designing my own. I am mostly comfortable with the OO paradigm (Java) but I have dabbled in a few others.   First, modern MT does not work at all like how I bet you you think it does. Classical MT did, and some people still do Classical MT, but it's not what powers Google Translate.

In terms of learning, start with this tutorial. It's dated, and it assumes knowledge of basic probability (you'll need it!), ww.isi.edu/natural-language/mt/wkbk.rtf

Then, go here: http://mt-class.org/ and have fun.

MT is hard. It's one of the hardest problems in NLP (natural language processing), and Google and DARPA have thrown tons of money and brilliant minds at it. 

The basic way modern MT works is like this: as "training data", you have a bunch of bilingual parallel texts (think: transcripts of the UN proceedings, translations of news articles, all done by humans), and a lot of monolingual text (just text) in the language you want to translate into, let's call it English. The former tells you how translation works, and  the latter is supposed to tell you what English looks like.

From the bilingual text, you extract a phrase dictionary, which is a (multiword expression) to (multiword expression) dictionary. Then, when you translate a new sentence, you break up the source language sentence (French) into phrases, look them up in the dictionary and then try to stitch them together in a way that looks like English (using the monolingual data as a guide). Sometimes you reorder the phrases, sometimes you don't. It's NP-complete to do all this, so we use approximations instead.

Now, you can also make a rule-based system which is what I assume you would do  on your own. Those are fun, but will not work particularly well for anything complicated. It's worth doing though! You'll learn a lot. I did. Thanks for all this! I'm a few pages into the first link, and this feels like a great place to start, though I can quickly see why this is one of the hardest problems. 

I was actually vaguely aware that Google translate used already-translated-by-humans papers, though beyond that, my knowledge was vague. And with my super vague understanding, my "brilliant" idea was to use rules specific to a language as to how that language is structured. Basically I will need a super lexographical understanding of the languages I'm making rules for. Basically it would know how language A is structured, adjectives come before nouns, Direct Objects at the end of the sentence, things like that. 

I am at lease aware that I would have no chance fighting against the likes of Google or DARPA (because DARPA does crazy shit... haha) but I mostly wanted to look into this as a sort of resume builder. When ever they ask me, "Working on any projects of your own?" I can proudly respond "Yes! I've been looking into Machine Translation and this is what I've learned!" Very laudable and I don't want to discourage you, just wanted to let you know what you're getting yourself into. :-)

Systran was the technology behind Babelfish, and essentially did what you want to do. It couldn't use data, and so can't live up to Google Translate's abilities, but truth be told Google is starting to look at adding grammar and "real linguistics" into their stuff.  Check out [DELPH-IN](http://www.delph-in.net/). Nifty, reading about it now. Thanks. 

Interestingly enough, I discovered a link to a website in Japanese, which Google so kindly translated for me.... I should warn you that I'm just an interested freshman (planning to be a Linguistics major, and, if the CS department for my school lets me in (they're VERY selective), a CS major) that happened to find this. I don't know enough about this to compare it to others I actually just got off the phone with my grandmother and we were discussing Chomsky and Wittgenstein. I am a philosophy fan as well, so I'd love to take some of those aspects and run with them.  Cool! That's very interesting. My first Chomsky book is slated to arrive in the mail sometime soon.

A pl that will be useful when using DELPH-IN's software is Common Lisp; most of their stuff is written in it (It also happens to be my favorite PL)

[Here's](http://www.cliki.net/Getting%20Started) a useful guide to getting started with the PL. Oh I have worked with Clisp. I was one of the few people who enjoyed it.... I also really want to do some simulations for a board game called Pandemic using lisp. The nodes would be perfect for it too.  Yay!

You have inspired joy within me. That is great. After being a little bit of a creeper, I see you are a fan of lisp?  Just so happens that this subject caught my interest too, yesterday. As dlwh mentioned, google translate is example-based. I am learning Korean language nowadays, and I have noticed patterns in google's translation that show this. 

If I were to translate very simple sentences from english to korean, I wouldn't be able to tell that google is example-based. I type in "I am eating a cake", and upon receiving the Korean translation I can change the equivalent of "eat" to a number of different tenses, and differently styled verb forms. I can change "cake" too ie: one is literally "cake" written in korean letters, one is a traditional korean cake-like desert. I can change "I".  

As far as I'd know, this could very well be a sophisticated dictionary-based translation, which parses the grammar and translates my phrases word-by-word. But as I input more complicated sentences, and more "expressiony" phrases, I suddenly receive korean texts that I can't modify in any way. Simply a "phrase equivalent".

The technical details of language parsing don't seem to me like the process should be as complicated as it appears to be. But then again, there is a lot of ambiguity in human languages (especially in damn korean &amp;gt;.&amp;lt;) -  meanings that we fill out from context. 

Still this is a very interesting subject, and I would be happy to hear of your progress, or things that you learned.

</snippet></document><document><title>Time Loop Logic</title><url>http://www.reddit.com/r/compsci/comments/y56qv/time_loop_logic/</url><snippet>I still can't get how this prime factor getting algorithm would work assuming Novikov self-consistency principle: [Time loop logic](http://en.wikipedia.org/wiki/Novikov_self-consistency_principle#Time_loop_logic). Could someone explain this in a level an undergrad com sci student would understand? Thanks.  Thursday.

If you wanted to be specific, 7:24am on Thursday morning.

Harry [Potter] was sitting on his bed, a textbook lying limp in his motionless hands.

Harry had just had an idea for a truly brilliant experimental test.

It would mean waiting an extra hour for breakfast, but that was why he had cereal bars. No, this idea absolutely positively had to be tested right away, immediately, now.

Harry set the textbook aside, leapt out of bed, raced around his bed, yanked out the cavern level of his trunk, ran down the stairs, and started moving boxes of books around. (He really needed to unpack and get bookcases at some point but he was in the middle of his textbook reading contest with Hermione and falling behind so he hadn't had time.)

Harry found the book he wanted and raced back upstairs.

The other boys were getting ready to go down to breakfast in the Great Hall and start the day.

"Excuse me can you do something for me?" said Harry. He was flipping through the book's index as he spoke, found the page with the first ten thousand primes, flipped to that page, and thrust the book at Anthony Goldstein. "Pick two three-digit numbers from this list. Don't tell me what they are. Just multiply them together and tell me the product. Oh, and can you do the calculation twice to double-check? Please make really sure you've got the right answer, I'm not sure what's going to happen to me or the universe if you make a multiplication error."

It said a lot about what life in that dorm had been like over the past few days that Anthony didn't even bother saying anything like "Why'd you suddenly flip out?" or "That seems really weird, what are your reasons for asking?" or "What do you mean, you're not sure what's going to happen to the universe?"

Anthony wordlessly accepted the book and took out a parchment and quill. Harry spun around and shut his eyes, making sure not to see anything, dancing back and forth and bouncing up and down with impatience. He got a pad of paper and a mechanical pencil and got ready to write.

"Okay," Anthony said, "One hundred and eighty-one thousand, four hundred and twenty-nine."

Harry wrote down 181,429. He repeated what he'd just written down, and Anthony confirmed it.

Then Harry raced back down into the cavern level of his trunk, glanced at his watch (the watch said 4:28 which meant 7:28) and then shut his eyes.

Around thirty seconds later, Harry heard the sound of steps, followed by the sound of the cavern level of the trunk sliding shut. (Harry wasn't worried about suffocating. An automatic Air-Freshening Charm was part of what you got if you were willing to buy a really good trunk. Wasn't magic wonderful, it didn't have to worry about electric bills.)

And when Harry opened his eyes, he saw just what he'd been hoping to see, a folded piece of paper left on the floor, the gift of his future self.

Call that piece of paper "Paper-2".

Harry tore a piece of paper off his pad.

Call that "Paper-1". It was, of course, the same piece of paper. You could even see, if you looked closely, that the ragged edges matched.

Harry reviewed in his mind the algorithm that he would follow.

If Harry opened up Paper-2 and it was blank, then he would write "101 x 101" down on Paper-1, fold it up, study for an hour, go back in time, drop off Paper-1 (which would thereby become Paper-2), and head on up out of the cavern level to join his dorm mates for breakfast.

If Harry opened up Paper-2 and it had two numbers written on it, Harry would multiply those numbers together.

If their product equaled 181,429, Harry would write down those two numbers on Paper-1 and send Paper-1 back in time.

Otherwise Harry would add 2 to the number on the right and write down the new pair of numbers on Paper-1. Unless that made the number on the right greater than 997, in which case Harry would add 2 to the number on the left and write down 101 on the right.

And if Paper-2 said 997 x 997, Harry would leave Paper-1 blank.

Which meant that the only possible stable time loop was the one in which Paper-2 contained the two prime factors of 181,429.

If this worked, Harry could use it to recover any sort of answer that was easy to check but hard to find. He wouldn't have just shown that P=NP once you had a Time-Turner, this trick was more general than that. Harry could use it to find the combinations on combination locks, or passwords of every sort. Maybe even find the entrance to Slytherin's Chamber of Secrets, if Harry could figure out some systematic way of describing all the locations in Hogwarts. It would be an awesome cheat even by Harry's standards of cheating.

Harry took Paper-2 in his trembling hand, and unfolded it.

....

Link : http://hpmor.com/chapter/17 . Fanfic by Eliezer Yudkowsky. Thanks! Now I think I get it. When the computer finished the verification step, it is already in the future, so if he sends back F+1, it contradicts what the computer got earlier: F. So the only possibility is that the correct solution is sent back. Is this correct? Thanks! Now I think I get it. When the computer finished the verification step, it is already in the future, so if he sends back F+1, it contradicts what the computer got earlier: F. So the only possibility is that the correct solution is sent back. Is this correct? You might want to continue reading at the link. The result is... unexpected. Thursday.

If you wanted to be specific, 7:24am on Thursday morning.

Harry [Potter] was sitting on his bed, a textbook lying limp in his motionless hands.

Harry had just had an idea for a truly brilliant experimental test.

It would mean waiting an extra hour for breakfast, but that was why he had cereal bars. No, this idea absolutely positively had to be tested right away, immediately, now.

Harry set the textbook aside, leapt out of bed, raced around his bed, yanked out the cavern level of his trunk, ran down the stairs, and started moving boxes of books around. (He really needed to unpack and get bookcases at some point but he was in the middle of his textbook reading contest with Hermione and falling behind so he hadn't had time.)

Harry found the book he wanted and raced back upstairs.

The other boys were getting ready to go down to breakfast in the Great Hall and start the day.

"Excuse me can you do something for me?" said Harry. He was flipping through the book's index as he spoke, found the page with the first ten thousand primes, flipped to that page, and thrust the book at Anthony Goldstein. "Pick two three-digit numbers from this list. Don't tell me what they are. Just multiply them together and tell me the product. Oh, and can you do the calculation twice to double-check? Please make really sure you've got the right answer, I'm not sure what's going to happen to me or the universe if you make a multiplication error."

It said a lot about what life in that dorm had been like over the past few days that Anthony didn't even bother saying anything like "Why'd you suddenly flip out?" or "That seems really weird, what are your reasons for asking?" or "What do you mean, you're not sure what's going to happen to the universe?"

Anthony wordlessly accepted the book and took out a parchment and quill. Harry spun around and shut his eyes, making sure not to see anything, dancing back and forth and bouncing up and down with impatience. He got a pad of paper and a mechanical pencil and got ready to write.

"Okay," Anthony said, "One hundred and eighty-one thousand, four hundred and twenty-nine."

Harry wrote down 181,429. He repeated what he'd just written down, and Anthony confirmed it.

Then Harry raced back down into the cavern level of his trunk, glanced at his watch (the watch said 4:28 which meant 7:28) and then shut his eyes.

Around thirty seconds later, Harry heard the sound of steps, followed by the sound of the cavern level of the trunk sliding shut. (Harry wasn't worried about suffocating. An automatic Air-Freshening Charm was part of what you got if you were willing to buy a really good trunk. Wasn't magic wonderful, it didn't have to worry about electric bills.)

And when Harry opened his eyes, he saw just what he'd been hoping to see, a folded piece of paper left on the floor, the gift of his future self.

Call that piece of paper "Paper-2".

Harry tore a piece of paper off his pad.

Call that "Paper-1". It was, of course, the same piece of paper. You could even see, if you looked closely, that the ragged edges matched.

Harry reviewed in his mind the algorithm that he would follow.

If Harry opened up Paper-2 and it was blank, then he would write "101 x 101" down on Paper-1, fold it up, study for an hour, go back in time, drop off Paper-1 (which would thereby become Paper-2), and head on up out of the cavern level to join his dorm mates for breakfast.

If Harry opened up Paper-2 and it had two numbers written on it, Harry would multiply those numbers together.

If their product equaled 181,429, Harry would write down those two numbers on Paper-1 and send Paper-1 back in time.

Otherwise Harry would add 2 to the number on the right and write down the new pair of numbers on Paper-1. Unless that made the number on the right greater than 997, in which case Harry would add 2 to the number on the left and write down 101 on the right.

And if Paper-2 said 997 x 997, Harry would leave Paper-1 blank.

Which meant that the only possible stable time loop was the one in which Paper-2 contained the two prime factors of 181,429.

If this worked, Harry could use it to recover any sort of answer that was easy to check but hard to find. He wouldn't have just shown that P=NP once you had a Time-Turner, this trick was more general than that. Harry could use it to find the combinations on combination locks, or passwords of every sort. Maybe even find the entrance to Slytherin's Chamber of Secrets, if Harry could figure out some systematic way of describing all the locations in Hogwarts. It would be an awesome cheat even by Harry's standards of cheating.

Harry took Paper-2 in his trembling hand, and unfolded it.

....

Link : http://hpmor.com/chapter/17 . Fanfic by Eliezer Yudkowsky. Ho...ly...SHIT! That was awesome. It is only the beginning. I would highly recommend HPMOR, even you like neither Harry Potter nor Fanfics. It just has great writing from Eliezer [(Less Wrong)](http://lesswrong.com/) It is only the beginning. I would highly recommend HPMOR, even you like neither Harry Potter nor Fanfics. It just has great writing from Eliezer [(Less Wrong)](http://lesswrong.com/) Thursday.

If you wanted to be specific, 7:24am on Thursday morning.

Harry [Potter] was sitting on his bed, a textbook lying limp in his motionless hands.

Harry had just had an idea for a truly brilliant experimental test.

It would mean waiting an extra hour for breakfast, but that was why he had cereal bars. No, this idea absolutely positively had to be tested right away, immediately, now.

Harry set the textbook aside, leapt out of bed, raced around his bed, yanked out the cavern level of his trunk, ran down the stairs, and started moving boxes of books around. (He really needed to unpack and get bookcases at some point but he was in the middle of his textbook reading contest with Hermione and falling behind so he hadn't had time.)

Harry found the book he wanted and raced back upstairs.

The other boys were getting ready to go down to breakfast in the Great Hall and start the day.

"Excuse me can you do something for me?" said Harry. He was flipping through the book's index as he spoke, found the page with the first ten thousand primes, flipped to that page, and thrust the book at Anthony Goldstein. "Pick two three-digit numbers from this list. Don't tell me what they are. Just multiply them together and tell me the product. Oh, and can you do the calculation twice to double-check? Please make really sure you've got the right answer, I'm not sure what's going to happen to me or the universe if you make a multiplication error."

It said a lot about what life in that dorm had been like over the past few days that Anthony didn't even bother saying anything like "Why'd you suddenly flip out?" or "That seems really weird, what are your reasons for asking?" or "What do you mean, you're not sure what's going to happen to the universe?"

Anthony wordlessly accepted the book and took out a parchment and quill. Harry spun around and shut his eyes, making sure not to see anything, dancing back and forth and bouncing up and down with impatience. He got a pad of paper and a mechanical pencil and got ready to write.

"Okay," Anthony said, "One hundred and eighty-one thousand, four hundred and twenty-nine."

Harry wrote down 181,429. He repeated what he'd just written down, and Anthony confirmed it.

Then Harry raced back down into the cavern level of his trunk, glanced at his watch (the watch said 4:28 which meant 7:28) and then shut his eyes.

Around thirty seconds later, Harry heard the sound of steps, followed by the sound of the cavern level of the trunk sliding shut. (Harry wasn't worried about suffocating. An automatic Air-Freshening Charm was part of what you got if you were willing to buy a really good trunk. Wasn't magic wonderful, it didn't have to worry about electric bills.)

And when Harry opened his eyes, he saw just what he'd been hoping to see, a folded piece of paper left on the floor, the gift of his future self.

Call that piece of paper "Paper-2".

Harry tore a piece of paper off his pad.

Call that "Paper-1". It was, of course, the same piece of paper. You could even see, if you looked closely, that the ragged edges matched.

Harry reviewed in his mind the algorithm that he would follow.

If Harry opened up Paper-2 and it was blank, then he would write "101 x 101" down on Paper-1, fold it up, study for an hour, go back in time, drop off Paper-1 (which would thereby become Paper-2), and head on up out of the cavern level to join his dorm mates for breakfast.

If Harry opened up Paper-2 and it had two numbers written on it, Harry would multiply those numbers together.

If their product equaled 181,429, Harry would write down those two numbers on Paper-1 and send Paper-1 back in time.

Otherwise Harry would add 2 to the number on the right and write down the new pair of numbers on Paper-1. Unless that made the number on the right greater than 997, in which case Harry would add 2 to the number on the left and write down 101 on the right.

And if Paper-2 said 997 x 997, Harry would leave Paper-1 blank.

Which meant that the only possible stable time loop was the one in which Paper-2 contained the two prime factors of 181,429.

If this worked, Harry could use it to recover any sort of answer that was easy to check but hard to find. He wouldn't have just shown that P=NP once you had a Time-Turner, this trick was more general than that. Harry could use it to find the combinations on combination locks, or passwords of every sort. Maybe even find the entrance to Slytherin's Chamber of Secrets, if Harry could figure out some systematic way of describing all the locations in Hogwarts. It would be an awesome cheat even by Harry's standards of cheating.

Harry took Paper-2 in his trembling hand, and unfolded it.

....

Link : http://hpmor.com/chapter/17 . Fanfic by Eliezer Yudkowsky. Apart from being hopelessly wordy, this seems to differ from the algorithm described at wikipedia by:

&amp;gt; If Harry opened up Paper-2 and it was blank, then he would write "101 x 101" down on Paper-1

I haven't read more of the WP page to know if there is a reason other than poor editing that this seed step is missing, but it seems vital to a muggle used to coding in directed time.

*EDIT:  I don't mean to belittle your post with the description "hopelessly wordy" .. it's entertaining and seems to match the OP's request :-)* Writing "101x101" is equivalent to a future party sending "F" to the past party - the Wiki article and the fan-fic description seem to match.

As for "hopelessly wordy": I didn't write it - so feel free to criticise :). The part I don't see in WP is "if the paper is blank".  There seems to be a bootstrap paradox (what's the "first" F that will be received?) - but I'm not familiar enough with time loops to be sure :-). Well, yes. They should have added on the WP -

4.1.3 If no F received, send some 'r' backward on channel C.

Otherwise there is a possible stable loop, in which no one sends or receives anything.  &amp;gt; Otherwise there is a possible stable loop, in which no one sends or receives anything.

That succinctly identifies the error.

Being pedantic, I don't think your clause is enough - only because there is no implied timeout on the "wait for a message" step. Being the lord god of pedantry, I will point out that there is no need for a timout. Either the message would be there, or it won't - if it isn't, no amount of waiting will make it appear, since the channel sends the message from the future to a *specific* point in time. You are absolutely right.  I have a hard time getting my head around dealing with *instantaneous*, but that's probably because I've been dealing with I/O a lot recently :-).

I still think step 3 needs to be re-worded to make clear that the message is a Maybe, but you have successfully out-pedanted me. Thursday.

If you wanted to be specific, 7:24am on Thursday morning.

Harry [Potter] was sitting on his bed, a textbook lying limp in his motionless hands.

Harry had just had an idea for a truly brilliant experimental test.

It would mean waiting an extra hour for breakfast, but that was why he had cereal bars. No, this idea absolutely positively had to be tested right away, immediately, now.

Harry set the textbook aside, leapt out of bed, raced around his bed, yanked out the cavern level of his trunk, ran down the stairs, and started moving boxes of books around. (He really needed to unpack and get bookcases at some point but he was in the middle of his textbook reading contest with Hermione and falling behind so he hadn't had time.)

Harry found the book he wanted and raced back upstairs.

The other boys were getting ready to go down to breakfast in the Great Hall and start the day.

"Excuse me can you do something for me?" said Harry. He was flipping through the book's index as he spoke, found the page with the first ten thousand primes, flipped to that page, and thrust the book at Anthony Goldstein. "Pick two three-digit numbers from this list. Don't tell me what they are. Just multiply them together and tell me the product. Oh, and can you do the calculation twice to double-check? Please make really sure you've got the right answer, I'm not sure what's going to happen to me or the universe if you make a multiplication error."

It said a lot about what life in that dorm had been like over the past few days that Anthony didn't even bother saying anything like "Why'd you suddenly flip out?" or "That seems really weird, what are your reasons for asking?" or "What do you mean, you're not sure what's going to happen to the universe?"

Anthony wordlessly accepted the book and took out a parchment and quill. Harry spun around and shut his eyes, making sure not to see anything, dancing back and forth and bouncing up and down with impatience. He got a pad of paper and a mechanical pencil and got ready to write.

"Okay," Anthony said, "One hundred and eighty-one thousand, four hundred and twenty-nine."

Harry wrote down 181,429. He repeated what he'd just written down, and Anthony confirmed it.

Then Harry raced back down into the cavern level of his trunk, glanced at his watch (the watch said 4:28 which meant 7:28) and then shut his eyes.

Around thirty seconds later, Harry heard the sound of steps, followed by the sound of the cavern level of the trunk sliding shut. (Harry wasn't worried about suffocating. An automatic Air-Freshening Charm was part of what you got if you were willing to buy a really good trunk. Wasn't magic wonderful, it didn't have to worry about electric bills.)

And when Harry opened his eyes, he saw just what he'd been hoping to see, a folded piece of paper left on the floor, the gift of his future self.

Call that piece of paper "Paper-2".

Harry tore a piece of paper off his pad.

Call that "Paper-1". It was, of course, the same piece of paper. You could even see, if you looked closely, that the ragged edges matched.

Harry reviewed in his mind the algorithm that he would follow.

If Harry opened up Paper-2 and it was blank, then he would write "101 x 101" down on Paper-1, fold it up, study for an hour, go back in time, drop off Paper-1 (which would thereby become Paper-2), and head on up out of the cavern level to join his dorm mates for breakfast.

If Harry opened up Paper-2 and it had two numbers written on it, Harry would multiply those numbers together.

If their product equaled 181,429, Harry would write down those two numbers on Paper-1 and send Paper-1 back in time.

Otherwise Harry would add 2 to the number on the right and write down the new pair of numbers on Paper-1. Unless that made the number on the right greater than 997, in which case Harry would add 2 to the number on the left and write down 101 on the right.

And if Paper-2 said 997 x 997, Harry would leave Paper-1 blank.

Which meant that the only possible stable time loop was the one in which Paper-2 contained the two prime factors of 181,429.

If this worked, Harry could use it to recover any sort of answer that was easy to check but hard to find. He wouldn't have just shown that P=NP once you had a Time-Turner, this trick was more general than that. Harry could use it to find the combinations on combination locks, or passwords of every sort. Maybe even find the entrance to Slytherin's Chamber of Secrets, if Harry could figure out some systematic way of describing all the locations in Hogwarts. It would be an awesome cheat even by Harry's standards of cheating.

Harry took Paper-2 in his trembling hand, and unfolded it.

....

Link : http://hpmor.com/chapter/17 . Fanfic by Eliezer Yudkowsky. YES. Beyond elated that you cited HPMOR Thursday.

If you wanted to be specific, 7:24am on Thursday morning.

Harry [Potter] was sitting on his bed, a textbook lying limp in his motionless hands.

Harry had just had an idea for a truly brilliant experimental test.

It would mean waiting an extra hour for breakfast, but that was why he had cereal bars. No, this idea absolutely positively had to be tested right away, immediately, now.

Harry set the textbook aside, leapt out of bed, raced around his bed, yanked out the cavern level of his trunk, ran down the stairs, and started moving boxes of books around. (He really needed to unpack and get bookcases at some point but he was in the middle of his textbook reading contest with Hermione and falling behind so he hadn't had time.)

Harry found the book he wanted and raced back upstairs.

The other boys were getting ready to go down to breakfast in the Great Hall and start the day.

"Excuse me can you do something for me?" said Harry. He was flipping through the book's index as he spoke, found the page with the first ten thousand primes, flipped to that page, and thrust the book at Anthony Goldstein. "Pick two three-digit numbers from this list. Don't tell me what they are. Just multiply them together and tell me the product. Oh, and can you do the calculation twice to double-check? Please make really sure you've got the right answer, I'm not sure what's going to happen to me or the universe if you make a multiplication error."

It said a lot about what life in that dorm had been like over the past few days that Anthony didn't even bother saying anything like "Why'd you suddenly flip out?" or "That seems really weird, what are your reasons for asking?" or "What do you mean, you're not sure what's going to happen to the universe?"

Anthony wordlessly accepted the book and took out a parchment and quill. Harry spun around and shut his eyes, making sure not to see anything, dancing back and forth and bouncing up and down with impatience. He got a pad of paper and a mechanical pencil and got ready to write.

"Okay," Anthony said, "One hundred and eighty-one thousand, four hundred and twenty-nine."

Harry wrote down 181,429. He repeated what he'd just written down, and Anthony confirmed it.

Then Harry raced back down into the cavern level of his trunk, glanced at his watch (the watch said 4:28 which meant 7:28) and then shut his eyes.

Around thirty seconds later, Harry heard the sound of steps, followed by the sound of the cavern level of the trunk sliding shut. (Harry wasn't worried about suffocating. An automatic Air-Freshening Charm was part of what you got if you were willing to buy a really good trunk. Wasn't magic wonderful, it didn't have to worry about electric bills.)

And when Harry opened his eyes, he saw just what he'd been hoping to see, a folded piece of paper left on the floor, the gift of his future self.

Call that piece of paper "Paper-2".

Harry tore a piece of paper off his pad.

Call that "Paper-1". It was, of course, the same piece of paper. You could even see, if you looked closely, that the ragged edges matched.

Harry reviewed in his mind the algorithm that he would follow.

If Harry opened up Paper-2 and it was blank, then he would write "101 x 101" down on Paper-1, fold it up, study for an hour, go back in time, drop off Paper-1 (which would thereby become Paper-2), and head on up out of the cavern level to join his dorm mates for breakfast.

If Harry opened up Paper-2 and it had two numbers written on it, Harry would multiply those numbers together.

If their product equaled 181,429, Harry would write down those two numbers on Paper-1 and send Paper-1 back in time.

Otherwise Harry would add 2 to the number on the right and write down the new pair of numbers on Paper-1. Unless that made the number on the right greater than 997, in which case Harry would add 2 to the number on the left and write down 101 on the right.

And if Paper-2 said 997 x 997, Harry would leave Paper-1 blank.

Which meant that the only possible stable time loop was the one in which Paper-2 contained the two prime factors of 181,429.

If this worked, Harry could use it to recover any sort of answer that was easy to check but hard to find. He wouldn't have just shown that P=NP once you had a Time-Turner, this trick was more general than that. Harry could use it to find the combinations on combination locks, or passwords of every sort. Maybe even find the entrance to Slytherin's Chamber of Secrets, if Harry could figure out some systematic way of describing all the locations in Hogwarts. It would be an awesome cheat even by Harry's standards of cheating.

Harry took Paper-2 in his trembling hand, and unfolded it.

....

Link : http://hpmor.com/chapter/17 . Fanfic by Eliezer Yudkowsky. Thursday.

If you wanted to be specific, 7:24am on Thursday morning.

Harry [Potter] was sitting on his bed, a textbook lying limp in his motionless hands.

Harry had just had an idea for a truly brilliant experimental test.

It would mean waiting an extra hour for breakfast, but that was why he had cereal bars. No, this idea absolutely positively had to be tested right away, immediately, now.

Harry set the textbook aside, leapt out of bed, raced around his bed, yanked out the cavern level of his trunk, ran down the stairs, and started moving boxes of books around. (He really needed to unpack and get bookcases at some point but he was in the middle of his textbook reading contest with Hermione and falling behind so he hadn't had time.)

Harry found the book he wanted and raced back upstairs.

The other boys were getting ready to go down to breakfast in the Great Hall and start the day.

"Excuse me can you do something for me?" said Harry. He was flipping through the book's index as he spoke, found the page with the first ten thousand primes, flipped to that page, and thrust the book at Anthony Goldstein. "Pick two three-digit numbers from this list. Don't tell me what they are. Just multiply them together and tell me the product. Oh, and can you do the calculation twice to double-check? Please make really sure you've got the right answer, I'm not sure what's going to happen to me or the universe if you make a multiplication error."

It said a lot about what life in that dorm had been like over the past few days that Anthony didn't even bother saying anything like "Why'd you suddenly flip out?" or "That seems really weird, what are your reasons for asking?" or "What do you mean, you're not sure what's going to happen to the universe?"

Anthony wordlessly accepted the book and took out a parchment and quill. Harry spun around and shut his eyes, making sure not to see anything, dancing back and forth and bouncing up and down with impatience. He got a pad of paper and a mechanical pencil and got ready to write.

"Okay," Anthony said, "One hundred and eighty-one thousand, four hundred and twenty-nine."

Harry wrote down 181,429. He repeated what he'd just written down, and Anthony confirmed it.

Then Harry raced back down into the cavern level of his trunk, glanced at his watch (the watch said 4:28 which meant 7:28) and then shut his eyes.

Around thirty seconds later, Harry heard the sound of steps, followed by the sound of the cavern level of the trunk sliding shut. (Harry wasn't worried about suffocating. An automatic Air-Freshening Charm was part of what you got if you were willing to buy a really good trunk. Wasn't magic wonderful, it didn't have to worry about electric bills.)

And when Harry opened his eyes, he saw just what he'd been hoping to see, a folded piece of paper left on the floor, the gift of his future self.

Call that piece of paper "Paper-2".

Harry tore a piece of paper off his pad.

Call that "Paper-1". It was, of course, the same piece of paper. You could even see, if you looked closely, that the ragged edges matched.

Harry reviewed in his mind the algorithm that he would follow.

If Harry opened up Paper-2 and it was blank, then he would write "101 x 101" down on Paper-1, fold it up, study for an hour, go back in time, drop off Paper-1 (which would thereby become Paper-2), and head on up out of the cavern level to join his dorm mates for breakfast.

If Harry opened up Paper-2 and it had two numbers written on it, Harry would multiply those numbers together.

If their product equaled 181,429, Harry would write down those two numbers on Paper-1 and send Paper-1 back in time.

Otherwise Harry would add 2 to the number on the right and write down the new pair of numbers on Paper-1. Unless that made the number on the right greater than 997, in which case Harry would add 2 to the number on the left and write down 101 on the right.

And if Paper-2 said 997 x 997, Harry would leave Paper-1 blank.

Which meant that the only possible stable time loop was the one in which Paper-2 contained the two prime factors of 181,429.

If this worked, Harry could use it to recover any sort of answer that was easy to check but hard to find. He wouldn't have just shown that P=NP once you had a Time-Turner, this trick was more general than that. Harry could use it to find the combinations on combination locks, or passwords of every sort. Maybe even find the entrance to Slytherin's Chamber of Secrets, if Harry could figure out some systematic way of describing all the locations in Hogwarts. It would be an awesome cheat even by Harry's standards of cheating.

Harry took Paper-2 in his trembling hand, and unfolded it.

....

Link : http://hpmor.com/chapter/17 . Fanfic by Eliezer Yudkowsky.  The idea is the following: you receive an answer from the future.
If it is correct, you send it back in time. Note that now you ARE in the
future from which the result originally came from---everything works nicely
in this case.

But, assume that you get a WRONG answer (in the case of factoring or
even an NP-complete problem, checking the answer is easy enough). Now
you send something *different* to the past. Clearly, this creates a paradox:
you (in the future) send something different than the thing you (in the past)
received! We expect that this is impossible, as events that we clearly witnessed
in the past now *cannot* have happened. I think, in rough terms, the concept is that if you start an unstable time loop, it will eventually stabilize. "Once" it stabilizes, there are an infinite number of universes in which it has stabilized, and a finite number required to reach that stable state, so the odds are Very Good that you live in one of the ones after it reached stability. Hm, I disagree. From wikipedia (I do not know anything beyond was is
said there):
&amp;gt; Stated simply, the Novikov consistency principle asserts that if an event exists that would give rise to a paradox, or to any "change" to the past whatsoever, then the probability of that event is zero

Under this premise you would simply get the solution on "the first try", because everything else creates a paradoxon and thus has probability zero. 

Maybe I misunderstood you: it is kind of awkward to talk about processes that "run" (already language fails here) outside of time. No, I think I am trying to say the same thing as that principle. N/(Inf+N) = zero (ish). Hm, I disagree. From wikipedia (I do not know anything beyond was is
said there):
&amp;gt; Stated simply, the Novikov consistency principle asserts that if an event exists that would give rise to a paradox, or to any "change" to the past whatsoever, then the probability of that event is zero

Under this premise you would simply get the solution on "the first try", because everything else creates a paradoxon and thus has probability zero. 

Maybe I misunderstood you: it is kind of awkward to talk about processes that "run" (already language fails here) outside of time.  I'm pretty sure it was [Scott Aaronson](http://www.scottaaronson.com/blog/) who a while ago had a post mentioning a similar strategy relying on the many-worlds interpretation of QM to solve NP problems in P time.  I'm probably butchering the algorithm, but this should get the idea across:

  * using an RNG based on quantum effects, choose a random candidate factor F
  * check if F divides N
  * if it doesn't, kill yourself

... maybe not as exciting, but amusing. I'm pretty sure it was [Scott Aaronson](http://www.scottaaronson.com/blog/) who a while ago had a post mentioning a similar strategy relying on the many-worlds interpretation of QM to solve NP problems in P time.  I'm probably butchering the algorithm, but this should get the idea across:

  * using an RNG based on quantum effects, choose a random candidate factor F
  * check if F divides N
  * if it doesn't, kill yourself

... maybe not as exciting, but amusing.  Yes, it seems more like a joke than something plausible, but who knows. One day we may be able to bend space-time just enough for this to work.

Reminds me of that story of [Lallafa](http://en.wikipedia.org/wiki/List_of_minor_The_Hitchhiker%27s_Guide_to_the_Galaxy_characters#Lallafa) from Hitchhiker's Guide. I think it's less a joke and more an esoteric complexity class. Yeah, I mean I see the practical use. NFAs always seemed like a digression until quantum computing became a possibility. </snippet></document><document><title>Straw poll:  Who really believes that P might just equal NP after all?</title><url>http://www.reddit.com/r/compsci/comments/y37m1/straw_poll_who_really_believes_that_p_might_just/</url><snippet>As programmers, it's in our blood to realize that some problems have quick and efficient solutions while others are seemingly intractable.  So the very notion that *any* problem can be solved easily seems to fly in the face of a lifetime of experience. For these reasons, everything seems to indicate that P =/= NP. It's like my college prof said: "Hard problems are hard."  

But then there's that other group, those conspiracy theorists types who suggest that P=NP after all.  These guys aren't dopes, they have a very deep point.

Here we are, very limited biological programmers, and our techniques are very crude. In this infinite space of algorithms, we've not even scratched the surface. How many among us avoid recursion in practical applications because the complications are too much for our little minds to keep track of?  (I plead guilty.) Who among us uses self-modifying code to get a job done?  (Nobody I know.) We're like monkeys arranging boxes to try to get to the banana.  Maybe if we had a God's-eye-view of all possible algorithms, there would no longer be any such thing as a "hard problem."

In short, my poll-question is this: How many of us are really believe we might just live in a P=NP universe?    [Here](http://www.cs.umd.edu/~gasarch/papers/poll.pdf) are the opinions of professionals. I get excited like a little school girl whenever I reference this paper to people because I know/am friends with the various professors they polled in this. &amp;gt;Jamie Andrews: (University of Waterloo, 2015, P=NP) There will be an O(n^log(n)) algorithm
&amp;gt;for an NP-complete problem, rendering the whole P vs. NP question essentially
&amp;gt;irrelevant. :-)

Does that 2015 means that he thinks that the proof will be made in 2015? Well, if he has made a bet, you cant say he doesnt have iron balls. &amp;gt;Jamie Andrews: (University of Waterloo, 2015, P=NP) There will be an O(n^log(n)) algorithm
&amp;gt;for an NP-complete problem, rendering the whole P vs. NP question essentially
&amp;gt;irrelevant. :-)

Does that 2015 means that he thinks that the proof will be made in 2015? Well, if he has made a bet, you cant say he doesnt have iron balls. &amp;gt;Jamie Andrews: (University of Waterloo, 2015, P=NP) There will be an O(n^log(n)) algorithm
&amp;gt;for an NP-complete problem, rendering the whole P vs. NP question essentially
&amp;gt;irrelevant. :-)

Does that 2015 means that he thinks that the proof will be made in 2015? Well, if he has made a bet, you cant say he doesnt have iron balls. &amp;gt;Jamie Andrews: (University of Waterloo, 2015, P=NP) There will be an O(n^log(n)) algorithm
&amp;gt;for an NP-complete problem, rendering the whole P vs. NP question essentially
&amp;gt;irrelevant. :-)

Does that 2015 means that he thinks that the proof will be made in 2015? Well, if he has made a bet, you cant say he doesnt have iron balls. It's his guess. Yes, I was just pointing that its a very optimistic guess. &amp;gt;Jamie Andrews: (University of Waterloo, 2015, P=NP) There will be an O(n^log(n)) algorithm
&amp;gt;for an NP-complete problem, rendering the whole P vs. NP question essentially
&amp;gt;irrelevant. :-)

Does that 2015 means that he thinks that the proof will be made in 2015? Well, if he has made a bet, you cant say he doesnt have iron balls. &amp;gt;Jamie Andrews: (University of Waterloo, 2015, P=NP) There will be an O(n^log(n)) algorithm
&amp;gt;for an NP-complete problem, rendering the whole P vs. NP question essentially
&amp;gt;irrelevant. :-)

Does that 2015 means that he thinks that the proof will be made in 2015? Well, if he has made a bet, you cant say he doesnt have iron balls. [Here](http://www.cs.umd.edu/~gasarch/papers/poll.pdf) are the opinions of professionals. I get excited like a little school girl whenever I reference this paper to people because I know/am friends with the various professors they polled in this. [Here](http://www.cs.umd.edu/~gasarch/papers/poll.pdf) are the opinions of professionals. I get excited like a little school girl whenever I reference this paper to people because I know/am friends with the various professors they polled in this. [Here](http://www.cs.umd.edu/~gasarch/papers/poll.pdf) are the opinions of professionals. I get excited like a little school girl whenever I reference this paper to people because I know/am friends with the various professors they polled in this.  proof by general consensus The very best kind of proof. No, no it's not.  If you're riffing on "technically correct" you got it backwards!  They're being sarcastic. It's a quote from futurama. The thing is, technically correct being the best kind of correct has a truthiness about it that proof by general consensus being the best kind of proof totally rejects.  The futurama quote is good.  vinfx's comment is bad, and he should feel bad.  I just wanted to remark that P=NP doesn't actually mean that any problem can be solved "easily".  There are lots of problems that require exponential time or worse, even if P=NP.  P=NP would just mean that problems which are "easy" to verify are "easy" to solve, which is a whole different thing. (where "easy" should be read as in polynomial time) Thanks for pointing out that important distinction. I actually meant to type *any* *NP* problem.  It's also technically improper to discuss NP problems as necessarily "hard" and P problems as "easy", but I wanted to discuss this on a philosophical level and be accessible to laymen. I just wanted to remark that P=NP doesn't actually mean that any problem can be solved "easily".  There are lots of problems that require exponential time or worse, even if P=NP.  P=NP would just mean that problems which are "easy" to verify are "easy" to solve, which is a whole different thing. (where "easy" should be read as in polynomial time) In fact, it's trivial to construct a problem that cannot be solved in less than exponential time.

Example: Given a number N, print every number from 0 to 2^N . That's somewhat cheating, in that P and NP are generally defined in terms of decision problems (output a yes or no), or more rarely in terms of search problems (find some element that satisfies these criteria).

A good example of a well-defined problem that cannot be solved at all (in the general case), let alone in polynomial time, is the halting problem. Every problem in P or NP reduces to it -- change the algorithm that you have to loop forever if it returns 0, then ask if the resulting algorithm halts -- so it's NP-hard. A less pathological example is [TQBF](http://en.wikipedia.org/wiki/True_quantified_Boolean_formula). TQBF is hard, but it doesn't necessarily require exponential time - it's PSPACE complete, but we don't know that P!=PSPACE.

A decision problem that requires exponential time is easy to come by: "Does the Turing machine *M* halt in *n* steps or less?" That one provably requires exponential time, provided *n* is encoded in a sensible number system (not unary).  1. P=NP will be resolved between 2002-2009: 5
2. P=NP will be resolved between 2010-2019: 12
3. P=NP will be resolved between 2020-2029: 13
4. P=NP will be resolved between 2030-2039: 10
5. P=NP will be resolved between 2040-2049: 5
6. P=NP will be resolved between 2050-2059: 12
7. P=NP will be resolved between 2060-2069: 4
8. P=NP will be resolved between 2070-2079: 0
9. P=NP will be resolved between 2080-2089: 1
10. P=NP will be resolved between 2090-2099: 0
11. P=NP will be resolved between 2100-2110: 7
12. P=NP will be resolved between 2100-2199: 0
13. P=NP will be resolved between 2200-3000: 5
14. P=NP will never be resolved : 5.

Source: http://www.cs.umd.edu/~gasarch/papers/poll.pdf 1. P=NP will be resolved between 2002-2009: 5
2. P=NP will be resolved between 2010-2019: 12
3. P=NP will be resolved between 2020-2029: 13
4. P=NP will be resolved between 2030-2039: 10
5. P=NP will be resolved between 2040-2049: 5
6. P=NP will be resolved between 2050-2059: 12
7. P=NP will be resolved between 2060-2069: 4
8. P=NP will be resolved between 2070-2079: 0
9. P=NP will be resolved between 2080-2089: 1
10. P=NP will be resolved between 2090-2099: 0
11. P=NP will be resolved between 2100-2110: 7
12. P=NP will be resolved between 2100-2199: 0
13. P=NP will be resolved between 2200-3000: 5
14. P=NP will never be resolved : 5.

Source: http://www.cs.umd.edu/~gasarch/papers/poll.pdf   This has to be the computer science equivalent of the Grand Unified Theory of Physics.  Seems like the debate will go on forever.  I have to say that if someone ever proved that P=NP it would be earth shattering in computer science.  I have to wonder if by proving it we would actually be able to use it it to solve problems more efficiently.  Does proving it actually give us the P solutions to the NP problems?  Or can we only say one exists and we still have to find it which is still hard. &amp;gt; Does proving it actually give us the P solutions to the NP problems? 

The answer is yes and no.

P = NP means, for every NP problem *there exists* a deterministic polynomial-time algorithm. In principle, this could be proven non-constructively, so that we know these things (say, P-algorithms for SAT) exist but don't know of any examples yet. That would suck right? We would know that P = NP but couldn't take advantage of it. (see a [similar thread](http://www.reddit.com/r/compsci/comments/xtwya/how_can_it_be_decidable_whether_%CF%80_has_some/) about nonconstructive proofs of decidability)

Except, I lied. P = NP can't be non-constructive. Suppose someone proves P = NP in a nonconstructive way. Still, we *would* have examples of P-algorithms for SAT -- see [this section](http://en.wikipedia.org/wiki/P_versus_NP_problem#Polynomial-time_algorithms) of the wikipedia article for P vs NP. This is an algorithm, that *we can write down today*, that solves SAT in polynomial time iff P = NP. However, the constants hidden in the big-O of this algorithm are astronomical, so it's not really that interesting at all.

So, if P = NP is proven, we will have at least one polytime algorithm for SAT that we can write down. But it is a ridiculously unrealistic algorithm. It's conceivable that we could prove P = NP and not know any "natural" polytime algorithms for NP-complete problems.  The idea is correct but the Wikipedia page isn't. The algorithm in the link doesn't decide anything as it runs forever on no instances.

The trick it uses is that P=NP implies that for any NP language L there exists an algorithm K that semi decides L in polynomial time. It then enumerates all algorithms, running them for a i steps, until it finds K (the time this takes doesn't depend on input size) and is able to run it for however many steps it takes to get the correct output (O(poly(n)) by P=NP).

But P = NP implies Co-NP = NP because P is closed under complement and so an algorithm exists that decides any NP language L in polynomial time if P = NP.

This algorithm should check for proofs of no instances as well as yes instances. Do you always know how to check a proof of a no instance (assuming we only have non-constructive proof for P=NP)? P=NP implies that a verifier for the no-case *exists* but it doesn't seem to be always easy to find one.

On the wiki page they use subset sum as example. Verifying a yes-proof is easy (as the problem is in NP) but I don't have an idea what a no-proof would even look like there. This has to be the computer science equivalent of the Grand Unified Theory of Physics.  Seems like the debate will go on forever.  I have to say that if someone ever proved that P=NP it would be earth shattering in computer science.  I have to wonder if by proving it we would actually be able to use it it to solve problems more efficiently.  Does proving it actually give us the P solutions to the NP problems?  Or can we only say one exists and we still have to find it which is still hard. This has to be the computer science equivalent of the Grand Unified Theory of Physics.  Seems like the debate will go on forever.  I have to say that if someone ever proved that P=NP it would be earth shattering in computer science.  I have to wonder if by proving it we would actually be able to use it it to solve problems more efficiently.  Does proving it actually give us the P solutions to the NP problems?  Or can we only say one exists and we still have to find it which is still hard. My gut instinct is that P != NP.

However, your mention of GUT makes me wonder if the "solution" will be a cheat - from the realm of physics. Maybe a generalized quantum superposition-based solver method - or literally bending time (example: the ships computers in Star Trek are themselves in a temporal warp field to allow for faster than light processing).

To me it feels like P != NP for the universe "as I understand it". It'll take someone proving the universe is more exotic to change that view. This has to be the computer science equivalent of the Grand Unified Theory of Physics.  Seems like the debate will go on forever.  I have to say that if someone ever proved that P=NP it would be earth shattering in computer science.  I have to wonder if by proving it we would actually be able to use it it to solve problems more efficiently.  Does proving it actually give us the P solutions to the NP problems?  Or can we only say one exists and we still have to find it which is still hard.  It's also possible that P=NP but that the constant factor on algorithms for solving NP problems is extremely high, making the P=NP result uninteresting in practice. Just as possible is that the polynomial terms are of high order, leading to the same conclusion.

My favourite is that it could be unprovable *and true* - polynomial-time algorithms might exist that simply can't be rigourously shown to give the right answer or run in polynomial time. Unprovable and false is depressing, though. [deleted] [deleted]    Try reading [The Fifth Gifft](http://www.kuro5hin.org/story/2005/8/19/21304/8493) (a sci-fi short story) with the interpretation that the fifth gift is a device which *causes* P=NP in a local region of space. I can't wait to see the [Travelling Salesman Movie](http://www.travellingsalesmanmovie.com/) as well!    &amp;gt;"So the very notion that any problem can be solved easily seems to fly..."

Sorry to be pedantic, but this doesn't follow from P=NP. There are problems that are provably exponential (rather than just being NP-complete) - David Harel's book "Computers Ltd" has quite a few good examples. (I don't remember any specifics, I'm afraid.)    evidence suggests not equal. What evidence is that? What evidence is that?   It would be a disservice to human inquiry as a whole to discount entirely the possibility that P=NP, just as it would to dismiss P!=NP out of hand. You want to convince me one way or the other? Show me the proof. Forming a consensus on what's "likely" or "unlikely" is all well and good, but until you have the proof you don't know, full stop. It would be a disservice to human inquiry as a whole to discount entirely the possibility that P=NP, just as it would to dismiss P!=NP out of hand. You want to convince me one way or the other? Show me the proof. Forming a consensus on what's "likely" or "unlikely" is all well and good, but until you have the proof you don't know, full stop.      I've been trying to solve the P vs NP problem for years now. I seem to convince myself I have the answer about once every 4-6 months. Currently, I think they're equal, although possibly only for probabilistic algorithms. This would involve finding an algorithm with a whole bag of properties, which we don't seem to have done yet. [deleted]  Highly improbable: so many bright young and older minds have tackled this problem, directly or indirectly, that if P was indeed equal to NP, then we would've found out about it many years ago.

It's also very good that there are NP problems, otherwise life would be so dull.  What if chess or go could be solved in polynomial time?  That'd be boring to know that rather than use imagination and instinct, a good game could be played by simply following a series of steps. Chess is actually EXPTIME-complete, which has been proven to [not be in P](http://en.wikipedia.org/wiki/Time_hierarchy_theorem). Highly improbable: so many bright young and older minds have tackled this problem, directly or indirectly, that if P was indeed equal to NP, then we would've found out about it many years ago.

It's also very good that there are NP problems, otherwise life would be so dull.  What if chess or go could be solved in polynomial time?  That'd be boring to know that rather than use imagination and instinct, a good game could be played by simply following a series of steps. Highly improbable: so many bright young and older minds have tackled this problem, directly or indirectly, that if P was indeed equal to NP, then we would've found out about it many years ago.

It's also very good that there are NP problems, otherwise life would be so dull.  What if chess or go could be solved in polynomial time?  That'd be boring to know that rather than use imagination and instinct, a good game could be played by simply following a series of steps.  I think P = NP, mainly because everyone seems so cocksure of the opposite and their reasoning is complete nonsense. (See: Scott Aronson.) Especially people who think it's some deep philosophical question: uh, no, P=NP would not give us some magical ability to do anything we wanted. &amp;gt; I think P = NP, mainly because everyone seems so cocksure of the opposite and their reasoning is complete nonsense.

Believing something just to be contrary is complete nonsense.

edit: It's one thing to say that the available information is not strong enough to justify leaning one way or the other on the P vs NP question. But objectively, there are currently more reasons to lean towards P != NP than P = NP, so it doesn't make sense to abandon P != NP just to jump all the way over to the P = NP camp. &amp;gt; But objectively, there are currently more reasons to lean towards P != NP than P = NP

This is what I'm skeptical of. If I saw some solid reasoning other than "it would be weird" or "it seems unintuitive" or "we would have found out by now if P were identical to NP" I might agree with you. I think P = NP, mainly because everyone seems so cocksure of the opposite and their reasoning is complete nonsense. (See: Scott Aronson.) Especially people who think it's some deep philosophical question: uh, no, P=NP would not give us some magical ability to do anything we wanted.  Only if N is 1.  [Here](http://www.scottaaronson.com/blog/?p=122) are some reasons why why it does not make sense for P to equal NP. Numerous theoreticians that I have spoken with think similarly.   I don't think P == NP when using conventional computing. 
Now, if we are using quantum computing...  You seem to be slightly misspeaking because I think P == NP would be a fundamental mathematical fact so it wouldn't change based on the type of computer you happen to be using! I think he means that we could get NP problems in P timeframes with quantum computing. precisely. As I see it, the main issue with breaking down the P == NP barrier would be the demise of RSA. I'm worried that quantum computing will eventually allow that. But I guess we'll just use quantum computing limitations to create a new encryption method. RSA breaks down with Shor's algorithm.  Factoring is not believed to be NP complete, so how is that relevant? There is more to RSA than factoring. I'd go into that, but I have to get back to work. I'm curious for your reply later, if you do so.  As far as I recall RSA, if you can factor the large semi prime number into its two prime factors, you can use those factors to create every other number in the private/public keys, breaking RSA.  Wikipedia is seemingly confirming this, as well.  

Edit: Scott Aaronson also discusses it on his [blog](http://www.scottaaronson.com/blog/?p=208). precisely. As I see it, the main issue with breaking down the P == NP barrier would be the demise of RSA. I'm worried that quantum computing will eventually allow that. But I guess we'll just use quantum computing limitations to create a new encryption method. Asymmetric encryption schemes have already been developed that are secure against quantum machines. Even if we were to find a problem in NP-Complete that was in BQP then our encryption mechanisms still wouldn't be in bad shape.  &amp;gt; Asymmetric encryption schemes have already been developed that are secure against quantum machines.

This is true -- in particular, certain problems on lattices are, to the best of our knowledge, hard for quantum computers.

&amp;gt; Even if we were to find a problem in NP-Complete that was in BQP then our encryption mechanisms still wouldn't be in bad shape.

If any NP-complete problem was in BQP, then NP &#8838; BQP and all bets are off for (asymptotic, complexity-based) cryptography. 

I think you might be mixed up about the implications of cryptography not being based on NP-complete problems. Even if all known cryptosystems are broken, it wouldn't necessarily imply P = NP. But if P = NP (or BQP = NP) then all cryptosystems are broken. My point wasn't to say what would happen if P=NP. My point was to say what would happen if we found a quantum algorithm to solve NP-Complete problems. 

I could be wrong (I've been wrong before. I do PL, not theory) but my understanding is that asymmetric encryption schemes have been developed that are not based around NP problems and are still "hard" for attackers with quantum machines. Of course if NP was a subset of BQP then we couldn't base our encryption schemes on NP problems since we couldn't get trapdoor one-way functions out of the NP problems.  Breaking any encryption scheme is an NP problem, due to the following algorithm: "nondeterministically guess a key, then verify that you have the right one in polynomial time". P = NP makes it possible to do this algorithm in deterministic polynomial time, breaking the scheme. BQP = NP makes it possible to do this algorithm on a quantum computer.

PS: I think you mean "NP-complete" where you say "NP." All cryptosystems are based on "NP problems" (as the above shows), but none are really based on NP complete problems. I can't find an argument with your first point. I will have to talk to my crypto friends when I am back at the lab.

I remember making an asymmetric scheme based on subset sum in undergrad. I see no reason why an asymmetric scheme cannot be based on an NP-Complete problem. Wouldn't that immediately prove P!=NP since integer factorization would have to not be in NP-Complete? Breaking any encryption scheme is an NP problem, due to the following algorithm: "nondeterministically guess a key, then verify that you have the right one in polynomial time". P = NP makes it possible to do this algorithm in deterministic polynomial time, breaking the scheme. BQP = NP makes it possible to do this algorithm on a quantum computer.

PS: I think you mean "NP-complete" where you say "NP." All cryptosystems are based on "NP problems" (as the above shows), but none are really based on NP complete problems. I don't think P == NP when using conventional computing. 
Now, if we are using quantum computing...  I don't think P == NP when using conventional computing. 
Now, if we are using quantum computing...  I don't think P == NP when using conventional computing. 
Now, if we are using quantum computing...  P and NP are defined in terms of classical computing. If you're allowed to add quantum into the mix, you get other classes like BQP.  some people believe in god. You are indeed correct. Some people do.

This discussion, however, is not about any god but about classes of computability. You must have misread.   It's probably just because the papers about it are over my head, but I don't buy the proof that solving one NP problem in P time will ultimately solve all of them. I think that someone will end up generalizing what we thought was a specific use case for neat data and publish a paper with a P solution to an NP problem in my lifetime, and I'll still feel reasonably safe using HTTPS the next morning. It's probably just because the papers about it are over my head, but I don't buy the proof that solving one NP problem in P time will ultimately solve all of them. I think that someone will end up generalizing what we thought was a specific use case for neat data and publish a paper with a P solution to an NP problem in my lifetime, and I'll still feel reasonably safe using HTTPS the next morning. It's probably just because the papers about it are over my head, but I don't buy the proof that solving one NP problem in P time will ultimately solve all of them. I think that someone will end up generalizing what we thought was a specific use case for neat data and publish a paper with a P solution to an NP problem in my lifetime, and I'll still feel reasonably safe using HTTPS the next morning. It's probably just because the papers about it are over my head, but I don't buy the proof that solving one NP problem in P time will ultimately solve all of them. I think that someone will end up generalizing what we thought was a specific use case for neat data and publish a paper with a P solution to an NP problem in my lifetime, and I'll still feel reasonably safe using HTTPS the next morning.  If a new model of computing could be developed, allowing you to solve non-linear time problems in linear time, then P = NP

:D No, that doesn't mean that P = NP, it just means non-deterministic polynomial time problems can be solved in a reasonable amount of time using different computing mediums.  That's like saying O(n^2) problems have a variable running time complexity based on the processor that is computing the problem.  It's not about how fast it can be processed, it's about how many steps it takes to do so.  Also, not all problems are linear (in fact, most aren't).  They can be O(nlogn), O(logn), O(n^3), just to name a few common ones.

Personally, it's hard to say (or even truly understand) how to approach an NP hard problem, and solving one without sacrificing output optimization, time or the ability to solve all possibilities of outcome.  I rationalize this as a 50/50 chance of P = NP.  I'm certainly not qualified to definitively say yes or no.</snippet></document><document><title>Why is quicksort better than other sorting algorithms in practice?</title><url>http://cs.stackexchange.com/q/3/98</url><snippet>  We all "know" Quicksort is widely considered the fastest/best sorting algorithm, but why exactly is that? Is has a quadratic time worst-case, after all, and there are true n*log n algorithms. Click through for some excellent reasons. As Tuna-Fish2 points out, there are other algorithms out there that do well. Generally you'll find that production sorting libraries, when they do use quicksort, will switch to insertion sort, or something else when they get down to a sufficiently small size for it to be a good trade-off.

The reason quicksort is often preferred over mergesort actually has nothing to do with the asymptotic time complexity. Rather, because of the way the algorithm works, it's very cache friendly - so even though the wost case is quadratic, and the best/avg case are "the same" as mergesort, it tends to preform better because it makes good use of the cpu caches.

Specifically, modern cpus will store a chunk of memory of a certain size in cache together. With quicksort, we find a pivot, divide the rest of the data into less than/greater than the pivot, and then recurse, working on one set of data or the other at a time. Since the data we're working on at any given time is close in memory, it's more likely it will be in the same cache line. Has there been attempts to create notation that acknowledges cache effects? At least one obvious way would be to have big-O of cache misses. Has there been attempts to create notation that acknowledges cache effects? At least one obvious way would be to have big-O of cache misses. We all "know" Quicksort is widely considered the fastest/best sorting algorithm, but why exactly is that? Is has a quadratic time worst-case, after all, and there are true n*log n algorithms. Click through for some excellent reasons. I wouldn't call Quicksort the fastest one anymore. For a lot of languages and cases, Timsort (Mergesort with some tweaks) is now faster. 

Timsort shines with pre-sorted data (best case, O(n)), but more importantly, it does much less comparisons than quicksort, at the cost of more array access. Since in most modern languages comparisons tend to be polymorphic and at least do a virtual call, while the array access is just a pointer copy, this makes boatloads of sense. Also, mergesorts treat memory as tape, and so will probably win any sorting problem that falls out of the cache in today's machines by a mile. I wouldn't call Quicksort the fastest one anymore. For a lot of languages and cases, Timsort (Mergesort with some tweaks) is now faster. 

Timsort shines with pre-sorted data (best case, O(n)), but more importantly, it does much less comparisons than quicksort, at the cost of more array access. Since in most modern languages comparisons tend to be polymorphic and at least do a virtual call, while the array access is just a pointer copy, this makes boatloads of sense. Also, mergesorts treat memory as tape, and so will probably win any sorting problem that falls out of the cache in today's machines by a mile. I wouldn't call Quicksort the fastest one anymore. For a lot of languages and cases, Timsort (Mergesort with some tweaks) is now faster. 

Timsort shines with pre-sorted data (best case, O(n)), but more importantly, it does much less comparisons than quicksort, at the cost of more array access. Since in most modern languages comparisons tend to be polymorphic and at least do a virtual call, while the array access is just a pointer copy, this makes boatloads of sense. Also, mergesorts treat memory as tape, and so will probably win any sorting problem that falls out of the cache in today's machines by a mile. We all "know" Quicksort is widely considered the fastest/best sorting algorithm, but why exactly is that? Is has a quadratic time worst-case, after all, and there are true n*log n algorithms. Click through for some excellent reasons. As Tuna-Fish2 pointed out, you can do a lot better than Quicksort. Quicksort is the fastest basic sorting algorithm for large sets of data, but you can do better with a really simple algorithm: 

For arrays less than a certain length (say, 16), use Insertionsort. For longer arrays, use recursive Mergesort. This beats Quicksort for any length array, although it still loses to Timsort.

Here's some pseudocode: 

    def sort(arr):
        if (len(arr) &amp;lt; 16):
            insertion_sort(arr)
        else:
            left = sort(arr[0...(len(arr) / 2)])
            right = sort(arr[(len(arr) / 2)...len(arr)])
            merge(left, right)  Tangential note: I've always hated how quicksort is described as "average n log n". The entire notion of average case is invalid without mentioning what distribution you're talking about. Most analyses just assume a uniformly random distribution without justifying it. What if you use a randomly selected pivot? Would the distribution matter in that case?  What if you use a randomly selected pivot? Would the distribution matter in that case?  Yeah, the point is that the word "average" only make sense for a given distribution of inputs. After you fix a particular implementation (for example, one that picks a random pivot), you still need to know the distribution of inputs to compute the average running time. Are you considering the selection of pivot an "input"? I would consider that part of the algorithm. What if you use a randomly selected pivot? Would the distribution matter in that case?  Randomly selected how? If you mean uniformly randomly selected, I believe that in that case no matter what distribution you pick you get an average case of n log n. That's a very special case, though. Not at all. Dumb example: pick a worst-case input. Assign it probability 1 and all the others 0. Relative to this (not very meaningful) distribution, the algorithm has quadratic expected time.

Less dumb: distributions that favor input lists with many duplicates, for example, drive many a Quicksort implementation into quadratic expected time behaviour. That can be a problem in practice (e.g. lists of booleans).
 Well, yes. I'm not sure what you're saying contradicts what I'm saying. "I believe that in that case no matter what distribution you pick you get an average case of n log n" -- I'm saying that statement is wrong, as proven by the example I offer. Not at all. Dumb example: pick a worst-case input. Assign it probability 1 and all the others 0. Relative to this (not very meaningful) distribution, the algorithm has quadratic expected time.

Less dumb: distributions that favor input lists with many duplicates, for example, drive many a Quicksort implementation into quadratic expected time behaviour. That can be a problem in practice (e.g. lists of booleans).
 Tangential note: I've always hated how quicksort is described as "average n log n". The entire notion of average case is invalid without mentioning what distribution you're talking about. Most analyses just assume a uniformly random distribution without justifying it. You can shuffle the list in linear time, so that justifies the uniformly random distribution. Not for input data that's "mostly duplicates", say, which is something quicksort has trouble with. A random permutation of data that's "mostly duplicates" is going to lead to more data that's "mostly duplicates".

And even if it did, it still doesn't justify the average case for the usual quicksort, since you've changed the algorithm.      Maybe I'm showing my age, but back when I was in college we were taught that the radix sort can break the n*ln(n) barrier because it doesn't actually require a comparison operation. That's because the barrier is only valid in the [comparison model](https://en.wikipedia.org/wiki/Decision_tree_model) which many popular [sorting algorithms](https://en.wikipedia.org/wiki/Comparison_sort) adhere to. In other algorithms, comparisons are clearly not the dominant operation so they are not related to runtime at all.

Another good example is [Bucket sort](https://en.wikipedia.org/wiki/Bucket_sort). Thats not a good explanation, in my opinion. 

A better one, I think, is this simple point.

Quick sort and Merge sort and others, will work on any object that has a notion of order. 

Radix sort does not work on just anything that has a notion of order. It reduces the scope of the algorithm to gain an advantage. Radix sort only works on integers in a finite range, or things that can be mapped to integers in a finite range. 

If you had integers of arbitrary size, then Radix sort could take an arbitrary amount of time to complete. Because the size of the integer is fixed, it becomes a constant - and falls out in the Big O notation.  I see, that makes perfect sense. I remember the examples of radix sorts in textbooks and they usually had a list of 4-digit numbers (leading zeros for the smaller ones). If they threw a few 20 digit integers into the mix it would change everything. Thats not a good explanation, in my opinion. 

A better one, I think, is this simple point.

Quick sort and Merge sort and others, will work on any object that has a notion of order. 

Radix sort does not work on just anything that has a notion of order. It reduces the scope of the algorithm to gain an advantage. Radix sort only works on integers in a finite range, or things that can be mapped to integers in a finite range. 

If you had integers of arbitrary size, then Radix sort could take an arbitrary amount of time to complete. Because the size of the integer is fixed, it becomes a constant - and falls out in the Big O notation.  Actually, it's both, that is restriction of the input set and not adhering to the comparison model.

"Falling out of the Big Oh notation" is a misleading statement. They have very valid asymptotic runtimes, just in a setting different from the usual one. I didnt say you were wrong, I said I thing your explanation does not do much explaining. I believe my explanation helped him understand why. 

EDIT: 
&amp;gt; "Falling out of the Big Oh notation" is a misleading statement. They have very valid asymptotic runtimes, just in a setting different from the usual one.

It is not misleading at all. Radix sort sets the size of a number constant (b/c hardware), and so the constant falls out int he Big O. If you could specify an arbitrary size of integers to be sorted, Radix sort would be O(n d), if d is the number of bits. </snippet></document><document><title>Resources for designing bytecode?</title><url>http://www.reddit.com/r/compsci/comments/y1dky/resources_for_designing_bytecode/</url><snippet>Are there any good articles or research papers that discuss designing a bytecode from a language specification?  Of various qualities and depths:

http://static.usenix.org/events/vee05/full_papers/p153-yunhe.pdf

http://stephane.ducasse.free.fr/FreeBooks/BlueBook/Bluebook.pdf (Starting after chapter 26)

http://www.lua.org/doc/jucs05.pdf

http://nekovm.org/lua#virtual_machine

http://www.cs.williams.edu/~bailey/06brh.pdf

http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.9849&amp;amp;rep=rep1&amp;amp;type=pdf I didn't think I'd be getting a response to this one. Thanks so much :) No problem. If you have more specific questions, let me know. Disregarding choices like how many bits to use for register labels; is it just picking/testing what the most frequent operations for basic usage are and making those the byte codes? Like Lua's seem to be mostly arithmetic/logical with the rest being direct support for their own peculiarities (like tables and functions).

It's like the bytecode is the easy part and the actual hard part is doing register picking / stack organization when compiling for the hypothetical VM? That's it in a nutshell. There are some primitives that you'll probably be generating no matter what -- conditional jumps, for example.

Then, depending on your language, there will probably be certain sequences that you generate over and over, and if you implemented them in your VM, you would spend most of your time dispatching to the simple operations. For example, in Lua, directly supporting tables is a good idea, because otherwise the table lookup would be dominated by the cost of interpreting bytecode. And then there's the possibility you'd want to JIT.

So, yes, the bytecode is fairly easy, especially if you don't set it in stone, and have no problems with deleting or adding operations. As far as register picking -- there's a good chance that you can keep it simple. I'd try to see if, say, 256 registers is good enough. If you run into the limit (I'd be surprised), you might be able to do some sort of "register extension" instead of trying to handle spills. I *so* want to write a JIT. That's phase II or III for this hobby language. It was much easier to find info for that -- I've got the source to LuaJIT, as well as the HotpathVM, Tracemonkey, and Jitting Prolog VM papers. I'd be really open to other suggestions if you have them.

I can see how 256 registers would be way more than necessary :) I'd use far less -- it would be neat to test speeds on keeping the entire set of registers in one or two cache lines... or exclusively in the hardware registers if possible.

I really don't see using hundreds of variables in a single function, and pushing frames on the stack seems simple to me, so I'd probably just do a VM analog of C calling (if that counts as spill) and never use half the registers.

Thanks again for you help on this. I don't want to trivialize writing a VM too much, but honestly I was expecting something vastly more difficult to conceptualize and was worried I wasn't getting it at all.</snippet></document><document><title>Random Observations: Analysis vs Algebra predicts eating corn?</title><url>http://bentilly.blogspot.pt/2010/08/analysis-vs-algebra-predicts-eating.html</url><snippet /></document><document><title>I am here to ask for direction. What topics in theoretical comsci would put into good use my BS degree in Math and would also utilize my interest in computer science? (x-post from r/math)</title><url>http://www.reddit.com/r/compsci/comments/xzlkd/i_am_here_to_ask_for_direction_what_topics_in/</url><snippet>I just graduated from BS Math last April. I am now working at the institute in our university as an instructor in Math. My undergraduate thesis was on elliptic curves. And I'm also interested in cryptography.

Moreover, I'm also interested in a lot of other stuff like combinatorics, graph theory, game theory and the like. I'm kind of confused since I feel I'm too excited about these topics.

I have competed in the ACM-ICPC regionals, along with some of my schoolmates and I believe most people in the institute of Math view me as a comsci person. I also think that the areas of research in our CS Department are not too much on the theoretical.

People have been suggesting that I might find myself more inclined to theoretical computer science, given what they know about me. I am having second thoughts since I do not know anyone in our university who specializes in this and might get lost along the way.

I am here to ask for direction.

* What topics in theoretical comsci would put into good use my BS degree in Math and would also utilize my interest in computer science (and quite possibly my ACM-ICPC skills)?
* Would these topics be too far from the research interests of my institute?
* Is this plan too far-fetched because no one else (none that I know of) in our country has tried these paths?

Thank you (in advance) for helping a confused Masters student. :)  algorithms, theory of computation and programming languages are some good topics. But since you said your school is not theoretical I doubt there are active professors in those fields.  I went from undergrad math at a top math research institution to graduate CS at a top CS research institution. My focus is on programming languages, where my grad school happens to have made significant historical contributions. I was much more comfortable with proofs and analysis than pure-CS students, and I dominated in algorithms, theory of computation, programming languages, database systems, compilers and operating systems. I did less well in CS courses heavy on probability, like artificial intelligence. My research focuses on novel computational platforms and programming languages, which is historically a strong area at my university. But almost everyone else in my group is knee deep in concurrent programming and static code analysis.

As a Masters student, you are virtually free from the pressures that can drive PhD students into super-specialized fields of marginal personal interest in order to get published. Find an advisor that you think will give you enough freedom to explore without also hanging you out to dry. algorithms, theory of computation and programming languages are some good topics. But since you said your school is not theoretical I doubt there are active professors in those fields.    Computer Algebra - getting computers to *do* mathematics. Involves a wide range of mathematics and I personally find it fascinating. Did an undergrad and masters in pure mathematics and now doing a PhD in CompSci.  I'm really interested in computer algebra. Are there any nice resources online that I may look into? You might be able to find some of the textbooks online (I think Davenport's "Computer Algebra" is available on his website). It's an awesome subject area and one I'm really glad I went in to. If you have any specific questions I'd be happy to answer them!    [deleted] [deleted]   Wow, from recent graduate to instructor? Nice work!     Type systems and programming languages are quite mathematical topics.

If you like category theory and algebraic geometry, I suggest you take a look in the relationship between types systems, lambda calculi, functors and category theory.
 </snippet></document><document><title>A New Way To Solve Linear Equations</title><url>http://rjlipton.wordpress.com/2012/08/09/a-new-way-to-solve-linear-equations/</url><snippet>  Checked the comments section and saw a response from Fields medalist Terence Tao. I think i need to start reading better blogs.  So am I right in assuming he is choosing random selections of linear combinations of random solution vectors by adding random basis solution vectors?  And then trimming the sets down until the last set which has a high probability of being non-empty?    Will have to read the paper.  Definitely sounds intriguing.   I wonder where I would start to start understanding some of the words in this article and their context together. Disclaimer: I'm neither an expert nor a mathematician, so take my post with a large cluster of salt

The most difficult part of this article is figuring out what the jargon means. This is an algorithm that can find integer solutions to systems of equation like

    x + y = 0
    y + z = 1

Here, a solution is basically an assignment of the variables (x,y,z) such that it satisfies both of the above equations. However, because we're working with something called a "finite prime field", every single term or literal on both the left hand side and the right hand side must be bounded between 0 &amp;lt;= x &amp;lt;= p-1, where p is some prime number.

For example, "a system within a finite 2-field" can be interpreted as a system of equation like above, with some arbitrary number of variables **n**, in some other number of **equations** such that every number/variable/coefficient on the left hand side and on the right hand side can only take the value of either 0 or 1. 

One further property within a finite field is that if adding or multiplying two numbers results in some number x that doesn't fit between 0 &amp;lt;= x &amp;lt;= p-1, then we keep on subtracting p, the prime number of the field, until it finally becomes less than p. For example, the operation (we also call this modular arithmetic)

    1 + 1 + 1 + 1

in the 2-field results in 4, which is outside the proper bound of 0 &amp;lt;= x &amp;lt;= 1, so we subtract 2 and get 2, which is still outside the bound, so we subtract 2 again, ending up with 1 + 1 + 1 + 1 = 0 \mod 2

While not intuitive, it can also be proven that within finite prime fields, there is a *unique* multiplicative identity, 1, and a *unique* additive identity, 0, such that for some integer x within F_p (a finite prime field with its characteristic prime being p), (x + 0 = x mod p) and no other integer y within F_P exists such that (x + y also = x mod p). The same can be said of the multiplicative identity 1, and that there exists one and only one additive and multiplicative inverse such that (x + (-x) = 0 \mod p) and (x * x^-1 = 1 \mod p)

---

Anyways, let's first consider arithmetic in F_2 of the form

     a + b + c + ... = num

Now, I'm going to claim that out of all possible ways of assigning values to (a,b,c,...), exactly 1/2 or half will satisfy the above equation. Let's look at an example first before proceeding

     a + b = 0

Now, there are 4 ways of assigning values to (a,b), they are (0,0), (0,1), (1,0), (1,1). Let's see which of these four satisfies the above equation

     0 + 0 = 0 &amp;lt;-
     0 + 1 = 1
     1 + 0 = 1
     1 + 1 = 0 &amp;lt;-

Only (0,0) and (1,1) satisfies a+b=0, which confirms that this property holds for 2 variables (or n=2 by the above definition). Let's look at the n=3 case. The viable assignments are 

    (0,0,0),(0,0,1), (0,1,0), (0,1,1), (1,0,0),(1,0,1),(1,1,0),(1,1,1) 

(notice that in order to enumerate the n case within field F_k, this is the same as counting all of the n-bit numbers in base-k, this is just a digression, so there are in general k^n possible assignment of the variables)

Now, at a quick glance, only (0,0,0),(0,1,1),(1,0,1),(1,1,0) satisfies a+b+c = 0. Since there are 8 possible assignments yet only 4 are satisfiable, once again confirming our hunch that exactly half of the assignments.

Informally, this is because of all of the values that *num* can take, the sum of the assignments are on average distributed evenly among each of the possible values. Since num can only take 2 values here, the probability that a random assignment satisfies one unitary (meaning no coefficients on any of the variables) equation is exactly 1/k. For the case of having coefficients, the brief analysis provided by the link will explain further). Take a little bit of time to convince yourself of this property.

Now, suppose that in general, any equation of the form

    c1 x + c2 y + c3 z + ... cn n = num

within field F_k has the property that any random assignment taken from F_k^n (this means taking n elements out of F_k as the assignment to (x,y,z,...,n)) has some probability within the neighborhood of 1/k of satisfying that one equation. Okay, then if we take a reasonably large sample of assignments, then there's a near perfect chance that the first equation will be satisfied.

Unfortunately, the probability of the i^th equation being satisfied is 1/(k^(i)). For example, the probability of a randomly chosen assignment satisfying a 10 equation system in F_11 is 1/11^10 ~ 4e-011, basically on the same order of magnitude as winning ten thousand megamillion lotteries.

So what can we do? Well, there's a nice property of F_k such that if (a,b,c,...) is a solution, and if (a',b',c',...) is a solution, then so is (a+a',b+b',c+c',...). Because of this, for each equation processed, we can expect to remove around (k-1)/k * 100 percent of the "initial solution candidates" that do not satisfy that equation. However, because we know that there are k^n expected solutions, we can have high confidence that randomly choosing some sets of the remaining solution candidates and adding them together will not create too many duplicates.

With this property in hand and the ability to confidently generate more solution candidates after each equation, we can always ensure that there could be large enough number of "randomly chosen" solution candidates such that on average, we can at least expect a few solutions to be left over.

---

Anyways, this is kind of contrived and is probably not very accurate (the thing about the 1/k probability is definitely not the end of the story and only serves to help illustrate the question), but I hope it at least gives you some idea of what the algorithm does and what why it does what it does &amp;gt; finite 2-field

Mathematicians usually say "finite field of order 2". A *field* is a set with addition and multiplication operations obeying the usual laws, for which every element other than 0 has a unique reciprocal (ie, division works as it does in more familiar settings). A *finite field* is just a field whose underlying set is finite (unlike the rational numbers, the real numbers and the complex numbers, which all form infinite fields). As you explain in your comment, the integers modulo any prime *p* forms a finite field. There's actually a nifty theorem due to Galois that *every* finite field must have *p^n* elements for some prime *p* and some integer *n*, and furthermore that there is only one such field for any given *p* and *n*, up to isomorphism (ie, relabelling of the elements). "Division works as expected" is a surprisingly strong condition!

We refer to the prime *p* as the *characteristic* of the finite field. It's a theorem that x + x + ... + x (*p* times) = 0 for any element x of the field.

&amp;gt; Well, there's a nice property of F_k such that if (a,b,c,...) is a solution, and if (a',b',c',...) is a solution, then so is (a+a',b+b',c+c',...).

This is the only mistake I spotted in your post. What you say is true if all the equations have 0 on their right-hand sides (because 0 + 0 = 0), but not otherwise. But we can rescue the situation! Here's where the assumption that our field (let's call it F) is finite comes in. Let *p* be the characteristic of F, and the right-hand-side of the equation be x. Since the sum of *p* x's in F is 0, the sum of (*p* + 1) x's must be x. So while adding two solutions won't give us a new solution (unless x = 0), adding *p*+1 solutions will!

As a concrete example, think about the finite field of order 2, F_2. F_2's characteristic is of course 2. Suppose we have three solutions to the equation c_1 \* a + c_2 \* b = x. So

    c_1 (a + a' + a") + c_2 (b + b' + b")   
    = (c_1 a + c_2 b) + (c_1 a' + c_2 b') + (c_1 a" + c_2 b")   
    = x + x + x

There are only two possibilities for x, namely 0 and 1. Now 0 + 0 + 0 = 0, and 1 + 1 + 1 = 1 modulo 2. So in both cases, (a + a' + a", b + b' + b") is a solution to the original equation. Thank you I never thought to consider that and just accepted it without proof after adding a few pairs of solutions together in a system in F_3  Maybe I'm missing something, but this is not a new method. This is the equivalent of projecting the system onto an eigenvalue basis where the eigenvalues were determined at random. This is a common method for solving the Schrodinger wave equation in semiconductors. Not really.

The linked algorithm is about solving linear systems in modular arithmetic. That's useless for numerical PDE.

Aside from the eigenvalue/eigenvector confusion: You can't randomly choose eigenvectors, you need to compute them first. Randomly projecting alone doesn't work. At the very least you need to iterate the linear map a few (or many) times.

And that's not what the linked algo does at all. It's not even meant for square matrices, but for m-by-n ones with m&amp;lt;&amp;lt;n.

Non-square matrices don't have eigenvectors nor eigenvalues, the next closest thing for them are singular values, but they aren't very helpful when you want to solve the equation for a single right hand side.

Even for square matrices A it's idiotic to search for eigenvalues/eigenvectors, at least exact ones, when you just want to solve A.x=b for one specific right hand side b.

________

Regarding the method. I do think it's new but, at least as far as proven by the guy, it's not competitive.

The proof seems to assume a very big amount N ~ O(n q^2 log(q) ) of starting vectors. That makes the method significantly slower than even basic Gaussian elimination.

But maybe a much smaller N is sufficient.

For one thing: the choice of N from the "paper" yields a very small failure probability (1-e-n ). Failure probability 1-e-log(n) = O(1/n) would be more than good enough. Not really.

The linked algorithm is about solving linear systems in modular arithmetic. That's useless for numerical PDE.

Aside from the eigenvalue/eigenvector confusion: You can't randomly choose eigenvectors, you need to compute them first. Randomly projecting alone doesn't work. At the very least you need to iterate the linear map a few (or many) times.

And that's not what the linked algo does at all. It's not even meant for square matrices, but for m-by-n ones with m&amp;lt;&amp;lt;n.

Non-square matrices don't have eigenvectors nor eigenvalues, the next closest thing for them are singular values, but they aren't very helpful when you want to solve the equation for a single right hand side.

Even for square matrices A it's idiotic to search for eigenvalues/eigenvectors, at least exact ones, when you just want to solve A.x=b for one specific right hand side b.

________

Regarding the method. I do think it's new but, at least as far as proven by the guy, it's not competitive.

The proof seems to assume a very big amount N ~ O(n q^2 log(q) ) of starting vectors. That makes the method significantly slower than even basic Gaussian elimination.

But maybe a much smaller N is sufficient.

For one thing: the choice of N from the "paper" yields a very small failure probability (1-e-n ). Failure probability 1-e-log(n) = O(1/n) would be more than good enough. it's true, i don't know linear systems in modular arithmetic. i know computational physics. see "maybe i'm missing something" in my original post. 

that said, this method looks an awful lot like other methods, perhaps this is a new application of it? Maybe I'm missing something, but this is not a new method. This is the equivalent of projecting the system onto an eigenvalue basis where the eigenvalues were determined at random. This is a common method for solving the Schrodinger wave equation in semiconductors. Maybe I'm missing something, but this is not a new method. This is the equivalent of projecting the system onto an eigenvalue basis where the eigenvalues were determined at random. This is a common method for solving the Schrodinger wave equation in semiconductors. [deleted] yeah it looks pretty exactly like Arnoldi but he doesn't mention ortho-normalizing the basis or anything, he just says 'recombination' a bunch Maybe I'm missing something, but this is not a new method. This is the equivalent of projecting the system onto an eigenvalue basis where the eigenvalues were determined at random. This is a common method for solving the Schrodinger wave equation in semiconductors. explain it like i'm five? how do you randomly determine the eigenvalues? how do you project a system of equations into eigenvalue basis?  &amp;gt; how do you project a system of equations into eigenvalue basis?

Probably meant eigenvector basis.

That person has no clue, the many upvotes he/she got make me LOL. &amp;gt; how do you project a system of equations into eigenvalue basis?

Probably meant eigenvector basis.

That person has no clue, the many upvotes he/she got make me LOL. explain it like i'm five? how do you randomly determine the eigenvalues? how do you project a system of equations into eigenvalue basis?  just as a musical instrument resonates greatest at certain frequencies, so does a system of equations. the resonant modes (wave shapes) are the eigenvectors, and the frequencies are the eigenvalues.

to find them randomly, you create a series of random vectors, and see which ones "resonate the loudest"

to fully project the system onto the vectors (that is, to use the vectors as a basis) you subtract the contribution from each basis function until you have the whole thing described...

so, like you're maybe 10, (not 5):
a system of equations is just a bunch of vectors together.
if you have a vector &amp;lt;5,3,12&amp;gt;, you immediately think "the x component is 5, the z component is 12" but the formal way to do that is to "project" the vector onto the basis vector &amp;lt;1,0,0&amp;gt; "x", &amp;lt;0,1,0&amp;gt; "y", and &amp;lt;0,0,1&amp;gt; for "z".

&amp;lt;5,3,12&amp;gt; projected onto &amp;lt;0,1,0&amp;gt; is 3, so we say the y component is 3, because &amp;lt;5,3,12&amp;gt; resonates with &amp;lt;0,1,0&amp;gt;, uh, 3 much. 

same thing with whatever basis vectors you choose.  Pure gibberish.  You're just making this up right? Well, to be fair, his explanation of eigenvectors in terms of resonating waveforms is actually quite thought provoking and has a certain novelty to it, and I can sort of see why he decided to use that example.

To extend his analogy, we consider one of the typical geometric interpretations of eigenvectors associated with some matrix A as the set of inputs for which transformation A will not cause a rotation. That is, for some x an eigenvector of A, Ax still points in the direction of x such that the angle/"phase" that x makes with respect to some set of basis will never be changed by these transformations. In that sense, consider an equivalence class of vectors in R^n based on their norm (metric from the origin) on some SPD matrix, then consider Schwartz inequality, we see that ||Ax|| &amp;lt;= ||A||||x|| or that sup||Ax|| = ||A|| ||x|| (where we can disregard the ||x|| since within each equivalence class, ||x|| is constant) ~ ||A|| = max(\lambda_i). That is, the supremum of ||Ax|| is in the direction of the dominant eigenvector. This is also intuitive geometrically. If I don't have to change the angle, then theoretically the effects of the transformation should be greater right?

Now, in the wave sense, suppose that we're looking at a beam of scattering light over a range of angle (analogous to the range of the angles between x's of just one norm-equiv class and the basis) behind two tiny slits of openings (a transformation that may change the phase of some of the waves). Now, just like the operation Ax, the slits could direct different beams from its "light bulb basis" towards the same spot and create observations that are the superposition/linear combination of a bunch of waves. However, in general, when the slit does not cause these combinations to destructively interfere (aka, do not change the phase of the final waveform, and analagous to not changing the direction of the eigenvector), then that's usually where the waveform "resonates" the strongest. (The characteristic or the eigenvalue of each slit is actually the intensity of the light as it exits out of the slits or at some fixed distance away, think about it for a bit and justify this to yourself)

Furthermore, the power law can be translated into this wave analogy. Suppose we stack together a large number of double slits and apply each to any beam of incoming light, then the diffraction behavior will weaken with each passing plate and the end result will have a "dominant" bright spot equivalent to how applying A to any x over and over again will eventually result in a vector close to the eigenvector.

Edit: I also highly recommend a classic in scientific computing titled "Conjugate Gradient without the Agonizing Pain" (warning, it still might be pain inducing, but it's also a great lin alg refresher as well) which does mention an interpretation of some of the krylov subspace methods as doing descent direction within the eigenvector space, which could be what he was originally referring to, albeit no rank reduction was mentioned which was what confused me too (however the style of iteration is very similar). Have a good day/night/morning now! just as a musical instrument resonates greatest at certain frequencies, so does a system of equations. the resonant modes (wave shapes) are the eigenvectors, and the frequencies are the eigenvalues.

to find them randomly, you create a series of random vectors, and see which ones "resonate the loudest"

to fully project the system onto the vectors (that is, to use the vectors as a basis) you subtract the contribution from each basis function until you have the whole thing described...

so, like you're maybe 10, (not 5):
a system of equations is just a bunch of vectors together.
if you have a vector &amp;lt;5,3,12&amp;gt;, you immediately think "the x component is 5, the z component is 12" but the formal way to do that is to "project" the vector onto the basis vector &amp;lt;1,0,0&amp;gt; "x", &amp;lt;0,1,0&amp;gt; "y", and &amp;lt;0,0,1&amp;gt; for "z".

&amp;lt;5,3,12&amp;gt; projected onto &amp;lt;0,1,0&amp;gt; is 3, so we say the y component is 3, because &amp;lt;5,3,12&amp;gt; resonates with &amp;lt;0,1,0&amp;gt;, uh, 3 much. 

same thing with whatever basis vectors you choose. </snippet></document><document><title>Joel Runyon meets Russell Kirch, inventor of the world's first internally programmable computer, in a coffee shop in Portland</title><url>http://joelrunyon.com/two3/an-unexpected-ass-kicking</url><snippet>   &amp;gt;Russell A. Kirsch (born 1929) led a team of colleagues which, between 1947 and 1950, created America&#8217;s first internally programmable computer ...

Wait, what?!  He was only 18 when he did it?!?!  Every time you see an "invented the first X" claim, it's time to be skeptical about the details. According to wikipedia, Kirsch led the team that built SEAC, the first *electronic* stored program computer. However, the [SSEC](http://en.wikipedia.org/wiki/IBM_SSEC) (1948) holds a claim to the title of first stored program computer (though not electronic), and both the [SSEM](http://en.wikipedia.org/wiki/Small-Scale_Experimental_Machine) (1948) and the [EDSAC](http://en.wikipedia.org/wiki/EDSAC) (1949) were electronic stored program computers. This doesn't diminish Kirch's contributions to computer engineering and computer science, but it does put them in a more interesting (if less simplistic) context. Don't forget [the world's first stored program electronic computer, The Baby](http://www.computer50.org/mark1/new.baby.html), 1948 as well.  I didn't, that's the nickname for the SSEM :).  &amp;gt;I&#8217;ve been against Macintosh company lately. They&#8217;re trying to get everyone to use iPads and when people use iPads they end up just using technology to consume things instead of making things. With a computer you can make things. You can code, you can make things and create things that have never before existed and do things that have never been done before.

I love that quote. I've only used an iPad for a couple hours, but it certainly does feel more like a theatre than a workshop or a laboratory.  Small touchpads like that will always be inferior tools because they are less capable of inputting information. That is a real keyboard and mouse is a far superior way to write or program or create art. 

The only time the iPad is a good tool is when your work requires the portability and then yes it can be better than say .. a laptop.   &amp;gt; Want to mess with your mind? Without the man in the photo, **someone else would have come up with the format for digital photos**. And yet, he was the first. It's far easier to make another of something than to create something out of whole cloth. Likewise, it's easier still to sit on the sidelines and yell "YOU'RE WRONG" or "THIS DOESN'T MATTER". And yet, he was the first. It's far easier to make another of something than to create something out of whole cloth. Likewise, it's easier still to sit on the sidelines and yell "YOU'RE WRONG" or "THIS DOESN'T MATTER". I'm not saying it wasn't important, or the guy isn't smart. I'm just saying that if he didn't make it, one of his contemporaries surely would have.  &amp;gt; Want to mess with your mind? Without the man in the photo, **someone else would have come up with the format for digital photos**. &amp;gt; Want to mess with your mind? Without the man in the photo, **someone else would have come up with this same lame-ass comment**

FTFY That doesn't even make sense. Your comment was lame. I agree! This one is much better.   "Nothing is withheld from us which we have conceived to do." 

That sounds nice and it's inspiration, but it's so not true. This guy lived a dream life and the disconnect is clear. He is smart and confident as he should be with many successes to his name, but the reality of life is not every is smart or motivated and simply dreaming they can do something is not enough. There are plenty of fat people dreaming of being skinny and plenty of nerds dreaming of human robots they can have sex with.

In fact lets take it a step further. Life would be horrible if we could not conceive things which were impossible to ever achieve. Even the most committed people eventually either stop trying or fail. Even Einstein was not sure of himself and was unable to complete the loose ends of his work and by his death that bothered him, so you know he was trying. Life is finite and so is your mind, you can't do anything you dream. You just have to do what you can and hope it was a valid contribution to humanity. 
</snippet></document><document><title>How will visualization programs handle the crush of exascale data?</title><url>http://ascr-discovery.science.doe.gov/newfaces/childs1.shtml</url><snippet /></document><document><title>If anyone is interested in Geometric Folding Algorithms, here's video/notes of a great MIT class on the subject</title><url>http://courses.csail.mit.edu/6.849/fall10/lectures/</url><snippet>   I saw a [ted talk](http://www.ted.com/talks/michael_hansmeyer_building_unimaginable_shapes.html) about geometric folding yesterday. Totally inspiring.. 
This course seems to be the perfect introduction to this, thank you so much!   I knew it was this dude before I clicked the link!  He did his PhD paper on this.
I watched a vid when we had class about red-black trees and noticed he was kinda young looked him up and found this.</snippet></document><document><title>How can it be decidable whether &#960; has some sequence of digits?</title><url>http://cs.stackexchange.com/q/367/773</url><snippet>      I don't buy this proof because of its similarity to the halting problem, I think the proof would require evidence that an algorithm exists to find (or show that there is no) n. That is the real work of the algorithm. This proof just abstracts that away and says that if we know the answer, an algorithm exists. It seems like this proof could apply to any decision problem.

There could be some math fact about pi that gives this away, but I don't know much about that.

EDIT: I get it now, this is a decidable problem because it is a single problem with a countable number of solutions. All such problems like this are decidable, although the solution may not be satisfying to anyone. I just want too much satisfaction :) Thanks to you guys for helping me understand what is going on here! The "proof" is twaddle from a computational or constructive point of view, and rather trivial in the presence of excluded middle, as shown by the second answer. The "proof" is twaddle from a computational or constructive point of view, and rather trivial in the presence of excluded middle, as shown by the second answer. Exactly, the problem can be reduced to finding N (the largest number of consecutive 0s in pi).  In order to find N, you would have to traverse the digits in pi, which of course cannot be done in a finite number of steps. I don't buy this proof because of its similarity to the halting problem, I think the proof would require evidence that an algorithm exists to find (or show that there is no) n. That is the real work of the algorithm. This proof just abstracts that away and says that if we know the answer, an algorithm exists. It seems like this proof could apply to any decision problem.

There could be some math fact about pi that gives this away, but I don't know much about that.

EDIT: I get it now, this is a decidable problem because it is a single problem with a countable number of solutions. All such problems like this are decidable, although the solution may not be satisfying to anyone. I just want too much satisfaction :) Thanks to you guys for helping me understand what is going on here! The thing is, the question only asks whether f(n) is computable, not an actual function that computes f(n).

For example, by the construction from JeffE, we know that one of the following *finite* set of turning machines is a correct description of f(n)

    procedure f(n)
        output 1

    prodedure f(n)
        output n &amp;lt;= 1

    procedure f(n)
        output n &amp;lt;= 2

    procedure f(n)
        output n &amp;lt;= 3

etc

Now, we have no idea which of these TMs are correct, in fact it can be easily shown that the problem of deciding which of the above is a description of f(n) is not only undecidable, but also unenumerable. However, we know that one of the above machines is the correct description, and the fact that it exists implies by definition of computability that f(n) is computable.

IE: We don't need to know which machine is the correct description, we only need to prove that such a description exists. Still you assume that n is computable, which it may not be.

take this proof,

We wish to know if a program will halt on some input

it either doesn't halt, or it halts in n steps. so either

    f(n) output false
    f(n) output false if program runs past 1 steps, if it halts by 1 step true
    " " 2 " "
    and so on...

I believe this takes the same logical steps and proves the halting problem decidable         You raise a good question, and it's kinda tricky, getting a feel of what decidability is thought to be defined as and what it actually is. In fact, I get it mixed up all of the time too. 

Decidability is defined as such:

___

* Suppose we have an augmented alphabet E extended with the start and halt operators. A language L is a subset of E. In our case, the alphabet E is the set of naturals and L is the subset of naturals that will always be accepted by f. (We can agree that L is at least recursively enumerable by the construction of the machine F such that given input n, F searches for the sequence 0^n and if found, outputs "yes", proving L is R.E)

* A turing machine M decides L if every computation of M with input from alphabet E halts, and if x \in L, then M(x) = "yes", otherwise M(x) = "no".

* L is decidable iff there exists a turing machine M that decides L

---

Hence, suppose that we have the conflicting realities that either all of 0^n are in pi or some 0^n are in pi, the proof of L's decidability depends on the question of whether there exists **a** turing machine F' such that for every computation of F' with any input from the naturals, F' outputs "yes" when 0^n is in pi and "no" when it is not, using **one** operation.

The difference in attempting to construct a similar argument for a machine H that decides the halting problem is that there are no realities where every program either never halts or always halts after some **finite** set of number of steps (so that we can just build a really large switch statement) because of the existence of the loop

    while True do ; end

Therefore, in order to construct the machine H using the above argument, it will need to incorporate an infinite number of cases, and hence will still be undecidable.

Anyways, I hope this clears it up a little bit and I'm glad you pointed that question out But similarly pi never stops and we never know what the next number looks like until we calculate it. You are assuming that you can calculate the longest string of 0s in pi, but I think that is just as impossible as calculating how many steps a program is going to run for. Okay, you're not getting my point. By the definition of decidability, you don't need an actual value of N; the point is that we don't need to compute pi in order prove that there exists a function that decides whether 0^n is in pi.

I showed F(n) only to show the trivial property that L must at the very least by R.E. I get your point but I don't believe that N exists. There might always be a bigger one waiting later on pi. We've already covered the case when N doesn't exist, in that case, the turing description f(n) = 1 decides the problem. 

Basically we divide the problem space into either N is finite or it doesn't exist because we can't be sure of which it is, we just know that naturally, it must be either one or the other. I was unclear in the previous post, I should have said doesn't exist as a computable number, but I think I see the difference in our arguments now.

For any decision problem I can say that two programs exist, one that always returns true and one that always returns false, and I just don't know which one to pick. The picking is part of the problem. I was unclear in the previous post, I should have said doesn't exist as a computable number, but I think I see the difference in our arguments now.

For any decision problem I can say that two programs exist, one that always returns true and one that always returns false, and I just don't know which one to pick. The picking is part of the problem. But similarly pi never stops and we never know what the next number looks like until we calculate it. You are assuming that you can calculate the longest string of 0s in pi, but I think that is just as impossible as calculating how many steps a program is going to run for. Still you assume that n is computable, which it may not be.

take this proof,

We wish to know if a program will halt on some input

it either doesn't halt, or it halts in n steps. so either

    f(n) output false
    f(n) output false if program runs past 1 steps, if it halts by 1 step true
    " " 2 " "
    and so on...

I believe this takes the same logical steps and proves the halting problem decidable         Still you assume that n is computable, which it may not be.

take this proof,

We wish to know if a program will halt on some input

it either doesn't halt, or it halts in n steps. so either

    f(n) output false
    f(n) output false if program runs past 1 steps, if it halts by 1 step true
    " " 2 " "
    and so on...

I believe this takes the same logical steps and proves the halting problem decidable         The halting problem requires that "a single program" make the decision. This is not a single program, but a countably-infinite set of them. Additionally, in order to determine which f(n) to use to solve the halting problem, you must solve the halting problem.

For the OP question: Determining n is not necessary. It is known that for any n the question is computable, as well as for no n. In all cases it is computable. The halting problem is known and proven to be undecidable, I used the same proof applied to the halting problem to "prove" it decidable.

I'll admit the problem is decidable by a countably-infinite set of programs, but then really, so is the halting problem.

In conclusion, I'm really not exactly sure what you are getting at above... The problem analogous to the halting problem would be one that decides whether the string 0^n appears in *any given* irrational number.

If we restrict ourselves to just pi, then we no longer have the same generality the halting problem has, which is where your argument falls apart. This is not true. While we might be able to prove that some programs halt or don't halt, there are some programs that we will never be able to prove either for. The common proof for the halting problem uses a specific program and an assumption of the existence of a halting problem solution to reach a contradiction. This is not true. While we might be able to prove that some programs halt or don't halt, there are some programs that we will never be able to prove either for. The common proof for the halting problem uses a specific program and an assumption of the existence of a halting problem solution to reach a contradiction. Yes there are, but that's because their behavior is much more complicated than the one in the OP.

Let's try this:

Do you agree that for any fixed N, then the function f_N(x) = 1 if x &amp;lt;= N and 0 if x &amp;gt; N is computable? (N may be infinity of course)

Do you agree that the every function in the *set* of functions {f_N(x) | 0 &amp;lt;= N &amp;lt;= infinity} described above is computable?

Do you agree that the function f in the OP is an element of this set, even if we don't know which specific element it is?

Then you must agree that f itself is computable. If you don't, which step did you disagree with? Then you can solve any decision problem
take the set of {f(\_) =&amp;gt; true, f(\_) =&amp;gt; false} and pick one. The selection of the function you choose has to be part of computability. This is not true. While we might be able to prove that some programs halt or don't halt, there are some programs that we will never be able to prove either for. The common proof for the halting problem uses a specific program and an assumption of the existence of a halting problem solution to reach a contradiction. The thing is, the question only asks whether f(n) is computable, not an actual function that computes f(n).

For example, by the construction from JeffE, we know that one of the following *finite* set of turning machines is a correct description of f(n)

    procedure f(n)
        output 1

    prodedure f(n)
        output n &amp;lt;= 1

    procedure f(n)
        output n &amp;lt;= 2

    procedure f(n)
        output n &amp;lt;= 3

etc

Now, we have no idea which of these TMs are correct, in fact it can be easily shown that the problem of deciding which of the above is a description of f(n) is not only undecidable, but also unenumerable. However, we know that one of the above machines is the correct description, and the fact that it exists implies by definition of computability that f(n) is computable.

IE: We don't need to know which machine is the correct description, we only need to prove that such a description exists. But such a "proof" relies on Excluded Middle, which implicitly assumes the existence of a correct description!  Constructively this just doesn't work...  Just to summarize

The question asks to prove that the function defined as follows is computable.

&amp;gt; f(n) = 1 if 0^n is in the number pi

&amp;gt;        = 0 otherwise

Equivalently, we can also ask whether the decision problem "does there exist the string 0^n in the number pi?" is decidable?

JeffE's response is to consider the following two cases

* For every integer n, 0^n appears in pi
* There exists some finite integer N such that 0^N is the largest string of zeroes that appears in pi.

Informally, if the string 0^N appears in pi, then 0^(N-1) must also appear in pi (as a subsequence of 0^(N)), hence JeffE contends that these two cases are the only possible cases. Under each case, there exists some computable function satisfying f. We'll see why

* Suppose all 0^n appears in pi, then the trivial function f(n) = 1 satisfies f because no matter what input n, 0^n will be in pi so f(n) will always output 1. Now this is clearly computable because the number of operations is bounded finitely.

* Suppose that there exists some maximum N such that 0^N appears in pi but not 0^(N+1). In this case, in order for us to say that 0^N appears in pi, the sequence must appear a finite digit away from the radix point, hence under this scenario,  the simple function f(n) = n &amp;lt;= N must also be decidable.

Now there's some reservation over at CS.SE due to the fact that we don't know which of the two cases are correct. In fact, it is an easy reduction problem into both Halt and CoHalt for the problems of deciding whether all 0^n appears in pi so we know that we can't decide which of the two cases are correct.

Furthermore, the machine that is valid in case 2 will never terminate in case 1 and hence is not decidable (albeit it is still a valid description of f); likewise, the machine that is valid in case 1 will produce incorrect results in case 2 and is not a valid description of f. 

However, asking for whether f(n) is computable is different from asking you to construct f(n). All we have to do is show that there exists a machine description of f(n) that is correct and terminates on all input. Because we know that one of the two machines constructed by JeffE is correct in all universes and exists, then obvious that machine exists. Hence, by the sole criterion of existence, we claim that f(n) is computable. [; \blacksquare ;] That isn't a valid proof of decidability, but rather demonstrates that the program is recursively enumerable.  The program might not be recursive though, in which case it's undecidable but recursively enumerable.

In other words you've shown only that the problem is at least semi-decidable. I'm pulling this from section 4 of http://www.cs.cornell.edu/courses/cs4820/2012sp/handouts/turingm.pdf

* M decides L if every computation of M halts in the "yes" or "no" state, and L is the set of strings occurring in starting congurations that lead to the "yes" state.
* L is decidable if there is a machine M that decides L.

exactly one of the turing machines F(n): 1, F(n): n &amp;lt;= 1, F(n): n &amp;lt;= 2, etc decides L as per above, hence via a classical interpretation of decidability, L is decidable and its associated function f(n) is therefore also computable I'm not saying the original problem is undecidable, I'm assuming it is decidable since it seems to have been assigned as a homework question.  However, your proof of its decidability is incomplete as it only establishes a worst case of being partially decidable.

In order to complete the proof, you'd need to show that N is computable.  There are numbers where such an N is not computable even though it obviously has to exist, for example Chaitin's number.  If N can not be computed, then the problem in general can not be decided. I'm not saying the original problem is undecidable, I'm assuming it is decidable since it seems to have been assigned as a homework question.  However, your proof of its decidability is incomplete as it only establishes a worst case of being partially decidable.

In order to complete the proof, you'd need to show that N is computable.  There are numbers where such an N is not computable even though it obviously has to exist, for example Chaitin's number.  If N can not be computed, then the problem in general can not be decided. I'm not saying the original problem is undecidable, I'm assuming it is decidable since it seems to have been assigned as a homework question.  However, your proof of its decidability is incomplete as it only establishes a worst case of being partially decidable.

In order to complete the proof, you'd need to show that N is computable.  There are numbers where such an N is not computable even though it obviously has to exist, for example Chaitin's number.  If N can not be computed, then the problem in general can not be decided. [deleted]  What the hell is 0^n? Isn't 0^n = 0?</snippet></document><document><title>Is there any concrete relation between G&#246;del's incompleteness theorem, the halting problem and universal Turing machines?</title><url>http://cs.stackexchange.com/q/419/773</url><snippet>   </snippet></document><document><title>How and where can I try to publish my B.Sc. thesis?</title><url>http://www.reddit.com/r/compsci/comments/xur9g/how_and_where_can_i_try_to_publish_my_bsc_thesis/</url><snippet>My B.Sc thesis was [The Arduino as a Hardwre Random Number Generator](http://benedikt.sudo.is/ardrand.pdf). My advisor wants me (and has strongly advised that I do so) try to get my paper published. 

It had been rumored (word on the internet) that the microcontroller was able to read analog noise that would be random. I attempted to utilize it as a cheap hardware RNG (not successfully).

The Arduino manual makes the claim that using this noise data to seed the rndom() function (from avr-libc). I showed that this wasn't true.

Where should I try to get this published? My advisor has suggested workshops, but I haben't found anything suitable. Advice?

EDIT: I accidentally link.    Density plots are your friend, all those horrible gnuplot red and white plots can be made pretty with density plots.

Check USENIX and the relevant USENIX venues. They like this sort of thing.

Frankly, no one really cares about the software bug that the arduino sucks at random numbers, but what they might care about is the temperature dependency. I think that might fly.

USENIX Embedded Security: https://www.usenix.org/conference/usenixsecurity12/session/embedded-security

If you want GOOD advice, go to the program committee of USENIX Embedded Security and ask 2 or 3 of them is A. it is an appropriate venue when you clean it up and B. what are other relevant venues.

e: What workshops did he suggest. addictedtomeme is right, you know nothing and don't know better than your advisor, listen to him first, me and addictedtomeme second, and reddit last.   your thesis has a broken link      Find a conference which fits the scope of the project. I would suggest checking the ACM and IEEE websites for a list of upcoming conferences. Once you've identified a conference, find their website and look over the paper submission procedures, these may vary but they should provide a document template, page limits, and submission deadlines.

Be aware, if your paper is accepted by the conference, you will likely be required to attend the conference and present a poster or presentation. The cost of registration and attendance will come out of pocket, but your university/department may have special funds set aside to reimburse these costs.

EDIT: A journal may be more appropriate depending on the scope and impact of the paper; however, acceptance is much more competitive.</snippet></document><document><title>Fast square root (inspired by id's fast invsqrt), also known as how to find a good first guess efficiently</title><url>http://www.phailed.me/2012/08/somewhat-fast-square-root/</url><snippet>  It's an aliasing violation and therefore undefined behavior in C to access a float value through an int pointer.  The compiler is allowed to launch nasal demons.
 Tricks like this are useful, and should only ever be used, when every cycle is hugely important and portability is not. Where portability is at all important, one calls sqrt() and kicks methods like this to the curb.

So the argument that "this invokes behavior marked UNDEFINED by the language spec" would gather this response in a code review: So?
 The only person that would respond with "So?" is someone who has never had the harrowing experience of having to track down an incredibly difficult bug that was the result of undefined behavior.  Modern compilers are ruthless about exploiting the standard to the letter to extract every performance gain, and so you must be absolutely certain that your code does not exhibit undefined behavior.
 Been there, done that, outlived the T-shirt. And yes, under nearly every conceivable circumstance, I'm known to be strict about compiler warnings. I've been known to file bugs starting "I just pulled the latest sources for GCC, and guess what, they are complaining about something that should be fixed" ...

But there is a time and a place for specific tricks to gain necessary performance when it can be done with an acceptable loss of portability, precision, and/or accuracy; when it is sufficient to assure that the behavior of the code is predictable on the current target.

It is in that context that the resounding response, around the table, from the real time kernel team, to someone pointing out that a bit of performance critical machine specific code has behavior that is "undefined" in ISO/IEC 9899:1999, will be: "So?"

 In which case they should not be writing that bit of code in invalid C and expect the compiler to 1) know what the fuck to do with it and 2) interpret it the same way they do. That may be fine if you write your own compiler, but then why not go ahead and design your own language where that is actually defined behaviour?

Regarding your statement in the other branch of the thread:
&amp;gt; In real time systems, a late answer (no matter how correct) is almost never preferred over a good approximation that arrives in time to make a correct decision.
Having an answer arrive a bit late is preferable to having the entire system fall down on your head because you hit a corner case that wasn't hit during testing.

Here's a story: your airplane's flight control system is using a fast square root algorithm, and periodically smoothes out the error. It needs to be fast, so you decide to squeeze out that one CPU instruction by writing invalid C code and trusting the compiler to do The Right Thing with it. Then one day, your plane falls out of the sky. The end.

TL;DR: I'd like to see them ask "So?" when their use of undefined behaviour kills people. There is no "trusting the compiler" involved. You *know* what the behavior is, on your platform, and if you change platform -- that is, if you change compilers, libraries, operating system or hardware -- you verify that it still does the right thing.

And having done that verification, you carry it forward as a unit test, to alert you if the behavior changes.

And having assured that the produced code does the right thing -- captures the bits that make up the floating point number in an integer variable, in this case -- you know that the code will not magically rewrite itself to do something significantly different.

On one note, I agree with you too strongly to just leave the point unanswered: I would not fly nonstandard coding tricks in a safety-critical embedded system. But not necessarily for the reason you indicate; rather, for the simple reason that, for such projects, you don't paint yourself into the performance corner of needing to do so; you make damn sure you have enough computing power on your target.

But you're wandering off into straw-man land. Designing my own language is neither necessary nor desirable; unless you count "ISO/IEC 9899:1999 with the following additional behavioral guarantees" ... nor can my flat assurance that there are times when this is appropriate be rationally extended into saying that I would do so in cases where it is not necessary, not desirable, and far too high risk.
 I'm sorry, I think we misunderstand each other about the meaning of "undefined behaviour". You seem to take it as meaning "implementation specified", whereas I'm taking it as "the compiler does not have to be consistent about what it produces.

In particular, the compiler could produce code that runs "fine" (read "as you expect") 99% of the time, and crashes spectacularly the remaining 1%. For example, it could produce code that crashes only if some weird concurrent stuff happens at the same time (and there is a lot of that going on). Unless you know exactly what the compiler does, you can't ever say "it produces the machine code I expect it to".

Now, where do you draw the line of "too high risk"? Is it fine for your car to run code that's written that way? Some computer in a hospital?

In what context do you expect to ever have timing requirements strong enough and safety requirements weak enough that you can go ahead and say "I'm not really relying on the fact that I ever get a result, but if I get one it better be on time".

That being said, nobody really gives a shit about most software (which may be seen as unfrotunate), so I guess that's fine and I agree with you, as long as the people who do write the code where it's important realize that it is. Tricks like this are useful, and should only ever be used, when every cycle is hugely important and portability is not. Where portability is at all important, one calls sqrt() and kicks methods like this to the curb.

So the argument that "this invokes behavior marked UNDEFINED by the language spec" would gather this response in a code review: So?
 Correctness is generally preferable to performance.
 In real time systems, a late answer (no matter how correct) is almost
never preferred over a good approximation that arrives in time to make
a correct decision.

When renormalizing unit vectors in a display subsystem of a game,
the error in the vector length being 1e-3 or 1e-6 matters a lot less
than whether or not you got the frame drawn in time.

There are more examples of when performance is specifically
preferred over correctness.

 Real time does not mean high performance. Real time means high *enough* performance. It's an aliasing violation and therefore undefined behavior in C to access a float value through an int pointer.  The compiler is allowed to launch nasal demons.
 I'm still checking under my bed to make sure that there aren't any monsters.

With that said, I'm not the greatest coder (did you see that loop guard in the main function?) and I understand that most compilers will associate known aliases with type information saved for optimization stages that needs to do alias analysis, but since &amp;amp;i was never changed and y isn't allocated into an integer register rather than &amp;amp;j, the alias type can't be violated since there's no defs on &amp;amp;i. I know, this isn't an excuse to break C, but using unions presents the same exact difficulty of dereferencing a memory location tagged with type *a* into a storage container of type *b* and hence also constitutes an aliasing violation;  the only way around would be to store f in memory, create a new int n in memory, and copy f into n, and then dereferencing n.

The main purpose of the article was to present the method for guessing sqrt(n), I wrote it in C because it's one of the few languages that allowed me to extract out the bit vector associated with floating point numbers It's an aliasing violation and therefore undefined behavior in C to access a float value through an int pointer.  The compiler is allowed to launch nasal demons.
 It's an aliasing violation and therefore undefined behavior in C to access a float value through an int pointer.  The compiler is allowed to launch nasal demons.
 Everyone does this so it's undefined as far as your mom is undefined. No, everyone does not do this, only people that do not know what they're doing.  Those who have been bitten by aliasing violations use a union or memcpy(). A union is no better. It is undefined to read from a member of a union other than the last member that was written to. `memcpy` or a manual object representation copy through a pointer to `char` type is likely the only portable way to read a `float` as if it were an `int`, and this makes the assumption that `int` and `float` have the object representations that you want. No, everyone does not do this, only people that do not know what they're doing.  Those who have been bitten by aliasing violations use a union or memcpy().  Impressive! And *woosh*

If you know most of your sqrts will be within a specific range, you can use a lookup table. For values outside of the lookup table you take the full hit of a sqrt. For values inside the lookup table you can interpolate between a low and high value. By changing your lookup table bounds, step size and (non-)linearity, you can get a fair bit of flexibilty and the code remains fairly simple to read, as well as being cross-platform. I've used this approach on iOS games (along with sin+cos lookup tables) and it worked well.  TIL Carmack was not the original designer of this method.  No benchmark vs Carmack's implementation? No timing measurements at all, as far as I could find.

Carmack's famous code calculates 1/sqrt(x) while this code calculates sqrt(x). What makes Carmack's code amazingly fast is that it does not do floating point divides; this code needs to use a divide in the solution improvement iteration step.

As author used the word "fast" in his title, I rather expected to see a direct comparison of benchmark times for built-in sqrt, this code, and Carmack's code with result multiplied by the input, to give sqrt().

As it stands, it is a decent article on how to manipulate the bits of IEEE floating point to generate an initial guess for sqrt(x), with a presentation of the method for improving the guess.

Actually, a time measurement that would really support the content would be to show how many iterations you save by using the fancy calculated guess over simpler starting guesses. Oh sorry, I meant fast only in the sense that the initial guess requires only two steps of newton for the level of accuracy on a scale of 2^-20 (~O(machine epsilon for fp)) with the property that the size of the input doesn't affect the relative error of the output as opposed to all linear estimators that break down linearly, and that the code to compute the initial value is faster on an Intel machine than each step of the newton refinements (1 fpu instruction + a few arithmetic instruction roughly takes just one cycle as opposed to multiple fpu instructions that blocks due to the fact that Intel only offers one module to do fp arithmetic).

However, the original fast invsqrt procedure is still faster than my code due to fewer ops and no fp instruction until they get to the refinement step. Since I've taylored the code to specifically calculate the square root as accurately as possible, I do achieve higher relative accuracy (1-2 points in the guess, 2-8 points after refinement) than fast invsqrt but at the cost of clumsier and slower code.

A few more observations: qsqrt's relative error is cyclic in O(x^(2)) (because perfect squares are computed perfectly without need of any refinement); wikipedia's implementation is cyclic in O(2^(n)) (what it does is basically it halves the exponent and zeroes out the mantissa. This works only when all 2^n bits in the mantissa are 0), and linear estimators are **linear** in O(n) (because Newton converges quadratically, this means that for every square, you need to add in an additional step of newton's in order to preserve the same accuracy as before)

Here's the usual way of estimating the first square root using x/2 [http://codepad.org/HLLGH8zF](http://codepad.org/HLLGH8zF). As you can see, the relative error seems to break down linearly &amp;gt; Since I've **taylored** the code ...


Hah!

 &amp;gt; Since I've **taylored** the code ...


Hah!

 Oh sorry, I meant fast only in the sense that the initial guess requires only two steps of newton for the level of accuracy on a scale of 2^-20 (~O(machine epsilon for fp)) with the property that the size of the input doesn't affect the relative error of the output as opposed to all linear estimators that break down linearly, and that the code to compute the initial value is faster on an Intel machine than each step of the newton refinements (1 fpu instruction + a few arithmetic instruction roughly takes just one cycle as opposed to multiple fpu instructions that blocks due to the fact that Intel only offers one module to do fp arithmetic).

However, the original fast invsqrt procedure is still faster than my code due to fewer ops and no fp instruction until they get to the refinement step. Since I've taylored the code to specifically calculate the square root as accurately as possible, I do achieve higher relative accuracy (1-2 points in the guess, 2-8 points after refinement) than fast invsqrt but at the cost of clumsier and slower code.

A few more observations: qsqrt's relative error is cyclic in O(x^(2)) (because perfect squares are computed perfectly without need of any refinement); wikipedia's implementation is cyclic in O(2^(n)) (what it does is basically it halves the exponent and zeroes out the mantissa. This works only when all 2^n bits in the mantissa are 0), and linear estimators are **linear** in O(n) (because Newton converges quadratically, this means that for every square, you need to add in an additional step of newton's in order to preserve the same accuracy as before)

Here's the usual way of estimating the first square root using x/2 [http://codepad.org/HLLGH8zF](http://codepad.org/HLLGH8zF). As you can see, the relative error seems to break down linearly The first time I used the "fast inverse square root" was on hardware that
could do floating point multiplies fairly rapidly, but divides took several times as long. So moving from "divide by sqare root, which has to do
divides" to "multiply by inverse-sqrt, which only multiplies" made that bit of code significantly faster.

But it did leave me sensitive to seeing divide as higher cost than multiply. Not sure whether this still holds, or if current FPUs can churn out divide results as fast as multiply results (or more precisely, does not take long enough to block the pipe, gotta love multi-issue ;)

Bottom line? many ways to skin this cat; and for any given target, have to at least consider coding up and evaluating timing of each one; thanks for adding another good alternative!
 No benchmark vs Carmack's implementation? Hypothetically, his is way faster than mine :) Did a benchmark (code [here](https://github.com/DavidHulsmanNL/RSqrtBenchmark), binary [here](http://www.mediafire.com/?25636558u7r21eu)) __note:__ needs ~800MiB RAM

name|t in sec|comment
-|-|-
Lee|2.81|Slowest and almost as accurate as original sqrt()
sqrt|1.36|Pretty slow, but most accurate
Quake3|0.24|Fast, but less accurate than sqrt()
Doom3|0.6|Slower than Q3's implementation, but more accurate. Uses a lookup table

Note: inlining your function makes no difference, whereas it does for the other implementations Thanks for putting in the effort, I really appreciate it, does changing the first line to 

    int i;
    memcpy(&amp;amp;i, &amp;amp;x, 4);

and compiling with -O3 have any difference?

I've noticed that -O default never ends up being aggressive enough to vectorize the code, but at -O3, the following listing gets generated

    qsqrt(float):
        movd	xmm1, DWORD PTR .LC3[rip]
        xor	eax, eax
        movd	xmm0, eax
        unpcklps	xmm1, xmm1
        cvtps2pd	xmm1, xmm1
        mulsd	xmm1, QWORD PTR .LC0[rip]
        movddup	xmm2, xmm1
        movd	xmm1, eax
        cvtpd2ps	xmm2, xmm2
        divss	xmm1, xmm2
        addss	xmm1, xmm2
        movss	xmm2, DWORD PTR .LC2[rip]
        mulss	xmm1, xmm2
        divss	xmm0, xmm1
        addss	xmm0, xmm1
        mulss	xmm0, xmm2
        ret
    .LC0:
        .long	1719614413
        .long	1073127582
    .LC2:
        .long	1056964608
    .LC3:
        .long	528482304</snippet></document><document><title>What is the Mars Curiosity Rover's software built in?</title><url>http://programmers.stackexchange.com/questions/159637/what-is-the-mars-curiosity-rovers-software-built-in</url><snippet>   FYI: I killed a lot of the comments here since most of them were unrelated to the submission.  Heh, 75% of the comments on reddit unrelated to the topic  [deleted] Nowhere does it mention Hiphop or PHP [here](http://compass.informatik.rwth-aachen.de/ws-slides/havelund.pdf)

Looks like just C and Python without the help of Hiphop. [deleted] [deleted] [deleted] Nowhere does it mention Hiphop or PHP [here](http://compass.informatik.rwth-aachen.de/ws-slides/havelund.pdf)

Looks like just C and Python without the help of Hiphop. Nowhere does it mention Hiphop or PHP [here](http://compass.informatik.rwth-aachen.de/ws-slides/havelund.pdf)

Looks like just C and Python without the help of Hiphop.  It was built with 2.5 million lines of code in C.

[source](http://compass.informatik.rwth-aachen.de/ws-slides/havelund.pdf) It was built with 2.5 million lines of code in C.

[source](http://compass.informatik.rwth-aachen.de/ws-slides/havelund.pdf)   My father is part of the Mars communications team for JPL, I'll get back to this thread with a reliable answer. Can you ask him for an AMA? He is currently swamped with work, as you can imagine. The EDL team is partying and chillin' to the max, but the communications team is just beginning a new stage of their work. When things settle down, I'll see if he'd be willing.   [deleted]</snippet></document><document><title>Decent readings on "Fundamentals of Computer System" </title><url>http://cs.brown.edu/courses/cs195s/reading.xml</url><snippet /></document><document><title>Author of popular PLAI textbook offers intro Programming 
Languages online for free [title fixed]</title><url>http://www.cs.brown.edu/courses/cs173/2012/OnLine/</url><snippet>  </snippet></document><document><title>Expanding my Computer Science Knowledge.</title><url>http://www.reddit.com/r/compsci/comments/xt2oa/expanding_my_computer_science_knowledge/</url><snippet>Hey guys, hopefully this is in the right section. Well, I'm in high school and aspiring to become a computer scientist for my professional career. I've been trying to get a head start since middle school by learning programming languages and some basic computer knowledge. But I would like to know what you guys would suggest to expand my knolwedge on everything related to computer science, including hardware, programming, etc. Like maybe book references or just ideas and concepts I should look up to study? I'm just trying to further my knowledge in the computer science field and I hope you guys can help me out!  &amp;gt; Hey guys, hopefully this is in the right section.

[not really, IMO](http://www.reddit.com/r/askcomputerscience)

Despite the fact that the word "computer" is in the name "Computer Science", CS has surprisingly little to do with computers.  If you can already program a bit, that's good.  I would recommend that you now start looking at the core of CS.  Pick up a discrete math or algorithms book and start working through it.  I don't know of a good introductory book (my first was the CLRS). Thanks! Besides discrete math and algorithms what other topics should I explore that are also at the "core" of of CS? Thanks! Besides discrete math and algorithms what other topics should I explore that are also at the "core" of of CS?  I recommend udacity.com for someone your age This. is. AMAZING. Thanks!  Depending on how you learn, books may be a good start. If you don't like books, or after you've gone through one-- [build something useless](http://blog.andrewmunsell.com/post/28852025566/build-something-useless#.UCCe5ql0Uy4). The cliche, "practice makes perfect," also applies to CS and programming. 

Look at sites like StackOverflow and answer questions to improve your coding skills as well-- essentially, each one of those questions is like a little CS problem to solve ;)

There's no particular area you "should" study-- it's up to you to determine if you're into design, programming, hardware hacking, etc. Explore some different fields (look into Raspberry Pi, Arduino for starting out hardware hacking since it's pretty cheap $$$ wise). 

If you're looking for CS work (ie. freelance), start building a portfolio by building stuff for yourself or for friends, family, etc. Start off small and gain some experience.   Computer Science its a wide area of knowledge. Most people in computer science focus in software development like programming, object oriented design, framworks. Other people are more interested in databases, datawarehouses, indices, distributed systems. Of course there are other topics, like networking, visual computing, etc. As a computer scientist you should be able to understand all of those concepts, but you can pick one to specialize yourself.   </snippet></document><document><title>Combining multiple languages from different paradigms into a common environment through a clever type system and careful memory management - possible?</title><url>http://www.reddit.com/r/compsci/comments/xs6i7/combining_multiple_languages_from_different/</url><snippet>This idea has been banging around in my head for a while and I thought that this would be a good place to bring it up for discussion as it would entail elements of language design, typeclasses, and so on. It stemmed from my often switching between several languages - C, Python, Haskell, Prolog, Java - based on what I was working on.

Everyone has their favorite language and each language has a domain in which it is most applicable. C is favored by people that like to be "close to the metal", Haskell is the champion of the functional world, Prolog is the leading declarative language, and Python is some amalgamation of several paradigms and is very popular. Each of these is different but they have some common elements.

Each has some sort of integer representation, for example. With care, one should be able to create a system that translates between, say, arrays in C and lists in Python, converting an (int*) to a Python list and back. Would it be possible to create an environment in which one could easily switch between languages for different sections of a program?

Here's a pseudocode example:

    common_variable List(Integer) myData;
    common_variable String filename;

    function C void complicatedSort(...) {}
    function Python List filterByFile(...) {}
    function Haskell List generateList(...) {}

    myData = filterByFile(generateList(myData), filename);
    complicatedSort(myData);

where the third function might use Haskell's expressivity to generate a list based on some logic that would be best expressed in Haskell and return a List type, which would be passed to the second function to be filtered in a Python list comprehension that compares against the contents of a file, and finally given to C to be sorted efficiently.

It's not the best example, but it illustrates what I had in mind - write the more abstract / higher level code in, say, a functional language, pass it to another for IO, pass it to a lower level language for computational efficiency. A better example might be to a definite clause grammar in Prolog to analyze some text, making into a tree with semantic labels and word senses attached, passed to Haskell for intelligent and concise tree operations, passed to C to be used for analysis using the GSL library on a supercomputing cluster using a gigantic dataset pulled from the internet and initially parsed by Python using the BeautifulSoup module.

It should be possible by carefully choosing a collection of primitive types that can be translated into the format a particular language uses, transform the data as necessary when passing it to a function in the language, transforming back that function's return value, and re-translating it for a function in another language, and so on. This common environment's main purpose would simply be formatting data in memory so as to be passed to function blocks written in whatever language is chosen for the actual work to be done, the environment not really being part of the program logic, which would be executed by a code block in an existing language.

This environment would have its own compiler that would generate a symbol table and the other standard portions of an executable. Each code block in its own language would then be compiled using that language's compiler but with the shared symbols table etc. passed so that the different parts could interoperate, and the final program generated would be native machine code. The result would be speedier development without much of a performance tradeoff. The programmer could switch between conciseness of expression and efficiency of computation as appropriate to the program subroutine being written.

Has something like this been attempted? Are there difficulties preventing such an environment that I'm not seeing? I think that it would be useful and that it would be feasible, but I've only barely scratched the surface of language design and so am not qualified to judge.

Thoughts?

**Edit**: Perhaps the main difficulty here would be that one would need a good programming language designer as well as at least one person per language that is familiar enough with the corresponding compiler to be able to help with interoperability. It would require an quite a collection of talent from separate areas.    This sort of thing has been a dream since the 1950s. Start by reading about [UNCOL](http://en.wikipedia.org/wiki/UNCOL), and move on from there.  The [Common Intermediate Language](http://en.wikipedia.org/wiki/Common_Intermediate_Language) implements this very nicely. Its a ECMA standard which defines a byte code (compiled but machine-independent language much like Java's class files) which can be run on any system that has the Common Language Runtime (CLR). On Windows this is the .NET Framework.

All .NET languages are compiled to CIL, not to machine code (unlike C/C++ which compiles to ASM which is then turned into machine code). When the application is executed the CLR compiles the byte code into machine code the CPU can understand using a [Just In Time \(JIT\) compiler](http://en.wikipedia.org/wiki/Just-in-time_compilation).

This means that any .NET language can use code from any other .NET language and be run on any machine the CLR supports (x86, x64, ARM, PPC, etc). I can code my project in C# and then you can go use it in a functional language like F#. In fact, there are [a lot of languages](http://en.wikipedia.org/wiki/List_of_.NET_languages) that have been made to conform to the CLR including C++ (through .NET) and Python (through IronPython). Even Java using [IKVM](http://www.ikvm.net/).  I had an idea for a meta-language that could be used to describe any language. Are you familiar with category theory? I'm no expert on it but from what I understand categories can mathematically describe certain concepts like types, sets, and functions that return things other than numbers. If you had a language that was "category-driven" you could potentially describe any language mathematically. This would enable the compiler/interpreter of the language to handle anything written in Python by looking at the "Python category". Its just a dream of mine, though. I'm no super genius and have nowhere near the mathematical ability to really work on something like this. I do believe that categories would be the first steps towards a universal programming language that can do it all. I had an idea for a meta-language that could be used to describe any language. Are you familiar with category theory? I'm no expert on it but from what I understand categories can mathematically describe certain concepts like types, sets, and functions that return things other than numbers. If you had a language that was "category-driven" you could potentially describe any language mathematically. This would enable the compiler/interpreter of the language to handle anything written in Python by looking at the "Python category". Its just a dream of mine, though. I'm no super genius and have nowhere near the mathematical ability to really work on something like this. I do believe that categories would be the first steps towards a universal programming language that can do it all. You might be interested in [denotational semantics](http://en.wikipedia.org/wiki/Denotational_semantics). Thanks for the link! I'm no mathematician or anything. I REALLY struggle with category theory but I do find it incredibly interesting. I think sometime in the future there will be the possibility for a universal language.</snippet></document><document><title>The One Instruction Wonder</title><url>http://www.drdobbs.com/embedded-systems/the-one-instruction-wonder/221800122</url><snippet>  Hmm seems kind of gimmicky.

It's like saying you invented a CPU with one single instruction called "do" that takes two parameters as follows:

    p1 : The mnemonic for an x86 instruction to execute.
    p2 : An address to the parameters to pass to x86 instruction in p1.

In other words, you just took a whole bunch of real, meaningful instructions and wrapped them as parameters to a single master instruction.

In this article it just isn't presented like this, instead it's presented in terms of moving a value from one address to another, but depending on that address there could be a side-effect which carries out the real, underlying instruction.  Like one address might perform an addition, another a multiplication, so on so forth.

I can see why an architecture like this would be created in practice, but it's not because of its single instruction.  It's more because in a TTA the functionality does not have to be fixed, allowing you to swap in/out different hardware components that will actually carry out the computation. Hmm seems kind of gimmicky.

It's like saying you invented a CPU with one single instruction called "do" that takes two parameters as follows:

    p1 : The mnemonic for an x86 instruction to execute.
    p2 : An address to the parameters to pass to x86 instruction in p1.

In other words, you just took a whole bunch of real, meaningful instructions and wrapped them as parameters to a single master instruction.

In this article it just isn't presented like this, instead it's presented in terms of moving a value from one address to another, but depending on that address there could be a side-effect which carries out the real, underlying instruction.  Like one address might perform an addition, another a multiplication, so on so forth.

I can see why an architecture like this would be created in practice, but it's not because of its single instruction.  It's more because in a TTA the functionality does not have to be fixed, allowing you to swap in/out different hardware components that will actually carry out the computation. &amp;gt; Hmm seems kind of gimmicky.

No more than, say, the Sheffer stroke/NAND or NOR operators being functionally complete.  And here I was hoping he'd attempted to implement a legitimate OISC set.  After all, there are sets that are proven to be OISC instruction sets.  (Notably, subtract and branch if less than or equal to zero/less than zero).

Of course, those instruction sets don't handle I/O, but that's not a requirement of such a computer.  I've been thinking about this for a while now. Would it be possible to implement a Turing complete machine using only add, subtract, multiply and exponentiation? I think that if you manage to implement a conditional checker using only those operations, it'd be possible (like the subleq machine shows). If, and only if, you get to store results into the IP.

If you have access to the IP, doing conditions is trivial. First, reduce your condition result to 1 or 0. Then use it to multiply the distance to the jump location for the "1" case, and add and store that to the IP.

Note that I think that computers (almost) like that have actually existed. Some early calculators had a really diminutive instruction set, and implemented most complex operations using the simple ones. Only they typically had no native mul or div, just shifts and bitops. I was actually thinking of a machine that would operate with only one integer, that is, that it was a function that would only take in one input. </snippet></document><document><title>what is the best and most fun to read comp science book?</title><url>http://www.reddit.com/r/compsci/comments/xpwwf/what_is_the_best_and_most_fun_to_read_comp/</url><snippet>i'm looking for something to help me think in a logical computer kind of way.    Really?  Structure and Interpretation of Computer Programs is a "fun to read" book to help someone "think in a logical computer kind of way"?  Give me a break.  I'm sure someone will come along and suggest you read through the Knuth series, too.

I'd suggest something like this:


The Tinkertoy Computer and Other Machinations, by A.K. Dewdney.

http://amzn.com/0716724898

Dewdney used to write a column in Scientific American in the '90s called "Computer Recreations," which explored interesting computer science ideas for a non-C.S. audience.  This book is a collection of some of those columns (there are a few other collections as well).  I used to read the column, and I've read much of this book and others, and they are fun and interesting.  They are not about programming directly, but explore algorithms and ideas in an accessible way.  Each article also stands alone, so you can thumb through a pick out an  interesting chapter to read without having to wade through the ones that don't appeal to you.

If you want to learn about programming, computer architecture, etc., there are some excellent textbooks, but if you want to explore the CS way of thinking I'd got for something written with that in mind.  I enjoyed "Algorithmics" by David Harel when I was starting out. It does a good job of explaining the why of comp sci. http://www.inf.ufrgs.br/~ssalamon/Books/Algorithmics%20-%20The%20Spirit%20of%20Computing.pdf http://www.inf.ufrgs.br/~ssalamon/Books/Algorithmics%20-%20The%20Spirit%20of%20Computing.pdf   Goedel Escher Bach is great but it might not be exactly what you're looking for.  There's a lot about AI and recursion and math. I fucking knew some cultist would come in here to advertise their bible.  
  
It's not math. It's not computer science. Hofstadter's book is about a fucking religion. Keep that shit out of /r/compsci. Someone's cranky. I fucking knew some cultist would come in here to advertise their bible.  
  
It's not math. It's not computer science. Hofstadter's book is about a fucking religion. Keep that shit out of /r/compsci. I fucking knew some cultist would come in here to advertise their bible.  
  
It's not math. It's not computer science. Hofstadter's book is about a fucking religion. Keep that shit out of /r/compsci. Making an arguement that the human mind can be attributed to known cycles of logic is not the same thing as preaching religion. Asking people to keep it out of compsci is similar to asking /r/science not to discuss darwin.  Making an argument that despair comes from souls trapped on earth by lord Xenu is not the same thing as preaching religion. Asking people to keep it out of cogsci is like asking /r/compsci not to discuss an absolutely untestable theory of mind that is praised by nerds who only wish they wore the clothes of emperor Hofstadter. Do you know the definition of religion? Its different from that of psychology. Godel, Escher, Bach is really a book about strange loops, formal mathematical systems, and the way in which complex logical systems can be built from small, logically pure subsystems. Its the definition of computer science. Have you actually read the book? If so, you might remember that the parts relating to the human mind are used more as an example of logical hierarchy than as a message in and of itself. But interpret as you will, it is the true fundamentalist who sees the specter of threats to their worldview in every field from psychology to mathematics. I'm sure you would love to know that MIT offers courses based both on that book, and on the "Society of Mind" artificial intelligence book which shares the same theses.  0. A religion is an untestable theory of nature.  
1. No it doesn't.  
2. When it did, it was a summer course geared toward high school students.  
3. For high school students, pretty much any student can set up whatever course he/she wants.  
4. It was run by idiot undergrads.  
5. Appeal to authority, bitch.  
6. MIT isn't even an authority. But... GEB's hypotheses aren't untestable. They're proven by the lambda calculus.  0. A religion is an untestable theory of nature.  
1. No it doesn't.  
2. When it did, it was a summer course geared toward high school students.  
3. For high school students, pretty much any student can set up whatever course he/she wants.  
4. It was run by idiot undergrads.  
5. Appeal to authority, bitch.  
6. MIT isn't even an authority. I fucking knew some cultist would come in here to advertise their bible.  
  
It's not math. It's not computer science. Hofstadter's book is about a fucking religion. Keep that shit out of /r/compsci.   I'd go for Jean-Yves Girard's *The Blind Spot* if you're really into logic (as in Proof theory) and trolling (as in Gratuitous agressivity).

After all he is the kind of guy who ends up writing articles about [mustard watches](http://iml.univ-mrs.fr/~girard/mustard/article.html) just to trash scientists living off of (according to him) stupid exotic extensions of poorly thought logics.         </snippet></document><document><title>Need opinion about online school</title><url>http://www.reddit.com/r/compsci/comments/xpbvp/need_opinion_about_online_school/</url><snippet>hey /r/compsci, pretty soon here im going to start taking online classes and i want to study computer science. i'm really just looking for some advice or input as far as which colleges have a good program. I've looked into it myself a bit and keep running into colleges like DeVry, ITT or Baker university. I would prefer to go to a more reputable college than these examples. Has anyone got a comp sci degree online, or know of any available. All input is much appreciated.  I would avoid DeVry, ITT, or any 100% online school (Phoenix, Westwood, Capella, etc). They offer stripped down, accelerated programs and I don't think you will learn as much as you would at a traditional university. They are also *greatly over priced* (by tens of thousands) and are not well respected among the IT community. Their credits will not transfer to another university, and I honestly think if you enroll at those colleges you will be making a grave mistake.

My advice is to enroll in an accredited brick and mortar university or community college and register for as many of their online courses as you can. Most schools offer online courses these days however you won't be able to go with 100% online courses. For in person classes, try registering all of those on the same days, such as Monday/Wed, Tues/Thurs, or all day Saturday to minimize your commutes to the school. You could also try registering for night courses if you need to work during the day.

My qualifications: I have a computer science degree and have worked as a software engineer for 15 years. I've interviewed for software development jobs and interviewed candidates for the same jobs. I do not respect nor am I impressed by degrees from the universities you listed. In Canada we have a University called Athabasca that is completely online and is a public university. Its courses transfer as easily as any other university up here. I would avoid DeVry, ITT, or any 100% online school (Phoenix, Westwood, Capella, etc). They offer stripped down, accelerated programs and I don't think you will learn as much as you would at a traditional university. They are also *greatly over priced* (by tens of thousands) and are not well respected among the IT community. Their credits will not transfer to another university, and I honestly think if you enroll at those colleges you will be making a grave mistake.

My advice is to enroll in an accredited brick and mortar university or community college and register for as many of their online courses as you can. Most schools offer online courses these days however you won't be able to go with 100% online courses. For in person classes, try registering all of those on the same days, such as Monday/Wed, Tues/Thurs, or all day Saturday to minimize your commutes to the school. You could also try registering for night courses if you need to work during the day.

My qualifications: I have a computer science degree and have worked as a software engineer for 15 years. I've interviewed for software development jobs and interviewed candidates for the same jobs. I do not respect nor am I impressed by degrees from the universities you listed. Believe it or not, most, if not all the schools you mentioned (I'm not sure about Westwood) have campuses where you can take classes in the building with live professors... the issue isn't between "online" vs. "brick and mortar" since a 100% online school is rare or an actual diploma mill, but rather between "For profit" and "not for profit". All the schools you mentioned to stay away from are all for-profit schools who have substandard recruiting practices with an incentive to keep as many students as possible which waters down their degree substantially. So, I would slightly modify your advice for the student to avoid "For-profit" education. MIT, Caltech, Harvard, and Yale are for profit organizations and obviously they are not degree mills. 

The schools I listed advertise degrees attainable 100% online and their online student enrollment greatly surpasses their in person enrollment. Their primary focus is clearly online education. Their brick and mortar locations are essentially just for keeping up appearances. According to [this article](http://www.thecrimson.com/article/2009/5/21/harvards-role-as-a-nonprofit-harvard/) in the Crimson, Harvard University is considered a nonprofit organization, which leads me to cast doubt upon your other examples, since being the oldest corporation in the US, Harvard College had the most chance of being a notable exception. If you have any proof that these other schools are for profit corporations, that would be interesting to me indeed, however, I wouldn't be surprised, because there is an exception to every general rule, and we are discussing generalities here, are we not?

That said, Harvard has [attainable degrees offered 100% online](http://www.extension.harvard.edu/distance-education). You're really reaching. Just can't let it go, can you?

What about the other universities? Are you going to say they're all online non-profits? How far do you need to take this to "win" your "debate"? I was agreeing with your main point that he shouldn't go to those schools, but wanted to clarify WHY he shouldn't since your reasoning was off a little- and to me these reasons are at least as important as the end result. In my response to your response, I just gave evidence that you were wrong about one of your counter-examples. I don't think it is spiteful or unusual for someone to point out misinformation like in your previous post. I was hoping that you'd respond with some source on your other examples, but instead you responded with empty rhetoric. I wasn't trying to "debate," just offer something extra for you to consider and then defend my point when you countered it with misinformation. As someone like yourself with so many degrees and qualifications in computer science, isn't learning a lifelong pursuit? For me to learn that Harvard did not have non-profit status, after my time with VES, would have been kind of a surprise to me, so I double checked it with a quick search on the Crimson, but the fact is, it is non-profit. So, point aside, I thought we were having a civil conversation, and if that ends, you're right, I'll let it go. I gave the person some advice and received nothing but nit picking responses from a self-appointed contrarian out to prove someone wrong, anyone wrong.

I seriously grow weary of Reddit.  Don't most schools like that churn out degrees to make a profit? Don't most schools like that churn out degrees to make a profit? Yup. My mom worked as an "advisor" for University of Phoenix for a few months.

She left because her job consisted solely of trying to get people who had just signed up to get federal loans to actually do the coursework, and then when they failed she had the pleasure of telling them that they now had a massive amount of debt and were no closer to getting a higher-paying job. It was really depressing work.

Go to a real school. CS isn't something you can bullshit. If anybody goes to any of these schools and ends up with a good programming gig, it won't be because of the school, and they probably could have gone just as far with self-study.  What's really important is that you identify the things you want from your education.  I won't go as far as to say that you should view it as an investment with a return, but many people do this because they want job training from a CS education.

Things to consider:

1. Do you want to look for work with this degree?
2. How much do you want to further my understanding of the study of computation and explore it for education's sake?
3. How much time can you devote to your studies?  Will you be able to make the costs worth your while?

Personally I think there exist a lot of good resources for exploring the study of computer science, but as far as online courses go, you should be careful.  Even respectable institutions have gotten into the game of offering distances learning packages and degrees, but in my experience it's really about padding the bottom line just about everywhere you look.  I've heard it put before that online courses at many universities are the same as 'charging students course fees and tuition in exchange for access to powerpoint slides'.  Don't believe everything you hear though, especially from people on the Internet like me.  There could exist the perfect school for you online, but I must stress that being skeptical and asking questions is really, really important.  If something doesn't smell right or you feel pressured to make a decision too quickly, back out.  All educational institutions are *selling* you an education and are doing so in a way that suits their own best interests.  You need to suit your own best interests, first and foremost.  Don't worry about hurting somebody's feelings by taking your time committing to a program.

That being said, I have a few broad suggestions to potentially help you out.

1.  If you are interested in just understanding and learning about CS, there are foundational bits of literature that are touched on in lists in this reddit, I suggest you look them up and check them out.

2. Getting reasonably priced access to these materials will probably require access to a college/university library.  If you're able, try to get access to one by enrolling part time in a class or if they offer it by buying a library membership (or just walking into the library and reading books in there without leaving....if it's a public institution you pay taxes, who cares).  I'm not quite sure how the latter (paying to get a library card) would be done, if it is possible or applicable.

3.  If you want to get a job with a degree, you may need to bite the bullet and go with an online course package.  Then again, maybe you don't.  Any more, hard work and diligence can propel you farther than many University programs can.  I'm in a CS program, and I see undergraduate students coasting through every year...you don't need a degree to "compete" with them on an actual skills-based level.

4.  Consider taking courses from a brick and mortar school in an ala cart fashion to make up for gaps in areas you find hard to do on your own.  If you're not interested in getting an actual degree with your coursework, you can sometimes audit the classes at a much lower rate (or maybe even at no cost to you, in my experience it costs *something* though).

5.  If you opt to go on your own path without the college model, plan to work in the CS/Programming/IT realm, you'll need to make up for your lack of college degree by marketing yourself.  This is generally accomplished with a number of different things you can easily do on your own.  Blog about your exploits.  Post your code to github.  Make friends and go to professional meetups (also check out your local startup scene, they're around).  All three of these things together make you look awesome, make you look like you take the initiative, make you friends, and make you look clutch.

       Any opinions about WGU?

http://www.wgu.edu/ It's a diploma mill. What's that opinion based on? It's a diploma mill. You might not think WGU is all that, but you shouldn't use the term "diploma mill" for a regionally-accredited nonprofit. There are actual diploma mills, where a few hundred dollars and some bullshit "life experience" form will get you a worthless degree--the kind of "degree" that people get fired for claiming on their job applications.       [deleted] For those of us who have mortgages, full time jobs, and don't live near a 4 year university, your suggestion is worthless. 

EDIT: coward poster who deleted himself was saying that all online degrees are a complete waste of time and that anyone wanting a degree should go attend a four year university in person. [deleted] </snippet></document><document><title>SynthBot: An Unsupervised Software Synthesizer Programmer</title><url>http://quod.lib.umich.edu/cgi/p/pod/dod-idx?c=icmc;idno=bbp2372.2008.139</url><snippet>   What is it, it just wants to download something. It's a pdf, but I'm whoever uploaded it didn't feel the need to include a file extension... [There is no need for file extensions on the internet.](http://en.wikipedia.org/wiki/Internet_media_type)  Nice! Eventually we will be able to give a computer a recorded song as input, and have it recreate the song in a DAW. So cool. Is there a source code download of Synthbot that I can look at?   The evaluation section of the paper is obviously incomplete, so we have to take with a pinch of salt the claim that it can match the input sound. Having said that, Yee-King has been working in the area for quite some time so I would expect it to be good.

Full disclosure/shameless plug: my PhD work was in this area, quite a few years ago now. Thesis and some code available here: http://www.skynet.ie/~jmmcd/thesis.html Very good. 

I am skeptical of the end goals from a musical viewpoint though. Not being able to recreate sounds easily could be seen as a reason for people to use different sounds. If automatic sound-matching becomes a viable technology (it's not, yet) it will allow a lot of new and exciting possibilities, in addition to some boring ones. If you could record any sound and then automatically derive parameters to play with, wouldn't you come up with some cool stuff? It'd be like sampling, except instead of just playing the sample faster or slower or in loops, you can do anything.

My attitude is that artists should be completely free, and the more tools the merrier. Then we can criticise the ones who abuse presets or just use the same sounds over and over, if we want.  True, it will also give people who try to get new sounds an exciting new field to play on.

What is the central problem in sound matching? Why does it not work yet?

I worked on HMM-based speech synthesis one and a half year ago. HMM-based speech synthesis uses a similar idea: Extract MFCCs, model their relation to phonemes with hidden Markov models, use an excitation signal on the MFCCs obtained from HMMs that correspond to given text. It was quite successful and robust. &amp;gt; What is the central problem in sound matching? Why does it not work yet?

Good question. I suppose there are two main problems. First, the space of all possible sounds is very large, and no practical synthesizer can synthesize all of it. So when trying to match an arbitrary sound, there is no guarantee that our synth is *capable* of matching it, even if we had a perfect search method. (This is in contrast to speech synthesis work, I think, where the synthesizer is usually capable of a good approximation to any normal speech sound. Right?) So, our first question is really which synthesizer should we use. Most research I've seen, including my own and the OP, just tries to do its best using a single synth, without really considering this question. Some alternative work (not mine) tries to evolve the synth itself, rather than the parameters of an existing synth. I don't think it worked well but it's an attractive approach.

Second, once we have chosen a synth, that synth and its parameters (together with the target sound) define a fitness landscape (in evolutionary terms) or error landscape or whatever your preferred terminology. The landscape is often quite ugly -- lots of local optima, non-linear dependencies, etc. Most people regard the synth as a black box, so calculating a gradient isn't possible. So it's a hard problem for this reason also. But there has been some good work on transforming the landscape in some ways too.

I also have some ideas involving these rather trendy deep learning architectures but I'm not really in the sound synthesis field for now so no time to pursue it :-/ The evaluation section of the paper is obviously incomplete, so we have to take with a pinch of salt the claim that it can match the input sound. Having said that, Yee-King has been working in the area for quite some time so I would expect it to be good.

Full disclosure/shameless plug: my PhD work was in this area, quite a few years ago now. Thesis and some code available here: http://www.skynet.ie/~jmmcd/thesis.html What will you nominally be a _doctor_ of? Music? Computer science? Some adjectified version of either one of these?</snippet></document><document><title>What is a computer?</title><url>http://www.reddit.com/r/compsci/comments/xkxtw/what_is_a_computer/</url><snippet>Simple question really. What is the accepted scientific definition of "a computer" in *computer* science? Sources appreciated.

(No, this isn't homework, I graduated years ago :) )  The [Turing Machine](http://en.wikipedia.org/wiki/Turing_machine) is the de-facto model and operating theory behind theoretical computer science. Thanks. Is this generally accepted and unspoken or is it stated in texts which I could cite if I wanted to convince others? Yes, generally accepted, oh, and Universal Turing Machine to be more specific. Most people wouldn't consider a watch to be a computer, though it is technically a Turing machine. The difference is that a UTM can simulate any other TM.

Source: Page 15 of this book gives a definition of a computer as consisting of five parts: input, output, memory, datapath, and control. If you think about it, those are parts of a UTM. The control is the states of the UTM, the input and output are on the tape, as is the memory and the datapath. This may seem like cheating, but the same disk in your computer stores programs and data (both input and output).
http://www.amazon.com/Computer-Organization-Design-Third-Edition/dp/1558606041/ref=cm_lmf_tit_7

http://en.wikipedia.org/wiki/Universal_Turing_machine

There, now a theoretical computer science doctorate told you. So computer science can be thought of as the science of Universal Turing Machines without losing or gaining any meaning? I've always thought the name of Computer Science is a bit wrong. I much prefer the term computING science. Calling it the "science of UTM's" is actually saying the same thing - because all a Turing machine does is calculate an output to a given input.

What we learn in universities is how to come up with algorithms, their speeds, positives and negatives, how to come up with heuristics, how to create various automota, etc. But it all comes down to the science of computing something. Dijkstra agrees. Dudes love that quote, but I don't think the boundary between CS and actual machines is as clear as Dijkstra claims. I'll give an example.

Power usage is a huge problem today. Mobile devices are hugely prevalent and are getting more powerful, but battery technology has not improved much. Wouldn't it be great if we had a way of measuring the "power complexity" of an algorithm? It wouldn't seem to match nicely with time or space complexity since it is more related to the density of expensive operations. It is pretty hard to argue that developing a method of finding the power complexity of an algorithm isn't Computer Science.

However, a UTM abstracts any notion of "expensive" operations away away, so we would need another model of computation that would allow us to do this. This model would have to be fundamentally connected to our real world implementations since power consumption is a physical property rather than an abstract mathematical one. So we have a problem that is clearly Computer Science that is inexorably tied to physical implementations of computing. 

Dijkstra's quote is fun to say because it lets us feel superior to engineers, but I think that it is missing a lot of important subtlety.  &amp;gt;Dijkstra's quote is fun to say because it lets us feel superior to engineers

I love to feel superior to engineers, could you link the quote? I've always thought the name of Computer Science is a bit wrong. I much prefer the term computING science. Calling it the "science of UTM's" is actually saying the same thing - because all a Turing machine does is calculate an output to a given input.

What we learn in universities is how to come up with algorithms, their speeds, positives and negatives, how to come up with heuristics, how to create various automota, etc. But it all comes down to the science of computing something. Yes, I've often wondered if they called it computer science instead of computation science in order to attract more undergrad students. My dean (think thats the name. Boss over my CS program) explained it like this: 

Computer Scientists use computers in the same way as astronomers use a telescope. You wouldnt say that the astronomer stody/work on telescopes. They figure out how the cosmos works. 

The telescope and the computer are just tools.

A Computer scientist work/study computing. We use a computer as it makes most of the number crunching and tedious task a breeze.

Dijkstras apparantelly said that CS students shouldnt be allowed to touch a computer the first few years :) Don't know who your Dean is, but that's a very popular quote ([misattributed to Dijkstra](http://en.wikiquote.org/wiki/Edsger_W._Dijkstra#Misattributed) in most cases): "Computer science is no more about computers than astronomy is about telescopes". Apparently its origins are from SICP. Ohh cool! Haven't gotten through SICP yet :( Will do next semester though.  I've always thought the name of Computer Science is a bit wrong. I much prefer the term computING science. Calling it the "science of UTM's" is actually saying the same thing - because all a Turing machine does is calculate an output to a given input.

What we learn in universities is how to come up with algorithms, their speeds, positives and negatives, how to come up with heuristics, how to create various automota, etc. But it all comes down to the science of computing something. In the first video of MIT's OpenCourseWare Lisp lessons, the instructor gave a definition of computer science that I like just a little more than computation science.

He made the analogy that when geometry was invented, they named it after the tools they used --tools for measuring the earth; geo means earth and metry means measurement-- while what they were *actually* studying was shapes and their relationships.  Likewise, computers are the tools we use in computer science, but it's not *really* the computers we study.  It's the study of *process*, and can be done with or without computers.  Computers just happen to be what we use for processing. Thanks for the response, that IS very interesting; It made me think about why I still don't like the term.

To me, everyone understands geometry. But people don't understand a degree in CompSci- instead they'll ask:

"fix my computer?"

"so like you learn JavaScript? Can you build my website?"

My favorite "oh, so you program? I always wanted to learn that, you think you could teach me?"

To which I responded "sure! I mean I went to school for three years to understand, but do you have 15 minutes?"

It's the perception of what computer science means that I dislike. Even if it's accurate and historical, it still misrepresents and i think trivializes my effort and time going to university. My brother did this:

"I want to build a program to do X for linux.  Can you help me?"

"Uh, sure."  So I spend 3 hours teaching him about variables, types, and assigning things.

"So when do we get to do actual stuff?"

"Got to walk before you can run, bro."  Nothing was ever mentioned about it again. I read a story about some guy, very enthusiastically, coming up to a programmer and saying "I have a fantastic idea for a football game, can you help me learn to program so I can do it?"

So they start off the same way you do, halfway through the lesson he stops the instructor and says "You know...nevermind...I always thought it was kind of like 'draw football stadium. throw football....not all this stuff'" and just left. I read a story about some guy, very enthusiastically, coming up to a programmer and saying "I have a fantastic idea for a football game, can you help me learn to program so I can do it?"

So they start off the same way you do, halfway through the lesson he stops the instructor and says "You know...nevermind...I always thought it was kind of like 'draw football stadium. throw football....not all this stuff'" and just left. I've always thought the name of Computer Science is a bit wrong. I much prefer the term computING science. Calling it the "science of UTM's" is actually saying the same thing - because all a Turing machine does is calculate an output to a given input.

What we learn in universities is how to come up with algorithms, their speeds, positives and negatives, how to come up with heuristics, how to create various automota, etc. But it all comes down to the science of computing something. I've always thought the name of Computer Science is a bit wrong. I much prefer the term computING science. Calling it the "science of UTM's" is actually saying the same thing - because all a Turing machine does is calculate an output to a given input.

What we learn in universities is how to come up with algorithms, their speeds, positives and negatives, how to come up with heuristics, how to create various automota, etc. But it all comes down to the science of computing something. [deleted] That's not really an accurate definition of a Turing machine.

According to wikipedia:

"A universal Turing machine (UTM) is a Turing machine that can simulate an arbitrary Turing machine on arbitrary input." and "A Turing machine is a device that manipulates symbols on a strip of tape according to a table of rules."

So it's not even the input or output that defines a Turing machine, it's how it goes about accomplishing the task. I disagree.  The only way "how" matters is that Turing's machine is remarkably simple and intuitive.  Otherwise the OP would be talking about how humans implement the lambda calculus.  It's obvious that humans are functionally equivalent to Turing machines but perhaps not so much to other models of computation.

What defines a Turing machine is what it's able to output based upon its input. [deleted] I've always thought the name of Computer Science is a bit wrong. I much prefer the term computING science. Calling it the "science of UTM's" is actually saying the same thing - because all a Turing machine does is calculate an output to a given input.

What we learn in universities is how to come up with algorithms, their speeds, positives and negatives, how to come up with heuristics, how to create various automota, etc. But it all comes down to the science of computing something. So computer science can be thought of as the science of Universal Turing Machines without losing or gaining any meaning? Complexity theory, a branch of CompSci, spends a lot of time discussing other abstract machines.  Some of the machines discussed are more powerful than a (deterministic) UTM, and some of them are less powerful.  Complexity theory is concerned with how to determine what the limits of a particular abstract machine are.

Basically UTM is equivalent to the kind of computer that is built in the real world.  The more powerful abstract machines are hypothetical, while the less powerful abstract machines are similar to parts of computers such as individual circuits. Hmm the plot thickens. With that in mind, would you have a broader definition of a computer, or would you say that the machines outside of the UTM model shouldn't be classed as computers? Well, I think if you want to be precise relative to complexity theory, you should just adopt the terms of complexity theory: 

* P or BPP* ~= desktops, supercomputers, smartphones
* NP, EXP, NEXP, R, RE, ALL ~= computers that are better than could exist in the real world
* BQP ~= Quantum Computers
* L, NL, NC, TC^0 , AC^0 ~= circuits or calculators (but not programmable calculators, that would be P or BPP)

If you don't care about being precise, I would suggest this simplified hierarchy:

* Programmable computers
* Calculators
* Rocks These are not classes of computation. They are classes of problems. EXP is the set of all languages that can be decided in exponential time by a TM, for example. Some of these sets are based on different computational models, like NP, but any computer can solve any problem from these classes. Non deterministic machines may be faster, but they cannot solve more problems that traditional machines. Thanks for the clarification.  I am careless when I use terms to refer to problems and machines interchangeably.  When I said "P or BPP ~= desktops, supercomputers, smartphones" what I meant was that P or BPP are sets of problems that are, generally speaking, tractable on computers that can be built in this universe.  Of course, there are P problems which are too big to solve, and NP problems which can be solved quickly; it all depends on the size of the problem. Well, I think if you want to be precise relative to complexity theory, you should just adopt the terms of complexity theory: 

* P or BPP* ~= desktops, supercomputers, smartphones
* NP, EXP, NEXP, R, RE, ALL ~= computers that are better than could exist in the real world
* BQP ~= Quantum Computers
* L, NL, NC, TC^0 , AC^0 ~= circuits or calculators (but not programmable calculators, that would be P or BPP)

If you don't care about being precise, I would suggest this simplified hierarchy:

* Programmable computers
* Calculators
* Rocks If you want to be incredibly pointlessly pedantic, the only computers  that we can *actually* build in the real world, are exceptionally complex FSA. Anything else is physically out of the reach of classical computing. Quantum computing could probably break out of that, but I am not familiar enough with it. As long as your problem takes less memory to solve than, let's say, 20GB of ram, then FSAs and UTMs are equivalent.  Once the problem size exceeds the memory capacity of the device then there is a significant difference. If you want to be incredibly pointlessly pedantic, the only computers  that we can *actually* build in the real world, are exceptionally complex FSA. Anything else is physically out of the reach of classical computing. Quantum computing could probably break out of that, but I am not familiar enough with it. &amp;gt; Quantum computing could probably break out of that, but I am not familiar enough with it.

If you accept the hypothesis, that there is a limited number of particles in the universe, you would have to find a way to store an infinite amount of information with a single particle, for *any* model to be able to build a true UTM. Hmm the plot thickens. With that in mind, would you have a broader definition of a computer, or would you say that the machines outside of the UTM model shouldn't be classed as computers? Complexity theory, a branch of CompSci, spends a lot of time discussing other abstract machines.  Some of the machines discussed are more powerful than a (deterministic) UTM, and some of them are less powerful.  Complexity theory is concerned with how to determine what the limits of a particular abstract machine are.

Basically UTM is equivalent to the kind of computer that is built in the real world.  The more powerful abstract machines are hypothetical, while the less powerful abstract machines are similar to parts of computers such as individual circuits. Wait.  The UTM is equivalent to the kind of computer built in the real world?  

The computers built in the real world are finite physical devices. Computers today are equivalent to Turing Machines that have a limited amount of tape, but as long as you only want to solve problems that are small enough there is no difference. So computer science can be thought of as the science of Universal Turing Machines without losing or gaining any meaning? That statement loses perspective of the field as a whole. First of all, computer scientists do not always work in one model of computation. There is a lot to be gained from *restricting* your notion of computation. For example, regular expressions are really useful because they are easier to reason about. There are programming languages that are *not* Turing-complete: [ACL2](http://en.wikipedia.org/wiki/ACL2), [Agda](http://en.wikipedia.org/wiki/Agda_\(programming_language\)), [Coq](http://en.wikipedia.org/wiki/Coq) (though these are all theorem proving languages).

Secondly, there are many sub-fields of computer science which are either oblivious to the underlying model of computation (e.g., HCI, Health Informatics, and others) or fields that choose the actual machine as their model (e.g., Systems, OS). Yet others like Programming Languages have adopted other models (e.g., the [Lambda calculus](http://en.wikipedia.org/wiki/Lambda_calculus), object calculi, combinatory calculi, etc.) as their principal model. &amp;gt;  Yet others like Programming Languages have adopted other models (e.g., the [4] Lambda calculus, object calculi, combinatory calculi, etc.) as their principal model.

Those models are all equivalent in power (or in some cases less powerful) than the UTM model we all know and love. Indeed, they are just different languages for stating the same thing, some being more useful for certain problem sets than others.

 Equivalent yes, but it's easier to reason about the lambda calculus when you're working with a functional programming language. Similarly with OO languages and object calculi, and so on. (we're just agreeing here, but just stating this for the benefit of others) So computer science can be thought of as the science of Universal Turing Machines without losing or gaining any meaning? So computer science can be thought of as the science of Universal Turing Machines without losing or gaining any meaning? That might give people the wrong idea, though. It's not as though most computer scientists sit around all day writing out instructions about jostling a head around or proving properties of UTMs. At least not at that level of abstraction. Well indeed, I wasn't suggesting we rename computer science :) So computer science can be thought of as the science of Universal Turing Machines without losing or gaining any meaning? So computer science can be thought of as the science of Universal Turing Machines without losing or gaining any meaning? So computer science can be thought of as the science of Universal Turing Machines without losing or gaining any meaning? Yes, generally accepted, oh, and Universal Turing Machine to be more specific. Most people wouldn't consider a watch to be a computer, though it is technically a Turing machine. The difference is that a UTM can simulate any other TM.

Source: Page 15 of this book gives a definition of a computer as consisting of five parts: input, output, memory, datapath, and control. If you think about it, those are parts of a UTM. The control is the states of the UTM, the input and output are on the tape, as is the memory and the datapath. This may seem like cheating, but the same disk in your computer stores programs and data (both input and output).
http://www.amazon.com/Computer-Organization-Design-Third-Edition/dp/1558606041/ref=cm_lmf_tit_7

http://en.wikipedia.org/wiki/Universal_Turing_machine

There, now a theoretical computer science doctorate told you. (Second reply after your edit adding the source.) Thanks for that. I guess by this definition, the claim that the [Antikythera Mechanism](http://en.wikipedia.org/wiki/Antikythera_mechanism) is a computer is accurate after all. I might have just lost a bet. Depends on your bet.

If you bet this is not a "computer" then you lose.  It computes.

If you bet something more sophisticated about it not being a general-purpose computer, then you win.  

I'll bet you lose.  :-)

*Edit: added the two "not"s because otherwise it didn't say what it meant.* Depends on your bet.

If you bet this is not a "computer" then you lose.  It computes.

If you bet something more sophisticated about it not being a general-purpose computer, then you win.  

I'll bet you lose.  :-)

*Edit: added the two "not"s because otherwise it didn't say what it meant.* "Computer" usually means a general-purpose computer. If a device that shows you astronomical positions when you turn a crank is a "computer", then one might argue that a rock that shows you a parabola when you throw it is also a "computer". Or less extremely, take a four-function calculator. Nobody would ever call one a "computer", even though it computes. Nor is a kaleidoscope, which computes fractal-like patterns, a computer.

FWIW, [Wikipedia](http://en.wikipedia.org/wiki/Computer) also agrees:

&amp;gt; A computer is a general purpose device that can be programmed to carry out a finite set of arithmetic or logical operations. Since a sequence of operations can be readily changed, the computer can solve more than one kind of problem. &amp;gt; "Computer" usually means a general-purpose computer. 

I don't agree.  For example, that is not the [historical meaning](http://www.globalnerdy.com/2008/09/11/multi-processor-computing-in-1924/) and that device is obviously historical.  In any event, [mirriam-webster.com](http://www.merriam-webster.com/dictionary/computer) says this.

&amp;gt; : one that computes; specifically : a programmable usually electronic device that can store, retrieve, and process data

My understanding of the device is that it is an analog computer; that you store data into it by entering the data, process the data by turning a crank, and then retrieve the results by looking at the dial. My understanding is that the mechanism essentially hard-codes the "program" and all you have control over is the input, from which it generates the output. A general-purpose computer, even according to your MW definition, is programmable, which that thing was not, so I would say he most likely won the bet, depending on the parameters of it. (Second reply after your edit adding the source.) Thanks for that. I guess by this definition, the claim that the [Antikythera Mechanism](http://en.wikipedia.org/wiki/Antikythera_mechanism) is a computer is accurate after all. I might have just lost a bet. Yes, generally accepted, oh, and Universal Turing Machine to be more specific. Most people wouldn't consider a watch to be a computer, though it is technically a Turing machine. The difference is that a UTM can simulate any other TM.

Source: Page 15 of this book gives a definition of a computer as consisting of five parts: input, output, memory, datapath, and control. If you think about it, those are parts of a UTM. The control is the states of the UTM, the input and output are on the tape, as is the memory and the datapath. This may seem like cheating, but the same disk in your computer stores programs and data (both input and output).
http://www.amazon.com/Computer-Organization-Design-Third-Edition/dp/1558606041/ref=cm_lmf_tit_7

http://en.wikipedia.org/wiki/Universal_Turing_machine

There, now a theoretical computer science doctorate told you. [deleted] The [Turing Machine](http://en.wikipedia.org/wiki/Turing_machine) is the de-facto model and operating theory behind theoretical computer science. Except where weaker automata, such as push-down automata, are studied, or other formalizations, such as the lambda calculus and mu-recursive functions are used.

- [Lambda Calculus](http://en.wikipedia.org/wiki/Lambda_calculus)

- [Mu-Recursive Functions](http://en.wikipedia.org/wiki/%CE%9C-recursive_function)

- [Automata](http://en.wikipedia.org/wiki/Automata_theory) Lambda calculus (as formulated by Church) has exactly the same expressive power as a Turing Machine, a fact that Turing himself proved. Indeed! It is one of the "other formalizations" I mentioned. However, push-down automata and finite state machines are less powerful, and are subject to study in computer science. The [Turing Machine](http://en.wikipedia.org/wiki/Turing_machine) is the de-facto model and operating theory behind theoretical computer science. The Turing machine is only *a* (as in one of many) model and operating theory.

It's still too ~~specific~~ vague to be used as *the* definition of "computer". This answer just says: I'm not going to define what a computer is, but here's the de-facto model of computation. What I'm trying to say is: "a model of computation" is not the same as "a computer".

See for instance [Biocomputers](http://en.wikipedia.org/wiki/Biocomputers).

EDIT: Clarification. True but others are generally considered to be Turing-equivalent per the Church-Turing thesis (note: not a scientific theory nor a mathematical theorem, just a generally accepted idea). A noted exception is the quantum computer, which is considered to be equivalent to an exponential version of a Turing machine, but with errors.
Edit: Biocomputers are most definitely Turing-equivalent; there are just a lot of them and they can perform relatively efficiently. Whether quantum computers can really work remains to be seen. &amp;gt; Biocomputers are most definitely Turing-equivalent; there are just a lot of them and they can perform relatively efficiently. Whether quantum computers can really work remains to be seen.

Yes, they are Turing-equivalent.

But OP asked for a general definition of "computer". So, is the answer: "everything that can emulate the workings of a Turing machine"? Since the Turing machine is only a model, we still haven't defined what a computer actually *does*, or how it *does* whatever it does. Maybe we don't want to define that. That's actually a good idea, I think. But just saying "The Turing Machine is the de-facto model and operating theory behind theoretical computer science."(AThousandTimesThis) does not really answer OPs question.

EDIT: The definition of a computer from that book you gave in [this comment](http://www.reddit.com/r/compsci/comments/xkxtw/what_is_a_computer/c5n9g97) seems to be of a rather technical, specific nature. It *is* a definition, but I'm not sure whether it's general enough.  Anyway, it seems OP was merely asking to figure out whether he lost a bet or not, and I guess he figured it out. &amp;gt; Biocomputers are most definitely Turing-equivalent; there are just a lot of them and they can perform relatively efficiently. Whether quantum computers can really work remains to be seen.

Yes, they are Turing-equivalent.

But OP asked for a general definition of "computer". So, is the answer: "everything that can emulate the workings of a Turing machine"? Since the Turing machine is only a model, we still haven't defined what a computer actually *does*, or how it *does* whatever it does. Maybe we don't want to define that. That's actually a good idea, I think. But just saying "The Turing Machine is the de-facto model and operating theory behind theoretical computer science."(AThousandTimesThis) does not really answer OPs question.

EDIT: The definition of a computer from that book you gave in [this comment](http://www.reddit.com/r/compsci/comments/xkxtw/what_is_a_computer/c5n9g97) seems to be of a rather technical, specific nature. It *is* a definition, but I'm not sure whether it's general enough.  Anyway, it seems OP was merely asking to figure out whether he lost a bet or not, and I guess he figured it out. True but others are generally considered to be Turing-equivalent per the Church-Turing thesis (note: not a scientific theory nor a mathematical theorem, just a generally accepted idea). A noted exception is the quantum computer, which is considered to be equivalent to an exponential version of a Turing machine, but with errors.
Edit: Biocomputers are most definitely Turing-equivalent; there are just a lot of them and they can perform relatively efficiently. Whether quantum computers can really work remains to be seen. Church-Turing isn't about whether already existing models are Turing equivalent, it's about whether there CAN BE any models that are greater than Turing. (It states that there are none.)

However, mu-recursive functions and lambda calculus, to name just two, are PROVABLY equivalent to Turing machines when considering partial N-&amp;gt;N functions. True but others are generally considered to be Turing-equivalent per the Church-Turing thesis (note: not a scientific theory nor a mathematical theorem, just a generally accepted idea). A noted exception is the quantum computer, which is considered to be equivalent to an exponential version of a Turing machine, but with errors.
Edit: Biocomputers are most definitely Turing-equivalent; there are just a lot of them and they can perform relatively efficiently. Whether quantum computers can really work remains to be seen. Think about what that equivalence means.

If there's a set of computational devices/models that are all "Turing equivalent" that means that they're *all* equivalent.  Any one can be used to perform the same computation as any of the other ones.

TMs are historically significant, and are useful because of their simplicity, but they are no "more" computation than any other equivalent model.  It's just a convention that this equivalence is labelled as "Turing equivalence". The Turing machine is only *a* (as in one of many) model and operating theory.

It's still too ~~specific~~ vague to be used as *the* definition of "computer". This answer just says: I'm not going to define what a computer is, but here's the de-facto model of computation. What I'm trying to say is: "a model of computation" is not the same as "a computer".

See for instance [Biocomputers](http://en.wikipedia.org/wiki/Biocomputers).

EDIT: Clarification. It's the only model that can be practically applied to modern computers.

Edit: Biocomputers can be described/simulated by Turing machines, as can anything that can be considered to be a computer. A computer is an implementation of a model of computation, and in theoretical computer science we use the Turing machine because it's the most universal. It allows us to discover the capabilities and limitations of computing. It's the only model that can be practically applied to modern computers.

Edit: Biocomputers can be described/simulated by Turing machines, as can anything that can be considered to be a computer. A computer is an implementation of a model of computation, and in theoretical computer science we use the Turing machine because it's the most universal. It allows us to discover the capabilities and limitations of computing. OP was not looking for a *practical* model, or a *practical* definition of a computer, or a definition of a *practical* computer. He/She asked for a general definition of what a computer actually is. What things are computers? What are the characteristics of a computer?
 It's the only model that can be practically applied to modern computers.

Edit: Biocomputers can be described/simulated by Turing machines, as can anything that can be considered to be a computer. A computer is an implementation of a model of computation, and in theoretical computer science we use the Turing machine because it's the most universal. It allows us to discover the capabilities and limitations of computing. &amp;gt; It's the only model that can be practically applied to modern computers.

Computer scientists use lambda calculus every day. Especially in programming languages and fields where programming languages have had high impact, such as some portions of the security field. It also has the advantage of being closely related to mathematical logic. Oh right. I overlooked lambda calculus because for some reason I never think of it in the same class as Turing machine. Thanks for mentioning that. The Turing machine is only *a* (as in one of many) model and operating theory.

It's still too ~~specific~~ vague to be used as *the* definition of "computer". This answer just says: I'm not going to define what a computer is, but here's the de-facto model of computation. What I'm trying to say is: "a model of computation" is not the same as "a computer".

See for instance [Biocomputers](http://en.wikipedia.org/wiki/Biocomputers).

EDIT: Clarification. The Turing machine is only *a* (as in one of many) model and operating theory.

It's still too ~~specific~~ vague to be used as *the* definition of "computer". This answer just says: I'm not going to define what a computer is, but here's the de-facto model of computation. What I'm trying to say is: "a model of computation" is not the same as "a computer".

See for instance [Biocomputers](http://en.wikipedia.org/wiki/Biocomputers).

EDIT: Clarification. The [Turing Machine](http://en.wikipedia.org/wiki/Turing_machine) is the de-facto model and operating theory behind theoretical computer science.  Good question.

Wikipedia says: "A computer is a general purpose device that can be programmed to carry out a finite set of arithmetic or logical operations. Since a sequence of operations can be readily changed, the computer can solve more than one kind of problem."

Seems general enough at first glance but I hardly doubt that captures the concept of a (general) computer entirely.
 Wouldn't a human fall under this definition as well? Wouldn't a human fall under this definition as well? Yes, it would. Is a human not a computer? If so, what exactly makes a human "not a computer"?

Others have defined "computer" as things that can do the things Turing machines can do (very popular in this submission). Well, humans can do that. They write down the definition and meticulously "calculate" state transitions, and write down "executions".

That's part of the question. Shall we exclude humans from the general definition of a computer? More precisely, other people have defined computers as things that are *equivalent* to turing machines.
Humans can act as computers, but it's not at all clear that computers and humans are equivalent.

Look up strong AI for more info. I am familiar with that. I did not intend to say that they are equivalent.

By the definition on Wikipedia, humans fall (amongst others) into the category of computers. But of course that does not automatically imply the converse.

I guess my use of the verb "to be" made this statement too vague.
 What makes a human not a computer is that humans can do many things that computers can't(at the current state of the art, at any rate).

Definitions are, in general, both necessary and complete.  That definition is a complete, concise, definition of a computer, but not of a human.  Hence, either our understanding of computers is wrong, our understanding of humans is wrong, or they're different things.  Currently, we believe they're different things.  It would be more accurate to say that humans can do computing. Wouldn't a human fall under this definition as well? Good question.

Wikipedia says: "A computer is a general purpose device that can be programmed to carry out a finite set of arithmetic or logical operations. Since a sequence of operations can be readily changed, the computer can solve more than one kind of problem."

Seems general enough at first glance but I hardly doubt that captures the concept of a (general) computer entirely.
 Yes, I don't think a computer has to be a general purpose device to be a computer, does it? The hugely broad definition of "a thing which carries out a calculation" is the best fit in my mind, but I don't know if the computer science community as a whole has a similar or alternative definition, if any. Yes, I don't think a computer has to be a general purpose device to be a computer, does it? The hugely broad definition of "a thing which carries out a calculation" is the best fit in my mind, but I don't know if the computer science community as a whole has a similar or alternative definition, if any. Yes, I don't think a computer has to be a general purpose device to be a computer, does it? The hugely broad definition of "a thing which carries out a calculation" is the best fit in my mind, but I don't know if the computer science community as a whole has a similar or alternative definition, if any. Good question.

Wikipedia says: "A computer is a general purpose device that can be programmed to carry out a finite set of arithmetic or logical operations. Since a sequence of operations can be readily changed, the computer can solve more than one kind of problem."

Seems general enough at first glance but I hardly doubt that captures the concept of a (general) computer entirely.
 One of my favorite examples: [the tinkertoy computer](http://museum.mit.edu/nom150/entries/1215). It might seem kind of like cheating, since it has to be rebuilt to perform different calculations, but that is how all machines thought of as computers began. You literally had to rewire computers to reprogram them.

The idea of computer can get a bit odd to people outside of computer science.

Some people argue whether the universe is a computer, or if it is merely a computation. These aren't crackpots either, but some of the best thinkers and theorists, from a variety of fields. 

From the perspective of computer science, there are many things that perform computations that are not traditionally thought of as "machines", "calculators", or "computers".

Chemical reactions can be thought of as computations. Given the same inputs, the reactions produce the same answer. Change the inputs, different answer.

A sundial is an analog computer. It "calculates" the time given the input of sunlight. Slightly more useful is my watch, which calculates the correct time without regard to sunlight.

My brain is a computer, which sadly doesn't always give the same answer given the same set of inputs. Apparently, it is a little buggy. Good question.

Wikipedia says: "A computer is a general purpose device that can be programmed to carry out a finite set of arithmetic or logical operations. Since a sequence of operations can be readily changed, the computer can solve more than one kind of problem."

Seems general enough at first glance but I hardly doubt that captures the concept of a (general) computer entirely.
 I think some notion of efficiency would be a useful addition to that description. I would disagree. Is a "___" (thing) that takes a year to (correctly) calculate "sqrt(x + 612 / 6) * 10" for a given input x not a computer? If it's not a computer, what is it?

EDIT: Well, taking the general definition I believe humans are computers, which is not that strange a thing IMO. I'm not saying we can set hard limits on anything, but that computers are designed for efficiency. If you construct a mechanical turing machine, then yes, what it does is computation, but only in a uselessly reductionist sense. Useful for theory, but "computer" is not really a word that we use in theory very often. If we mean turing machine, we say turing machine. Computers are things that are designed to be used. This is connotational. First of all, are "computers" (a term which we still haven't defined for the sake of discussion) designed to be used? Why concentrate so much on practicability? OP wants to find out whether the term "computer" has ever been precisely defined.

Even assuming we are looking at practicability. Let's say we have a computer that is terribly inefficient (but correct), taking twice as long as it would take you to calculate by hand. Still, making it work for you instead of yourself saves you work / allows you to do other things in parallel. So it would be practical after all.
 Let's look at space complexity, then, and perhaps my point will become more lucid.

I'm going to assume that you would agree that the phone/laptop/etc with which you are browsing Reddit is a computer. From the strict mathematical view of which you are so fond, then, we can conclude that computers are not turing machines and cannot solve general problems - because your device has a finite memory instead of an infinite tape, and is therefore nothing but a glorified regular expression parser.

To say that your device cannot solve general problems in the same way that an LED on a dimmer switch cannot solve general problems is, of course, a reduction to absurdity. It reasonable to call a laptop a computer because it is an approximation of a turing model. The state space is so phenomenally large that we can often conveniently forget its finite boundary.

And here's where we come back around to efficiency. Suppose the memory hardware encodes state as a unary, rather than binary, string. It still works - it's just far less efficient. Enough so that you can give up any hope of utilizing it for nontrivial computational tasks, and you can now feel the constraints of the space limitation such that you're now going to start thinking about this device as a finite state machine. You need "very large" memory to pretend that a device simulates a powerful computational model, and you need some consideration to efficiency to physically realize a large state space. &amp;gt; From the strict mathematical view of which you are so fond

Am I? First of all, are "computers" (a term which we still haven't defined for the sake of discussion) designed to be used? Why concentrate so much on practicability? OP wants to find out whether the term "computer" has ever been precisely defined.

Even assuming we are looking at practicability. Let's say we have a computer that is terribly inefficient (but correct), taking twice as long as it would take you to calculate by hand. Still, making it work for you instead of yourself saves you work / allows you to do other things in parallel. So it would be practical after all.
 I think some notion of efficiency would be a useful addition to that description.  How is computter formed? How is computter form? How data get praccessed? How is computter formed? How is computter form? How data get praccessed? What? I think it was a play on [How is babby formed](http://www.somethingawful.com/flash/shmorky/babby.swf) Thank you, that was fucking hilarious   Your question is **very** broad.. You need to be more specific. 
As far as I know, there is no "accepted scientific definition" of a computer. The word "computer" stems from the "to compute", meaning "to determine something by calculation". 

By that definition, I believe it's the Abacus that's the earliest "computing machine" which was used first by the Mesopotamians to perform arithmetic. So really in a sense, computers aren't as rigidly defined as say.. cars, or refrigerators, since anything that has "the ability to compute" is considered a computer. 

This might have raised more questions for you than it has answered :P can you be more specific? Well this is how the question arose. The term *is* very broad, but since computer science is called computer science I figured there must be an accepted definition within the field. Well this is how the question arose. The term *is* very broad, but since computer science is called computer science I figured there must be an accepted definition within the field.    On Turing machines, from Scott Aaronson's [wonderful lecture series](http://www.scottaaronson.com/democritus/lec3.html):

&amp;gt; In 1936, the word "computer" meant a person (usually a woman) whose job was to compute with pencil and paper. Turing wanted to show that, in principle, such a "computer" could be simulated by a machine. What would the machine look like? Well, it would have to able to write down its calculations somewhere. Since we don't really care about handwriting, font size, etc., it's easiest to imagine that the calculations are written on a sheet of paper divided into squares, with one symbol per square, and a finite number of possible symbols. Traditionally paper has two dimensions, but without loss of generality we can imagine a long, one-dimensional paper tape. How long? For the time being, we'll assume as long as we need.

&amp;gt; What can the machine do? Well, clearly it has to be able to read symbols off the tape and modify them based on what it reads. We'll assume for simplicity that the machine reads only one symbol at a time. But in that case, it had better be able to move back and forth on the tape. It would also be nice if, once it's computed an answer, the machine can halt! But at any time, how does the machine decide which things to do? According to Turing, this decision should depend only on two pieces of information: (1) the symbol currently being read, and (2) the machine's current "internal configuration" or "state." Based on its internal state and the symbol currently being read, the machine should (1) write a new symbol in the current square, (2) move backwards or forwards one square, and (3) switch to a new state or halt.

&amp;gt; Finally, since we want this machine to be physically realizable, the number of possible internal states should be finite. These are the only requirements.

&amp;gt; Turing's first result is the existence of a "universal" machine: a machine whose job is to simulate any other machine described via symbols on the tape. In other words, universal programmable computers can exist. You don't have to build one machine for email, another for playing DVD's, another for Tomb Raider, and so on: you can build a single machine that simulates any of the other machines, by running different programs stored in memory.

It should be noted that a Turing machine isn't really a definition of what a computer "is". No computers we use today resemble a Turing machine in their mechanical operation. (Rather, the computers we use are generally [Von Neumann architectures](http://en.wikipedia.org/wiki/Von_Neumann_architecture). Instead, a Turing machine defines (as far as we know) what physically-achievable computers are *able to do*. That is, as far as we know, any computer we build (whether using classical physics, quantum mechanics, or something else), can at most solve exactly the problems a Turing machine can and no others.  [Von Neumann architecture](http://en.wikipedia.org/wiki/Von_Neumann_architecture)      </snippet></document><document><title>Complexity theory gedanken experiment</title><url>http://www.reddit.com/r/compsci/comments/xlgrq/complexity_theory_gedanken_experiment/</url><snippet>If the universe around us is a machine, of sorts (NB the **if** in this part of the question) - a machine as in all the stuff we observe around us is framed within a story that this stuff is or can be doing some kind of computation (as defined by complexity theory), is there a way to figure out the power of that machine, quantify it in some way?

We're already looking closely at "quantum computation" - using quantum states to perform calculations, and there are lots of quantum states all around us already.

How would you even start to attack that question?  While previous studies in grad school included CS, they didn't include anything on complexity theory or how to evaluate computing power in various types of machines.
  There already a lot of discussion on this topic: can our universe be computable? or can you reboot the universe? thanks.  googling brought up this
http://en.wikipedia.org/wiki/Digital_physics
which pretty much directly answers that question  I consider the universe to be a computer with a dataset the size of the universe that runs at the speed of the universe. Computers are like the VMs of the universe. I presume we could be construed as vms as well, or perhaps applications depending on your view of the human mind.   [deleted] In what sense? A Godelian one? If so, what exactly does that mean? 
  
A TM can certainly encode a TM, so I'm not sure the naive interpretation of your comment is correct.  
  
Of course, if there exist certain oracles then you might not be able to identify which ones due to obscurity, but you could in principle try to learn them. [deleted] [deleted]  Pack another bowl; take another hit. LoL - no, haven't needed or used such substances in quite a while.</snippet></document><document><title>Why Computer Science? - (Question)</title><url>http://www.reddit.com/r/compsci/comments/xlh6q/why_computer_science_question/</url><snippet>Hi,

I apologize in advance if this is the wrong place.... But here goes.

I go to a university, as an applied math major. I would like to change over to Computer Science. To do that, I have to write this essay on why I would like to change majors.

I've been trying to get reasons down why I love Comp Sci, but have been getting stuck.

So far, here's the things I have:

1) Comp Sci is interesting, because it combines the aspects of a hardcore science with an engineering.

2) Comp Sci is useful for solving real-world problems, but it's also abstract like Math.

3) Computer Science has more control over variables than physical sciences. Since you are using memory, and virtual space, it is not as constraining.

Basically, I'm just kinda looking for ideas that remind me of other reasons I like the subject.

Anyways, Here's my real question:

TD:DR;What do you like about Computer Science, and why?

Thanks a bunch!

EDIT: Thank you all for the advice and help. lots of great ideas :)  I'm with CrazySpaniard, writing an essay to change majors is kind of stupid.

As for a switch from Applied Math to Computer Science, what you have sounds like a good basis. I think, condensed in one sentence, what you're saying is that Computer Science has the models and hard data of Applied Math, the formal logic of pure Mathematics, and the real-world utility and creative applications of an Engineering discipline. You rock. The creativity, utility, and modeling parts were really what I couldn't think of for some reason.

Thanks a bunch!  &amp;gt;Basically, I'm just kinda looking for ideas that remind me of other reasons I like the subject.

Computer Science is an art.  Most people see what code monkeys do as building. Someone else tells us what to make, and we build it. But that's not it at all. 

We are architects. With an extensive knowledge of the materials, tools, patterns, and methods available to him, an architect designs a building that is both functional and aesthetically beautiful. A computer scientist does the exact same thing, with one small difference: using the resources of his craft, he creates a program that is both functional and *logically* beautiful - that is, efficient. &amp;gt;Basically, I'm just kinda looking for ideas that remind me of other reasons I like the subject.

Computer Science is an art.  Most people see what code monkeys do as building. Someone else tells us what to make, and we build it. But that's not it at all. 

We are architects. With an extensive knowledge of the materials, tools, patterns, and methods available to him, an architect designs a building that is both functional and aesthetically beautiful. A computer scientist does the exact same thing, with one small difference: using the resources of his craft, he creates a program that is both functional and *logically* beautiful - that is, efficient.  because it's fun as fuck

why do you have to write an essay to switch majors? that seems dumb to me. "I like CS more than math so I wanna do CS now" isn't enough? Yes, I agree that should be good, but my school needs it :(. What school do you go to? Ucla because it's fun as fuck

why do you have to write an essay to switch majors? that seems dumb to me. "I like CS more than math so I wanna do CS now" isn't enough? I think it's more of an administrative thing than a logical prerequisite. If it was truly that easy to switch majors, I would imagine some kids would switch them all the time, complicating course scheduling and course availability for other students. That being said, yeah it's still dumb as shit. There should be a more streamlined work around, I agree. &amp;gt; If it was truly that easy to switch majors, I would imagine some kids would switch them all the time

I'd think the waste of time and money is a lot bigger deterrent than an essay is... I think it's more of an administrative thing than a logical prerequisite. If it was truly that easy to switch majors, I would imagine some kids would switch them all the time, complicating course scheduling and course availability for other students. That being said, yeah it's still dumb as shit. There should be a more streamlined work around, I agree.       </snippet></document><document><title>CS research project: the game Synonymy</title><url>http://gamer.cs.ualberta.ca/</url><snippet>  </snippet></document><document><title>Peer fortress: The scientific battlefield</title><url>http://matt.might.net/articles/peer-fortress/</url><snippet>      </snippet></document><document><title>Using NFAs for font compression</title><url>http://www.reddit.com/r/compsci/comments/xhu88/using_nfas_for_font_compression/</url><snippet>I want to compress a font (consisting only of A-Z, 0-9) for a 7-segment LCD display. For that, I thought of using a NFA: take a 6-bit character code, feed it bit by bit into the NFA and the *number* of the ending state will reflect the code to send to the display. (Each segment of the display can be mapped to a single bit, so every pattern can be mapped to 7 bits = 1 byte.)

For simple and small implementation, I will convert this NFA to a DFA.

Now, I thought of "somehow" renumbering the DFA states so that the jumps between states will never exceed +-7 positions, meaning that a single state transition can be packed into 1 nybble (4 bits). This renumbering will probably require that the bits of the character code are fed in some permuted order, which is not a problem.

Does this map to a known problem? If so, can you give some references?

I have tried looking through IEEE and ACM archives, but the nearest thing I could find was [PDF]:

http://cial.csie.ncku.edu.tw/presentation/group_pdf/(ICC%202011)%20Reorganized%20and%20Compact%20DFA.PDF

This paper is, though, about regexp matching and doesn't account for the fact that 1) my automaton has output, and 2) that the *state number* is output itself.  What do you want to reduce?

Cost? Size? Energy usage?

&amp;gt; NFA

An NFA can have multiple states (at the same time). I don't even know how it would help you at all to start from a NFA. Converting a 2^7 state NFA to a DFA means having 2^(2^7) = 2^128 states in the DFA.

&amp;gt; [...] so that the jumps between states will never exceed +-7 positions, meaning that a single state transition can be packed into 1 nybble (4 bits)

So you want to store 4 bits per character instead of 7 bits, so you only need 32 bytes instead of 64 bytes. But then you need an adder.

Have you tried doing the reordering on paper, because I doubt it is possible (my reasoning: the input has 6 bits, which means you can have a most 6 state transitions. If each can only have a "jump" of 7, you have a maximum of 6\*7=42 states to cover in each direction, meaning all in all 85 states, which is less than the 128 possible settings for a 7 segment display, not even considering that you sometimes will have to jump fewer than 7 positions) This is in the context of writing a program for a microcontroller with limited storage. I want to minimize memory used for the table. NFA can have multiple transitions for the same input character, so it would be of very much help.

The input has 6 bits, but in reality only 36 different patterns.

 Ok, then let's do some math: the straight forward way would be to use one byte per character, which means you have 36 bytes. If you can compress it that way, you only have 18 bytes, so you save 18 bytes with the data. That means, to make it worthwhile, the whole algorithm should take less than 18 bytes of machine code.

That sounds pretty impossible to me.  Can you draw your automata? I'm having trouble understanding your question. Yes, I'll try to do that tonight or tomorrow. In the mean-time, I think I have found a more formal description of the problem:

Mapping of 6 bit character codes -&amp;gt; 7-segmment LCD display can be described with truth tables, which can be minimized and implemented as a combinatorial circuit. I was conjecturing that converting the combinatorial circuit into a sequential one would result in a *smaller* combinatorial circuit that drives the transitions of the sequential circuit.

I believe that my hunch is justified as CPUs are huge sequential circuits that, at the expense of time, execute logic that would require vastly larger (in comparison to the CPU's state transition table) pure combinatorial circuits to implement. (For example, converting truetype font descriptions into antialiased bitmap with 256 shades of gray.)

Does this make more sense? :) </snippet></document><document><title>Interactive proofs &#8212; mathematical games that underlie much modern cryptography &#8212; work even if players try to use quantum information to cheat.</title><url>http://web.mit.edu/newsoffice/2012/interactive-proofs-work-even-if-quantum-information-is-used-0731.html</url><snippet>  Is anyone an expert to explain the class MIP*? Or just some basics on interactive proofs and what they have to do with quantum information? A computationally unbounded *prover* tries to convince a polynomial-time *verifier* that a certain statement is true. We're talking about statements whose truth the verifier can't determine on his own (so things outside of P -- think of your favorite [fully quantified boolean formula](http://en.wikipedia.org/wiki/True_quantified_Boolean_formula) with a million clauses).

The verifier asks multiple "challenges" to the prover, the prover responds. They do this over and over (thus, it's an **interactive** protocol). At the end of the protocol, the verifier either accepts the statement as true, or rejects it as false.

There are two "security" requirements of the protocol:

* If the statement is true, then a prover who runs the protocol honestly will always convince a verifier who runs the protocol honestly. This isn't so hard.

* If the statement is false, then there is no way a cheating prover could (wrongly) convince an honest verifier that it is true. Or at lest, the verifier can make the probability of this event happening as small as he wants. This is the big challenge: the verifier is polynomial-time, but the cheating prover could wield computationally unbounded attacks against him.

The second statement (along with interaction) makes it different than a "classical" proof -- here, you won't be convinced with 100% certainty, but maybe something like (100 - 1/2^n )% certainty.

That's a basic interactive proof. The class IP is the set of languages that have an interactive proof satisfying these conditions. A famous result of complexity theory is that IP = PSPACE. So a polynomial-time verifier can get **reliable answers** to **very difficult** (PSPACE) questions by recruiting the help of an **untrusted**, computationally unbounded oracle.

This article is about the **multi-prover** setting. Here, a verifier talks to **two** provers simultaneously. If the provers were allowed to be totally in collusion, then this would be no different than the single-prover case (think of a single mastermind controlling both malicious provers). So to make it different, we assume that the two provers, if they are trying to cheat, can't talk to each other during the proof protocol (they are *independent* provers). They can coordinate their strategies beforehand, and share secrets, but they're not allowed to compare notes "online."

Now the verifier has a little more advantage in catching cheating, as it can "cross-check" the answers from one prover against the other. 

The set of languages that have multi-prover interactive proofs is called MIP. It is a bigger class than IP because the verifier has a little more leverage. A famous result in complexity theory is that MIP = NEXP.

This paper is about what happens when the provers are entangled with quantum data. It's not at all obvious that entanglement *doesn't* help. Imagine if the verifier asked prover #1 a question, and then the prover made a measurement of some entangled qubit. Then when prover #2 measures it, he might see something useful that will help him keep up the ruse with the verifier. Intuitively we know that quantum entanglement can't be used to send information between the provers, but that's not enough for a proof.. it's a completely different adversarial model for the provers.

Anyway, this result is that MIP is the same even when the cheating provers are entangled (I believe the verifier and the protocol are still classical here). That is, entanglement won't help the provers to cheat. These are the best kinds of results. The good guys can still be classical, but they can be assured of their security even when the bad guys are quantum.

----

followup edit: There is another "security" property that an interactive proof might have:

* Even if the verifier is not following the protocol, he doesn't learn anything about the statement beyond the fact that it is true.

If you're wondering what the verifier could learn, consider the difference between: "this number is an RSA modulus" vs "here is the factorization of this RSA modulus".

An interactive proof with this property is called **zero-knowledge**. This is much more non-obvious (and very interesting) to define formally than the other properties. A famous result showed that you can get zero-knowledge "for free" for any (single-prover) interactive proof. That is, if something has an interactive proof, then it also has one with the zero-knowledge property. I don't remember whether a corresponding statement is true for the multi-prover case.  [This was posted to /r/programming several minutes ago - my in-thread response there would be much better answered by people from here, I'm sure](http://www.reddit.com/r/programming/comments/xg2e4/10yearold_problem_in_theoretical_computer_science/).

**(edit: fixed link)** Recursive link is recursive.</snippet></document><document><title>Art expressing CS / Math.</title><url>http://www.reddit.com/r/compsci/comments/xhgzs/art_expressing_cs_math/</url><snippet>** First and foremost, I'm warning anyone who reads this that I am creating this post for market research for my own career. ** I'd really appreciate any constructive criticism or ideas you'd like to offer, because you people are really my audience in this.

With that in mind;

My **main question** to ask is: Would any of you, as computer scientists, programming enthusiasts, algorithmic theorists, analysists and general other such cool people: 

* *Would you at all ever consider purchasing paintings that depict mathematical and computational theoretical imagery?* 

I can't totally explain what I plan to do, but basically the way I've learned computer science and math is entirely visual and sort of imaginary (in an inside my head) way - that allows me to understand concepts in the fields. I'd like to translate these concepts into something tangible that can be appreciated by people such as yourselves. 

My main goal out of this is to get people to understand the necessity of security in a complicated, technological world, as well as the intrinsic beauty of mathematics and well designed, elegant code. 

I don't have any examples yet of what I'm going to make, I just wanted really some feedback on whether this is an idea worth pursuing, and I'd really love to hear any possible desires from you guys, if you were to buy a painting or a piece of art that reflects your passion - computer science. 

For a little background, I studied for 3 years computer engineering, electrical engineering, got a bachelors in art and technology (new program). I then got a masters in CS, and am working towards a PhD in CS, where I focused for about 8 months on various aspects of security. I'd like ultimately to create a new kind of field as a combination of art and computer science, but in a tangible way that people can appreciate as having a piece of art in their own homes - instead of a video or photograph that feels a little less personalized.   Reminds me of [Low-complexity art](http://www.idsia.ch/~juergen/locoart/node12.html).          </snippet></document><document><title>Question on the analysis of an algorithm (SICP exercise 1.15 spoilers)</title><url>http://www.reddit.com/r/compsci/comments/xh32v/question_on_the_analysis_of_an_algorithm_sicp/</url><snippet>    (define (cube x) (* x x x))

    (define (p x) (- (* 3 x) (* 4 (cube x))))

    (define (sine angle)
        (if (not (&amp;gt; (abs angle) 0.1))
            angle
        (p (sine (/ angle 3.0)))))

In exercise 1.15 of the SICP, it's asked what the space and time complexity of the above algorithm is. The answer is the ceiling of log_3(n) - log_3(0.1), where n is the number you input to the sine function. I understand taking the logarithm of the input, as you're dividing the input by 3.0 in the body of the sine function. However, I'm not sure why you're subtracting the logarithm of 0.1, or rather the thought process that allows you to jump from seeing that the angle must be less than 0.1 to subtracting the logarithm of 0.1 from the logarithm of 3.0. Could someone explain it? Thank you!  </snippet></document><document><title>ELI5: How do random number generators work?</title><url>http://www.reddit.com/r/compsci/comments/xes5c/eli5_how_do_random_number_generators_work/</url><snippet>Not sure if this should go here, in /r/programming or /r/algorithms etc. Anyway, I am a beginner programmer, I've only taken one CS class so far (in Python) and have taught myself a little bit of Javascript (using Codecademy) and Java (using a book). I am basically curious to know how the random module in Python, or Math.random() in JS, (and I'm sure there are similar things in many other languages) works. How random is it actually, and how does it come up with the numbers it returns? There must be some sort of algorithm to generate the number, so how does it work? I'm sorry if this has been answered before, I searched and didn't find anything. 
P.S., you don't actually have to ELI5, but I would like a basic, beginner-level explanation. Thanks! :)  You are actually talking about pseudo-random number generators.  They are not actually random.  They all take a seed value, and produce a sequence of numbers from that seed.  The sequence is always the same for a given seed, but if you pick a seed based on something like the current epoch time, then it gives the illusion of being somewhat random.

The simplest is a [linear congruential generator](http://en.wikipedia.org/wiki/Linear_congruential_generator).  The Python standard library uses the [Mersenne twister](http://en.wikipedia.org/wiki/Mersenne_twister) which is a popular algorithm that is somewhat higher quality (but still a PRNG.)
 Thanks a lot! I guess my follow-up would be, is it possible to create an *actual* random number generator? If so, how does that work? **tl;dr: To answer your question: No. True RNG needs special hardware**

No software can implement a true RNG because software is 100% deterministic (or at least in theory, more on that later). It would not be possible to write a true RNG purely in code. Basicaly what you need is an external source of entropy (ie. randomness). 

*nix systems have a "special file" known as /dev/random which takes entropy from devices connected to the system. Sources of entropy can come from the keyboard, mouse, or disk drive (the speed at which it spins is determined by air resistance). Some systems have special hardware-based RNGs installed included the newest of the Ivy Bridge by Intel but these solutions are expensive and rare. /dev/random simply waits until there is entropy that has been generated by some source and provides it to the program asking for it.

But sometimes you need lots of random bits and there simply isn&#8217;t enough entropy and /dev/random "blocks" (ie. waits) for more before giving a random stream. So developers turn to another *nix structure called /dev/urandom which does not block but instead always returns with something by filling in the blanks with a PRNG. This effectively gives PRNG performance while making the developer better than just using rand().

This is all fine and dandy for end user systems (with people sitting at them) but it falls short for many servers (no keyboard/mouse and using SSD so no disk spin speed), embedded (like routers or cell phones), or virtual machine solutions. There are many attacks out there (specifically targeted against RSA encryption) which relies on starving the system of entropy and forcing the target to use the backup PRNG which is (by definition since it is just software) deterministic.

However, there is some hope. Some very smart people out there have come up with the hacks to make software give non-deterministic outputs. These really are hacks since true RNG performance can only come from a hardware RNG. The trick is to abuse the multi-core processors in computers to act non-deterministically. Usually programmers go through much pain and suffering to get multi-threading applications to act nice with each other and not to stomp all over each other's data. But by having two threads acting on the same value non-atomically it is possible to get non-deterministic data. It&#8217;s slow, but it works (in theory).

Another hack is to get two different clocks on the system that are incremented separately (such as CPU tick count which increments every clock cycle and the Real Time Clock which increments at 2^15 hz) and determine the lag time between them.

These methods right now are just theory. They are both talked about by the brilliant Dan Kaminsky in his talk at DefCon this year and on his website. Neither is proven to be truly random but as Kaminsky says: "it can't be worse." Kaminsky also has some other methods which he talked about at DefCon which he should be releasing soon.

**Source**: [Dan Kaminsky's Blog](http://dankaminsky.com/2012/02/17/primalfear/)

**Source**: [truerand](http://cfs.sourcearchive.com/documentation/1.4.1-19/truerand_8c-source.html) **tl;dr: To answer your question: No. True RNG needs special hardware**

No software can implement a true RNG because software is 100% deterministic (or at least in theory, more on that later). It would not be possible to write a true RNG purely in code. Basicaly what you need is an external source of entropy (ie. randomness). 

*nix systems have a "special file" known as /dev/random which takes entropy from devices connected to the system. Sources of entropy can come from the keyboard, mouse, or disk drive (the speed at which it spins is determined by air resistance). Some systems have special hardware-based RNGs installed included the newest of the Ivy Bridge by Intel but these solutions are expensive and rare. /dev/random simply waits until there is entropy that has been generated by some source and provides it to the program asking for it.

But sometimes you need lots of random bits and there simply isn&#8217;t enough entropy and /dev/random "blocks" (ie. waits) for more before giving a random stream. So developers turn to another *nix structure called /dev/urandom which does not block but instead always returns with something by filling in the blanks with a PRNG. This effectively gives PRNG performance while making the developer better than just using rand().

This is all fine and dandy for end user systems (with people sitting at them) but it falls short for many servers (no keyboard/mouse and using SSD so no disk spin speed), embedded (like routers or cell phones), or virtual machine solutions. There are many attacks out there (specifically targeted against RSA encryption) which relies on starving the system of entropy and forcing the target to use the backup PRNG which is (by definition since it is just software) deterministic.

However, there is some hope. Some very smart people out there have come up with the hacks to make software give non-deterministic outputs. These really are hacks since true RNG performance can only come from a hardware RNG. The trick is to abuse the multi-core processors in computers to act non-deterministically. Usually programmers go through much pain and suffering to get multi-threading applications to act nice with each other and not to stomp all over each other's data. But by having two threads acting on the same value non-atomically it is possible to get non-deterministic data. It&#8217;s slow, but it works (in theory).

Another hack is to get two different clocks on the system that are incremented separately (such as CPU tick count which increments every clock cycle and the Real Time Clock which increments at 2^15 hz) and determine the lag time between them.

These methods right now are just theory. They are both talked about by the brilliant Dan Kaminsky in his talk at DefCon this year and on his website. Neither is proven to be truly random but as Kaminsky says: "it can't be worse." Kaminsky also has some other methods which he talked about at DefCon which he should be releasing soon.

**Source**: [Dan Kaminsky's Blog](http://dankaminsky.com/2012/02/17/primalfear/)

**Source**: [truerand](http://cfs.sourcearchive.com/documentation/1.4.1-19/truerand_8c-source.html) Pity they cant use the decay of voltage in a power supply capacitor or electrical line noise.  **tl;dr: To answer your question: No. True RNG needs special hardware**

No software can implement a true RNG because software is 100% deterministic (or at least in theory, more on that later). It would not be possible to write a true RNG purely in code. Basicaly what you need is an external source of entropy (ie. randomness). 

*nix systems have a "special file" known as /dev/random which takes entropy from devices connected to the system. Sources of entropy can come from the keyboard, mouse, or disk drive (the speed at which it spins is determined by air resistance). Some systems have special hardware-based RNGs installed included the newest of the Ivy Bridge by Intel but these solutions are expensive and rare. /dev/random simply waits until there is entropy that has been generated by some source and provides it to the program asking for it.

But sometimes you need lots of random bits and there simply isn&#8217;t enough entropy and /dev/random "blocks" (ie. waits) for more before giving a random stream. So developers turn to another *nix structure called /dev/urandom which does not block but instead always returns with something by filling in the blanks with a PRNG. This effectively gives PRNG performance while making the developer better than just using rand().

This is all fine and dandy for end user systems (with people sitting at them) but it falls short for many servers (no keyboard/mouse and using SSD so no disk spin speed), embedded (like routers or cell phones), or virtual machine solutions. There are many attacks out there (specifically targeted against RSA encryption) which relies on starving the system of entropy and forcing the target to use the backup PRNG which is (by definition since it is just software) deterministic.

However, there is some hope. Some very smart people out there have come up with the hacks to make software give non-deterministic outputs. These really are hacks since true RNG performance can only come from a hardware RNG. The trick is to abuse the multi-core processors in computers to act non-deterministically. Usually programmers go through much pain and suffering to get multi-threading applications to act nice with each other and not to stomp all over each other's data. But by having two threads acting on the same value non-atomically it is possible to get non-deterministic data. It&#8217;s slow, but it works (in theory).

Another hack is to get two different clocks on the system that are incremented separately (such as CPU tick count which increments every clock cycle and the Real Time Clock which increments at 2^15 hz) and determine the lag time between them.

These methods right now are just theory. They are both talked about by the brilliant Dan Kaminsky in his talk at DefCon this year and on his website. Neither is proven to be truly random but as Kaminsky says: "it can't be worse." Kaminsky also has some other methods which he talked about at DefCon which he should be releasing soon.

**Source**: [Dan Kaminsky's Blog](http://dankaminsky.com/2012/02/17/primalfear/)

**Source**: [truerand](http://cfs.sourcearchive.com/documentation/1.4.1-19/truerand_8c-source.html) &amp;gt; (the speed at which it spins is determined by air resistance)

I thought the motor is completely enclosed inside, so no changes in air resistance. The random read/write speed fluctuations are more to do with random fluctuations in the power supplied to it. The air resistance will change as the temperature varies (the pressure is kept at atmospheric pressure by the holes in the HDD case), but I suspect other factors may contribute more entropy. You're right, I forgot to take temperature into account, but I thought the drive is filled with vacuum, so the temperature will affect the motor itself... And they're disks spinning, and pardon my lack of physics knowledge, but does air resistance affect the spinning of a disk? It's most certainly not a vacuum.  The design requires there to be air for the head to "float" on (like a plane using the surface effect) so that it does not crash into the surface of the platter.  If you've ever taken apart a dead drive, there are little ventilation ports with tiny air filters, so that air can flow in and out to equilibrate with changes in temperature and pressure.  The ports are sometimes labeled on the outside with stickers that say "Do Not Cover ---&amp;gt;" with the arrow pointing to a small hole.
 You're right, I forgot to take temperature into account, but I thought the drive is filled with vacuum, so the temperature will affect the motor itself... And they're disks spinning, and pardon my lack of physics knowledge, but does air resistance affect the spinning of a disk? As long as the disk is not a perfect, mathematical cylinder, then yes, air resistance will affect the spinning. Imagine spinning a disk in air, then in water, then in butter and finally in half-dry concrete.

In fact, only imagine spinning the disk in water. You can probably imagine that it would cause some turbulence in the water. The displacement of water is a loss of energy in the spin of the disk. As long as the disk is not a perfect, mathematical cylinder, then yes, air resistance will affect the spinning. Imagine spinning a disk in air, then in water, then in butter and finally in half-dry concrete.

In fact, only imagine spinning the disk in water. You can probably imagine that it would cause some turbulence in the water. The displacement of water is a loss of energy in the spin of the disk. **tl;dr: To answer your question: No. True RNG needs special hardware**

No software can implement a true RNG because software is 100% deterministic (or at least in theory, more on that later). It would not be possible to write a true RNG purely in code. Basicaly what you need is an external source of entropy (ie. randomness). 

*nix systems have a "special file" known as /dev/random which takes entropy from devices connected to the system. Sources of entropy can come from the keyboard, mouse, or disk drive (the speed at which it spins is determined by air resistance). Some systems have special hardware-based RNGs installed included the newest of the Ivy Bridge by Intel but these solutions are expensive and rare. /dev/random simply waits until there is entropy that has been generated by some source and provides it to the program asking for it.

But sometimes you need lots of random bits and there simply isn&#8217;t enough entropy and /dev/random "blocks" (ie. waits) for more before giving a random stream. So developers turn to another *nix structure called /dev/urandom which does not block but instead always returns with something by filling in the blanks with a PRNG. This effectively gives PRNG performance while making the developer better than just using rand().

This is all fine and dandy for end user systems (with people sitting at them) but it falls short for many servers (no keyboard/mouse and using SSD so no disk spin speed), embedded (like routers or cell phones), or virtual machine solutions. There are many attacks out there (specifically targeted against RSA encryption) which relies on starving the system of entropy and forcing the target to use the backup PRNG which is (by definition since it is just software) deterministic.

However, there is some hope. Some very smart people out there have come up with the hacks to make software give non-deterministic outputs. These really are hacks since true RNG performance can only come from a hardware RNG. The trick is to abuse the multi-core processors in computers to act non-deterministically. Usually programmers go through much pain and suffering to get multi-threading applications to act nice with each other and not to stomp all over each other's data. But by having two threads acting on the same value non-atomically it is possible to get non-deterministic data. It&#8217;s slow, but it works (in theory).

Another hack is to get two different clocks on the system that are incremented separately (such as CPU tick count which increments every clock cycle and the Real Time Clock which increments at 2^15 hz) and determine the lag time between them.

These methods right now are just theory. They are both talked about by the brilliant Dan Kaminsky in his talk at DefCon this year and on his website. Neither is proven to be truly random but as Kaminsky says: "it can't be worse." Kaminsky also has some other methods which he talked about at DefCon which he should be releasing soon.

**Source**: [Dan Kaminsky's Blog](http://dankaminsky.com/2012/02/17/primalfear/)

**Source**: [truerand](http://cfs.sourcearchive.com/documentation/1.4.1-19/truerand_8c-source.html) **tl;dr: To answer your question: No. True RNG needs special hardware**

No software can implement a true RNG because software is 100% deterministic (or at least in theory, more on that later). It would not be possible to write a true RNG purely in code. Basicaly what you need is an external source of entropy (ie. randomness). 

*nix systems have a "special file" known as /dev/random which takes entropy from devices connected to the system. Sources of entropy can come from the keyboard, mouse, or disk drive (the speed at which it spins is determined by air resistance). Some systems have special hardware-based RNGs installed included the newest of the Ivy Bridge by Intel but these solutions are expensive and rare. /dev/random simply waits until there is entropy that has been generated by some source and provides it to the program asking for it.

But sometimes you need lots of random bits and there simply isn&#8217;t enough entropy and /dev/random "blocks" (ie. waits) for more before giving a random stream. So developers turn to another *nix structure called /dev/urandom which does not block but instead always returns with something by filling in the blanks with a PRNG. This effectively gives PRNG performance while making the developer better than just using rand().

This is all fine and dandy for end user systems (with people sitting at them) but it falls short for many servers (no keyboard/mouse and using SSD so no disk spin speed), embedded (like routers or cell phones), or virtual machine solutions. There are many attacks out there (specifically targeted against RSA encryption) which relies on starving the system of entropy and forcing the target to use the backup PRNG which is (by definition since it is just software) deterministic.

However, there is some hope. Some very smart people out there have come up with the hacks to make software give non-deterministic outputs. These really are hacks since true RNG performance can only come from a hardware RNG. The trick is to abuse the multi-core processors in computers to act non-deterministically. Usually programmers go through much pain and suffering to get multi-threading applications to act nice with each other and not to stomp all over each other's data. But by having two threads acting on the same value non-atomically it is possible to get non-deterministic data. It&#8217;s slow, but it works (in theory).

Another hack is to get two different clocks on the system that are incremented separately (such as CPU tick count which increments every clock cycle and the Real Time Clock which increments at 2^15 hz) and determine the lag time between them.

These methods right now are just theory. They are both talked about by the brilliant Dan Kaminsky in his talk at DefCon this year and on his website. Neither is proven to be truly random but as Kaminsky says: "it can't be worse." Kaminsky also has some other methods which he talked about at DefCon which he should be releasing soon.

**Source**: [Dan Kaminsky's Blog](http://dankaminsky.com/2012/02/17/primalfear/)

**Source**: [truerand](http://cfs.sourcearchive.com/documentation/1.4.1-19/truerand_8c-source.html) **tl;dr: To answer your question: No. True RNG needs special hardware**

No software can implement a true RNG because software is 100% deterministic (or at least in theory, more on that later). It would not be possible to write a true RNG purely in code. Basicaly what you need is an external source of entropy (ie. randomness). 

*nix systems have a "special file" known as /dev/random which takes entropy from devices connected to the system. Sources of entropy can come from the keyboard, mouse, or disk drive (the speed at which it spins is determined by air resistance). Some systems have special hardware-based RNGs installed included the newest of the Ivy Bridge by Intel but these solutions are expensive and rare. /dev/random simply waits until there is entropy that has been generated by some source and provides it to the program asking for it.

But sometimes you need lots of random bits and there simply isn&#8217;t enough entropy and /dev/random "blocks" (ie. waits) for more before giving a random stream. So developers turn to another *nix structure called /dev/urandom which does not block but instead always returns with something by filling in the blanks with a PRNG. This effectively gives PRNG performance while making the developer better than just using rand().

This is all fine and dandy for end user systems (with people sitting at them) but it falls short for many servers (no keyboard/mouse and using SSD so no disk spin speed), embedded (like routers or cell phones), or virtual machine solutions. There are many attacks out there (specifically targeted against RSA encryption) which relies on starving the system of entropy and forcing the target to use the backup PRNG which is (by definition since it is just software) deterministic.

However, there is some hope. Some very smart people out there have come up with the hacks to make software give non-deterministic outputs. These really are hacks since true RNG performance can only come from a hardware RNG. The trick is to abuse the multi-core processors in computers to act non-deterministically. Usually programmers go through much pain and suffering to get multi-threading applications to act nice with each other and not to stomp all over each other's data. But by having two threads acting on the same value non-atomically it is possible to get non-deterministic data. It&#8217;s slow, but it works (in theory).

Another hack is to get two different clocks on the system that are incremented separately (such as CPU tick count which increments every clock cycle and the Real Time Clock which increments at 2^15 hz) and determine the lag time between them.

These methods right now are just theory. They are both talked about by the brilliant Dan Kaminsky in his talk at DefCon this year and on his website. Neither is proven to be truly random but as Kaminsky says: "it can't be worse." Kaminsky also has some other methods which he talked about at DefCon which he should be releasing soon.

**Source**: [Dan Kaminsky's Blog](http://dankaminsky.com/2012/02/17/primalfear/)

**Source**: [truerand](http://cfs.sourcearchive.com/documentation/1.4.1-19/truerand_8c-source.html) **tl;dr: To answer your question: No. True RNG needs special hardware**

No software can implement a true RNG because software is 100% deterministic (or at least in theory, more on that later). It would not be possible to write a true RNG purely in code. Basicaly what you need is an external source of entropy (ie. randomness). 

*nix systems have a "special file" known as /dev/random which takes entropy from devices connected to the system. Sources of entropy can come from the keyboard, mouse, or disk drive (the speed at which it spins is determined by air resistance). Some systems have special hardware-based RNGs installed included the newest of the Ivy Bridge by Intel but these solutions are expensive and rare. /dev/random simply waits until there is entropy that has been generated by some source and provides it to the program asking for it.

But sometimes you need lots of random bits and there simply isn&#8217;t enough entropy and /dev/random "blocks" (ie. waits) for more before giving a random stream. So developers turn to another *nix structure called /dev/urandom which does not block but instead always returns with something by filling in the blanks with a PRNG. This effectively gives PRNG performance while making the developer better than just using rand().

This is all fine and dandy for end user systems (with people sitting at them) but it falls short for many servers (no keyboard/mouse and using SSD so no disk spin speed), embedded (like routers or cell phones), or virtual machine solutions. There are many attacks out there (specifically targeted against RSA encryption) which relies on starving the system of entropy and forcing the target to use the backup PRNG which is (by definition since it is just software) deterministic.

However, there is some hope. Some very smart people out there have come up with the hacks to make software give non-deterministic outputs. These really are hacks since true RNG performance can only come from a hardware RNG. The trick is to abuse the multi-core processors in computers to act non-deterministically. Usually programmers go through much pain and suffering to get multi-threading applications to act nice with each other and not to stomp all over each other's data. But by having two threads acting on the same value non-atomically it is possible to get non-deterministic data. It&#8217;s slow, but it works (in theory).

Another hack is to get two different clocks on the system that are incremented separately (such as CPU tick count which increments every clock cycle and the Real Time Clock which increments at 2^15 hz) and determine the lag time between them.

These methods right now are just theory. They are both talked about by the brilliant Dan Kaminsky in his talk at DefCon this year and on his website. Neither is proven to be truly random but as Kaminsky says: "it can't be worse." Kaminsky also has some other methods which he talked about at DefCon which he should be releasing soon.

**Source**: [Dan Kaminsky's Blog](http://dankaminsky.com/2012/02/17/primalfear/)

**Source**: [truerand](http://cfs.sourcearchive.com/documentation/1.4.1-19/truerand_8c-source.html) Thanks a lot! I guess my follow-up would be, is it possible to create an *actual* random number generator? If so, how does that work? [deleted] [deleted] Thanks a lot! I guess my follow-up would be, is it possible to create an *actual* random number generator? If so, how does that work? Thanks a lot! I guess my follow-up would be, is it possible to create an *actual* random number generator? If so, how does that work? Thanks a lot! I guess my follow-up would be, is it possible to create an *actual* random number generator? If so, how does that work? Thanks a lot! I guess my follow-up would be, is it possible to create an *actual* random number generator? If so, how does that work? Thanks a lot! I guess my follow-up would be, is it possible to create an *actual* random number generator? If so, how does that work?  Truly random numbers are hard to come by, and often use some physical phenomena, such as photons traveling through semi-transparent surfaces, thermal, atmospheric, or audible noise, or nuclear decay rates.

Pseudo-random numbers (pretty much all software-based RNGs) often use some mathematical function that is difficult to predict.

An [LCG](http://en.wikipedia.org/wiki/Linear_congruential_generator) is one example of a fairly straight-forward PRNG that is used by everybody and their grandmother.  It's not terrific, but it's good enough for non-secure purposes (and it's fairly fast).

A lot of math-oriented languages use the [Mersenne Twister](http://en.wikipedia.org/wiki/Mersenne_twister), but I don't know much about it.

Some people use [Logistic Maps](http://en.wikipedia.org/wiki/Logistic_map) for their PRNGs, but these are *very* sensitive to initial conditions that will render the function either trivially predictable or very chaotic with small changes to the initial conditions.  There are some other maps that are better suited to this task, though.

{begin edit}

IIRC, I remember reading something about Wolfram using Cellular Automata for RNG, but I can't recall much about it.  I'll have to look into that some more.

{end edit}

If you only need a small amount of random bits (for seeding a PRNG, for example), you can also hash various pieces of data from around the system: thermal data, mouse movement, keystrokes, time, memory usage, CPU usage, process IDs, etc.  Individually, each of these can be moderately easy to guess at, but the resulting hash of multiple sources can be hard to predict.

Similar methods are used by many *nixes for /dev/random, which often collect entropy from various difficult-to-predict events around the system.

Then you have the whole field of cryptographically secure PRNGs, about which I don't know nearly enough.  These are basically PRNGs which either can't be predicted, we don't know how to predict, or are provably extremely hard to predict. &amp;gt; IIRC, I remember reading something about Wolfram using Cellular Automata for RNG, but I can't recall much about it. I'll have to look into that some more.

You're thinking about [rule 30](http://en.wikipedia.org/wiki/Rule_30). It's not very good, though. With a sufficiently large sample, one can reverse the calculation and effectively discover past values and predict future values. 

Edit: I can't find a paper for it, but [here](http://www.pouet.net/topic.php?which=8665&amp;amp;page=1) is a hobbyist demo programmer doing it on his own. 

Edit: [Paper](http://thales.math.uqam.ca/~rowland/papers/Local_nested_structure_in_rule_30.pdf) found :)   I don't have any answers, but I have similar questions about random generators and thought it would be just a waste of space to create another thread for it...

Say you have a true random generator that has binary output, is it possible to create a random number generator from it that has 3 outputs with equal probability and ~~with a 100% chance of~~ must terminate? I'm suspecting that it isn't possible, but I don't have any proof for it and my google-fu isn't good enough. I don't have any answers, but I have similar questions about random generators and thought it would be just a waste of space to create another thread for it...

Say you have a true random generator that has binary output, is it possible to create a random number generator from it that has 3 outputs with equal probability and ~~with a 100% chance of~~ must terminate? I'm suspecting that it isn't possible, but I don't have any proof for it and my google-fu isn't good enough. I don't have any answers, but I have similar questions about random generators and thought it would be just a waste of space to create another thread for it...

Say you have a true random generator that has binary output, is it possible to create a random number generator from it that has 3 outputs with equal probability and ~~with a 100% chance of~~ must terminate? I'm suspecting that it isn't possible, but I don't have any proof for it and my google-fu isn't good enough. I don't have any answers, but I have similar questions about random generators and thought it would be just a waste of space to create another thread for it...

Say you have a true random generator that has binary output, is it possible to create a random number generator from it that has 3 outputs with equal probability and ~~with a 100% chance of~~ must terminate? I'm suspecting that it isn't possible, but I don't have any proof for it and my google-fu isn't good enough. You can't do it exactly, but you can come arbitrarily close.  For example, if you sample 32 bits and divide that into three outcomes, you end up with two of probability 0.33333333325572311878204345703125 and one of probability 0.3333333334885537624359130859375, or an error of about 0.2 ppb.  If you need better you can just use more digits.
 You can't do it exactly, but you can come arbitrarily close.  For example, if you sample 32 bits and divide that into three outcomes, you end up with two of probability 0.33333333325572311878204345703125 and one of probability 0.3333333334885537624359130859375, or an error of about 0.2 ppb.  If you need better you can just use more digits.
 Couldn't you take just the first 30 bits and check which of the three 10-bit blocks has the highest value, and if multiple have the same one, repeat?

I really don't know, could you? Couldn't you take just the first 30 bits and check which of the three 10-bit blocks has the highest value, and if multiple have the same one, repeat?

I really don't know, could you? Couldn't you take just the first 30 bits and check which of the three 10-bit blocks has the highest value, and if multiple have the same one, repeat?

I really don't know, could you? </snippet></document><document><title>Are there any masters in CS programs for people without undergraduate CS degrees?</title><url>http://www.reddit.com/r/compsci/comments/xemn7/are_there_any_masters_in_cs_programs_for_people/</url><snippet>I will graduate in two semesters with a degree in finance. While I like finance I also really enjoy computer science. I started with python and have been learning R and C++. Mainly, I use these to program finance programs as a hobby. I eventually want to get a PhD in finance and getting a masters degree in CS would help me get into a computational finance PhD program. Although that's not what I'm set on doing, that's one path I would love to take. 


Another thing that would hamper me is my lack of mathematics, I only took one calculus course in undergrad. I'm not sure how pervasive math is in a graduate CS program, but I'm sure it's rather heavy.


So are there any masters programs that one can get into without a CS undergrad?   There are, but you'll need to take a lot of undergrad CS courses.  Just be warned that CS has surprisingly little to do with programming.  Past undergrad classes, most of the degree will be heavily math-based, or will at least assume that you have a lot of mathematical knowledge.

If you want to follow a programming-heavy degree plan, you may want to look for a university with a software engineering study plan.  Yes. My undergrad is not in CS, but I enrolled in a CS program at my undergrad university. You will have to take quite a few undergrad-level courses, and the programs are more accepting if you have a good math/science background. Thanks for the info. What was your UG in, if you don't mind answering? 


Ok, I've taken a bunch of finance/economics classes and one econometrics classes that are math heavy. No matter what I do I'll probably end up taking calc classes because I enjoy math and I'd like to get a PhD and would feel outgunned if I didn't, so levelling classes don't really bother me.


Big question: Does this hurt my placement into programs? I'd like to at least go to a school a little more prestigious than my current institution. 


Anyone else done this same thing? I was going for an Electrical Engineering major, with a Psychology minor. I had a run of bad luck and difficulty with the EE program, so I ended up with just a degree in Psychology. I liked the programming aspect of the EE, so, after being graduated for a couple of years, I applied to the CS Grad program at my former institution. I don't know if they showed favoritism for me being an alumnus, but I was accepted to the program. Maybe they liked my GRE scores, not sure. Interesting, but did you have a few CS classes under your belt? And probably a bit of math by that point too.  May apply to my current institution as well. It's not very high ranked though.

What do you do now?  I had Calc I/II/II, DiffEq, and linear algebra, Physics (E&amp;amp;M and Optics), Chem I/II, and some programming courses. Currently, I am a Java application programmer. It may interest you to know that, due to financial reasons, I never finished my CS graduate degree. I am self taught in many areas, and I got very lucky with timing and generous employers. I had Calc I/II/II, DiffEq, and linear algebra, Physics (E&amp;amp;M and Optics), Chem I/II, and some programming courses. Currently, I am a Java application programmer. It may interest you to know that, due to financial reasons, I never finished my CS graduate degree. I am self taught in many areas, and I got very lucky with timing and generous employers. Thanks for the info. What was your UG in, if you don't mind answering? 


Ok, I've taken a bunch of finance/economics classes and one econometrics classes that are math heavy. No matter what I do I'll probably end up taking calc classes because I enjoy math and I'd like to get a PhD and would feel outgunned if I didn't, so levelling classes don't really bother me.


Big question: Does this hurt my placement into programs? I'd like to at least go to a school a little more prestigious than my current institution. 


Anyone else done this same thing? Thanks for the info. What was your UG in, if you don't mind answering? 


Ok, I've taken a bunch of finance/economics classes and one econometrics classes that are math heavy. No matter what I do I'll probably end up taking calc classes because I enjoy math and I'd like to get a PhD and would feel outgunned if I didn't, so levelling classes don't really bother me.


Big question: Does this hurt my placement into programs? I'd like to at least go to a school a little more prestigious than my current institution. 


Anyone else done this same thing? Yes. My undergrad is not in CS, but I enrolled in a CS program at my undergrad university. You will have to take quite a few undergrad-level courses, and the programs are more accepting if you have a good math/science background.  One of my CS professors was a Finance major and he got his masters and PhD in Computer Science.  Don't finance majors take a lot of math or am I thinking of financial math?

What computational finance PhD programs were you thinking about applying to? That sounds really cool. Financial math. I can discount/compound anything and work with basic derivatives/integrals, but we don't learn anything else really. Sucks, I wish I'd majored in math/engineering/comp sci and then gone into finance at the graduate level. 

Purdue has one, it's in the statistics department though. A bunch of unis have quantitative finance PhD programs which peak my interest as well.  Ooh, cool. I'm assuming the quantitative finance stuff isn't as much CS and algorithms though, right?     I have an UG degree in Economics and am currently getting my MS in CS. I had to take 5 pre-reqs but was able to get into a CS program. My undergrad math was...lacking. I had Business Calc, Discrete Math and stats as well as an econometrics class and that was about it. I had to take Discrete Math again as a pre-req (due to not taking it seriously in UG and getting a C). 

One thing I would point out is that some schools (such as [Depaul](http://www.cdm.depaul.edu/academics/Pages/MSinComputationalFinance.aspx) ) actually have Computational Finance MS programs. If you are more interested in the finance aspect, that may be something you could look into. That's the exact amount of math I'll have, even the econometrics class. 

Will take the comp. finance thing into consideration. </snippet></document><document><title>CS Grad students, I'm having trouble deciding what subfield of Computer Science to go in to. I want something involving math/prob/statistics but need help weighing out the options. Computational Science or AI/ML?</title><url>http://www.reddit.com/r/compsci/comments/xd0rv/cs_grad_students_im_having_trouble_deciding_what/</url><snippet>I'm currently doing research as an undergrad in biological modeling (simulation) and I'm really enjoying myself. I'm a double major in CS and Mathematical Statistics.

I really like what I'm doing because I get to write algorithms, which I find fun, but I also get to look at heavy math probability and statistics in papers and apply all that theory I'm learning in my courses to my research.

So I'm weighing out continuing with simulation, when I applying to grad school, but I'm not sure how what kind of simulation. I like the biological modeling stuff I've been doing, but I don't know too much biology and I'm not sure how much demand there is out there for that type of knowledge.

Computational finance seems attractive because it seems to utilize the subjects I like (simulation, numerical analysis, statistics, algorithms), but it doesn't seem like I'd be contributing something useful to the world (no offense computational finance people).

On the other hand machine learning, data mining, and AI seem really cool and I've heard it's all statistics, but it doesn't involve much simulation which has recently become a passion of mine. The other advantage with this subfield is that it there seems to be a great demand for that type of knowledge right now and funding and startups popping up everywhere there.

Any grad students out there have advice for me, and can anyone recommend good grad programs in the US, and maybe Canada for the subjects I listed? By the way, I am actually going to be a senior, but won't be able to apply to grad schools until next year because I am graduating a semester late, since I'm double majoring. So I still have time to decide, but I wanted to get some grad students thoughts, maybe a possible IAMA? Would be possible to continue research after I graduate, but before I apply to grad school to boost my odds of acceptances?    Talk to professors in the areas you are interested in.  Especially talk to directors of graduate studies in programs like those that you might like to apply to.  Talk to the directors of graduate studes in programs at the University where you are an undergrad, or if you are at a college at some nearby university.   Undergraduates have weird ideas about graduate school, as do graduate students.  I learned in graduate school that everything my fellow students told me about how the program worked, what courses one has to take, what research is like, and so forth, is wrong.  So talk to some professors.  They actually like to talk to eager interested students.
I'm not a CS professor, I teach statistics, so I know something about what I call the "bandwagon of the oughts" (machine learning, regularization, model selection, model averaging, small n large p, genomics and other "omics", and so forth) but nothing I could tell you would tell you about what grad school in CS is really like.  Don't know, haven't been there, haven't done that. Hmm, I'm applying to grad school soon as well. What's the best way to get in contact with a professor from another university? I feel like they probably wouldn't respond to a cold email. What are some good things to ask them?  If you want a somewhat different area to think about, you might consider computer graphics and rendering (my area) specifically.  Modern rendering is essentially a statistical simulation of light transport throughout a scene.  There's a heavy dose of statistics, probability and simulation involved plus algorithmic effort to make the simulation run as quickly as possible.

[This paper](http://www.cs.cornell.edu/projects/manifolds-sg12/), for example, is a reasonable representation of the kinds of things that are state of the art these days and that you might like.                </snippet></document><document><title>Texts/Reading list on OS history?</title><url>http://www.reddit.com/r/compsci/comments/xc8n5/textsreading_list_on_os_history/</url><snippet>Is there any text out there that charts the technical progress of OSes from the 60s to present day? I'd like to get an overview of the field but can't seem to find anything.  If not, is there a comprehensive reading list?   I heard this spends a lot of time on hardware, but there may be some OS/software details as well.  Take a look at the TOC.  I'm on my phone, otherwise I'd look through it to make sure.  
http://www.amazon.com/History-Modern-Computing/dp/0262532034/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1343588292&amp;amp;sr=1-1&amp;amp;keywords=History+of+modern+computing

Plus, it's pretty cheap for an MIT Press.book. I would say that back in the 60s and 70s that OSs were tied closely to the hardware on which they ran. So, anytime you read about the OS software, you'll see that they were written for specific hardware.  So the HW layout and design heavily influenced the OS software design.  Am I off base here?  I've read  a bit about the history of computing, but I'm still exploring and learning. Um, yes and no. Computers from the 1970's up until the widespread adoption of SMP in the 90's and 00's more or less look the same for the most part: A single address space containing memory and bus devices. Some with dual address spaces (like the Intel x86 series which has IOPORT addressing in addition to physical memory). A PDP-11 looks much like a VAX, looks much like a SPARC looks much like an x86, the ISA is different, and the MMU has differences, but the memory model is basically the same.

What I find more interesting is the less common machines like the Lisp machines and stack machines, because they represent the road-not-taken. I often wonder if today, now that we have/have almost hit the thermal wall for CPUs, whether some of the hardware support for object and type tagging has something to offer computers today.

I am convinced that the flat, untyped, globally addressed memory model is actually technologically inferior, but that at any given time, the cost of building a new architecture and porting relevant software to it is higher than the cost of developing the next in the line of legacy-compatible hardware. Heh, reminds me of this blog post: http://www.yosefk.com/blog/the-high-level-cpu-challenge.html

We're at the point where a light sprinkling individual instructions to, say, test bits in a register are almost negligible in cost when you compare them to the cost of a branch misprediction, a cache miss, a pipeline stall, or similar. And folding the tests into an instruction doesn't hide the stalls that acting on them causes. Ok, I'll bite.

The specific feature I'd like to see:

Hardware memory allocation, I want a hardware blob that has interfaces like:

&amp;gt;CREATE-OBJECT &amp;lt;size&amp;gt; -&amp;gt; &amp;lt;ref-or-null&amp;gt;

&amp;gt;RESIZE-OBJECT &amp;lt;ref&amp;gt; &amp;lt;newsize&amp;gt; -&amp;gt; &amp;lt;ref-or-null&amp;gt;

&amp;gt;DESTROY-OBJECT &amp;lt;ref&amp;gt; -&amp;gt; nil

Here an object simply refers to as a variable sized array of words that can be accessed by reference+offset.

Then I want to be able to access the memory inside those objects by ref:offset duples. I want the hardware to automatically compact the heap with negligible influence on access latency. You might say "that sounds a lot like malloc, realloc, free" you'd be right, except RESIZE-OBJECT shouldn't change the value of ref, so that the program doesn't have to take out a lock and replace all the references to that object. This can be implemented in software by using indirect references, and looking them up in a dictionary (any function that returns a pointer from an integer, eg. hash table, array, btree).

I suppose it can also be implemented using sparse mmu allocations, with the upper bits of an address representing an object tag, and the lower bits an index, and mapping physical addresses to those addresses, but this results in a 4k granularity to object size, resulting in memory use inefficiency, and also requires an excessively large page table making it infeasible in practice.

I suppose the flaw in this type of addressing scheme, is that the hardware would then have to implement a reference-to-memory dictionary, and the space-efficient algorithms you would use in soft-dereferencing would be similarly time-inefficient as they are when implemented in software.

Now my next idea was to take this concept of a hardware heap with immutable references (the references don't change when the physical address of the storage changes), and combine it with a network of small 16-bit processors, each with their own 64k of physical memory, and a DMA mechanism adding two additional instructions to the above:

&amp;gt;GET-OBJECT &amp;lt;ref&amp;gt; &amp;lt;offset&amp;gt; &amp;lt;dest&amp;gt; &amp;lt;limit&amp;gt;

&amp;gt;RETURN-OBJECT &amp;lt;ref&amp;gt; &amp;lt;offset&amp;gt; &amp;lt;dest&amp;gt; &amp;lt;limit&amp;gt;

Which respectively DMA from and to the hardware heap manager, with transactional locking, so another processor making a GET-OBJECT request willl always see the previous version of the object, or the current one. It may require more operations or combinations of these operations rolled into atomic operations to be effective, for example it may be necessary to combine creating an object with it's initialisation, or returning an object with resizing it.

That is what I would like to see in hardware, though I have no idea if it is feasible or would result in performance gains.  &amp;gt; . I want the hardware to automatically compact the heap with negligible influence on access latency.

So the hardware would implement a conservative collector? or would you tag pointers as pointers somehow? I am by no means a GC expert.

The interface I defined has explicit allocation and deallocation, so it doesn't command any particular implementation of the hardware heap. I imagined a move-down type arangement, where one or more free-holes is tracked, and the heap controller continuously moves objects to fill the hole, effectively moving the whole up until it is contiguous with the bulk free memory. This would be a background activity, that is suspended when a request is made on the controller. I suspect this scheme is naive and a much better scheme is possible. Another scheme I imagined is to have a free-page bitmap and a discontiguous-page bitmap, and at the start of each page keep an index to the first free word in the page. When an object is deleted, the discontiguous bit for that page is set, which marks it a candidate for compaction.

The key question in all of this is how implementing it in silicon could provide performance improvements, and the only mechanisms I can see for that is either that a hardware heap controller is cheaper to implement than a dedicated CPU/timeslice for that purpose, or because some steps of heap compaction or tracking can be implemented as a parallel reduction, for example the bitmap memory could be connected to logic which evaluates the index of the first set bit of the bitmap, eliminating the scan that would be necessary in a software implementation.

One other possible strategy would be to have some kind of DRAM shift register combo which vastly increases the rate at which memory can be relocated. I don't know if that is feasible because I am not a semiconductor engineer.

You've really challenged me to think it through in more detail. Thank you.    I'm on my phone right now, but I'll be able to point you to some stuff when I'm back home.</snippet></document><document><title>Don Knuth releases Volume 4, Pre-fascicle 6A: A (Very Incomplete) Draft of Section 7.2.2.2: Satisfiability</title><url>http://www-cs-faculty.stanford.edu/~knuth/fasc6a.ps.gz</url><snippet>  We've gone from books, to fascicles, to pre-fascicles. His progress is becoming asymptotic. I salute him for facing up to the reality of the situation and trying to get what he can done and out while he can.

It's going to be a sad, sad day when he's gone. Maybe I'm just being negative but I can't help but think his project is somewhat misguided. If your book takes so long to write that the beginning is obsolete before you get to the end, I think you have to take a careful look at whether the book's scope was too large.  Having a single person try to summarize the entire field of computer science is just not viable in 2012.  It might not have even been viable when he started in in the 1960s.

I'm reminded of the Cathedral and the Bazaar.  Don Knuth is writing a book using the cathedral model. But for a good overview of an arbitrary topic in computer science, I turn first to the bazaar-constructed computer science areas of Wikipedia. It's hard for one person, even someone as remarkable as Knuth, to match the collective efforts of hundreds of people. I was actually thinking about starting to read his art of computer programming books, but now after your comment I'm wondering: how obsolete are his first books? Obviously there will be things that are obsolete, but is it to the point that it's not worth reading anymore? i suppose the amazon reviews are somewhat biased as they mostly admire knuth (obviously he's a genius, no doubt, but i would like an opinion of a person that disregards the fact that it was written by knuth and that the series *was* a milestone in his field).  A lot of algorithms are fundamental and haven't changed, so his description of those is in many cases quite relevant. His writing style is excellent. What I found most frustrating about the books was that in doing complexity analysis, he often painstakingly counts individual instructions; this is just not a level of scrutiny that any but the most specialized programs these days ever need. Even after his "MIX" assembly language was updated to "MMIX", I still found it far too low-level to be relevant to most tasks.

Honestly, though, my original comment was mostly motivated by Knuth himself, who says on [his web page](http://www-cs-faculty.stanford.edu/~uno/taocp.html)
&amp;gt; As I continue to write Volumes 4 and 5, I'll need to refer to topics that belong logically in Volumes 1--3 but weren't invented yet when I wrote those books.

 I suggest that the real value in TAoCP is not the detail of the algorithms themselves but the illustration of how to do a certain level and kind of thinking about algorithms. A level and kind of which, in fact, almost no&#8211;one needs to do, you're right. TAoCP isn't a programmers handbook or an encyclopaedia of algorithms, it's an exploration of a mode of thought.  A lot of algorithms are fundamental and haven't changed, so his description of those is in many cases quite relevant. His writing style is excellent. What I found most frustrating about the books was that in doing complexity analysis, he often painstakingly counts individual instructions; this is just not a level of scrutiny that any but the most specialized programs these days ever need. Even after his "MIX" assembly language was updated to "MMIX", I still found it far too low-level to be relevant to most tasks.

Honestly, though, my original comment was mostly motivated by Knuth himself, who says on [his web page](http://www-cs-faculty.stanford.edu/~uno/taocp.html)
&amp;gt; As I continue to write Volumes 4 and 5, I'll need to refer to topics that belong logically in Volumes 1--3 but weren't invented yet when I wrote those books.

 I was actually thinking about starting to read his art of computer programming books, but now after your comment I'm wondering: how obsolete are his first books? Obviously there will be things that are obsolete, but is it to the point that it's not worth reading anymore? i suppose the amazon reviews are somewhat biased as they mostly admire knuth (obviously he's a genius, no doubt, but i would like an opinion of a person that disregards the fact that it was written by knuth and that the series *was* a milestone in his field).  I was actually thinking about starting to read his art of computer programming books, but now after your comment I'm wondering: how obsolete are his first books? Obviously there will be things that are obsolete, but is it to the point that it's not worth reading anymore? i suppose the amazon reviews are somewhat biased as they mostly admire knuth (obviously he's a genius, no doubt, but i would like an opinion of a person that disregards the fact that it was written by knuth and that the series *was* a milestone in his field).  I was actually thinking about starting to read his art of computer programming books, but now after your comment I'm wondering: how obsolete are his first books? Obviously there will be things that are obsolete, but is it to the point that it's not worth reading anymore? i suppose the amazon reviews are somewhat biased as they mostly admire knuth (obviously he's a genius, no doubt, but i would like an opinion of a person that disregards the fact that it was written by knuth and that the series *was* a milestone in his field).  I was actually thinking about starting to read his art of computer programming books, but now after your comment I'm wondering: how obsolete are his first books? Obviously there will be things that are obsolete, but is it to the point that it's not worth reading anymore? i suppose the amazon reviews are somewhat biased as they mostly admire knuth (obviously he's a genius, no doubt, but i would like an opinion of a person that disregards the fact that it was written by knuth and that the series *was* a milestone in his field).  I was actually thinking about starting to read his art of computer programming books, but now after your comment I'm wondering: how obsolete are his first books? Obviously there will be things that are obsolete, but is it to the point that it's not worth reading anymore? i suppose the amazon reviews are somewhat biased as they mostly admire knuth (obviously he's a genius, no doubt, but i would like an opinion of a person that disregards the fact that it was written by knuth and that the series *was* a milestone in his field).   I sincerely fear losing Don Knuth. Dude, the man is so badass, that wouldn't stop him. Dude, the man is so badass, that wouldn't stop him.  where's the rest of the 7th chapter that has been made so far? can anyone post links if they're online? It has been published as volume 4A. You can find links to drafts [here](http://cs.utsa.edu/~wagner/knuth/)  When will Knuth release a bible of parallel computing algorithms. When will Knuth release a bible of parallel computing algorithms. Probably never, he doesn't really believe in benefits of parallel computing.</snippet></document><document><title>Followup by the Comp Sci professor leaving academia to work at Google</title><url>http://cs.unm.edu/~terran/academic_blog/?p=131</url><snippet>   The comment by "Neil DeGrasse Fan" at the bottom of that post is a classic example of the obnoxious anti-intellectualism academics encounter. I too am choosing to leave academia for many of the same reasons as Terran. 

On the more negative side, I must admit that people like "Neil DeGrasse Fan" have made me care *far* less about the general public than I used to. This is especially obnoxious

&amp;gt;He lets emotions dictate his personal opinions; this is indicative by his misunderstanding with who actually paid his way; he didn&#8217;t do it himself; someone else funded it and built it. The Republicans.

So this individual clearly thinks that working for the government means "you don't earn your own way in the world." I guess that must go for all employees of companies too, since someone "pays their way". No doubt this individual puts them in a different bucket though. Truly obnoxious.
 why not respond there, saying what you just did? The comment by "Neil DeGrasse Fan" at the bottom of that post is a classic example of the obnoxious anti-intellectualism academics encounter. I too am choosing to leave academia for many of the same reasons as Terran. 

On the more negative side, I must admit that people like "Neil DeGrasse Fan" have made me care *far* less about the general public than I used to. This is especially obnoxious

&amp;gt;He lets emotions dictate his personal opinions; this is indicative by his misunderstanding with who actually paid his way; he didn&#8217;t do it himself; someone else funded it and built it. The Republicans.

So this individual clearly thinks that working for the government means "you don't earn your own way in the world." I guess that must go for all employees of companies too, since someone "pays their way". No doubt this individual puts them in a different bucket though. Truly obnoxious.
 I think he was mocking Obama, for saying business owners didn't build it or do it themselves and it is justified since the professor incorrectly went on a tirade against Republicans. Maybe, but I've seen this kind of mentality many times before directed towards people who get paid via government money. Interestingly, the same "didn't do it yourself" mentality is never directed towards military or law enforcement, only academics, teachers, and other public sector government employees.

His attack on Terran's research ability is also awful: 

&amp;gt;You don&#8217;t have to be a genius to see that this &#8220;professor&#8221; is completely low-tier all around.

&amp;gt;1) His publication record is not that strong; only one first author paper in an elite conference.

&amp;gt;2) He was a professor at UNM &#8211; not a strong school by any stretch. Cleary, not good enough for more elite schools.

Seriously? This is Terran's publication record:

http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lane:Terran.html

The first author comment is cute. Many computer science journals follow mathematics and thus use alphabetical order. When names are listed by order of contribution, it's usually the case that those in more senior supervisory positions end up *last*. You can see this on many papers. 

But all of that is pretty irrelevant. Wether he is a top tier, middle tier, or even low tier researcher, that doesn't invalidate his observations. Attacking his professional ability to try to weaken his message is just the sort of slimy thing that is common in politics.  &amp;gt;The first author comment is cute. Many computer science journals follow mathematics and thus use alphabetical order.

For theory ones, yes.  However, as you can see, that isn't the case with his top-tier publications.  They're not alphabetical.

&amp;gt; it's usually the case that those in more senior supervisory positions end up last. You can see this on many papers. 

True, and that is true for a few of his top publications.  The rest were when he was also a student at Purdue or doing his postdoc. 

&amp;gt;Attacking his professional ability to try to weaken his message is just the sort of slimy thing that is common in politics. 

That's what he incorrectly did, though.  He went on a tirade and didn't even have the facts straight.  He opened up the door for this by making it political, and then getting it wrong. 

 What facts did he have wrong, exactly? He never said that science gets more funding during Democratic administrations than Republican administrations. (Which, despite the fact that NDT made it, is a silly comparison, since Congress sets the budget, not the executive branch.)

The only mention he made of Republicans was:

&amp;gt;  The current Republican-led poisonous political climate and Republican-orchestrated congressional melt-down has destroyed any chance of coherent, reasoned budget planning. In the face of these pressures, we have seen at least seven years of flat or declining funding for federal science programs and state legislatures slashing educational funding across the country.

And there are links for more details. &amp;gt;What facts did he have wrong, exactly? He never said that science gets more funding during Democratic administrations than Republican administrations.

He directly states that lack of science funding is the Republicans fault.  That isn't true.

&amp;gt;(Which, despite the fact that NDT made it, is a silly comparison, since Congress sets the budget, not the executive branch.)

Not entirely.  And, either way, all through Clinton and Bush, the Republicans controlled the House and Senate.  The Democrats didn't get in control until 2007.  

Since things like the DoE (the largest science body the US has) is directly below the President (it's a cabinet administration), he can allocate how DoE funds are split up.  

Allow me to elaborate with another quote.

"During President Bush&#8217;s first term, Federally funded R&amp;amp;D grew in the health sciences field, which was driven by Congress&#8217;s mission to double funding in this field by 2004. In his 2006 State of the Union address, President Bush unveiled his plan to overhaul education and Federally funded R&amp;amp;D in math and the physical sciences&#8212;chemistry, physics, Earth science, and astronomy. He vowed to substantially increase funding for basic research in science by increasing Federal investments in the NSF, DOE, and the NIST. His plan, the American Competitiveness Initiative, aimed to double for the budget for research in basic science to $50 billion by 2016. "

Similar data is found with Reagan and Bush Sr. 
 &amp;gt; all through Clinton and Bush, the Republicans controlled the House and Senate. The Democrats didn't get in control until 2007.

False. Democrats were the majority party in the Senate from June 6, 2001 through the end of the 107th Congress in 2003.

Again, I don't know how you missed this, but _he's not talking about that period!_ He's talking about the last 7 years, as you can see in my quote. For all we know, he thinks Republicans up through 2005 were excellent stewards of the government.

&amp;gt; During President Bush&#8217;s first term... Similar data is found with Reagan and Bush Sr.

Don't you have anything to back up your assertion that "at least seven years of flat or declining funding for federal science programs and state legislatures slashing educational funding across the country" is not the Republicans' fault?

EDIT: to get you started, [here](http://appropriations.house.gov/news/documentsingle.aspx?DocumentID=250023)'s what the Republican-led House appropriations committee decided to do with Obama's 2012 budget request: Fund NIST at 70% of the request, NOAA at 82%, NSF at 88%, NASA at 90%, the DEA at 97%, the FBI at 100%, and the Patent and Trademark Office at 100%. &amp;gt;False. Democrats were the majority party in the Senate from June 6, 2001 through the end of the 107th Congress in 2003.

False.  It was a 50-50 split. 

107th Congress (2001-2003)
Majority Party (Jan 3-20, 2001): Democrat (50 seats)
Minority Party: Republican (50 seats)

http://www.senate.gov/pagelayout/history/one_item_and_teasers/partydiv.htm

And the House was Republican.. and the Presidency was Republican.  You lose, easily.

&amp;gt;but he's not talking about that period! He's talking about the last 7 years

If you understood anything, you'd see that this is a perfect case of "rhetoric vs. policy".  People from 3rd tier institutions like yourself are not capable of differentiating between the two. 

&amp;gt; is not the Republicans' fault?

Do you have any proof that it *is* the Republicans fault?  Do you have anything other than rhetoric?  What about Executive Order 13589, which drastically cuts funding for the DoE, the largest pure science organization the US has?  In fact, the executive order is so severe, national labs aren't even publishing to conferences like the major Super Computing conference.  Even ORNL had to cancel their booth and demo this year.  All of that kills public science.

&amp;gt;Fund NIST at 70% of the request

NIST is the smallest organization and doesn't conduct much research to start with.

And your statement on the NSF is highly misleading; from your actual link:

&amp;gt;Within this funding, NSF&#8217;s core research is increased by $43 million to enhance basic research that is critical to innovation and U.S. economic competitiveness. 

So in reality, the House (R) increased science funding through the NSF.  Good job.

And things like NIST and NASA don't really fund research through universities, anyways.  Not that much, at least.  Things like the DOE and NSF do, which directly affect the author.  NASA, not so much.  &amp;gt; &amp;gt; Democrats were the majority party in the Senate from June 6, 2001 through the end of the 107th Congress in 2003.

&amp;gt; False. It was a 50-50 split.

&amp;gt; 107th Congress (2001-2003) Majority Party (Jan 3-20, 2001): Democrat (50 seats) Minority Party: Republican (50 seats)

Holy cow. I said starting June 6th 2001. From your own link:

&amp;gt; Majority Party (June 6, 2001-November 12, 2002 --): Democrat (50 seats)

&amp;gt; Minority Party: Republican (49 seats)

&amp;gt; Other Parties: 1

This is a really indisputable fact that's not open to interpretation. If you can admit that you were incorrect when you said "all through Clinton and Bush, the Republicans controlled the House and Senate" then I'd be happy to address your other claims that require more interpretation. But if you can't, you're just showing yourself to be willing to ignore basic facts, and I don't see any point continuing. &amp;gt;What facts did he have wrong, exactly? He never said that science gets more funding during Democratic administrations than Republican administrations.

He directly states that lack of science funding is the Republicans fault.  That isn't true.

&amp;gt;(Which, despite the fact that NDT made it, is a silly comparison, since Congress sets the budget, not the executive branch.)

Not entirely.  And, either way, all through Clinton and Bush, the Republicans controlled the House and Senate.  The Democrats didn't get in control until 2007.  

Since things like the DoE (the largest science body the US has) is directly below the President (it's a cabinet administration), he can allocate how DoE funds are split up.  

Allow me to elaborate with another quote.

"During President Bush&#8217;s first term, Federally funded R&amp;amp;D grew in the health sciences field, which was driven by Congress&#8217;s mission to double funding in this field by 2004. In his 2006 State of the Union address, President Bush unveiled his plan to overhaul education and Federally funded R&amp;amp;D in math and the physical sciences&#8212;chemistry, physics, Earth science, and astronomy. He vowed to substantially increase funding for basic research in science by increasing Federal investments in the NSF, DOE, and the NIST. His plan, the American Competitiveness Initiative, aimed to double for the budget for research in basic science to $50 billion by 2016. "

Similar data is found with Reagan and Bush Sr. 
 the fallacy fallacy. just because he argued his point badly doesn't make him wrong about everything. I don't have the knowledge of academia to know whether the rest of his point was right, but I'm inclined to just ignore the politics part because, well, right or wrong, it doesn't invalidate the rest of his point. &amp;gt;just because he argued his point badly doesn't make him wrong about everything.

Stawman fallacy.  I never said he was wrong about everything.

&amp;gt;but I'm inclined to just ignore the politics part because, well, right or wrong, it doesn't invalidate the rest of his point.

Great, but he opened himself up to attacks when he decided to get political.  Can I have his post?</snippet></document><document><title>First glider discovered in a cellular automata on an aperiodic tiling.</title><url>https://plus.google.com/110214848059767137292/posts/ZBdwythH2RF</url><snippet>  This is interesting, but I'm somewhat baffled by the popularity of cellular automata (as opposed to other research areas). 

Accessibility helps, I realize, but there isn't much here besides closely examining a specific system and tailoring a set of rules to it. 

There is some very interesting work being done on the relations of automata to words, but the existence of gliders doesn't seem to be much more than a cute application.  Just my two cents:

There is a very close analogy to fundamental physics and cellular automata.  Notice that right off the bat you get things that behave like particles, locality and a natural concept of "speed of light", that is, a maximum speed at which "particles" can travel.  CA won't ever have a hope of explaining some of the more sensational predictions of quantum mechanics, but as a model it might give us some fundamental insight.  For example, [Miller and Fredkin](http://arxiv.org/abs/1206.2060) just published a paper on finding patterns in a (regular) cellular automata that travel in circular paths, giving an analogy of orbital motion of particles.

But backing up even further and addressing your point about tailoring rules to find patterns, this is still "active research", as far as I know, but the idea is that this might not be true at all.  The concept of complexity is captured by Turing machine equivalence.  You might have heard that [Rule 110 is Turing machine equivalent](http://en.wikipedia.org/wiki/Rule_110#The_proof_of_universality) or that [Conway's game of life is Turing machine equivalent](http://rendell-attic.org/gol/tm.htm).  The idea is that these might be the rule rather than the exception.  Turing machine equivalence might be something that naturally and commonly falls out of any system that isn't pathological.  This is what Wolfram describes as the [principle of computation equivalence](http://mathworld.wolfram.com/PrincipleofComputationalEquivalence.html).

While researches might tailor a system in order to discover a particular feature, the idea is that observations about a particular system can probably be made to most systems, regardless of the differences in rules.

And now it looks like this statement can be made regardless of cellular structure! This is interesting, but I'm somewhat baffled by the popularity of cellular automata (as opposed to other research areas). 

Accessibility helps, I realize, but there isn't much here besides closely examining a specific system and tailoring a set of rules to it. 

There is some very interesting work being done on the relations of automata to words, but the existence of gliders doesn't seem to be much more than a cute application.  Automata to words? You mean for NLP? Got links?  Can someone explain to me like I'm a fifth grader what I'm looking at? Hacker News has some simple explanations of this.
https://news.ycombinator.com/item?id=4298515 Did you discover this glider? Can someone explain to me like I'm a fifth grader what I'm looking at?  Shit!

I thought that was impossible!

Edit: Ah, not classic Life. Does classic Life even make sense on a Penrose tiling? Well, the rules only mention 'live' neighbors, so it should 'work'.

This is the first I read about Penrose tiling automata, but if neighborhood is defined by sharing edges, then it seems that every cell has exactly 4 neighbors. On the other hand, considering vertex neighborhood, it seems to me that some cells have 8 neighbors and some have 9. The patterns should be completely different either way Does classic Life even make sense on a Penrose tiling?</snippet></document><document><title>Xpost from r/askcomputerscience: Question about factoring</title><url>http://www.reddit.com/r/AskComputerScience/comments/x9v1j/question_about_factoring/</url><snippet /></document><document><title>edX full course catalogue now available: includes Intro CS (MIT + Harvard), SaaS &amp;amp; AI (Berkeley)</title><url>https://www.edx.org/</url><snippet>  HarvardX's CS50x or MITx's 6.00x... HarvardX's CS50x or MITx's 6.00x...  So I'm paying $11,000 a year for some shit they are giving away for free? Only $11k/year? Which school are *you* going to? Berkeley So I'm paying $11,000 a year for some shit they are giving away for free?     Just registered for two classes. Quick question though: does this count towards college credits whatsoever? If so, that would be even more amazing! They award a certificate which is GPG signed by the respective institution but unfortunately it does not count towards college credit.  </snippet></document><document><title>How long does it take you guys to program? (give a basis if you can)</title><url>http://www.reddit.com/r/compsci/comments/x5hkb/how_long_does_it_take_you_guys_to_program_give_a/</url><snippet>I'm trying to figure out if I'm extremely slow or not since I'm learning and a perfectionist, but when you start programming something you never did before.. typically what is the difference between how long you think it will take you and how long it actually takes you?

I recently wrote some code that I could practically visualize the whole thing in my had beforehand so I figured 2-3 hours to write.  ended up taking 20+.  I think its because I do not type very fast.  

follow up question, what slows you down while programming?  I tend to feel physically limited a lot when I write.

*Meant to say Code not program in title, but you catch my drift  Always longer than I expect, even when I expect it to take longer than I expect. Always longer than I expect, even when I expect it to take longer than I expect. this is how I feel. this is how I feel.  How long does it take you to solve a problem?

Seriously - it depends.  Back in freshman college, I translated an old high school program of mine from Pascal to C for practice (I was learning C at the time) - it didn't work right.  I spent over a month trying to debug the damn thing, and when I finally found the error, it was a '-' sign where there should have been a '+' sign in a rotation matrix.  I remember distinctly changing the sign, compiling, the program running perfectly, then sitting there in a silent fuming mixture of pent-up hatred, stupidity, frustration, and pure loathing directed at this one tiny little error that had f'd up my program for weeks... for *hundreds* of hours of programming time.... and then I turned off the computer and didn't touch it again for at least a month.

It took me a long time to get over that - it was worse than any breakup I'd ever had. &amp;gt; I spent over a month trying to debug the damn thing, and when I finally found the error, it was a '-' sign where there should have been a '+' sign in a rotation matrix.

Too many programmers spend an exorbitant amount of time debugging a problem only to feel that it was all wasted because the fix for the problem was "a single line of code".  We've all had that head-slapping moment when it all becomes clear and the solution seems so simple.  The reason the solution seems so simple is that you spent an exorbitant amount of time debugging the problem.

The time you spend debugging is worth every bit as much as the time you spend writing code.  The important thing is that each time you debug a problem, it will get easier and faster.  You spent an entire month debugging a problem that next time will take you 10 seconds.

Debugging code is a skill; and a very important one at that.    As a bonus, it will also help you write better code, faster, and on the first try.  After all, if you know the solution to anything that could go wrong, you can just avoid them to begin with.

So please don't look at your hundreds of hours of programming time as having gone to waste; it's quite the opposite.  It was hard work, and I'm sure it made you understand every single line in that program.  That's some good shit. &amp;gt;So please don't look at your hundreds of hours of programming time as having gone to waste; it's quite the opposite. It was hard work, and I'm sure it made you understand every single line in that program. That's some good shit.

Heh... that was over ten years ago for me - nearly twenty, actually, and I've been working in software development for the past fifteen, so while the kind encouragement is appreciated as intended, it's not necessary in my case.  I've worked well past the point of thinking it was wasted effort, but at the time, I wanted to take a bat to my CRT in the worst way. I had a feeling that this was the case, but I wanted to make the point in general. &amp;gt; I spent over a month trying to debug the damn thing, and when I finally found the error, it was a '-' sign where there should have been a '+' sign in a rotation matrix.

Too many programmers spend an exorbitant amount of time debugging a problem only to feel that it was all wasted because the fix for the problem was "a single line of code".  We've all had that head-slapping moment when it all becomes clear and the solution seems so simple.  The reason the solution seems so simple is that you spent an exorbitant amount of time debugging the problem.

The time you spend debugging is worth every bit as much as the time you spend writing code.  The important thing is that each time you debug a problem, it will get easier and faster.  You spent an entire month debugging a problem that next time will take you 10 seconds.

Debugging code is a skill; and a very important one at that.    As a bonus, it will also help you write better code, faster, and on the first try.  After all, if you know the solution to anything that could go wrong, you can just avoid them to begin with.

So please don't look at your hundreds of hours of programming time as having gone to waste; it's quite the opposite.  It was hard work, and I'm sure it made you understand every single line in that program.  That's some good shit. &amp;gt; I spent over a month trying to debug the damn thing, and when I finally found the error, it was a '-' sign where there should have been a '+' sign in a rotation matrix.

Too many programmers spend an exorbitant amount of time debugging a problem only to feel that it was all wasted because the fix for the problem was "a single line of code".  We've all had that head-slapping moment when it all becomes clear and the solution seems so simple.  The reason the solution seems so simple is that you spent an exorbitant amount of time debugging the problem.

The time you spend debugging is worth every bit as much as the time you spend writing code.  The important thing is that each time you debug a problem, it will get easier and faster.  You spent an entire month debugging a problem that next time will take you 10 seconds.

Debugging code is a skill; and a very important one at that.    As a bonus, it will also help you write better code, faster, and on the first try.  After all, if you know the solution to anything that could go wrong, you can just avoid them to begin with.

So please don't look at your hundreds of hours of programming time as having gone to waste; it's quite the opposite.  It was hard work, and I'm sure it made you understand every single line in that program.  That's some good shit. &amp;gt; I spent over a month trying to debug the damn thing, and when I finally found the error, it was a '-' sign where there should have been a '+' sign in a rotation matrix.

Too many programmers spend an exorbitant amount of time debugging a problem only to feel that it was all wasted because the fix for the problem was "a single line of code".  We've all had that head-slapping moment when it all becomes clear and the solution seems so simple.  The reason the solution seems so simple is that you spent an exorbitant amount of time debugging the problem.

The time you spend debugging is worth every bit as much as the time you spend writing code.  The important thing is that each time you debug a problem, it will get easier and faster.  You spent an entire month debugging a problem that next time will take you 10 seconds.

Debugging code is a skill; and a very important one at that.    As a bonus, it will also help you write better code, faster, and on the first try.  After all, if you know the solution to anything that could go wrong, you can just avoid them to begin with.

So please don't look at your hundreds of hours of programming time as having gone to waste; it's quite the opposite.  It was hard work, and I'm sure it made you understand every single line in that program.  That's some good shit. I agree generally. But I also think that programming languages should be better than that-- It should be obvious when a mistake is being made. I think asking a programming language to adequately predict what you intend to do with Euclidian geometry is a little bit much at this point... I agree generally. But I also think that programming languages should be better than that-- It should be obvious when a mistake is being made. How long does it take you to solve a problem?

Seriously - it depends.  Back in freshman college, I translated an old high school program of mine from Pascal to C for practice (I was learning C at the time) - it didn't work right.  I spent over a month trying to debug the damn thing, and when I finally found the error, it was a '-' sign where there should have been a '+' sign in a rotation matrix.  I remember distinctly changing the sign, compiling, the program running perfectly, then sitting there in a silent fuming mixture of pent-up hatred, stupidity, frustration, and pure loathing directed at this one tiny little error that had f'd up my program for weeks... for *hundreds* of hours of programming time.... and then I turned off the computer and didn't touch it again for at least a month.

It took me a long time to get over that - it was worse than any breakup I'd ever had. Every time I solve a problem like that, I notice that in the future my brain gets better at knowing exactly what tests to use to make debugging shorter. I mean, obviously, you do something and you get better at it, but it's interesting to just *know* what's probably wrong with a program.

The most amazing thing, though, is when a program works as expected, with no bugs, at first compile. I feel like a wizard. When something complex compiles the first time you try, it is time to be scared, very scared. Compiling on the first try is not too scary. Now, a complex piece of software actually apparently working in the first try, now that's downright terrifying! Compiling on the first try is not too scary. Now, a complex piece of software actually apparently working in the first try, now that's downright terrifying! Compiling on the first try is not too scary. Now, a complex piece of software actually apparently working in the first try, now that's downright terrifying! If it makes you any better.. given infinite amount of time, even a monkey can write a complex piece of software that actually works on the first try. When something complex compiles the first time you try, it is time to be scared, very scared. When something complex compiles the first time you try, it is time to be scared, very scared. If your having problem making your program compile, you should be scared, very scared. I don't know why this is being down voted. Modern IDEs do wonders to ensure you write syntactically correct code Because no one was talking about syntax errors.  They were talking about semantic errors. Because no one was talking about syntax errors.  They were talking about semantic errors. I don't know why this is being down voted. Modern IDEs do wonders to ensure you write syntactically correct code How long does it take you to solve a problem?

Seriously - it depends.  Back in freshman college, I translated an old high school program of mine from Pascal to C for practice (I was learning C at the time) - it didn't work right.  I spent over a month trying to debug the damn thing, and when I finally found the error, it was a '-' sign where there should have been a '+' sign in a rotation matrix.  I remember distinctly changing the sign, compiling, the program running perfectly, then sitting there in a silent fuming mixture of pent-up hatred, stupidity, frustration, and pure loathing directed at this one tiny little error that had f'd up my program for weeks... for *hundreds* of hours of programming time.... and then I turned off the computer and didn't touch it again for at least a month.

It took me a long time to get over that - it was worse than any breakup I'd ever had. There are two ways to spend that hundreds of hours of programming time looking for that bug.  The difference is that when you are done, to show for it you have either:

1. A fix for that one-character error, or
2. A large test suite with good code coverage _and_ a fix for that one-character error.

Do the latter. How long does it take you to solve a problem?

Seriously - it depends.  Back in freshman college, I translated an old high school program of mine from Pascal to C for practice (I was learning C at the time) - it didn't work right.  I spent over a month trying to debug the damn thing, and when I finally found the error, it was a '-' sign where there should have been a '+' sign in a rotation matrix.  I remember distinctly changing the sign, compiling, the program running perfectly, then sitting there in a silent fuming mixture of pent-up hatred, stupidity, frustration, and pure loathing directed at this one tiny little error that had f'd up my program for weeks... for *hundreds* of hours of programming time.... and then I turned off the computer and didn't touch it again for at least a month.

It took me a long time to get over that - it was worse than any breakup I'd ever had. How long does it take you to solve a problem?

Seriously - it depends.  Back in freshman college, I translated an old high school program of mine from Pascal to C for practice (I was learning C at the time) - it didn't work right.  I spent over a month trying to debug the damn thing, and when I finally found the error, it was a '-' sign where there should have been a '+' sign in a rotation matrix.  I remember distinctly changing the sign, compiling, the program running perfectly, then sitting there in a silent fuming mixture of pent-up hatred, stupidity, frustration, and pure loathing directed at this one tiny little error that had f'd up my program for weeks... for *hundreds* of hours of programming time.... and then I turned off the computer and didn't touch it again for at least a month.

It took me a long time to get over that - it was worse than any breakup I'd ever had. How long does it take you to solve a problem?

Seriously - it depends.  Back in freshman college, I translated an old high school program of mine from Pascal to C for practice (I was learning C at the time) - it didn't work right.  I spent over a month trying to debug the damn thing, and when I finally found the error, it was a '-' sign where there should have been a '+' sign in a rotation matrix.  I remember distinctly changing the sign, compiling, the program running perfectly, then sitting there in a silent fuming mixture of pent-up hatred, stupidity, frustration, and pure loathing directed at this one tiny little error that had f'd up my program for weeks... for *hundreds* of hours of programming time.... and then I turned off the computer and didn't touch it again for at least a month.

It took me a long time to get over that - it was worse than any breakup I'd ever had. How long does it take you to solve a problem?

Seriously - it depends.  Back in freshman college, I translated an old high school program of mine from Pascal to C for practice (I was learning C at the time) - it didn't work right.  I spent over a month trying to debug the damn thing, and when I finally found the error, it was a '-' sign where there should have been a '+' sign in a rotation matrix.  I remember distinctly changing the sign, compiling, the program running perfectly, then sitting there in a silent fuming mixture of pent-up hatred, stupidity, frustration, and pure loathing directed at this one tiny little error that had f'd up my program for weeks... for *hundreds* of hours of programming time.... and then I turned off the computer and didn't touch it again for at least a month.

It took me a long time to get over that - it was worse than any breakup I'd ever had. I was recently helping a friend of mine do a Pascal's Triangle in Java -- probably the most trivial programming project I've done in a while.  After writing it perfectly, I spent hours wondering why the numbers were completely thrown off.  It turned out that I forgot to add a 1 to the next row array.  I felt incredibly stupid and foolish. How long does it take you to solve a problem?

Seriously - it depends.  Back in freshman college, I translated an old high school program of mine from Pascal to C for practice (I was learning C at the time) - it didn't work right.  I spent over a month trying to debug the damn thing, and when I finally found the error, it was a '-' sign where there should have been a '+' sign in a rotation matrix.  I remember distinctly changing the sign, compiling, the program running perfectly, then sitting there in a silent fuming mixture of pent-up hatred, stupidity, frustration, and pure loathing directed at this one tiny little error that had f'd up my program for weeks... for *hundreds* of hours of programming time.... and then I turned off the computer and didn't touch it again for at least a month.

It took me a long time to get over that - it was worse than any breakup I'd ever had. How long does it take you to solve a problem?

Seriously - it depends.  Back in freshman college, I translated an old high school program of mine from Pascal to C for practice (I was learning C at the time) - it didn't work right.  I spent over a month trying to debug the damn thing, and when I finally found the error, it was a '-' sign where there should have been a '+' sign in a rotation matrix.  I remember distinctly changing the sign, compiling, the program running perfectly, then sitting there in a silent fuming mixture of pent-up hatred, stupidity, frustration, and pure loathing directed at this one tiny little error that had f'd up my program for weeks... for *hundreds* of hours of programming time.... and then I turned off the computer and didn't touch it again for at least a month.

It took me a long time to get over that - it was worse than any breakup I'd ever had. How long does it take you to solve a problem?

Seriously - it depends.  Back in freshman college, I translated an old high school program of mine from Pascal to C for practice (I was learning C at the time) - it didn't work right.  I spent over a month trying to debug the damn thing, and when I finally found the error, it was a '-' sign where there should have been a '+' sign in a rotation matrix.  I remember distinctly changing the sign, compiling, the program running perfectly, then sitting there in a silent fuming mixture of pent-up hatred, stupidity, frustration, and pure loathing directed at this one tiny little error that had f'd up my program for weeks... for *hundreds* of hours of programming time.... and then I turned off the computer and didn't touch it again for at least a month.

It took me a long time to get over that - it was worse than any breakup I'd ever had. Sorry, but I don't understand these mistakes.  Wouldn't stepping through with a debugger once take less than an hour and find mistakes like these?  The only thing that should take hundreds of hours of time is huge overall design flaws.

I know you were a freshman and everything, but I just see these types of posts everywhere like "oh man I forgot a semi colon and it took days to fix" and get so confused. Debugger would not have helped in that case - I was not then, am not now, and seriously doubt I will *ever* be good enough to catch incorrect floating point and trigonometric values from a matrix.  There are some errors that only code inspection can fix. Debugger would not have helped in that case - I was not then, am not now, and seriously doubt I will *ever* be good enough to catch incorrect floating point and trigonometric values from a matrix.  There are some errors that only code inspection can fix. Sorry, but I don't understand these mistakes.  Wouldn't stepping through with a debugger once take less than an hour and find mistakes like these?  The only thing that should take hundreds of hours of time is huge overall design flaws.

I know you were a freshman and everything, but I just see these types of posts everywhere like "oh man I forgot a semi colon and it took days to fix" and get so confused. Sorry, but I don't understand these mistakes.  Wouldn't stepping through with a debugger once take less than an hour and find mistakes like these?  The only thing that should take hundreds of hours of time is huge overall design flaws.

I know you were a freshman and everything, but I just see these types of posts everywhere like "oh man I forgot a semi colon and it took days to fix" and get so confused.  If your typing speed is the limiting factor in your coding, then you're a genius! Or quadriplegic.  (Which is not to imply that they're mutually exclusive.) If your typing speed is the limiting factor in your coding, then you're a genius! I tend not to write a line unless I know *exactly* what is going on.  What happens though, is I'll spend a lot of time researching my thought before writing anything, checking for libraries and such, and then I get a burst of code I know I want to write.. but by the time I finish the first couple lines I get too mixed up to remember the rest.  this leads to a lot of half finished thoughts which I was trying to avoid in the first place.

If I ever can sit back and say "I dont quite know what I did, but it works" I get extremely uncomfortable and won't move on until I know. I tend not to write a line unless I know *exactly* what is going on.  What happens though, is I'll spend a lot of time researching my thought before writing anything, checking for libraries and such, and then I get a burst of code I know I want to write.. but by the time I finish the first couple lines I get too mixed up to remember the rest.  this leads to a lot of half finished thoughts which I was trying to avoid in the first place.

If I ever can sit back and say "I dont quite know what I did, but it works" I get extremely uncomfortable and won't move on until I know.  Oh, estimates! The bane of every programmer, project manager, and city official. Us poor humans are naturally bad at estimating things; especially things we have little experience with.

If you're curious, I recommend a book by Daniel Kahnman titled "Thinking: Fast and Slow". He details many biases and heuristics that we apply without even knowing it and one of them is the "planning fallacy". The problem basically comes down to the fact that we don't tend to think about what could go wrong. Instead, we visualize the best-case and maybe add some small fudge factor. The less experience we have, the less we realize how many things could go wrong and the worse we underestimate the time needed.

The best thing you can do to improve your estimates is to keep track of past endeavors. Record your estimate, what things happened that you didn't expect, and how long it really took. Use that as a baseline when predicting future projects. Try to avoid thinking about what you "expect" will happen and instead refer only to your recorded history. You can make small adjustments based on the current situation, if you want, but always use the historical record as the initial estimate and only adjust from there.

Aside from that, know that programming is a skill and with experience comes expertise. Even with 10 years of practiced experience, I still find myself taking far longer than I expect when met with a new challenge. For example, I typically write Python code, but took on a side project last year that was more appropriate to do in C#. I spent weeks ahead of time reading up on the language, but every time I sat down to code, I would spend 80-90% of my time reading documentation to figure out the various classes, arguments, and other things that I couldn't possibly have memorized. The same thing happens to me when I use a new module or framework in Python. If I was just writing a simple Python script, I could write a screen full in minutes, but if there is anything I'm not familiar with (be it a module, class, etc.), it could be over an hour while I pour over the documentation. Likewise if a bug occurs that I don't understand; it could be hours to debug it.

And the difference in ability between coders can vary a lot. Some past research has shown productivity to vary by a factor of up to 20x, even for people with similar experience in the same company. The difference between a freshman in college and an experienced, full-time programmer is likely much more than that. As you learn and gain experience, you will improve. Typing speed, memory, debugging skills; everything becomes easier and faster.

As others have mentioned, one thing you can do to help yourself out is to test as you go. Writing a thousand lines of code, compiling, and running, will almost never end well. Finding a bug in a thousand lines of code is hard; finding the same bug in a single 20 line function is much easier. Unit tests will help narrow down the problem and catch bugs as they occur. Starting small and building iteratively will help catch mistakes sooner, reduce the scope of things that need to be checked when a bug occurs, and prevent you from wasting lots of effort going down a blind alley.

Last, but not least, never measure your progress in lines of code. Some of the best programmers are the ones that think, read, and plan for an hour only to write a 5 line function that does exactly what they want. Writing lots of code quickly is rarely a good sign. The best code is well-understood, well-considered, and does what is intended in the simplest, cleanest way possible. Even then, almost nobody writes "perfect" code the first time. Your first attempt probably won't work. Your second attempt probably works, but it's poorly written. Maybe by the third time you'll have something half maintainable. Six months later, somebody else will point out a mistake you completely missed. Like a sculpture, good code starts with a rough idea and is hammered out and whittled down, taking a bit here and there, until every mark is perfect. And then somebody changes the requirements and you rip half of it up to start again.

All in all, don't be concerned about your speed. Good code takes time. Learn, understand, and perfect; don't rush. Oh, estimates! The bane of every programmer, project manager, and city official. Us poor humans are naturally bad at estimating things; especially things we have little experience with.

If you're curious, I recommend a book by Daniel Kahnman titled "Thinking: Fast and Slow". He details many biases and heuristics that we apply without even knowing it and one of them is the "planning fallacy". The problem basically comes down to the fact that we don't tend to think about what could go wrong. Instead, we visualize the best-case and maybe add some small fudge factor. The less experience we have, the less we realize how many things could go wrong and the worse we underestimate the time needed.

The best thing you can do to improve your estimates is to keep track of past endeavors. Record your estimate, what things happened that you didn't expect, and how long it really took. Use that as a baseline when predicting future projects. Try to avoid thinking about what you "expect" will happen and instead refer only to your recorded history. You can make small adjustments based on the current situation, if you want, but always use the historical record as the initial estimate and only adjust from there.

Aside from that, know that programming is a skill and with experience comes expertise. Even with 10 years of practiced experience, I still find myself taking far longer than I expect when met with a new challenge. For example, I typically write Python code, but took on a side project last year that was more appropriate to do in C#. I spent weeks ahead of time reading up on the language, but every time I sat down to code, I would spend 80-90% of my time reading documentation to figure out the various classes, arguments, and other things that I couldn't possibly have memorized. The same thing happens to me when I use a new module or framework in Python. If I was just writing a simple Python script, I could write a screen full in minutes, but if there is anything I'm not familiar with (be it a module, class, etc.), it could be over an hour while I pour over the documentation. Likewise if a bug occurs that I don't understand; it could be hours to debug it.

And the difference in ability between coders can vary a lot. Some past research has shown productivity to vary by a factor of up to 20x, even for people with similar experience in the same company. The difference between a freshman in college and an experienced, full-time programmer is likely much more than that. As you learn and gain experience, you will improve. Typing speed, memory, debugging skills; everything becomes easier and faster.

As others have mentioned, one thing you can do to help yourself out is to test as you go. Writing a thousand lines of code, compiling, and running, will almost never end well. Finding a bug in a thousand lines of code is hard; finding the same bug in a single 20 line function is much easier. Unit tests will help narrow down the problem and catch bugs as they occur. Starting small and building iteratively will help catch mistakes sooner, reduce the scope of things that need to be checked when a bug occurs, and prevent you from wasting lots of effort going down a blind alley.

Last, but not least, never measure your progress in lines of code. Some of the best programmers are the ones that think, read, and plan for an hour only to write a 5 line function that does exactly what they want. Writing lots of code quickly is rarely a good sign. The best code is well-understood, well-considered, and does what is intended in the simplest, cleanest way possible. Even then, almost nobody writes "perfect" code the first time. Your first attempt probably won't work. Your second attempt probably works, but it's poorly written. Maybe by the third time you'll have something half maintainable. Six months later, somebody else will point out a mistake you completely missed. Like a sculpture, good code starts with a rough idea and is hammered out and whittled down, taking a bit here and there, until every mark is perfect. And then somebody changes the requirements and you rip half of it up to start again.

All in all, don't be concerned about your speed. Good code takes time. Learn, understand, and perfect; don't rush.      I am assignment-in-conditional's bitch

    if (a = b)...

I have lost hours and hours to that over the years. I know that feel, bro.     gcc -Wall Or a half decent IDE will yellow underline "possible assignments in conditional statement" I am assignment-in-conditional's bitch

    if (a = b)...

I have lost hours and hours to that over the years. I know that feel, bro. most of the time I wonder why they even let you do that, and then once in a great while I use it effectively, and feel like a good C hacker. still not sure it's worth it. most of the time I wonder why they even let you do that, and then once in a great while I use it effectively, and feel like a good C hacker. still not sure it's worth it. most of the time I wonder why they even let you do that, and then once in a great while I use it effectively, and feel like a good C hacker. still not sure it's worth it. most of the time I wonder why they even let you do that, and then once in a great while I use it effectively, and feel like a good C hacker. still not sure it's worth it. I am assignment-in-conditional's bitch

    if (a = b)...

I have lost hours and hours to that over the years. I know that feel, bro. Doesn't help in the variable to variable comparison case, but to avoid the issue when comparing to a constant, it helps to write your comparisons with the constant term on the left:

&amp;gt; if (1 == a)...

would become the invalid

&amp;gt; if (1 = a) ...

if you omitted an = by accident, and that would just refuse to compile in most languages. 

But if you'd done:

&amp;gt; if (a == 1)

and omitted the = you'd have

&amp;gt; if (a = 1)

which would be your old friend accidental assignment.

But more generally, just set your compiler or IDE to warn you about these.

 Doesn't help in the variable to variable comparison case, but to avoid the issue when comparing to a constant, it helps to write your comparisons with the constant term on the left:

&amp;gt; if (1 == a)...

would become the invalid

&amp;gt; if (1 = a) ...

if you omitted an = by accident, and that would just refuse to compile in most languages. 

But if you'd done:

&amp;gt; if (a == 1)

and omitted the = you'd have

&amp;gt; if (a = 1)

which would be your old friend accidental assignment.

But more generally, just set your compiler or IDE to warn you about these.

    I find that testing the shit out of my code **as I'm writing it** winds up being a huge time-saver in the long run (although it obviously takes longer in the short run).

What I mean is that after I write any piece of non-trivial code that can produce some output I can look at, I hop into another terminal and start running it and playing with it. The amount of code I write before I test is often just a single line or a single loop. 

The idea is that it's waaaaaaaay easier to debug a single line than it is to write dozens or hundreds of lines and *then* debug.

Of course, you can't wholly depend on this approach with code whose behavior depends on random events, or events outside of your control, or synchronization issues, although I feel like this approach is still a great way to tackle complex programs that deal with these (and other) issues. It's also an excellent way to carefully write programs in a language you're unfamiliar with.   Typing is not the limiting factoring on writing code.  Studies (which I can't find now) have shown that if you were to sit down and retype all the code you will write in a year, it would only take a few weeks.  Programming is a thinking/problem solving game.

Also, programmers are inherently optimistic.  Estimation is one of the hardest parts of the job and I strongly recommend any new developers put special effort into trying to learn that skill (it'll save you a lot of grey hair) I didn't mean to imply that overall typing speed is the limiting factor.. for so for localized things im working on.  

After reading everyones responses though I think my problem with typing speed will go away as I write more code and get more comfortable with general syntax. At the moment,  I will work out the syntax for a couple lines in my head and then forget it when I start writing     How long does it take you to walk?                             </snippet></document></searchresult>
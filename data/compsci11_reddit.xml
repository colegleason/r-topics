<searchresult><compsci /><document><title>Interesting blog post by a tenured computer science professor on why he is leaving academia for Google.</title><url>http://cs.unm.edu/~terran/academic_blog/?p=113</url><snippet>  Well that was depressing. Agreed. I am entering my junior year of Uni and my future plans of employment have been battling with one another and will probably continue to do so for a while. I am torn between academia, federal employment, and the private sector. They all have their benefits and this article just gives me more to think about. It's unnerving and slightly depressing not having any plans for life after graduation. Agreed. I am entering my junior year of Uni and my future plans of employment have been battling with one another and will probably continue to do so for a while. I am torn between academia, federal employment, and the private sector. They all have their benefits and this article just gives me more to think about. It's unnerving and slightly depressing not having any plans for life after graduation. [deleted]  Oh weird. Terran's office ~~is~~ was two doors down from mine. AMA. Just kidding, or not.

This makes me doubly sad too as someone who currently thinks they want to be a college professor of computer science (I'm a phd student), and because I dropped his machine learning class before I realized I would never have the opportunity to take it again! Haha, Terran has filled in for a few lectures that I took with Luger.  He also ran the colloquium I attended last semester.  Small world, I guess. Oh weird. Terran's office ~~is~~ was two doors down from mine. AMA. Just kidding, or not.

This makes me doubly sad too as someone who currently thinks they want to be a college professor of computer science (I'm a phd student), and because I dropped his machine learning class before I realized I would never have the opportunity to take it again!  Interesting material, but I think his reasons regarding cultural misunderstanding of science and the behavior of the Republican party seems silly. Low salaries, inflexible bosses, and overspecialization are all fine reasons to leave the field but anti-intellectualism among some politicians just seems like a bizarre reason to change careers. I he intending this to be a protest piece against the current political climate, because I guarantee that nobody who currently distrusts science will change their mind because of this.  It's a fine reason when your job depends on the whims of those politicians. Funding of public universities has been rough lately. Interesting material, but I think his reasons regarding cultural misunderstanding of science and the behavior of the Republican party seems silly. Low salaries, inflexible bosses, and overspecialization are all fine reasons to leave the field but anti-intellectualism among some politicians just seems like a bizarre reason to change careers. I he intending this to be a protest piece against the current political climate, because I guarantee that nobody who currently distrusts science will change their mind because of this.  the administration of academia is subject to those larger cultural forces.
 [Republicans fund science](http://www.youtube.com/watch?v=x7Q8UvJ1wvk) more than the Democrats do.  Someone posted this below and it's an interesting watch; that's why politicians pander to the extreme parts of their base, but never rule that way. Someone mentioned that when republicans were in charge; the democrats were in charge of congess and vice-versa. Someone mentioned that when republicans were in charge; the democrats were in charge of congess and vice-versa. If you look, that wasn't true.  The Republicans were completely in charge during Bush, until 2007. [Republicans fund science](http://www.youtube.com/watch?v=x7Q8UvJ1wvk) more than the Democrats do.  Someone posted this below and it's an interesting watch; that's why politicians pander to the extreme parts of their base, but never rule that way. That's all true and well and good, but if the "new leaders" of the Republican party have their way, it's not going to be pretty, so they need to not "take our country back" they need to take their party back. [Republicans fund science](http://www.youtube.com/watch?v=x7Q8UvJ1wvk) more than the Democrats do.  Someone posted this below and it's an interesting watch; that's why politicians pander to the extreme parts of their base, but never rule that way. Interesting material, but I think his reasons regarding cultural misunderstanding of science and the behavior of the Republican party seems silly. Low salaries, inflexible bosses, and overspecialization are all fine reasons to leave the field but anti-intellectualism among some politicians just seems like a bizarre reason to change careers. I he intending this to be a protest piece against the current political climate, because I guarantee that nobody who currently distrusts science will change their mind because of this.  He was pretty clear that his beef with the GOP is that they are reducing funding for basic science. This doesn't seem "silly" to me, but rather on point. Interesting material, but I think his reasons regarding cultural misunderstanding of science and the behavior of the Republican party seems silly. Low salaries, inflexible bosses, and overspecialization are all fine reasons to leave the field but anti-intellectualism among some politicians just seems like a bizarre reason to change careers. I he intending this to be a protest piece against the current political climate, because I guarantee that nobody who currently distrusts science will change their mind because of this.  If he doesn't like inflexible bosses and overspecialization, wait until he gets into the private sector. I think Google is one of the few companies that does this part of business right.  Engineers doing engineering. Interesting material, but I think his reasons regarding cultural misunderstanding of science and the behavior of the Republican party seems silly. Low salaries, inflexible bosses, and overspecialization are all fine reasons to leave the field but anti-intellectualism among some politicians just seems like a bizarre reason to change careers. I he intending this to be a protest piece against the current political climate, because I guarantee that nobody who currently distrusts science will change their mind because of this.   From the tone of his post, he sounds as he is not particularly fond of teaching. That seems to contradict his view on education. Why not leave the teaching to a few and he can go on his jolly way doing research?  Although his article is a bit political, including unnecessary attacks on the two wars and Republican politicians, he is right about the trend of anti-intellectualism, anti-education, and attacks on science in the United States, as well as recent budget cuts and how some universities are driving professors to chase funding. Because politicians and politics have no effect on the funding of public universities. It's more about constituents' view on education. Take Maryland for example. Constituents voted that funding higher education is one of their highest priorities, so USM's funding is relatively untouched. Kansas? Not so much. So it really comes down to how educated constituents are. The more educated the person is, the higher value he/she places on higher education.     Okay, before people cling to his nuts, let's just point out the facts: he's not a very good researcher, and his publication record is mediocre (2nd and 3rd tier conferences).  

You can tell just simply reading his blog that he makes a poor researcher; he just simply cites things he agrees with (other far left blogs) and uses that to validate his world view; confirmation bias to the extreme. 

I mean, the guy doesn't even bother to look up facts for any of his political opinions.  If he did, he'd know that the [republicans have been more pro-science](http://www.youtube.com/watch?v=x7Q8UvJ1wvk), historically, than the democrats; even Bush was better than Obama and Clinton in this regard.

But, nope, not to this guy.  This guy's an idiot.  He's not even a researcher at Google; just a pauper software engineer.  &amp;gt;he's not a very good research, and his publication record is mediocre (2nd and 3rd tier conferences).

This attitude is a great example of what's wrong with academia. Check his comment history.  This guy is a [professional troll](http://www.reddit.com/r/politics/comments/fhdfv/bill_oreilly_managed_to_interrupt_the_president/c1g076i)--a self-righteous [first year PhD student at Georgia Tech](http://www.reddit.com/r/rit/comments/k2q4d/rit_is_a_3rd_tier_university/c2h837n) who decided to start lobbing "third-tier university" as an anemic insult since his _meteoric_ rise in academia.  He's [never stuck his head outside of academia](http://www.reddit.com/r/softscience/comments/h6nrb/got_into_a_few_decent_math_schools_was_told/c1t0fxa), but he's a "big shot" because his uncle is a professor. Check his comment history.  This guy is a [professional troll](http://www.reddit.com/r/politics/comments/fhdfv/bill_oreilly_managed_to_interrupt_the_president/c1g076i)--a self-righteous [first year PhD student at Georgia Tech](http://www.reddit.com/r/rit/comments/k2q4d/rit_is_a_3rd_tier_university/c2h837n) who decided to start lobbing "third-tier university" as an anemic insult since his _meteoric_ rise in academia.  He's [never stuck his head outside of academia](http://www.reddit.com/r/softscience/comments/h6nrb/got_into_a_few_decent_math_schools_was_told/c1t0fxa), but he's a "big shot" because his uncle is a professor. Nope.  3rd year PhD student, no uncles there.  PhD admissions don't consider legacy family members anyways.

Nothing I said in my comment was wrong.  He's a low level researcher and an ignorant partisan hack.  [Third year](http://www.reddit.com/r/gatech/comments/kweps/favorite_eating_place_on_campus/) my [ass](
http://www.reddit.com/r/softscience/comments/h6nrb/got_into_a_few_decent_math_schools_was_told/). Okay, before people cling to his nuts, let's just point out the facts: he's not a very good researcher, and his publication record is mediocre (2nd and 3rd tier conferences).  

You can tell just simply reading his blog that he makes a poor researcher; he just simply cites things he agrees with (other far left blogs) and uses that to validate his world view; confirmation bias to the extreme. 

I mean, the guy doesn't even bother to look up facts for any of his political opinions.  If he did, he'd know that the [republicans have been more pro-science](http://www.youtube.com/watch?v=x7Q8UvJ1wvk), historically, than the democrats; even Bush was better than Obama and Clinton in this regard.

But, nope, not to this guy.  This guy's an idiot.  He's not even a researcher at Google; just a pauper software engineer.  While I didn't like that he made it kind of political with the jabs at Republicans, it is true that Republicans *right now* are fighting for huge cuts in government spending, much larger than Democrats. Admittedly I don't know how much of these cuts are for sciences/research funding. But you're saying his facts are wrong because of historical Republican/Democrat funding trends, when the author said nothing about historical trends. He was just speaking about the current political climate for funding (which, again, I don't know enough about to say whether or not he's right; just seemed like you put up a straw man). How historical? The past dozen years, definitely. I don't think I've even heard much debate within the republican party about it. 

Even though they supported defense research, fighting 2 wars on credit passed the buck to the present day. They couldn't even raise the debt ceiling for the debts they incurred and now the fiscal cliff option would cut defense research in a big way. 
It's not all republicans, just the anti-intellectual ones.  Sure, but you're protecting the entire Republican brand and saying the current funding climate is the Democrats fault when you just admitted that it is at least *some* of the Republicans pushing the issue. Right now Democrats are much bigger advocates for govt' spending than the Republicans as a whole, as the Republicans are being held hostage by the tea party.

I don't really want to get into a political debate, I just wanted to point out that you're accusing him of being "an idiot" and not having his facts straight when it seems he does, or at the very least the facts are not as cut and dry as your initial post made them out to be. Okay, before people cling to his nuts, let's just point out the facts: he's not a very good researcher, and his publication record is mediocre (2nd and 3rd tier conferences).  

You can tell just simply reading his blog that he makes a poor researcher; he just simply cites things he agrees with (other far left blogs) and uses that to validate his world view; confirmation bias to the extreme. 

I mean, the guy doesn't even bother to look up facts for any of his political opinions.  If he did, he'd know that the [republicans have been more pro-science](http://www.youtube.com/watch?v=x7Q8UvJ1wvk), historically, than the democrats; even Bush was better than Obama and Clinton in this regard.

But, nope, not to this guy.  This guy's an idiot.  He's not even a researcher at Google; just a pauper software engineer.  Well, I know him personally (I submitted the same article in r/education).  Gotta say, he's a pretty sharp crayon, he was even my wife's undergrad research advisor.  He created a machine learning lab from scratch at UNM, led it with several students for several years, and to the best of my understanding, brought good funding to boot.

I took a very good grad level machine learning course from him too.

Remind me, how well do you know him? Well, I know him personally (I submitted the same article in r/education).  Gotta say, he's a pretty sharp crayon, he was even my wife's undergrad research advisor.  He created a machine learning lab from scratch at UNM, led it with several students for several years, and to the best of my understanding, brought good funding to boot.

I took a very good grad level machine learning course from him too.

Remind me, how well do you know him? I don't need to know him to know his publication record is mediocre.  All you're showing is that he's good at administration. You don't seem to have any idea what the word "administration" refers to.  If anything, you meant to say "management"...but if that would be your criticism, then you really really don't understand how real-world research is conducted.  The 19th century vision of lone researchers doing all their own work is naive to the point of silliness.  Was that really what you trying to imply?

Administrators, to a significant degree, aren't involved in research at all.  Researchers (a term that convolutes with managers and/or scientists) run lab meetings, govern the course of numerous simultaneous projects, and guide and advise younger, less experience researchers under them (as they climb the ladder, which you seem to detest as nonresearch, they also spend an inordinate amount of time securing funding).  They're directly involved in the design of experiments, the analysis of data, and in general, the overall scientific process of which they are a part.

You're an idiot. Considering that I do research at a much better school than yours for computer science, it's safe to know that I'm a little bit closer to the elite of my field than you are.

What you described was him doing administration.  He was responsible for getting equipment, office space, lab space, faculty most likely, etc.  All you showed was that he was good at that; that means nothing about his research.

If you want to see how good at *research* he is, you look at his publication record.  His is mediocre.   Considering that I do research at a much better school than yours for computer science, it's safe to know that I'm a little bit closer to the elite of my field than you are.

What you described was him doing administration.  He was responsible for getting equipment, office space, lab space, faculty most likely, etc.  All you showed was that he was good at that; that means nothing about his research.

If you want to see how good at *research* he is, you look at his publication record.  His is mediocre.   As has been pointed out in another response, you're a first-year grad student.  Trust me, I was pretty arrogant when I got to grad school too.  A few years should cure you right up.  In truth, having been there for so short a time, you haven't really demonstrated that you're a great grad student or researcher yet.  What you've demonstrated is that you can get into a good school.  Congratulations, you should be proud of that, but I wouldn't consider you a judge of other researchers.  You can't even distinguish the concepts of lab-leadership and department-administration.

I clearly stated that he created a lab and led it, and taught courses.  In a follow up I clarified (which should not have been necessary) that he was intimately involved in the design and implementation of projects along with the students under his guidance.  I don't understand why you continue to confuse what is obviously the running a university research lab with conventional administration.

Your obstinacy on this matter calls into question the academic credentials you so proudly flourish.
 Nope.  3rd year CS student.

&amp;gt;A few years should cure you right up.

Considering I've already finished my 2nd year...

&amp;gt;Congratulations, you should be proud of that, but I wouldn't consider you a judge of other researchers

3rd tier is 3rd tier, regardless.  You don't need to be a genius to see he's a 3rd tier researcher.

&amp;gt;I clearly stated that he created a lab and led it, and taught courses. 

None of which directly deals with his research credentials. 
 Okay, now we're well into the realm of basic logical comprehension:

I said:

 - I clearly stated that he created a lab and led it, and taught courses.

To which you said:

 - None of which directly deals with his research credentials. 

...which is a logical fail in that my statement was not in regard to his research credentials, but rather the other topic of discussion, his role as either administrator or researcher. Research credentials deal with his ability to publish high quality papers.  He hasn't shown that ability.   Nope.  3rd year CS student.

&amp;gt;A few years should cure you right up.

Considering I've already finished my 2nd year...

&amp;gt;Congratulations, you should be proud of that, but I wouldn't consider you a judge of other researchers

3rd tier is 3rd tier, regardless.  You don't need to be a genius to see he's a 3rd tier researcher.

&amp;gt;I clearly stated that he created a lab and led it, and taught courses. 

None of which directly deals with his research credentials. 
 Okay, before people cling to his nuts, let's just point out the facts: he's not a very good researcher, and his publication record is mediocre (2nd and 3rd tier conferences).  

You can tell just simply reading his blog that he makes a poor researcher; he just simply cites things he agrees with (other far left blogs) and uses that to validate his world view; confirmation bias to the extreme. 

I mean, the guy doesn't even bother to look up facts for any of his political opinions.  If he did, he'd know that the [republicans have been more pro-science](http://www.youtube.com/watch?v=x7Q8UvJ1wvk), historically, than the democrats; even Bush was better than Obama and Clinton in this regard.

But, nope, not to this guy.  This guy's an idiot.  He's not even a researcher at Google; just a pauper software engineer.  You are trolling, but for the other people reading this: Terran Lane has, among many others, a first author publication at ICML, which is definitely not a "2nd and 3rd tier conference" as it were:

    Lane, T. and Brodley, C. E. (2000). "Data Reduction Techniques for
    Instance-Based Learning of Human/Computer Interface Data." 
    In Langley, P. (Ed.), Proceedings of the Seventeenth International
    Conference on Machine Learning (ICML-2000), pp 519&#8211;526. Congrats.  One 1st tier conference with him as first author.  One.  Wow.  He's great! TIL great ideas exclusively come from "first-tier" conferences. Get your head out of your ass. Okay, before people cling to his nuts, let's just point out the facts: he's not a very good researcher, and his publication record is mediocre (2nd and 3rd tier conferences).  

You can tell just simply reading his blog that he makes a poor researcher; he just simply cites things he agrees with (other far left blogs) and uses that to validate his world view; confirmation bias to the extreme. 

I mean, the guy doesn't even bother to look up facts for any of his political opinions.  If he did, he'd know that the [republicans have been more pro-science](http://www.youtube.com/watch?v=x7Q8UvJ1wvk), historically, than the democrats; even Bush was better than Obama and Clinton in this regard.

But, nope, not to this guy.  This guy's an idiot.  He's not even a researcher at Google; just a pauper software engineer.  Okay, before people cling to his nuts, let's just point out the facts: he's not a very good researcher, and his publication record is mediocre (2nd and 3rd tier conferences).  

You can tell just simply reading his blog that he makes a poor researcher; he just simply cites things he agrees with (other far left blogs) and uses that to validate his world view; confirmation bias to the extreme. 

I mean, the guy doesn't even bother to look up facts for any of his political opinions.  If he did, he'd know that the [republicans have been more pro-science](http://www.youtube.com/watch?v=x7Q8UvJ1wvk), historically, than the democrats; even Bush was better than Obama and Clinton in this regard.

But, nope, not to this guy.  This guy's an idiot.  He's not even a researcher at Google; just a pauper software engineer.  Did you read the blog? His blog is mainly about systemic problems in academia. Where did you get the "poor researcher" idea from? Seems like your some conservative looking to spread lies, and misconception. You call this guy an idiot, almost like you're an anti-intellectual that is mentioned in the blog post. Yes, I read the blog. 

He's a poor researcher because his publication record shows this; all 2nd and 3rd tier conferences.

How am I a conservative spreading lies and misconceptions?  His entire blog is a lie and misconception!  You just agree with his point, so you don't care.

Republicans fund science more heavily than do the democrats. &amp;gt;Republicans fund science more heavily than do the democrats.

[Citation Needed].

Especially since it's entirely Republicans who try to promote an anti-intellectual, anti-professor narrative painting them as elitists. Not **all** Republicans, but **only** Republicans. &amp;gt;Republicans fund science more heavily than do the democrats.

[Citation Needed].

Especially since it's entirely Republicans who try to promote an anti-intellectual, anti-professor narrative painting them as elitists. Not **all** Republicans, but **only** Republicans. Okay, before people cling to his nuts, let's just point out the facts: he's not a very good researcher, and his publication record is mediocre (2nd and 3rd tier conferences).  

You can tell just simply reading his blog that he makes a poor researcher; he just simply cites things he agrees with (other far left blogs) and uses that to validate his world view; confirmation bias to the extreme. 

I mean, the guy doesn't even bother to look up facts for any of his political opinions.  If he did, he'd know that the [republicans have been more pro-science](http://www.youtube.com/watch?v=x7Q8UvJ1wvk), historically, than the democrats; even Bush was better than Obama and Clinton in this regard.

But, nope, not to this guy.  This guy's an idiot.  He's not even a researcher at Google; just a pauper software engineer.  Putting politics above scientific discourse is way worse than cutting science funding.

http://www.nytimes.com/2006/01/29/health/29iht-web.0129nasafull.html?pagewanted=all

For the most part, there was a Republican controlled senate/house of representatives during Clinton's presidency, and a ~~Democratic controlled house/senate~~ (edit: wrong, I suck at reading wikipedia tables) during W's presidency.  Congress controls the budget. Putting politics above scientific discourse is way worse than cutting science funding.

http://www.nytimes.com/2006/01/29/health/29iht-web.0129nasafull.html?pagewanted=all

For the most part, there was a Republican controlled senate/house of representatives during Clinton's presidency, and a ~~Democratic controlled house/senate~~ (edit: wrong, I suck at reading wikipedia tables) during W's presidency.  Congress controls the budget. The Democrats only got control in 2007.  For the remaining 6 years, it was all R.  And that's just Bush.  If you actually watch the video, it's clear that Republicans *historically* out-fund the Democrats when it comes to science.

And this guy entirely puts politics above science; he goes on rants about "Republicans filibustering Obama!" and all that; he doesn't even know who's cutting his research funding and who's advancing it!  What a fool!</snippet></document><document><title>New advances in the Iterated Prisoner's Dilemma game.</title><url>http://bosker.wordpress.com/2012/07/23/the-prisoners-dilemma/</url><snippet>  Anyone have the golden balls link where he promises to defect and share his half then splits the money? That's my favorite strategy. Ask [and ye shall receive](http://youtu.be/S0qjK3TWZE8). I *love* this clip.
 Ok, anyone care to explain the logic? I'm feeling too dumb to understand. &amp;gt; Think about Nick's strategy. He can't trust that Abraham will
&amp;gt; split. More importantly, he can't trust that Abraham will do what he
&amp;gt; said, because it's in Abraham's best interest to say one thing and
&amp;gt; do another. So he changes the game. He offers to split the pot
&amp;gt; outside the game -- set up a meta-game of sorts -- and removes
&amp;gt; Abraham's incentive to lie.

&amp;gt; In effect, Nick turns the Prisoner's Dilemma, where both players
&amp;gt; make their decisions simultaneously, into a sort of Trust game:
&amp;gt; where one player makes a decision, and then the other does. In a
&amp;gt; classic Trust game, Player A gets a pot of money. He gives some
&amp;gt; percentage of it to Player B. It is then multiplied by some amount,
&amp;gt; and Player B gives some percentage of it back to Player A. In a
&amp;gt; classic rational self-interest model, it makes no sense for Player B
&amp;gt; to give any of the money back to Player A. Given that, it makes no
&amp;gt; sense for Player A to give any of the money to Player B in the first
&amp;gt; place. But if Player A gives player B 100%, and Player B gives
&amp;gt; Player A back 50% of the increased pot, they both end up the
&amp;gt; happiest.

&amp;gt; Nick sets himself up as Player B, promising to give Abraham 50% of
&amp;gt; the jackpot outside of the game. Abraham is now Player A, deciding
&amp;gt; whether to give Nick the money in the first place. But unlike a
&amp;gt; classic Trust game, Abraham can't keep the money if he doesn't give
&amp;gt; it to Nick. So he might as well give the money to Nick. The game is
&amp;gt; turned on its head; trusting Nick now means letting him have all the
&amp;gt; money. Not trusting Nick means...well, it doesn't mean anything
&amp;gt; anymore. All that's left is not letting Nick have the money out of
&amp;gt; spite -- and that emotion seems out of place in the
&amp;gt; conversation. Abraham decides to trust Nick, because it's the only
&amp;gt; option that makes any sense. (Although the process is funny to
&amp;gt; watch. Abraham can't figure out what's going on. He tries to steer
&amp;gt; the conversation back to mutual trust -- how important his word is
&amp;gt; -- but it's no longer relevant. And, at the end, he first picks up
&amp;gt; the "Steal" ball before saying "okay, I'll tell you what, I'm going
&amp;gt; to go with you" and changing his mind. That bit of honesty
&amp;gt; demonstrates how effectively subterfuge was removed from Abraham's
&amp;gt; game.)

&amp;gt; Nick, for his part, having removed the subterfuge from the game, can
&amp;gt; safely choose "Split." Although notice that he does so before
&amp;gt; Abraham chooses, so he's sure his psychological manipulation
&amp;gt; worked. I wouldn't have been so cocky.

This is [Bruce Schneier's explanation.](http://www.schneier.com/blog/archives/2012/04/amazing_round_o.html#c746020)
 Ok, anyone care to explain the logic? I'm feeling too dumb to understand. Anyone have the golden balls link where he promises to defect and share his half then splits the money? That's my favorite strategy. In the traditional version of the Prisoner's Dilemma this strategy doesn't work because the two criminals do not get to communicate and strike up such deals. In the traditional version of the Prisoner's Dilemma this strategy doesn't work because the two criminals do not get to communicate and strike up such deals.   TL:DR for those of us who don't want to read a full history of the PD?  As soon as I saw Hofstadter's name I knew that, just like GEB, this article was going to be composed entirely of fluff, presenting no actual information about the aforementioned new advances.  
  
Needless to say, I was right. As soon as I saw svsvsvs's name, I knew that, just like [/user/svsvsvs](http://reddit.com/user/svsvsvs), this comment was going to be composed entirely of ASCII characters, presenting no actual justification about the aforementioned bash attempt.

Needless to say, I spent too long writing this comment. [You sound butthurt.](http://cf2.8tracks.us/mix_covers/000/678/666/73991.max1024.jpg)</snippet></document><document><title>FREAK: Fast Retina Keypoint</title><url>http://www.ivpe.com/freak.htm</url><snippet /></document><document><title>Neuroscience Meets Cryptography:
Designing Crypto Primitives Secure Against Rubber Hose Attacks [PDF]</title><url>http://bojinov.org/professional/usenixsec2012-rubberhose.pdf</url><snippet /></document><document><title>Turing Test &#8216;reworked for programmers&#8217;, predicts Hoare</title><url>http://eandt.theiet.org/news/2012/jun/turing-test-programmers.cfm#.UA3KH_7dBpM.reddit</url><snippet>  &amp;gt; that a computer could understand its own program and assist the programmer to make changes and improvements.  
  
I can't even understand the programs I wrote 5 minutes ago. How the hell is an automated tool going to do this?  Sounds like formal verification! Here's some recent work on it that's interesting.

http://www.seas.harvard.edu/news-events/press-releases/nacl-to-give-way-to-rocksalt   Interesting idea, but not much of a Turing Test.

I'd be happy just to see the Turing test as originally described run; I have never seen any documentation of it actually ever being carried out in a formal setting. 

Things called "Turing tests" have been run, even as competitions, but they've been done incorrectly. In the ones that have been run, a set of terminals are laid out as a mix of computers and humans, and the judges vote for each terminal which it is.

Instead, they should run Turing's original: Several pairs of terminals, each pair consisting of one human and one computer both of whom speak to the same judge. The judge must decide which is the human. I haven't watched the talk, but I don't think Hoare is suggesting this as an alternative to the Turing test.  He's suggesting an analogous test for a different community.  Turing's test was designed as a challenge problem for the field of AI, and as a benchmark for deciding when a machine can reasonably be called intelligent.  Hoare is suggesting a challenge problem for the field of formal methods (and a benchmark for a "sufficiently good program analyzer"), and phrasing it as a Turing test because he was speaking at the Turing Centennial conference. Was it really as a challenge test, or more as a thought experiment that would break people's (possible) assumption that computerized intelligence couldn't be done? 

Since in a 'real' setting the Turing test is very adversarial and it well, it likely would fail for many possible real AIs? Ie, ones that aren't into faking being a human.   The way this is written makes it look like Hoare has a fundamental misunderstanding of the implications of Turing's work.  Given who Hoare is, I find this hard to believe.</snippet></document><document><title>Poison attacks against machine learning</title><url>http://www.kurzweilai.net/poison-attacks-against-machine-learning</url><snippet>  Uhm do real world machine learning algorithms actually learn while they're live? Surely you could simply train them using only curated datasets to avoid this problem entirely. Sure! The article mentions spam filters and credit card fraud alarms, for instance. Why would spam filters and credit card fraud alarms have to be trained on live data? You could simply retrain it with updated datasets every few days  and performance would barely be affected. If you update the filter frequently, won't you suffer the same way that you do when you update it continously? Except that you now have a window of vulnerability that could potentially be exploited by spammers/fraudsters, until you get around to updating your filter. Why would spam filters and credit card fraud alarms have to be trained on live data? You could simply retrain it with updated datasets every few days  and performance would barely be affected. Uhm do real world machine learning algorithms actually learn while they're live? Surely you could simply train them using only curated datasets to avoid this problem entirely. Yes, it's called [online machine learning](http://en.wikipedia.org/wiki/Online_machine_learning). Those would work with similar efficiency as long as you periodically feed it curated datasets, which encompass newer patterns and turn learning off during actual classification. As long as a good classification doesn't depend on very short term trends, there's no reason for an online machine learning algorithm to be continuously learning, even if it is online.  Ok, I might be missing something here, but won't any kind of wrong data poison the well? Especially if in such a high enough quantity that you force false positives?

I'm so convinced of this that I keep changing my facebook settings, close friends, likes and dislikes, just to mess their algorithms (:  I think a real, and worrying, application of this idea would be to poison [spamassassin](http://spamassassin.apache.org/).  This is a perfect example of a "learning" algorithm that can be fed just about anything... Not really. SVMs handle noisy data poorly, which is why parameter selection is a very important part of SVM training. 

Also, SpamAssasin is shit without adding your own data, so each installation will end up with a different training set. This attack requires knowing the exact training set. So it wouldn't even be effective if it was using SVMs  A program that learns from data can be made to learn the wrong things from bad data! News at 11.    I think what other comments are missing that are interesting about this is that it's like a high level computational disease. The fanatical side of me likes to treat it as a metaphor for how the cerebral mind can organize data poorly by receiving bad data, like religion. Except that is a really shitty analogy to what is going on.  Would love a more elaborate response. Would you care to elaborate on how there is a god? Its an unprovable statement. Similarly, your analogy does not connect with anything about how this attack is carried out. As such, there is not much there for me to point at and say "This is wrong because X" until you state a connection. You've simple said "Its like X" you have to state how it is like X for there to even be any resemblance of a debate. 

Unless religion can somehow take a person, examine all the factual events of their life experiences, remove experiences from their brain, and insert new experiences with incorrect labeling, that are designed to confuse and disorient the individual, and then re-train their brain using the whole of their previous and now altered life experinces. I dont see how this anology holds. 

Unless your making a feeble "religion is always bad and its machine 'learning' and I'm going to take the word learning literally and these bad data points are religion because they are both bad." In which case, you are making gross oversimplifications of two things and trying to equate the simplifications. Which is just silly and useless.  Not sure if you meant to come off like an ass or not, but you did. You also didn't seem to thoroughly read my first comment, because your argument is about something else entirely.

I was sincerely interested as to why you thought the metaphor was wrong. I'm a software engineer who has implemented machine learning at relatively high scale and have had much success in pitching the approaches to management/bosses by making similar analogies.

I made the self-stated "fanatical" metaphor for how minds can be tricked into perceiving, remembering, distorting and believing things that are false by input you provide it. It wasn't meant entirely for literacy than the homage it makes to the complexity of these systems and how interesting it is. Religion was an example of bad data that fucks up an algorithm or how the brain problem solves (like a machine learning algorithm) input.

You should read Snow Crash by Neal Stephenson, it's underlying theme makes similar metaphors that really piqued  my interest in machine learning which I disovered when working with the guy who organized the NYC Machine Learning events. </snippet></document><document><title>CS Conference Paper Cheat Sheet</title><url>http://www.reddit.com/r/compsci/comments/x0xib/cs_conference_paper_cheat_sheet/</url><snippet>I am doing my masters in CS and am in my first semester.  My supervisor and I did an experiment that we are writing into a paper for a conference this fall.  I seem to be left on my own to write the paper which I have no experience in.  

As I am reading related papers I see there is a general theme in the different sections and what is included.  

Do you know of a good cheat sheet to cover writing a CS paper and what should be covered in each section?  I know papers seem to be like government writing in that they love as much detail as possible in them.  There is a standard research paper sectioning...
first there is an abstract(this is written LAST!!!, after you have written the whole paper and have the best overview of the whole work)
then there is some background story about the problem, what motivated you, what are the key challenges etc...
then there is related work section, who did what related to the problem...this gives you the authority on the matter as you show the knowledge about the research area you're writing about

now..here goes your main part of the paper:motivation, findings, basically, the stuff that YOU developed in your research :)
then after this part there is an evaluation(if you need one)...this is actually very important part of your work..you test your findings, your ideas, show some real world examples etc... 

then there are "closing sections of the paper"...
there is a future work section where you show what are the possible extensions and what you plan to do (or as a suggestion to someone else to continue on your work)

and finally..there is a conclusion which is very very important as a lot of people first read only abstract and conclusions and only after they are interested in the details they read the whole paper....


this is not CS specific, this is how most of the research paper are written..just Google how to write scientific papers or something like that......i personally watched this (as I really really like the author :))) )

http://research.microsoft.com/en-us/um/people/simonpj/papers/giving-a-talk/giving-a-talk.htm  I haven't found one yet. I've just had someone sit down and show me. I will do the same.

 * Introduction - what is the problem? what do I need to know about to understand the rest of your paper?  why is your problem worth solving? what are you going to show in this paper?
 * Motivation -  outline your problem. how do you know there's a problem? what are the issues that need to be addressed? (edit: this used to say 'why is your problem worth solving', but I think that's not a good question)
 * Challenges - why was what you did hard?
 * Related Work - has anyone tried this before? what makes you different from them?
 * Methodology - how did you do it? how do you know you were doing it right?
 * Evaluation - how well did you do?
 * Conclusion - tell me again what you solved and how well you did it. are you happy with the outcome? what do you want to do in the future? 

There's lots of options and variations, like where to put related work, or how long your methodology section is. Also, this tells you nothing about what a good structure is for each section. It's a start, though.

Oh, and as far as writing technique overall goes. You probably know this, but don't BS. State everything as plainly and clearly as possible with minimal repetition. Your goal is to inject new knowledge directly into your reader's brain, not entertain them with turns of phrase. :) &amp;gt; &#8226;Motivation - why is your problem worth solving?

I hate this part of CS publishing. Mathematicians never have to answer this question.  Which sub-field are you and your advisor in? (Algorithms, compilers, etc) Natural Language Processing, so AI.   I would look at top conferences in your field on a site like [this](http://academic.research.microsoft.com/RankList?entitytype=3&amp;amp;topDomainID=2&amp;amp;subDomainID=5&amp;amp;last=0&amp;amp;start=1&amp;amp;end=100). Read papers from the most recent years and make a mental note of the section headings (i.e., Background, Related Work, Experiment, etc). If you find a conference website for prior years ([example](http://cs.nyu.edu/~stoc2012/accepted.htm)), you can usually find a list of papers accepted for that year. To actually download the paper for free, search scholar.google.com Thanks.  That's what I am currently doing, but I am looking to see if someone has already done this.

Economy of Effort and all that good stuff :P  Have you read and digested *The Adversarial Reviewer*?</snippet></document><document><title>Fiction writer seeking input on text regarding cryptography</title><url>http://www.reddit.com/r/compsci/comments/wyxxm/fiction_writer_seeking_input_on_text_regarding/</url><snippet>Hey guys. I'm an author and I'm working with a chunk of text that gets a bit technical. I know enough about computing to not make an ass out of myself in your average conversation, but I feel like I don't have enough authority over this subject to write something that rings true. Please, take a look at the following text and tell me what may need fixing.
---------------
She quickly loaded her design environment and opened the main class file. Bumps rose on her arm like sandpaper grit. She looked at the class header.

/* 
=======================================

Program: Fetch

Version: 03.15.02

Developer: (-)

Developer: Trm7kB0hMTAhdmk=

Developer: ITEwId2EpEgRr2wWZCPfFHEt3Q==

Purpose: Study in AI and interactivity

=======================================

Version Notes

Then there was a long series of notes regarding incremental improvements of the program. She leaned back in her chair and stared at the pixels on her screen. There were three developers. The first had to be Minus. The second two used long, alpha-numeric, case-sensitive ciphers that included nonstandard glyphs: the equal signs. That amounted to a character set with ninety-four possibilities. She counted how many characters were in the first encrypted name: sixteen. She did the math.

3.7 x 10^31

There were more combinations than there were grains of sand in the Sahara desert. Her estimate assumed that there was a one-to-one relationship between these characters and the characters in the name &#8211; which would require that the name also used a ninety-four letter alphabet. Either there were numbers and symbols in the name, or the one-to-one relationship didn&#8217;t exist. Some letters could be garbage. Some could represent more than one letter. The capital &#8220;T&#8221; could translate to a common pairing, &#8220;th&#8221; or &#8220;sa.&#8221;

Further, the meaning of each repeated character could be different at each instant, based on its position.

&#8220;Well,fine,&#8221; she said. She called a friend and downloaded government-strength code-breaking software within a few minutes. She fed the names into the program and watched the console brute-force its way downward. The program made more guesses per second than her eyes could follow. A few minutes passed, she did more math.

She found that it could take several years at the speed that her processor moved.

She made another call. A shadow at the other end blew her a kiss and then mobilized a botnet. Computers all over the world quietly dedicated a fraction of their processing power to breaking the code. Faceless users noticed slight dips in frame-rates and browsing speeds.

*****

**Update:** After trying to rework this a couple times, I've decided to take a slightly different approach. Per one of the suggestions on this thread, I've ordered "The Code Book."

I'm going to encrypt the names with something that a human can solve with old-fashioned pen, paper, and smarts. This will be more in line with the secret developer's intention. Also, the process will probably be more engaging for my readers, as I can give hints for how they can solve it themselves.

I want to say thanks again to all of you super smart and helpful people. I hope I haven't wasted anybody's time. The response I got from this community was overwhelming in its positivity, intelligence, and helpfulness. I like you folks.

I'll be back in a few weeks with the revised section, for anyone interested. At that point, it might not be truly relevant to /r/compsci - but as OP, i feel an obligation to deliver. And, again, I'll be sure to acknowledge you folks if this book ever sees print.  The first issue that immediately jumps out at me is ending your string with "=" or "==". That to me looks like [base64 padding](http://en.wikipedia.org/wiki/Base64#Padding).

If your character is any kind of technical wizard (or any sort of computer programmer, server administrator, basically any skilled position in the field) they would see that and immediately recognize it as likely base64.

In decoding both, I notice they both contain the string "!10!" and some high-ascii/non-printable characters. I'm guessing you used some sort of actual program/algorithm to generate these. A quick trip to Google doesn't turn up what kind of algorithm might be putting "!10!" in both (it may be some kind of indicator of the algorithm, or just a coincidence).

The output from most encryption and hashing algorithms is a set of bytes (value 0-255). In order to more easily and reliably transmit this data around (as the binary data will be easily butchered when stored somewhere only printable characters are expected), it's often encoded into printable characters. Hashes will often simply represent the bytes in hexadecimal (using only 0-9 A-F), encryption algorithms and keys tend towards using base64 as it can more succinctly encode the longer streams of data they tend to deal with.

So the entire couple paragraphs about how it could be decoded talking about the text as-is are pretty meaningless... I guess it's kind of like looking at a picture of a lock and saying "This lock is really flat, it appears to have a glossy finish. Perhaps the glossy finish is to help reflect against attackers that don't have sunglasses. The key for this must be infinitely small. There is the word "Kodak" scrawled across the back repeatedly... Maybe it provides some sort of clue."

Most modern encryption algorithms don't really do much in the way of substitution like using "T" to represent a common pairing like "th". They don't usually operate on language itself, but on binary data... They see a file (such as an image, say a JPG) the same way they see text - a stream of bytes, of numbers that get run through a [usually complex set of operations and formulas](http://en.wikipedia.org/wiki/Blowfish_\(cipher\)).

As for your character trying to brute force the text, that operation would take a lot longer than you think, even with a botnet. The biggest problem being that she has *no idea* what algorithm was used to generate it. There are dozens of symmetric encryption algorithms in use, and at least a few fairly popular ones. Without any sort of algorithm you're pretty much shit out of luck... There are simply too many to try.

Depending on how it all fits in to your story, you may want to look at using a hash for their names instead. A hash is simply one-way encryption. It would allow someone who already knew the name to confirm that said developer had worked on it, but never allow (other than through brute force) anyone to reverse it into their name. Something like:

&amp;gt; Developer: da6645f6e22bf5f75974dc7eed5fcd6160d6b51e

As a developer running across that, my first thought would be to simply try and decode it and see if it comes out looking like ASCII/Unicode (maybe the developer had some funky characters in his name that his editor didn't agree with?). However, if I had reason to believe it would be somehow secured, it's fairly easy to take a guess at the algorithm simply from the length of the hash.

40 hex characters is a 20 byte hash (160 bits). A quick reference to the [hash algorithms](http://en.wikipedia.org/wiki/Cryptographic_hash_function#Cryptographic_hash_algorithms) page on Wikipedia shows me five possible algorithms: HAVAL, RIPEMD-160, SHA-0, SHA-1 and Tiger-160.

From that list, we can already drop SHA-0 (it's an old, broken version of SHA-1). Of the remainder, the one I see most commonly is probably SHA-1, but that could be a result of the kind of software I work with. Perhaps your character could get an idea of which of the remaining hashes it is by something to do with the hashes history. (EG: SHA was developed by the NSA, read the wiki articles).

Alternatively, make it a SHA-384 hash (it will result in a long 96 character string) but 384 bits is a fairly uncommon output size for a hashing algorithm.

Bonus: Brute forcing hashes is pretty common. There are a lot of people that have software to do it on GPUs (much faster than doing it on CPU).

If you have any questions, feel free to ask. I'd be happy to help. Amazingly helpful. Thank you so much. I'll incorporate some of this into the narrative and come back to /compsci to show you guys the result. Personally, I'd aim for an MD5 or SHA-1 hash. Those have known flaws that will make an attack feasible. The newer algorithms are on the order of "make every atom in the universe into a tiny computer and we'll be almost done when all the stars turn into black holes".

If you could set the story ~10 years ago, before the flaws were publicly known, then you could plausibly claim that a secret "government-strength code-breaking software" already had an exploit for those flaws. In a modern setting, nobody competent uses MD5 or SHA-1 anymore.

(Unfortunately, there are a lot of incompetent people in this field.) Pfft.  I see SHA-1 used all the time still... and there is certainly no algorithm that can "easily" find a preimage for either MD5 or SHA-1 (as opposed to finding collisions). MD-5 and SHA-1 are baked into all versions of TLS (SSL) before 1.2, which most clients and servers still don't use - so yes, odds are if you have a secure tab open right now, it used a combination of MD-5 and SHA-1 to create the key that's used to encrypt your data exchange. Pfft.  I see SHA-1 used all the time still... and there is certainly no algorithm that can "easily" find a preimage for either MD5 or SHA-1 (as opposed to finding collisions). There can be no possible algorithm at all since hashing is by definition not one to one. There is an infinite amount of possible pre-images. Personally, I'd aim for an MD5 or SHA-1 hash. Those have known flaws that will make an attack feasible. The newer algorithms are on the order of "make every atom in the universe into a tiny computer and we'll be almost done when all the stars turn into black holes".

If you could set the story ~10 years ago, before the flaws were publicly known, then you could plausibly claim that a secret "government-strength code-breaking software" already had an exploit for those flaws. In a modern setting, nobody competent uses MD5 or SHA-1 anymore.

(Unfortunately, there are a lot of incompetent people in this field.) Amazingly helpful. Thank you so much. I'll incorporate some of this into the narrative and come back to /compsci to show you guys the result.  Modern encryption is not based on characters, but on bits. It is the 1s and 0s that get scrambled, the result can be represented alphanumerically but but the reasoning your character is doing does not make much sense in the digitial era.

That being said though, I do not think it matters. The text is engaging and interesting. If you want to get it 100% technically correct you would probably end up with something really boring that almost noone in your audience understands. Seconded, it's not like most technical people don't have the ability to suspend disbelief just like everyone else. If you want to go all the way on making sure to use real science and tech in your story, then you'll get a small portion of techies, maybe.

in other words: technobabble isn't really the end of the world. better to use something that most people will understand intuitively than something that actually makes technical sense. Also, as a graphic designer the words "Zoom! Enhance!" fill me with the urge to kill. I couldn't bear to do that to the crypto-guys out there. Or to my own credibility. Seconded, it's not like most technical people don't have the ability to suspend disbelief just like everyone else. If you want to go all the way on making sure to use real science and tech in your story, then you'll get a small portion of techies, maybe.

in other words: technobabble isn't really the end of the world. better to use something that most people will understand intuitively than something that actually makes technical sense. Modern encryption is not based on characters, but on bits. It is the 1s and 0s that get scrambled, the result can be represented alphanumerically but but the reasoning your character is doing does not make much sense in the digitial era.

That being said though, I do not think it matters. The text is engaging and interesting. If you want to get it 100% technically correct you would probably end up with something really boring that almost noone in your audience understands. Modern encryption is not based on characters, but on bits. It is the 1s and 0s that get scrambled, the result can be represented alphanumerically but but the reasoning your character is doing does not make much sense in the digitial era.

That being said though, I do not think it matters. The text is engaging and interesting. If you want to get it 100% technically correct you would probably end up with something really boring that almost noone in your audience understands.  the text strings you got for the two developers look like [base64 with padding](http://en.wikipedia.org/wiki/Base64#Decoding_Base64_with_padding).

and between what she's deduced, and the "ciphertext" samples available to her (2), decoding is literally impossible. she effectively needs to locate one single function which:

1. maps each of 94 input symbols to a subset of the 94 output symbols. (there are 2^94 such subsets.)
2. might change that mapping at every input. (so it includes at least another function for rearranging 94 things. there are 94! such functions.)

a fairly significant number of those functions will map those two strings to things which look like names. in fact, there will probably be a function which can map one of those developer names to her own, and the other to the name of her cat. Note that her estimates are just estimates - she doesn't know the encryption scheme.

Spoiler alert: I used this to encrypt the names...

http://textmechanic.com/Encryption-Generator.html

So, the scheme isn't too complicated. It's been cracked before. The developers aren't trying to hide themselves forever so much as play a little game. &amp;gt; Note that her estimates are just estimates - she doesn't know the encryption scheme.

that is sort of my point. she doesn't know the scheme, so she has to search the entire mapping space, with so little to go on that she can find positively whatever answer she wants to find. Yeah...I understand what you mean. So if a programmer wanted to make his name difficult --but not impossible-- to find, how would he go about it?

And the programmer that's trying to figure it out, how would they go about it? Yeah...I understand what you mean. So if a programmer wanted to make his name difficult --but not impossible-- to find, how would he go about it?

And the programmer that's trying to figure it out, how would they go about it? If I just wanted to tease someone, I would probably use some md5 or SHA hash of my name. The main point of these hashing algorithms is, that they can't be calculated backwards, even if you know the complete algorithm. They also map many-to-one, meaning two different names could result in the same string.

There are two ways, you could try to find out the real name:

Brute Force: Takes lots and lots of time, but assuming you know that the name is something that makes sense, like "Crash Override", "Zero Cool" or even something with letters echanged for signs (like "Fatal1ty") you could limit the search space massively by using a dictionary and some substitution rules.
Basically you try all possible names and see which one results in the same hash that you are looking for.

[Rainbow tables](http://en.wikipedia.org/wiki/Rainbow_table): For many hash algortihms there are prepared lists of "plain text - hash result" pairs. If you are lucky your adversary is on the list and you just need to look up the hash value.

If I wanted to make things really easy for the adversary I would use a simple substitution cipher or maybe a Vigen&#232;re cipher, however than the text would need to be long enough to allow frequency analysis to work. This wouldn't work for names, just for longer texts.

----
If I was on the other side of the monitor, the first thing I need to do, would be to identify the cipher used. Base64 is easily recognized, but it's just a convinient way to transmit information. The underlying cipher could be anything. If it is a hash, it should be identifiable [based on its length](http://en.wikipedia.org/wiki/List_of_hash_functions#Cryptographic_hash_functions).
Otherwise I would try out some of the simple ciphers, but if I have no other information, like a list of possible suspects, some information about the adversaries usual MO, or anything at all, it would be very unlikely to decrypt the data. In the worst case it is encrypted with a random One-TimePad and not decryptable at all.
 Interesting. Walk with me for a second.

The two mystery-developers in the text above are actually just one guy. The two strings form his first and last name.

I want to change my approach so that the first cryptogram names the kind of algorithm used to form the second cryptogram. The second cryptogram asks for a pet's name. When the name is used as a keyphrase for a third cryptogram, the mystery-developer is revealed.

What would something like that look like? Before I can really answer that, I will need more information.

The most obvious is question is about the setting. I assume your story takes place in a present day society without any radical advancements in processing power or quantum computing, correct?

Secondly and more important: What exactly is happening, is it playfull banter between old frenemies, or is it a criminal taunting a cop, or something else altogether?

Thirdly and most important: *Know your enemy* -This is part is going to be exessively long

Is the unknown adversary a criminal mastermind, taunting your heroine with his superior skill? Is it someone secretly hoping to get caught, so he can finally gloat about all the amazing hacks he accomplished? Os is it just some script kiddie feeling clever and arrogantly thinking to be unbeatable? What advantages has the adversary, which the heroine must overcome? Is he more intelligent or skilled? Does he posses his own super computers or maybe even a working quantum computer, allowing him to break any encryption instantaneously?

Let's go into the mindset of a hacker for a bit:

There are script kiddies whith no real skill, that can only repeat what they have seen others do and use existing tools, but don't really innovate.

There are those who break into computer systems mostly for financial gain and contract their work out for the highest bidder, these got skills, but no morals and no honour.

There are the social engineers, the confidence men for a modern world. Artists, who can steal your companies biggest secrets, just with a smile and a web browser. They call you on you office phone or have nice chat with your secretary, and without you thinking about it twice, you just tell them your most private information. These can be said to be the oldest hackers there ever were, long before Charles Babbage even thought about gears and levers.

And there are the more technically inclined old school hackers, skilled programmers who are interessted in a challenge and in furthering their own knowledge.

I'm a hopeless romantic, so I still believe in the idea of Hacking as a way of life. A true hacker is not someone who just knows how to break into computer systems. A true hacker is almost consumed by a neverending pursuit for knowledge and advancement. He knows that to achieve the fullest potential of his brain, you can't just focus on one singular topic, you have to keep it agile and active. A true hacker wants to take the world apart to see how it works, he does lockpicking as a sport, he learns different languages, computer and otherwise. He can play an instrument or two. And he knows that to achieve a good performance you need to have good control over your body. So he does Tai Chi, meditates or does martial arts - not to ever use it in defense, since logic and intelligence should always be able to beat raw strength.

----

Back to your story:
I'm a huge fan of the intellectual banter between to highly intelligent adversaries. I love Sherlock holmes and Moriati (especially in the new TV Series). I also liked your poetic description of the botnet so i would love to keep that in. I hate illogical puzzles, there should be some kind of increasing complexity.

So keeping that in mind here is how I would to this:

* All ciphertexts are base64 encoded for convenience.
* The first ciphertext is an SHA1 hash. The heroine identifies this based on its length. She looks it up in an online rainbow table and finds out it maps to "Shamir's baby" - this is relatively unusual as a password, and it's strange, that it would be in the table. But it might be that the adversary has put it in there himself. It's an invitation - he wants to play!
* Shamir's baby is a reference to the [RSA algorithm](http://en.wikipedia.org/wiki/RSA_%28algorithm%29) - a rather safe algorithm, if the key is long enough.
* The second ciphertext is a large semiprime number. It is the basis for an RSA Key. It is only 640 Bits, on a single computer it would take [years](http://en.wikipedia.org/wiki/RSA_numbers#RSA-640) to break this number into its two prime components, but the botnet reduces this to mere days.
* The third cipher is the adversaries name (better yet a taunt that causes the heroine to realize, she's met the adversary before) encrypted with the pbulic RSA Key, since the heroine has factored the semiprime number, she can coumpute the secret key and decrypt the message.

**Comment:**

I would love to have the first clue be "Bruces baby", since Bruce Schneier is a lot better known in the general computer world, but his algorithm are either to easy, or to complicated. Note that her estimates are just estimates - she doesn't know the encryption scheme.

Spoiler alert: I used this to encrypt the names...

http://textmechanic.com/Encryption-Generator.html

So, the scheme isn't too complicated. It's been cracked before. The developers aren't trying to hide themselves forever so much as play a little game.  This isn't really an answer to your question, but if you haven't already you should definitely read "The Code Book" By Simon Singh. It has a few very technical chapters, but overall is a great book for the cryptography newb. It talks about the mechanics of crypto, and also where/how it's used. It begins with stories about Mary Queen of Scots, goes into detail about Bletchely Park and Alan Turing's work during WWII, and goes into a lot about the mechanics and legality of modern computer crypto.

I'm only saying this because, to be blunt, the scenario you describe doesn't really make sense with modern crypto.

Also, here is a handy brute force calculator https://www.grc.com/haystack.htm that should give you a better idea of what is a reasonable timeframe. Also, with encryption done right, you shouldn't be able to tell how many characters you are searching for. Oh I have a stack to read that's about four feet tall, but I'll keep that in mind. I'm happy to say that reddit is proving more than adequate as a resource into these things. Oh I have a stack to read that's about four feet tall, but I'll keep that in mind. I'm happy to say that reddit is proving more than adequate as a resource into these things. This isn't really an answer to your question, but if you haven't already you should definitely read "The Code Book" By Simon Singh. It has a few very technical chapters, but overall is a great book for the cryptography newb. It talks about the mechanics of crypto, and also where/how it's used. It begins with stories about Mary Queen of Scots, goes into detail about Bletchely Park and Alan Turing's work during WWII, and goes into a lot about the mechanics and legality of modern computer crypto.

I'm only saying this because, to be blunt, the scenario you describe doesn't really make sense with modern crypto.

Also, here is a handy brute force calculator https://www.grc.com/haystack.htm that should give you a better idea of what is a reasonable timeframe. Also, with encryption done right, you shouldn't be able to tell how many characters you are searching for. Okay, I broke down and ordered "The Code Book."

I'll move it to the top of the pile. Once I study up, I'm going to refactor this section of the story (see the update in my original post).

Thanks so much for the recommendation.  [deleted] The government-grade password super awesome cracking cracker device is kind of lame. Granted. I'll scrub it out and put something less dumb there.

The story does take place some unspecified time in the future, so I didn't want to give a hard estimate on the actual computation time. Moore's law and all. In terms of years, we're usually talking "heat death of the universe" type lengths of time, no matter how many modern computers you throw at it. For example, if you had started attacking 256-bit AES at the big bang, and it finished now, you'd have needed 10^50 2.58Ghz cores, if they could all compute an encryption in a single cycle. This would be a nontrivial portion of the atoms in the universe dedicated to this task.  So you're saying there's a chance... So you're saying there's a chance... The government-grade password super awesome cracking cracker device is kind of lame. Granted. I'll scrub it out and put something less dumb there.

The story does take place some unspecified time in the future, so I didn't want to give a hard estimate on the actual computation time. Moore's law and all.   I'm not following your female protagonist's actions here. If she is trying to decode the developers' names, I'm not entirely sure if a cipher is the relevant point. Wouldn't they just use pseudonyms? Most standard ciphers are used to encrypt large files, not short strings. Yes, she's trying to decode the developers' names. They serve as a kind of test. The developers did this as sort of a game, to see who could figure out who they really were. Then I don't see why they would use a cipher, especially not one that would require a lot of raw processing power. It'd make more sense to present a riddle or obscure reference of some kind, that would require a lot of in-depth knowledge in their field to understand. Well, the key-phrase is the name "Laika," which is a recurring motiff throughout the book and in these fictional programs. "Laika" paired with the Tiny Encryption Algorithm yields the developer's names. So, in a way, it is a riddle. It does employ an obscure reference that makes sense to people who follow the lead programmer's work. I mean the guy that signed his name (-).

What do you think about that?   I would change it so that she somehow knows the encryption scheme ( md5, sha, etc.)

Then she does the math.  Then she tries a rainbow table, which doesn't find any hits after X time.  Then she sighs, makes the phone call, gets the botnet, and brute forces it.

That seems more realistic to me, and also less detached than the explanation of the alphanumeric cipher.

edit: on second thought, she technically wouldn't have to know the encryption scheme, so she would have to try more rainbow tables and also take longer for the brute force.

edit: never mind, misunderstood the story, they wouldn't use an encryption scheme because they don't need quick one way verification.  disregard this suggestion :)

edit: wait, but if she doesn't know the cipher, how does she recognize the right answer when she sees it? &amp;gt; on second thought, she technically wouldn't have to know the encryption scheme, so she would have to try more rainbow tables and also take longer for the brute force.

You can usually get a pretty good idea of the algorithm in use by the output length and context.

&amp;gt; wait, but if she doesn't know the cipher, how does she recognize the right answer when she sees it?

You don't for sure. Take anything that results in only printable ASCII characters and manually examine the results (there shouldn't be a whole lot) for one that looks like a name, y'know, "Bob Bobbington" instead of "F234ojr34QF$3ff4". &amp;gt;You can usually get a pretty good idea of the algorithm in use by the output length and context.

Well, that's because we right now only use a few number of common encryption schemes.  Here, the cipher seems to be in character space, and can be as complicated and elaborate as the cipher creator wants (doesn't have to be quick or standardized).
&amp;gt;
&amp;gt;You don't for sure. Take anything that results in only printable ASCII characters and manually examine the results (there shouldn't be a whole lot) for one that looks like a name, y'know, "Bob Bobbington" instead of "F234ojr34QF$3ff4".

the first developer's name is (-), the other names could be even less obvious :\ &amp;gt;You can usually get a pretty good idea of the algorithm in use by the output length and context.

Well, that's because we right now only use a few number of common encryption schemes.  Here, the cipher seems to be in character space, and can be as complicated and elaborate as the cipher creator wants (doesn't have to be quick or standardized).
&amp;gt;
&amp;gt;You don't for sure. Take anything that results in only printable ASCII characters and manually examine the results (there shouldn't be a whole lot) for one that looks like a name, y'know, "Bob Bobbington" instead of "F234ojr34QF$3ff4".

the first developer's name is (-), the other names could be even less obvious :\ I would change it so that she somehow knows the encryption scheme ( md5, sha, etc.)

Then she does the math.  Then she tries a rainbow table, which doesn't find any hits after X time.  Then she sighs, makes the phone call, gets the botnet, and brute forces it.

That seems more realistic to me, and also less detached than the explanation of the alphanumeric cipher.

edit: on second thought, she technically wouldn't have to know the encryption scheme, so she would have to try more rainbow tables and also take longer for the brute force.

edit: never mind, misunderstood the story, they wouldn't use an encryption scheme because they don't need quick one way verification.  disregard this suggestion :)

edit: wait, but if she doesn't know the cipher, how does she recognize the right answer when she sees it? I think this is the way I'll go. Thanks. I would change it so that she somehow knows the encryption scheme ( md5, sha, etc.)

Then she does the math.  Then she tries a rainbow table, which doesn't find any hits after X time.  Then she sighs, makes the phone call, gets the botnet, and brute forces it.

That seems more realistic to me, and also less detached than the explanation of the alphanumeric cipher.

edit: on second thought, she technically wouldn't have to know the encryption scheme, so she would have to try more rainbow tables and also take longer for the brute force.

edit: never mind, misunderstood the story, they wouldn't use an encryption scheme because they don't need quick one way verification.  disregard this suggestion :)

edit: wait, but if she doesn't know the cipher, how does she recognize the right answer when she sees it?  Like others have said I don't think it would be too interesting to make it completely accurate.  One thing that I would like to mention though is that the "She called a friend and downloaded..." part seemed really lame compared to the rest of the piece.  It seemed like you didn't know how to introduce having this program, but I think there are a lot better ways than just "she called a friend".  Other than that it sounds pretty cool! Maybe she could compile the program from source code herself. That'd kind-of make sense, since the compiler could optimize the code for her machine, making it a little faster than generic downloaded code. That's a neat suggestion.  "She ran the makefile and compiled the cracker against the system's architecture.  This would speed things up tremendously to take advantage of &amp;lt;xyz&amp;gt;.  After the compilation she opened up a terminal and ran it with &amp;lt;xyz&amp;gt; flags to further optimize the speed."  Of course I'm not much of a writer. :)   &amp;gt;Her estimate assumed that there was a one-to-one relationship between these characters and the characters in the name

I'm not sure how her estimate could do this, padding could always be added, it depends on the method used to cipher the text. A simple Caesar Cipher would work this way but more complex ciphers could transpose requirements that need a particular length.

&amp;gt;Further, the meaning of each repeated character could be different at each instant, based on its position.

Everything up to this point that you are talking about deals with simplistic ciphers such as the Caesar Cipher, but with an expanded alphabet, at this point you are proposing a rotating cipher which is very complex in implementation and I'm not aware of any such cipher existing.

&amp;gt;Faceless users noticed slight dips in frame-rates and browsing speeds.

Wait, what? Frame Rates are handeled by graphics cards not CPUs. The CPU just requests the image like normal and the GPU does the processing.
 To clarify:

The result, 3.7x10^31, comes from calculating 94^16 - which assumes a one-to-one relationship between letters in the code and letters in the answer.

The character realizes that number would only pertain to a simple cipher - and the narrative goes on to explain some of the ways that her estimate would be wrong. It implies that the cipher could have many more possibilities than the simple 3.7x1031

I don't know if cracking such a code is possible right now, but the text takes place somewhere in the future. So we can pretend that there are ways.

Good catch on the frame-rate thing. I'll have to find some other way of saying that users experience a dip in performance.
 No, frame rate can easily be affected by CPU usage.  The CPU has to process information to give to the GPU, otherwise the GPU has no work to do.

For example, the Source game engine ( HL2, TF2, etc) is heavily heavily CPU bound because of the physics engine.  Run some 100% cpu process on every core with prime95, and you'll see your FPS drop like a rock. Thanks for clarifying. another way of showing the cpu usage are fans spinning up (laptops especially). Anyone who's into their computers and does a lot of intensive processing probably has something monitoring CPU temp too.

This is even more likely if they're overclocking.

I'm a developer, and I still love to watch the temp of my CPU, there's an automatic cutoff of the machine at 80&#176;(it's supposed to protect the machine, all it does is annoy me), so I am constantly making sure it doesn't go above 75&#176;, even if that means I need to pause tasks while it cools back down. Thanks for clarifying. To clarify:

The result, 3.7x10^31, comes from calculating 94^16 - which assumes a one-to-one relationship between letters in the code and letters in the answer.

The character realizes that number would only pertain to a simple cipher - and the narrative goes on to explain some of the ways that her estimate would be wrong. It implies that the cipher could have many more possibilities than the simple 3.7x1031

I don't know if cracking such a code is possible right now, but the text takes place somewhere in the future. So we can pretend that there are ways.

Good catch on the frame-rate thing. I'll have to find some other way of saying that users experience a dip in performance.
  You sort of contradicted yourself here. When you said "there was a one-to-one relationship between these characters and the characters in the name", you imply that a caesar shift or something similar is being used, but then you say "the meaning of each repeated character...based on position", that's no longer one-to-one.     Have you been published? I suppose you intended to remain anonymous, so maybe you could pm me your name? I'm just curious; no big deal if you'd rather not. I have been published in small magazines. I've written two books, but I didn't try to get them published. They say every writer has a million lousy words at the top of the pile. They need to burn through those before they have anything worthwhile to say. So, those two books were my payment of dues.

I do hope to get my third book published when all is said and done. I've written about 40K words for it so far (not counting the other 40K I threw away). For me, that's a 240 page manuscript. But that's pretty abstract - a manuscript is printed on a full 8.5" x 11" in 12pt w/ double spaced lines. The average contemporary novel is between 60k and 80k words.

All this to say that no, you wouldn't be able to find my work anywhere. Yet.

But maybe I'll get lucky. Maybe I'll be back here in a year with a picture of my acknowledgements page, showing where it says "and thanks to all the folks at Reddit for answering my dumb questions about technical things. Thanks for answering. I love literature and go through periods of fantasizing about writing a novel, but hearing from actual writers about how difficult that task is always brings me down to Earth. Good luck!   </snippet></document><document><title>Charts and heuristics for grasping the advance of computational power from birth of computing to present?</title><url>http://www.reddit.com/r/compsci/comments/wwrop/charts_and_heuristics_for_grasping_the_advance_of/</url><snippet>A friend of mine recently asserted that a contemporary iPhone has a bit more power than all the computers used in the initial moon landing combined.  I took him at his word on that point.  More than anything, it struck me that as a relative cs novice I have only the most basic conception of how computing power has developed.  Do you know of any charts or other heuristic devices for easily apprehending the accelerating rate of increase in computing power?     This is probably what you're looking for: http://computer.howstuffworks.com/moores-law.htm

Also, there's a reason that the Apollo 11 computer system was so simple. It's because the simpler the system, the fewer the potential points of failure. One bug in the software could potentially mean catastrophic failure, loss of life, astronomical amounts of money wasted, a national catastrophe, and the Soviets laugh at the USA. This is true. In *addition*, radiation, from being outside Earth's magnetic field, drastically raises the chances that typical semiconductor-based circuits have to fail. The physical design of the Apollo 11 hardware had to vary.  Latching on to this question, are there any estimates for advances in *algorithmic* power? I've heard that it's also somewhat logarithmic, but I'm curious if there is any attempt at quantifying a statement like that. Things like fast Fourier transforms, merge sort, and RK4 did not always exist. Some of the algorithms we use predate computers, but I'm also curious about their adoption into programming. Latching on to this question, are there any estimates for advances in *algorithmic* power? I've heard that it's also somewhat logarithmic, but I'm curious if there is any attempt at quantifying a statement like that. Things like fast Fourier transforms, merge sort, and RK4 did not always exist. Some of the algorithms we use predate computers, but I'm also curious about their adoption into programming. How would you compare the algorithmic power of a fast algorithm which can only run on one machine to a somewhat slower algorithm which can be distributed across many machines?   </snippet></document><document><title>What is a Loop Invariant for linear search?</title><url>http://www.reddit.com/r/compsci/comments/wvyvs/what_is_a_loop_invariant_for_linear_search/</url><snippet>So I m going through Intro to Algs by CLRS and they don't mention what would be a loop invariant for search algorithms in general nor do they mention it for linear search. So here's my implementation in python:

    v=23
    a= [8,2,10,1,9,23,4]
    for i in range(len(a)):
        if a[i] == v
            print i
            break
        else: continue


I m currently working on binary search and I can see that it is faster than this. In fact, I m pretty sure T(n)= O(n) for linear search whereas T(n)=O(lg n) for binary search. But I can't figure out what a loop invariant would be for either one of them. The only thing i can think of as a loop invariant is: v is in a or it is not.

So yeah,  help?  For binary search, you have a range that you are constantly shrinking until its only a range of 1 number. So at each iteration, you know that your answer is not outside of that range.
For linear search, your result is not in the first i entries of a. For binary search you have to expand it a little. You know what ranges the search value is **not** in, but you do not know if it's actually there. For binary search, you have a range that you are constantly shrinking until its only a range of 1 number. So at each iteration, you know that your answer is not outside of that range.
For linear search, your result is not in the first i entries of a.    Your loop invariant is a good start, but it can also depend on i. how can i turn this loop invariant: "v is in a or it is not" into one that one that involves i?

for all i, a[i] ==v or a[i] != v ?

 how can i turn this loop invariant: "v is in a or it is not" into one that one that involves i?

for all i, a[i] ==v or a[i] != v ?

 I would recommend writing your algorithms in psuedo-code and not in a specific language, this will make analysis much easier.  You should not be hard-coding in the search values, instead they should be inputs to the algorithm.  
   
**Algorithm**:  
Linear Search  
  
**Input**:  
* An integer v  
* An array of integers A of positive length, indexed from zero  
  
**Output**:  
* The index in A of the element with the value v  
* NULL if A does not have an element with the value v.
  
    LinearSearch(v : int, A : int[])  
    1. Let i = 0  
    2. While i &amp;lt; A.length)  Do  
        3. If A[i] = v Then  
            4. Ret i  
           End If  
        5. i := i + 1  
     End While
    6. Ret NULL
         


 I would recommend writing your algorithms in psuedo-code and not in a specific language, this will make analysis much easier.  You should not be hard-coding in the search values, instead they should be inputs to the algorithm.  
   
**Algorithm**:  
Linear Search  
  
**Input**:  
* An integer v  
* An array of integers A of positive length, indexed from zero  
  
**Output**:  
* The index in A of the element with the value v  
* NULL if A does not have an element with the value v.
  
    LinearSearch(v : int, A : int[])  
    1. Let i = 0  
    2. While i &amp;lt; A.length)  Do  
        3. If A[i] = v Then  
            4. Ret i  
           End If  
        5. i := i + 1  
     End While
    6. Ret NULL
         


 if i were taking this as an intro to algs course at a uni, would I be expected to turn in programs or pseudo code? if i were taking this as an intro to algs course at a uni, would I be expected to turn in programs or pseudo code? I can only speak of my university (University of Calgary).  We always used a high-level psuedo-code.  For example, a FOR loop would be easier to analyze (eg. unit cost criterion, assertions, correctness) when it is converted to a simple WHILE loop.  Also the assertion would be easier to place.  

   I would **highly** recommend [this](http://www.amazon.ca/Introduction-Algorithms-Second-Edition-Thomas/dp/0262032937) book, if you are at all interested in algorithm design and analysis.  
  
[EDIT] Just finsihed a very difficult algorithms course, this was my bible.</snippet></document><document><title>How do I train for/learn to solve problems like the ones in Google Code Jam?</title><url>http://www.reddit.com/r/compsci/comments/wt9n6/how_do_i_train_forlearn_to_solve_problems_like/</url><snippet>Hey all,

I'm starting college this fall and while I can do a fair few of the Google Code Jam practice problems, I look at some of the solutions and they are ridiculously awesome. How can I learn to do this? Will it come naturally after 4 years of a CS degree?

Cheers  Very good question. A lot of the basics will be taught in computer science and discrete maths courses -- things like graph theory, computational geometry, dynamic programming, harder data structures, etc. Apart from that, you need to practice. TopCoder runs algorithm competitions like Code Jam every few weeks. I've heard SPOJ has good problem sets, but I personally don't use them. Get involved with your college's ACM-ICPC team/s. CodeSprint runs regular contests, and ProjectEuler.net is great for training math/programming skills. In fact, if you focus on Project Euler for a few months/years, you'll be able to solve most code jam problems.

Algorithm competitions are also run for high school students, and they can get very very hard. Most countries have a dedicated training site (USA = usaco.org, Australia = orac.amt.edu.au, etc.) and they usually have loads of practice problems. When you get good, you'll want to start training on problems from the International Olympiad of Informatics (problems here: http://ioinformatics.org/contest/prev.shtml).

I'm heavily involved in the algorithms/informatics scene and though I'm no genius, I know lots of people who are, including multiple international medallists and high-ranking CodeJam contestants (one of whom came first in a round). If you have any specific questions (e.g. about how to approach a problem) I can try to answer. Thanks for the detailed answer! I'll definitely be looking up those resources :) I've noticed a lot of the solutions are done in C++ - any idea why this is? I don't know C++ all the well yet, but I suppose I'll learn it fairly soon :) No problem. The C++ used by contestants is very different to the sort you'll see in industry -- it's more like pure C with a couple of C++ features. C provides a good balance between runtime speed, memory usage, high-level abstraction and low-level control. The first two are particularly important because contests will usually put time and memory limits on your program (this is less relevant in Code Jam where you're submitting an output file instead of your code, but you still want it to run fast on your machine).

C++ has all of C's benefits, and it also has an awesome thing called the STL (standard template library). This gives you fast implementations of common data structures and algorithms and makes them really easy to invoke. For instance, you can either code up a binary heap from scratch in C which will take a while (especially if you also mess it up and need to debug it), or you can #include &amp;lt;queue&amp;gt; and initialise a binary heap with priority_queue&amp;lt;int&amp;gt; pq; In addition to this, the STL has useful algorithms including next_permutation, nth_element, and binary_search, that C doesn't.

And on some level, it's just a vicious cycle. Some contests only support C/++ and Java, so programmers learn to compete in C/++ or Java, so other contests decide to only support C/++ and Java, etc. Awesome, guess I better learn C++ asap then :) Awesome, guess I better learn C++ asap then :) No problem. The C++ used by contestants is very different to the sort you'll see in industry -- it's more like pure C with a couple of C++ features. C provides a good balance between runtime speed, memory usage, high-level abstraction and low-level control. The first two are particularly important because contests will usually put time and memory limits on your program (this is less relevant in Code Jam where you're submitting an output file instead of your code, but you still want it to run fast on your machine).

C++ has all of C's benefits, and it also has an awesome thing called the STL (standard template library). This gives you fast implementations of common data structures and algorithms and makes them really easy to invoke. For instance, you can either code up a binary heap from scratch in C which will take a while (especially if you also mess it up and need to debug it), or you can #include &amp;lt;queue&amp;gt; and initialise a binary heap with priority_queue&amp;lt;int&amp;gt; pq; In addition to this, the STL has useful algorithms including next_permutation, nth_element, and binary_search, that C doesn't.

And on some level, it's just a vicious cycle. Some contests only support C/++ and Java, so programmers learn to compete in C/++ or Java, so other contests decide to only support C/++ and Java, etc. I look forward to the day where Python et al are seen as viable languages for these sorts of competitions because of expressiveness of the language, rather than trying to eke out a few extra milliseconds of runtime efficiency. I feel like C/++ is a "boy's club", a sort of self-fulfilling prophesy where one cannot be taken seriously or be seen as competitive unless they are hyper-literate in C/++, rather than appreciating that C/++, while general purpose, has perhaps outlived its usefulness.

That said, I am literate in C/++, and have had to remain so to continue my participation in these sorts of competitions, despite having moved to Python as my language of choice.

**EDIT:** I'll remind the people downvoting that downvotes are reserved for things that are off-topic. Downvoting because you disagree is childish. Furthermore, I expect better from a focused, academic community like r/compsci. This "hurr durr, I feel differently, taste my downvotes" mentality should be left at r/f7u12 or r/wtf. &amp;gt; I look forward to the day where Python et al are seen as viable languages for these sorts of competitions because of expressiveness of the language, rather than trying to eke out a few extra milliseconds of runtime efficiency.

Python is very roughly 10x as slow as C++, and contest input limits are often designed to require around a second to solve, so we're talking about a very big difference here. The problem is that you can have a right answer in the wrong language, and you will fail. This is a fundamental flaw with the programming competition paradigm. I understand that Python is slower, but realistically, this won't be the case forever. And then Python will jump in the mix, just as Java has. Wouldn't it be better to have a structure that rewards a correct solution rather than a fast one?

EDIT: Also, you are perhaps too kind, giving Python the accolade of being only 10x slower. In reality, it can be orders of magnitude worse. Being fast enough is part of being correct. Thank goodness, too, because a contest that didn't care about speed would be really boring. By correct, I'm referring to having the most algorithmically efficient solution (magnitude of complexity). If we could somehow divorce the word "fast" from the number of clock cycles, students could grow as algorithmists, rather than just being really good at bit twiddling and writing the same damn min spanning tree over and over. The problem is, we don't really know which operations to minimize in our algorithms unless we consider the actual amount of time needed as well as the nature of the computer's architecture. C++ exposes more of the computer's internals than other languages, thus it is kind of a baseline. If you write an awesome algorithm in another language and it's not as fast as C++, you won't know if the problem is a bug or the interaction of the algorithm and the machine. Encouraging C++ use in competitions is also good because C++ is everywhere, it's free and widely used in industry, and using the same language puts everyone on the same footing in some sense. By correct, I'm referring to having the most algorithmically efficient solution (magnitude of complexity). If we could somehow divorce the word "fast" from the number of clock cycles, students could grow as algorithmists, rather than just being really good at bit twiddling and writing the same damn min spanning tree over and over. The problem is that you can have a right answer in the wrong language, and you will fail. This is a fundamental flaw with the programming competition paradigm. I understand that Python is slower, but realistically, this won't be the case forever. And then Python will jump in the mix, just as Java has. Wouldn't it be better to have a structure that rewards a correct solution rather than a fast one?

EDIT: Also, you are perhaps too kind, giving Python the accolade of being only 10x slower. In reality, it can be orders of magnitude worse. &amp;gt; I understand that Python is slower, but realistically, this won't be the case forever.

Um, no. It really *will* be the case forever. Python's a great language, but it is simply impossible to do certain types of optimization on a language that supports only dynamic typing. Statically typed languages will always have an edge in performance. I look forward to the day where Python et al are seen as viable languages for these sorts of competitions because of expressiveness of the language, rather than trying to eke out a few extra milliseconds of runtime efficiency. I feel like C/++ is a "boy's club", a sort of self-fulfilling prophesy where one cannot be taken seriously or be seen as competitive unless they are hyper-literate in C/++, rather than appreciating that C/++, while general purpose, has perhaps outlived its usefulness.

That said, I am literate in C/++, and have had to remain so to continue my participation in these sorts of competitions, despite having moved to Python as my language of choice.

**EDIT:** I'll remind the people downvoting that downvotes are reserved for things that are off-topic. Downvoting because you disagree is childish. Furthermore, I expect better from a focused, academic community like r/compsci. This "hurr durr, I feel differently, taste my downvotes" mentality should be left at r/f7u12 or r/wtf. &amp;gt;EDIT: I'll remind the people downvoting that downvotes are reserved for things that are off-topic. Downvoting because you disagree is childish. Furthermore, I expect better from a focused, academic community like r/compsci. This "hurr durr, I feel differently, taste my downvotes" mentality should be left at r/f7u12 or r/wtf.


From the Reddiquette:

&amp;gt;[Please don't] Complain about downvotes on your posts. Millions of people use reddit; every story and comment gets at least a few downvotes. 

Personally I find downvote complaints one of the most annoying things one can do. As we move further and further from the topic at hand...

I edited my post (not made a new one) when my posts were getting hidden just because some people didn't agree with my opinion. I was merely pointing out that discussion is hard when your posts get hidden. I'd much rather have an intelligent discussion in an intelligent subreddit, but if you'd like to talk about Reddiquette... I look forward to the day where Python et al are seen as viable languages for these sorts of competitions because of expressiveness of the language, rather than trying to eke out a few extra milliseconds of runtime efficiency. I feel like C/++ is a "boy's club", a sort of self-fulfilling prophesy where one cannot be taken seriously or be seen as competitive unless they are hyper-literate in C/++, rather than appreciating that C/++, while general purpose, has perhaps outlived its usefulness.

That said, I am literate in C/++, and have had to remain so to continue my participation in these sorts of competitions, despite having moved to Python as my language of choice.

**EDIT:** I'll remind the people downvoting that downvotes are reserved for things that are off-topic. Downvoting because you disagree is childish. Furthermore, I expect better from a focused, academic community like r/compsci. This "hurr durr, I feel differently, taste my downvotes" mentality should be left at r/f7u12 or r/wtf. I look forward to the day where Python et al are seen as viable languages for these sorts of competitions because of expressiveness of the language, rather than trying to eke out a few extra milliseconds of runtime efficiency. I feel like C/++ is a "boy's club", a sort of self-fulfilling prophesy where one cannot be taken seriously or be seen as competitive unless they are hyper-literate in C/++, rather than appreciating that C/++, while general purpose, has perhaps outlived its usefulness.

That said, I am literate in C/++, and have had to remain so to continue my participation in these sorts of competitions, despite having moved to Python as my language of choice.

**EDIT:** I'll remind the people downvoting that downvotes are reserved for things that are off-topic. Downvoting because you disagree is childish. Furthermore, I expect better from a focused, academic community like r/compsci. This "hurr durr, I feel differently, taste my downvotes" mentality should be left at r/f7u12 or r/wtf. I look forward to the day where Python et al are seen as viable languages for these sorts of competitions because of expressiveness of the language, rather than trying to eke out a few extra milliseconds of runtime efficiency. I feel like C/++ is a "boy's club", a sort of self-fulfilling prophesy where one cannot be taken seriously or be seen as competitive unless they are hyper-literate in C/++, rather than appreciating that C/++, while general purpose, has perhaps outlived its usefulness.

That said, I am literate in C/++, and have had to remain so to continue my participation in these sorts of competitions, despite having moved to Python as my language of choice.

**EDIT:** I'll remind the people downvoting that downvotes are reserved for things that are off-topic. Downvoting because you disagree is childish. Furthermore, I expect better from a focused, academic community like r/compsci. This "hurr durr, I feel differently, taste my downvotes" mentality should be left at r/f7u12 or r/wtf. Very good question. A lot of the basics will be taught in computer science and discrete maths courses -- things like graph theory, computational geometry, dynamic programming, harder data structures, etc. Apart from that, you need to practice. TopCoder runs algorithm competitions like Code Jam every few weeks. I've heard SPOJ has good problem sets, but I personally don't use them. Get involved with your college's ACM-ICPC team/s. CodeSprint runs regular contests, and ProjectEuler.net is great for training math/programming skills. In fact, if you focus on Project Euler for a few months/years, you'll be able to solve most code jam problems.

Algorithm competitions are also run for high school students, and they can get very very hard. Most countries have a dedicated training site (USA = usaco.org, Australia = orac.amt.edu.au, etc.) and they usually have loads of practice problems. When you get good, you'll want to start training on problems from the International Olympiad of Informatics (problems here: http://ioinformatics.org/contest/prev.shtml).

I'm heavily involved in the algorithms/informatics scene and though I'm no genius, I know lots of people who are, including multiple international medallists and high-ranking CodeJam contestants (one of whom came first in a round). If you have any specific questions (e.g. about how to approach a problem) I can try to answer. Very good question. A lot of the basics will be taught in computer science and discrete maths courses -- things like graph theory, computational geometry, dynamic programming, harder data structures, etc. Apart from that, you need to practice. TopCoder runs algorithm competitions like Code Jam every few weeks. I've heard SPOJ has good problem sets, but I personally don't use them. Get involved with your college's ACM-ICPC team/s. CodeSprint runs regular contests, and ProjectEuler.net is great for training math/programming skills. In fact, if you focus on Project Euler for a few months/years, you'll be able to solve most code jam problems.

Algorithm competitions are also run for high school students, and they can get very very hard. Most countries have a dedicated training site (USA = usaco.org, Australia = orac.amt.edu.au, etc.) and they usually have loads of practice problems. When you get good, you'll want to start training on problems from the International Olympiad of Informatics (problems here: http://ioinformatics.org/contest/prev.shtml).

I'm heavily involved in the algorithms/informatics scene and though I'm no genius, I know lots of people who are, including multiple international medallists and high-ranking CodeJam contestants (one of whom came first in a round). If you have any specific questions (e.g. about how to approach a problem) I can try to answer. Very good question. A lot of the basics will be taught in computer science and discrete maths courses -- things like graph theory, computational geometry, dynamic programming, harder data structures, etc. Apart from that, you need to practice. TopCoder runs algorithm competitions like Code Jam every few weeks. I've heard SPOJ has good problem sets, but I personally don't use them. Get involved with your college's ACM-ICPC team/s. CodeSprint runs regular contests, and ProjectEuler.net is great for training math/programming skills. In fact, if you focus on Project Euler for a few months/years, you'll be able to solve most code jam problems.

Algorithm competitions are also run for high school students, and they can get very very hard. Most countries have a dedicated training site (USA = usaco.org, Australia = orac.amt.edu.au, etc.) and they usually have loads of practice problems. When you get good, you'll want to start training on problems from the International Olympiad of Informatics (problems here: http://ioinformatics.org/contest/prev.shtml).

I'm heavily involved in the algorithms/informatics scene and though I'm no genius, I know lots of people who are, including multiple international medallists and high-ranking CodeJam contestants (one of whom came first in a round). If you have any specific questions (e.g. about how to approach a problem) I can try to answer. Very good question. A lot of the basics will be taught in computer science and discrete maths courses -- things like graph theory, computational geometry, dynamic programming, harder data structures, etc. Apart from that, you need to practice. TopCoder runs algorithm competitions like Code Jam every few weeks. I've heard SPOJ has good problem sets, but I personally don't use them. Get involved with your college's ACM-ICPC team/s. CodeSprint runs regular contests, and ProjectEuler.net is great for training math/programming skills. In fact, if you focus on Project Euler for a few months/years, you'll be able to solve most code jam problems.

Algorithm competitions are also run for high school students, and they can get very very hard. Most countries have a dedicated training site (USA = usaco.org, Australia = orac.amt.edu.au, etc.) and they usually have loads of practice problems. When you get good, you'll want to start training on problems from the International Olympiad of Informatics (problems here: http://ioinformatics.org/contest/prev.shtml).

I'm heavily involved in the algorithms/informatics scene and though I'm no genius, I know lots of people who are, including multiple international medallists and high-ranking CodeJam contestants (one of whom came first in a round). If you have any specific questions (e.g. about how to approach a problem) I can try to answer. Are you writing code for Project Euler?    No it will definitely not come naturally. you have to work at it over a long period of time and keep revisiting problem classes after gaining more knowledge in certain classes. And it helps to get over the fact that some people are just born with supernatural problem solving skills. &amp;gt;And it helps to get over the fact that some people are just born with supernatural problem solving skills.

I've always disagreed with this. I don't think that anyone is literally *born with* problem solving skills. Some people have just been working at it for a much longer time, since childhood. It may be due to being lucky enough to have parents that bought their kid a computer when he was 6, or having a parent that is a professor and has a propensity to instill curiosity from a young age, or reading a lot of books since elementary school and not balking at being teased for being "a nerd", and so on.

It always irks me when a student starts a major, is exposed to the topic for effectively the first time, sees classmates that have been at it for years already, and complains that it isn't fair that some people are "born with supernatural skills". It cheapens the amount of work that those people have put into developing and polishing a myriad of skills and is an anti-intellectual attitude. Instead of feeling inadequate or developing jealousy, make friends with and learn from these people. Put in enough work and you very well may outperform them in a while, as long as you aren't put off by the fact that it will be difficult.

**Edit**: To expand on my rant a bit more: nothing inherently comes naturally to anyone. Take a child that has just begun to speak and try to accurately guess whether they will become an engineer, programmer, artist, musician, writer, or anything else. You probably won't, save a few exceptional cases that science hasn't explained, like the "idiot savant". This should suffice to demonstrate that it is silly to suggest to someone to "accept that some people are just born with supernatural problem solving skills". Would it irk you if that student found the subject simple on her first exposure? &amp;gt;And it helps to get over the fact that some people are just born with supernatural problem solving skills.

I've always disagreed with this. I don't think that anyone is literally *born with* problem solving skills. Some people have just been working at it for a much longer time, since childhood. It may be due to being lucky enough to have parents that bought their kid a computer when he was 6, or having a parent that is a professor and has a propensity to instill curiosity from a young age, or reading a lot of books since elementary school and not balking at being teased for being "a nerd", and so on.

It always irks me when a student starts a major, is exposed to the topic for effectively the first time, sees classmates that have been at it for years already, and complains that it isn't fair that some people are "born with supernatural skills". It cheapens the amount of work that those people have put into developing and polishing a myriad of skills and is an anti-intellectual attitude. Instead of feeling inadequate or developing jealousy, make friends with and learn from these people. Put in enough work and you very well may outperform them in a while, as long as you aren't put off by the fact that it will be difficult.

**Edit**: To expand on my rant a bit more: nothing inherently comes naturally to anyone. Take a child that has just begun to speak and try to accurately guess whether they will become an engineer, programmer, artist, musician, writer, or anything else. You probably won't, save a few exceptional cases that science hasn't explained, like the "idiot savant". This should suffice to demonstrate that it is silly to suggest to someone to "accept that some people are just born with supernatural problem solving skills". Autism. What about it? It gives superpowers!

Couldn't help it sorry, I shouldn't be kidding, it's a serious issue, but it is a condition you can born with, and while you don't get superpowers you get amazing logical and rational skills.

If not affected by other worse deceases, autistic persons can do something that for the others is way more difficult, they can shut themselves in and block all the incoming 'noise' around them, this provides a excellent environment for thinking through this problems.

I've had a colleague who suffered a mild case of autism, it was border line between serious autism and asperger syndrome, he floated back and forth between them and had some cases of random hysteria and suffered a little bit with them (socially), but on the programming field, man was he great, he produced the cleanest most thought through code of all class, we were learning advanced algorithms in Java at the time and I remember struggling almost a full weekend to get a efficient fibonacci-queue code passing all the tests in the given time (we used automation tests that were also time measured to prove efficiency) and him being able to solve it in the CLASS the teacher presented it. All by himself, just by not talking to anyone, and thinking it through.

It's just a case I know, but thinking "superpowers" on autistic people is more than well documented and yes, it's something you can be born with. Again, they are *not born with any supernatural skills*. They are born with a mental condition that causes a deficiency in several aspects, and some of them compensate by obsessing with a certain topic. They still put in tremendous amounts of time and effort to develop whatever skills they may have. Most college kids could probably do it too if they weren't off getting wasted three nights a week and getting high the rest of the week.

Also, anecdotes are not data. Yes is a mental condition and yes it gives advantages and disadvantages like all condition's, but they don't "compensate" nor they put tremendous effort, our brains are wired differently and take different paths to solve the same problem

Also way to go being an asshole and downvoting me just because you don't share the same opinion, but fine be an internet jerk, what did you want? a video of me and my colleague on the class with an "I'm autistic" postit stapled to his head? ffs If you want hard evidence why don't you just pull a book about autistic behavior from the shelf or search the wikipedia? It gives superpowers!

Couldn't help it sorry, I shouldn't be kidding, it's a serious issue, but it is a condition you can born with, and while you don't get superpowers you get amazing logical and rational skills.

If not affected by other worse deceases, autistic persons can do something that for the others is way more difficult, they can shut themselves in and block all the incoming 'noise' around them, this provides a excellent environment for thinking through this problems.

I've had a colleague who suffered a mild case of autism, it was border line between serious autism and asperger syndrome, he floated back and forth between them and had some cases of random hysteria and suffered a little bit with them (socially), but on the programming field, man was he great, he produced the cleanest most thought through code of all class, we were learning advanced algorithms in Java at the time and I remember struggling almost a full weekend to get a efficient fibonacci-queue code passing all the tests in the given time (we used automation tests that were also time measured to prove efficiency) and him being able to solve it in the CLASS the teacher presented it. All by himself, just by not talking to anyone, and thinking it through.

It's just a case I know, but thinking "superpowers" on autistic people is more than well documented and yes, it's something you can be born with.  Some CS degrees will teach you knowledge needed to do those efficiently, but only experience and a very good cheat sheet will get you to the top. What would said cheat sheet contain? And do you know where I can find more problems like these? They seem fun :) To get started, take a look at some of the winning solutions in Code Jam.  A lot of them contain a lot of quick #defines where they define a bunch of common shortcut functions:

    using namespace std;
    #define VAR(a,b) __typeof(b) a=(b)
    #define FOR(i,a,b) for (int _n(b), i(a); i &amp;lt; _n; i++)
    #define FORD(i,a,b) for(int i=(a),_b=(b);i&amp;gt;=_b;--i)
    #define FOREACH(it,c) for(VAR(it,(c).begin());it!=(c).end();++it)
    #define REP(i,n) FOR(i,0,n)
    #define ALL(c) (c).begin(), (c).end()
    #define SORT(c) sort(ALL(c))
    #define REVERSE(c) reverse(ALL(c))
    #define UNIQUE(c) SORT(c),(c).resize(unique(ALL(c))-(c).begin())
    #define INF 1000000000
    #define X first
    #define Y second
    #define pb push_back
    #define SZ(c) (c).size()
    typedef pair&amp;lt;int, int&amp;gt; PII;
    typedef vector&amp;lt;int&amp;gt; VI;
    typedef vector&amp;lt;PII&amp;gt; VPII;
    typedef vector&amp;lt;VI&amp;gt; VVI; This stuff is helpful, but only a little. Petr has been among the best on TopCoder for ages, and he doesn't use defines. Agreed.  Once you get to a certain level where the problems become significantly more difficult, then it becomes more "do you have the correct mindset to break down this problem", rather than having any cheat sheets help you. This stuff is helpful, but only a little. Petr has been among the best on TopCoder for ages, and he doesn't use defines. What would said cheat sheet contain? And do you know where I can find more problems like these? They seem fun :)  No, this will not come naturally after college, trust me. It takes a lot of dedication and practice outside of your normal classwork.

I did competitive programming all 4 years in high school in ACSL and TCEA. Contests were usually 2-5 hours on a team of 3 people with 1 computer. In college did the same thing a couple times with ACM.

I dedicated many hours a week during my peak competitive times to practicing problem solving, especially rewriting algorithms constantly and learning how to make them generic to apply to many different types of problems.

The practice pays off. I can approach any code at work with ease, and formulate solutions and design ridiculously fast. Many people often think I'm a genius, but I'm not, I just have extensive experience and A LOT of programming hours under my belt. I highly recommend you keep on this path, it will pay off and definitely help your career as a software developer.

Now in my free time I just like to solve problems on sites like http://projecteuler.net/ and a few others when I see them pop up, and they look interesting and fun. I have never seen Google Code Jam before, will definitely check it out, though I don't have the time anymore for any actual competitions. Let's say I wanted to go with your route. I'm taking a math degree right now with some comp sci courses on the side. I'm taking my 2nd one right now (during the summer). 

How do I go about getting as good as you? Like what did you do every week? What were the major courses that helped you out where as the other ones you could pass on them?  

I want to be a good software developer but i only have 2 months of programming experience.  The only computer science courses that will have a huge impact are data structures, algorithms, and maybe software design.
Electives like compiler design and programming language paradigms will also teach you a lot about advanced coding concepts.

I would say there are essential math courses if you really want to be able to nail some advanced algorithm concepts: Discrete math, linear algebra, numerical methods, numerical analysis and of course graph theory.

To me it was really about everything you teach yourself outside of class. Doing code problems, and then refactoring your code to make it better, and doing them in multiple languages and paradigms.

**Ultimately, in my opinion, the "fast-track" to becoming excellent at these problems is to do them in a functional programming language. See http://en.wikipedia.org/wiki/Functional_programming**

My first functional language was Clojure. I felt a HUGE increase in my understanding of advanced concepts after I forced myself to tackle problems in a functional style rather then object-oriented. I felt like I was "reborn" as a developer when I programmed in functional. I grew up mostly only knowing object-oriented, and then late in college I was introduced to functional programming. It is much more difficult, but your code is often much shorter, and the solutions look truly elegant.

So do this: solve every problem in an object-oriented language, and then in a mostly functional language. Go back to problems and refactor your code to make it more efficient. On stuff like project euler, after you have a solution, go to the forums and look at other peoples solutions and work to understand them. &amp;gt; It is much more difficult

Disagree. Functional code is easier to write. Well I agree with your statement that "functional code is easier to write" but what I'm saying is "functional programming is harder to learn".

Especially considering that all introductory CS courses seem to only focus on OO languages.

EDIT: In fact I'm surprised focus isn't put on functional programming in schools. I think it would be easier to learn coding if you started doing functional first, then learned about OOP later. I think developers coming out of college would be better overall. &amp;gt;Well I agree with your statement that "functional code is easier to write" but what I'm saying is "functional programming is harder to learn".

I disagree with this as well. From personal experience TA'ing, those who have not programmed before find it easier to learn functional programming than object-oriented. Our intro CS class teaches Haskell first, then some Java, with a bit of machine code and circuits. The Java is always harder for the students than the Haskell. Really, Haskell? I have played with it a few times and it is super cool. What college do you TA at?

I feel that this is probably the exception rather then the norm for intro classes, but kudos to your school for doing so. &amp;gt; It is much more difficult

Disagree. Functional code is easier to write. The only computer science courses that will have a huge impact are data structures, algorithms, and maybe software design.
Electives like compiler design and programming language paradigms will also teach you a lot about advanced coding concepts.

I would say there are essential math courses if you really want to be able to nail some advanced algorithm concepts: Discrete math, linear algebra, numerical methods, numerical analysis and of course graph theory.

To me it was really about everything you teach yourself outside of class. Doing code problems, and then refactoring your code to make it better, and doing them in multiple languages and paradigms.

**Ultimately, in my opinion, the "fast-track" to becoming excellent at these problems is to do them in a functional programming language. See http://en.wikipedia.org/wiki/Functional_programming**

My first functional language was Clojure. I felt a HUGE increase in my understanding of advanced concepts after I forced myself to tackle problems in a functional style rather then object-oriented. I felt like I was "reborn" as a developer when I programmed in functional. I grew up mostly only knowing object-oriented, and then late in college I was introduced to functional programming. It is much more difficult, but your code is often much shorter, and the solutions look truly elegant.

So do this: solve every problem in an object-oriented language, and then in a mostly functional language. Go back to problems and refactor your code to make it more efficient. On stuff like project euler, after you have a solution, go to the forums and look at other peoples solutions and work to understand them. Let's say I wanted to go with your route. I'm taking a math degree right now with some comp sci courses on the side. I'm taking my 2nd one right now (during the summer). 

How do I go about getting as good as you? Like what did you do every week? What were the major courses that helped you out where as the other ones you could pass on them?  

I want to be a good software developer but i only have 2 months of programming experience.   TopCoder.com should help. Oh wow, this seems pretty neat. Thanks! Topcoder has nice tutorials also. They also have practice rooms for past contests.

[Codeforces](http://codeforces.com/) have regular contests and there (like in codejam) you can see all successful submissions after the contest is over.

[Project Euler](http://projecteuler.net/) is awesome.

[Code Chef](http://www.codechef.com/) has regular contests. They have 2-3 monthly contests, I think.

Before you start solving these problems, you should make sure that your algorithms knowledge is decent. Without that, it will be hard to improve even by looking at solutions.  1. Study algorithms

2. Try a few problems

3. Review published solutions for problems you get stuck on or don't even know how to begin. After reading how it's done, try to make your own version. Copy/reference the original code a little as possible.   You might also be interested in the [AI Challenge](http://aichallenge.org) (started by the University of Waterloo Computer Science Club and sponsored by Google) and [Programming Praxis](http://programmingpraxis.com).          </snippet></document><document><title>DBLP: Live search of CS related paper bibliographies.</title><url>http://www.dblp.org/search/</url><snippet /></document><document><title>Can someone help me identify and learn how to read this data type diagram?</title><url>http://www.reddit.com/r/compsci/comments/wtlch/can_someone_help_me_identify_and_learn_how_to/</url><snippet>Fig 1. on page 3 of [this paper here](http://arxiv.org/pdf/0707.0744v1.pdf) (PDF warning). I don't know if I've ever seen this diagram type before and I'd like to learn about it as well as how to read it. I appreciate any help or insights in advance.    email Inge, she's still working here and I believe she teaches this stuff @ UvA ;) email is on the paper   My instant reaction is, it looks like a state machine.

No idea what it is specifically though.</snippet></document><document><title>Introduction to Queuing Theory - Cooper [PDF, full text]</title><url>http://www.cse.fau.edu/~bob/publications/IntroToQueueingTheory_Cooper.pdf</url><snippet /></document><document><title>Find an error, get a Bitcoin.</title><url>http://carlos.bueno.org/2012/07/paper-bitcoins.html</url><snippet>  On the design,

The Julia sets make it look like a 90's graphics programming book. I'd replace it with some microprint. The penrose tiling is pretty sweet, though.

The font is too round for money. It needs to be stern, like it means business. I'd suggest a monospace font for the number literals and keys, increasing readability and avoiding alignment problems like you have with the key on the front.

The QR codes are great. I really like the idea.

You also used a bunch of soft effects, like drop down shadows and fading borders. Those aren't very professional, either. You can easily do without. Note that QR codes don't need to be black on white, you could make them black and transparent on the penrose pattern. I think that will look cool.

I'm not so sure about the code on the backside, but it's okay. 

OP, If you don't mind, I'll try to make a new version of the design later this day. I'm a fan of the idea. If you want to help me, I could use any source you used or made. Sure! That's the idea. This was a one-day project and I couldn't think of anything clever to warrant microprinting. The code is linked in the post: https://github.com/aristus/bitcoin-printer The source .pxm file is not in the repo (it's 20MB) but the .png blanks for front and back are.

I learned graphic design back when it was still called "desktop publishing", so I suppose that shows. :) I had a hard time finding good monospaced typeface that was readable at three different sizes, but maybe you can. Perhaps an oldskool OCR A check typeface.

Another idea: the Penrose was generated by a Python program and manually included in the blank. It might be neat to generate a new one for each bill, perhaps seeded by the public address.

Edit: Just added the source graphics as well. Enjoy! Sure! That's the idea. This was a one-day project and I couldn't think of anything clever to warrant microprinting. The code is linked in the post: https://github.com/aristus/bitcoin-printer The source .pxm file is not in the repo (it's 20MB) but the .png blanks for front and back are.

I learned graphic design back when it was still called "desktop publishing", so I suppose that shows. :) I had a hard time finding good monospaced typeface that was readable at three different sizes, but maybe you can. Perhaps an oldskool OCR A check typeface.

Another idea: the Penrose was generated by a Python program and manually included in the blank. It might be neat to generate a new one for each bill, perhaps seeded by the public address.

Edit: Just added the source graphics as well. Enjoy! You are the first person I've met, besides myself, that uses pixelmator. Congratulations? :D I started using it because PXM1 was a bug-for-bug clone of Photoshop 5. Not CS5, version 5 from like 1997. All my shortcut memory worked. Version 2 is a step down, I think. They removed useful things like always showing the foreground/background colors and many of the one-key commands. On the design,

The Julia sets make it look like a 90's graphics programming book. I'd replace it with some microprint. The penrose tiling is pretty sweet, though.

The font is too round for money. It needs to be stern, like it means business. I'd suggest a monospace font for the number literals and keys, increasing readability and avoiding alignment problems like you have with the key on the front.

The QR codes are great. I really like the idea.

You also used a bunch of soft effects, like drop down shadows and fading borders. Those aren't very professional, either. You can easily do without. Note that QR codes don't need to be black on white, you could make them black and transparent on the penrose pattern. I think that will look cool.

I'm not so sure about the code on the backside, but it's okay. 

OP, If you don't mind, I'll try to make a new version of the design later this day. I'm a fan of the idea. If you want to help me, I could use any source you used or made.   Once you "spend" the paper Bitcoin, it becomes worthless.  Or, the Bitcoin, after used, could be worth more or less than what the paper counterpart shows.  If I'm not mistaken, the pub/priv keypair used by Bitcoin represents a bitcoin address, rather than a unit of currency.  I guess it would make more sense to have a "check card" with the keypairs of your address or addresses? It's somewhat of a problem, but les than you might think. Given a Bitcoin address you can query the log for all transactions in and out, and see how much is in the "account".

The only point of trust is that you are in sole possession of the private key, ie that the person giving you the token didn't keep a copy or post a photo of both sides to Facebook, etc. Given that I'm "minting" these at a par value of about 50 cents, it's not a big deal. These are like Knuth's "hexadecimal dollars", literally symbolic money.

But if I wanted to print, say, 100BTC bills, I'd use scratchoff technology and post signed lists of the public keys, etc. With a sufficient paper trail I think it'd be ok. It looks very nice, but could never work as a way to transfer bitcoins.

Suppose I have receive a 10 bitcoin note. I then reconstruct the private key from the note and save it. I then spend the 10 bitcoin note on something else at a shop. Next, I go to my computer and with the private key for the wallet holding the 10 bitcoins I spend them online, thus emptying the wallet and leaving the 10 bitcoin note I gave to the shop worthless.

Edit: Even supposing you made them tamper evident, there's nothing stopping someone from simply counterfeiting the notes.  The notes could be produced by a third party, which holds the money until the note is cashed.

This is kind of like how a debit card or traveler's cheques work.   Again, what stops counterfeiting? What stops the third party from printing a bunch of fake notes and then running with the money? There is no innate mechanism that would prevent it. It would require the backing of whatever government the notes are being spent in, which probably won't happen or at least not any time soon.

One of bitcoins' biggest advantages over other currencies is the fact that it it's security relies only on the strength of the underlying cryptosystem, any attempt to create a static physical representation ultimately removes that advantage.

If you want something convenient like a debit card, a smart card containing the private key encrypted with a password would be a much better solution and would not rely on trusting a third party. The notes could have all the security band notes have now (UV, IR, holograms...), and there could be a module to check with the third party.

Of course, if you're going that route, you're better of using actual debit cards...

Anyways, I like the concept. This is never going to happen, but it can be an inspiration to others. This project is worth a little thought. Once you "spend" the paper Bitcoin, it becomes worthless.  Or, the Bitcoin, after used, could be worth more or less than what the paper counterpart shows.  If I'm not mistaken, the pub/priv keypair used by Bitcoin represents a bitcoin address, rather than a unit of currency.  I guess it would make more sense to have a "check card" with the keypairs of your address or addresses? Once you "spend" the paper Bitcoin, it becomes worthless.  Or, the Bitcoin, after used, could be worth more or less than what the paper counterpart shows.  If I'm not mistaken, the pub/priv keypair used by Bitcoin represents a bitcoin address, rather than a unit of currency.  I guess it would make more sense to have a "check card" with the keypairs of your address or addresses?  Instead of storing half of the key on each side, why not encrypt the key with a one-time pad, storing the (one-time pad) key on one side and encrypted key on the other? Then you provably couldn't derive the key from only one side.  Not 1 BTC; 0.1 BTC.  Bitcoin bills like this should have a place to rip the code in half when you spend it.  And provide an easy way to verify the balance before redeeming it. That's an interesting idea. Of course there are lots of high-tech ways to tamper-proof something. I'm generally more interested in low-tech, distributed ideas. Nail polish covered with a signature can be an effective tamper-proof, assuming the integrity of the signature. 

Checking the balance is simple and non-destructive as all transactions in and out of an address, as I understand it, is public information. </snippet></document><document><title>QuickCite: the lazy academic&#8217;s friend</title><url>http://rjpower.org/wordpress/quickcite-the-lazy-academics-friend/</url><snippet>  Cool stuff. Here are some things that would be nice.

- Support for \citet and \citep citations (they are just like \cite from a citation retrieval point of view).
- Display a bit more information about each match to disambiguate cases where the title and authors are the same (for example, the venue or journal, or the year).

I've also run into a couple of bugs that I'll try to report, but I have lots of writing to do at the moment :) </snippet></document><document><title>Macro Programming Language</title><url>http://www.reddit.com/r/compsci/comments/wrrt4/macro_programming_language/</url><snippet>I am looking for a language which compiles into source code for another language. I would like it to be able to automate certain simple tasks and have more concise code.

I also want it to be simple enough that if you want, you can just give it source code for the target language and it will just spit it back out, without needing any special syntax. This will help me transfer existing code from the base language to the metalanguage.

There are a few main applications of this.

1) I would like to be able to do something like

    ##constructor(varname1, varname2, varname3)

and have it write a simple constructor which takes those arguments and sets the local variables without having to type it out every time. It doesn't take long to type it out manually each time, but enough features like this could make it so that you can get to the interesting part of programming more quickly.

2) I have implemented Dijkstra's algorithm dozens of times in several different contexts. I would like to write one implementation of it (per target language), then be able to "call" the macro with the appropriate code snippets, and have the (host language) compiler write it out and inline it for me. The target language compiler would then be able to compile it as if I had written it by hand. 

I realize that this can be faked to some extent with function pointers, but sometimes you want something to be entirely inlined. For example, if you are searching for a file recursively, it's equivalent to graph traversal. It would be very nice if you could just say graphDepthFirstSearch(...) with a little code as arguments and then not have to write it out.

Furthermore, the more basic algorithms are automated and standardized (even if it's just standardized to a particular programmer), the less code you have to debug. If you know that your implementation of A* works, and your code that uses it doesn't work, the problem must be somewhere else.

3) I would like to be able to add features to the target language. I would like to be able to use Matlab's multiple return values in C++, or Python's array slices in C, or C++'s operator overloading in Java. I would, of course, accept that my implementations would be far less efficient than they would be if they were built in to the target language's compiler, but I would accept this in exchange for faster coding.

4) I'd like compile-time for loops and expansion of constant-sized arrays. This way you can define a Point class which has 3 or 4 coordinates, but you don't have to write out getters and setters for each coordinate. But inside the Point class, you could just refer to "coordinates[2]" and have the macro compiler change that to "y".

---

I am not really interested in a base language which allows for these particular features, mainly because these are just examples. I'd like something that can work for ANY target language. It does not have to work for every target language simultaneously: I'm ok with writing Dijkstra's algorithm once for each target language I'm going to use.

I've done a little searching, but I haven't found anything that sounds quite like what I want. This idea seems pretty simple, so I'm sure I'm not the first.

Does anyone know of a good language which does this? Have you worked with it much?

edit: Merged in a few things that I had in the comments.  `cpp` is shipped with most C compilers, and allows you to run the C preprocessor over any source. It sounds like that fits the bill for you.

`M4` is the venerable classic macro processor shipped with every Unix. It might also do the job.
 M4 looks very useful. I'll look into that.

I think cpp would work for the simple stuff, but I don't know if it supports things like compile-time for loops or expanding a constant-sized array into multiple variables. If it does I certainly don't know how to do it. `cpp` is shipped with most C compilers, and allows you to run the C preprocessor over any source. It sounds like that fits the bill for you.

`M4` is the venerable classic macro processor shipped with every Unix. It might also do the job.
    Which languages do you want it to work with? The languages I use most often are Java, C++, C, and Scheme, approximately in that order. But ideally it ought to be able to work with anything.

There are other languages I'd like to learn, but sometimes they don't have features that I'd like. For example, lua doesn't have a continue keyword. With a macro language, I could work around that more elegantly and not have to type out silly workarounds every single time. The languages I use most often are Java, C++, C, and Scheme, approximately in that order. But ideally it ought to be able to work with anything.

There are other languages I'd like to learn, but sometimes they don't have features that I'd like. For example, lua doesn't have a continue keyword. With a macro language, I could work around that more elegantly and not have to type out silly workarounds every single time. So, you would like to be able to write &amp;lt;...this language...&amp;gt; code, and have it run as Java, C++, C, Scheme, JavaScript, Haskell, Lua, Scala, Basic, lisp, Common Lisp, lambda calculus, Perl, Assembly, B, D, APL, Ada, BrainFuck, Whitespace, Shakespeare, and Malbolge code (in approximate order of preference)?


You do realize that some of those languages don't allow assignment, right? Some don't have loops.  Some don't even have variables.  Some don't let you modify anything, while others let you evaluate entire expressions at compile time.


The basic data structures in each are so horribly incompatible that there is literally no common basis between them other than English (or your spoken language of choice).

If you want to write a piece of code in one language and use it in another, you can use the [Foreign Function Interface](http://en.wikipedia.org/wiki/Foreign_function_interface), or stay within a Java/.Net/parrot virtual machine. (LLVM compiles C and can target the Java virtual machine, for instance.)

If you want to have high-level metaprogramming, try reading up on C++'s templated code, Standard Template Library, template meta-programming, C-style preprocessor, and X-macros.

There are some special-purpose crosscompilers that compile, for example, CoffeeScript to JavaScript but they are not often used and produce messy, unreadable, and unmaintainable code.



How much experience do you have with the C++ STL and C++ metaprogramming?
 I think you misunderstood what I meant. Despite English being my first language I'm having trouble saying this, so bear with me. The following is NOT an example of what I would expect such a language to do: allow me to write Dijkstra's algorithm once and then compile it into any language. What I WOULD like is a language that lets me write Dijkstra's algorithm once per target language and allow me to invoke it as needed. I would like the macro language to be able to support any given target language (one at a time). The main point is that I'd like to be able to abstract over the context of an algorithm, whether I have an explicit Graph class, and if so, which one, and so on.

Also, I'd like to add features from other languages: use Matlab's multiple return values in C++, or use Python's string slices in C, or use C++'s operator overloading in Java. I would, of course, accept that my implementations would be far less efficient than they would be if they were built in to the target language's compiler, but I would accept this in exchange for faster coding.

---

I've worked a little bit with C++ templates. I find them slightly counter-intuitive, and I'm a bit annoyed that certain things have to be all defined in one file, unlike normal classes. It's been a while since I attempted it so I can't remember precisely what my complaint was. 

I'm familiar with very basic C++ macros and precompiler directives. The most complicated stuff I know is something like #define ADD(a,b) (a)+(b) So, you want a language with all of the features  you like? Of course. Why wouldn't I? Specifically you, and no one else?  Sorry.  That one hasn't been written yet.  And no, the meta, "smash all the languages together and get something out" thing doesn't exist.  But it does remind me of one language.

Perl has a lot of features that you've mentioned.

* Multiple return values 
* string slices
* operator overloading
* ability to inline other languages
* steals its syntax from other languages like a bloody thief


It doesn't produce other code, but if you use this one language, you'll get a lot of the features you were looking for.^\*

\*Fair warning, though.  Perl is crazy.  All of these interesting features produce a lot of "interesting" code.  Which is what your meta "smash all languages together" thing would do anyway. Exceptionally few ideas are original. I simply wanted to ask if the idea had been implemented before, and if not, if there is a compelling reason why it has not been implemented. I didn't want to write half a compiler only to realize that there's a better alternative.

I will try Perl again. Thank you for the suggestion.

---
Re: specifically me:

If I only wanted one language with all of the features I wanted, I wouldn't be asking about a metalanguage. I would be looking for just one language. But different languages are not equally good for all purposes. C is (as far as I know) the fastest except maybe for assembly, Java has huge standard libraries and in my opinion helpful error messages, etc, etc. A metalanguage that can accelerate coding in many different target languages would be a useful tool. That is all that I meant. Exceptionally few ideas are original. I simply wanted to ask if the idea had been implemented before, and if not, if there is a compelling reason why it has not been implemented. I didn't want to write half a compiler only to realize that there's a better alternative.

I will try Perl again. Thank you for the suggestion.

---
Re: specifically me:

If I only wanted one language with all of the features I wanted, I wouldn't be asking about a metalanguage. I would be looking for just one language. But different languages are not equally good for all purposes. C is (as far as I know) the fastest except maybe for assembly, Java has huge standard libraries and in my opinion helpful error messages, etc, etc. A metalanguage that can accelerate coding in many different target languages would be a useful tool. That is all that I meant. Exceptionally few ideas are original. I simply wanted to ask if the idea had been implemented before, and if not, if there is a compelling reason why it has not been implemented. I didn't want to write half a compiler only to realize that there's a better alternative.

I will try Perl again. Thank you for the suggestion.

---
Re: specifically me:

If I only wanted one language with all of the features I wanted, I wouldn't be asking about a metalanguage. I would be looking for just one language. But different languages are not equally good for all purposes. C is (as far as I know) the fastest except maybe for assembly, Java has huge standard libraries and in my opinion helpful error messages, etc, etc. A metalanguage that can accelerate coding in many different target languages would be a useful tool. That is all that I meant. Here's the problem, though: Java gave up the speed of C in order to have platform agnosticism. If you want the Java libraries, you're constrained to only run at Java speeds. 

The python strings are implemented completely differently than C strings. If you want python slicing in C, you lose the speed and compatibility of C strings.  

Anytime you bring in a new feature to what you're writing, you also bring all the points where that paradigm fails and all the incompatibilities of that solution. 

At every interface between languages, you'd need a new piece of glue that would slow you down and make the program perform worse. 

The .Net and Java virtual machines mitigate that a little bit by providing standard String, Set, etc. implementations for all the languages they host. But those at not shared by any other language, which is why I mentioned them earlier.  You can write a Scala function and the call it from JRuby.

But you can't mix languages in the same source file. They have different grammars and will assign conflicting meanings to various lines of code.  You cannot parse them together without breaking deep, meaningful parts of each language.  You would need each compiler to produce the same output format and work cooperatively with each other.  They were never designed to do that. They implement separate languages in the most efficient way for that language. If your meta language has any hope of emulating any of these behaviours, it needs to do them itself.


In order to do what you want, you would have to re-implement each and every part of each language in a common form that would allow such incompatible parts to work together. Let me say that again: you would have to re-implement each and every part of every language you want to implement.  At that point, you've implemented your own custom language.  It would have to re-implement each and every bug, quirk, and design mistake.

And once you're done? You don't have a metalanguage, you just have a new language that looks like a lot of other languages. 


So, the compilers are incompatible. The code is incompatible. The languages were never designed to interoperate. The syntaxes conflict with each other. The data structures are completely different. 

No matter how you slice it, you will never get all of these languages to behave properly with each other without crippling the system as a whole
  </snippet></document><document><title>CompSci syllabus</title><url>http://www.reddit.com/r/compsci/comments/wrsnm/compsci_syllabus/</url><snippet>Looking for a CompSci syllabus for self-taught students.  The curriculum of any given school reflects their faculty's interests and strengths, as much as anything else.  I would suggest also looking over the [ACM's suggestions for an ideal curriculum](http://www.acm.org/education/curricula-recommendations), which includes the objectives/rationale for each course.  There was a time, years ago, when book publishers tailored text books to specific ACM idealized courses.    </snippet></document><document><title>Mathematics for Computer Science [800 page PDF, as used at MIT, CC licensed]</title><url>http://courses.csail.mit.edu/6.042/spring12/mcs.pdf</url><snippet>   I recognize that link title. Thanks for following! :-) :) should I had mention you ? anyway, thanks, I admire the goodies you share. I recognize that link title. Thanks for following! :-) Well, this one has been posted here multiple times, including [this one](http://www.reddit.com/r/compsci/comments/dk8rf/mathematics_for_computer_science_fall_2010_mit/) by me about 1 year ago :) Yeah, I "steal" links from everywhere and sometimes people "steal" the exact descriptions I rewrite for them. It's all good. My comment was just a verbal fist bump really. I recognize that link title. Thanks for following! :-)  Are there solutions available for the problems? Or at least, answers? Are there solutions available for the problems? Or at least, answers? No. Well yes, they do have very nice written out solutions to the problems, but they are not made available for distribution. Bummer. :( I love learning by comparing my solution to THE solution (especially knowing it does exist), learn from my mistakes and also confirm I understood everything correctly. Can it be bought somewhere at least? Thanks for the info! I don't know if they intend to make them available in the future. But when I took the class, many of the in class problems and some of the problems on the assignments were taken from this book and they always had very nice solutions available for the students afterwards. Maybe the solutions can be found at OCW. What's the name of the class?  Hi! I was a TA for this class (6.042) at MIT last year. I'd be happy to answer questions about how the class is run, if people are interested. Please do. I have a colleague who won't shut up about going to MIT and he is always a downer because 'his math just is isn't strong enouth'. What can you tell me about the average level of the students regarding mathematical knowledge? 

Anything that I could tell him to give him insight and information would be helpful. Also, we are not from the USA, so anything you know regarding foreigner's applications for doctorate programs at MIT would be greatly appreciated. 

But yeah, mostly just lay down the pointers for a guy who wants to go to MIT and thinks he does not know enough math.

edit: said colleague is a fellow researcher, we are both researching automated planning systems going for a master's degree in Information Systems/Computer Science (depending on what you mean by one or the other). I give this information in case you want to focus on the level of students of technology  Is this a good resource for self study? It appears to cover the same topics &#8211; give or take a little &#8211; as my 3 year undergraduate computer science degree (excluding maths used in elective and specialist topics).

I'm not sure how useful it would be in isolation though. I don't know if I would have absorbed this information without tutorials and labs to actually use the underlying ideas. Well, I am wondering about the calculus for the most part. I just finished my undergraduate degree in CS, so my math isn't terrible, I feel confident in probability and statistics along with discrete structures and linear algebra. However, calculus, not so much.

If you have any recommendations for resources it would be much appreciated. For probability and statistics I enjoyed (and by enjoyed I really mean I freaking loved it) studying from [Statistics for Engineering and the Sciences](http://www.amazon.com/Statistics-Engineering-Sciences-5th-Edition/dp/0131877062).  Well, I am wondering about the calculus for the most part. I just finished my undergraduate degree in CS, so my math isn't terrible, I feel confident in probability and statistics along with discrete structures and linear algebra. However, calculus, not so much.

If you have any recommendations for resources it would be much appreciated. For probability and statistics I enjoyed (and by enjoyed I really mean I freaking loved it) studying from [Statistics for Engineering and the Sciences](http://www.amazon.com/Statistics-Engineering-Sciences-5th-Edition/dp/0131877062).  Well, there's only a tiny bit of simple calculus in the OP's link &#8211; mostly discrete stuff in chapter 13.

Serious calculus (excluding basic summing and rate determination) tends to be limited to special fields within computer science: graphics, video encoding, modelling of real-world phenomena, etc. You can do a lot of computer science with zero calculus.

As for your question about textbooks though: I'm not sure I ever read a text that worked well for me. I don't think I'm a person that ever learned abstract subjects well from textbooks.

I took a double undergraduate degree (comp sci and control systems engineering). I did a lot of maths as part of the control systems so it's often pretty hard to remember what I actually studied for computer science &#8211; particularly when it comes to calculus (which I remember being entirely on the engineering side).

Frankly I don't think I learned much from my actual maths courses except how to bluff during exams. I ended up needing to learn everything when I reached my last two years of engineering and they expected a huge amount of maths. I cobbled it together by leaning on lab tutors, other students and banging my head on the table a lot.

I ended up in computer vision (which is a maths heavy field) but I don't think I'm "good" at maths [for my field &#8211; I'm sure that still puts me better than 99% of the population]. I can do what I need to do and I can read the research papers. I learned that if you are good at mental calculations you often don't get credit.  Twice I had to retake a test (without writing anything down) to demonstrate that's how I worked.  I'm ADHD (rather severly evidently though this escaped everyone until my wife and I went for marriage counseling and the counselor asked us during the first session about it).  Back from the tangent- so if I wrote anything down during calculations I'd completely lose my train of thought and collapse.  Same thing with papers- write the whole thing in my head before putting anything on paper. 

Like yourself (I was CS/EE which is close) I have never felt I was in any way "good" at maths and never enjoyed them in the slightest- it was just something you had to do to get through the courses. My son on the other hand loves math, excels at it and is completely uninterested in engineering because he doesn't like "science".  I blame his high school.  It appears to cover the same topics &#8211; give or take a little &#8211; as my 3 year undergraduate computer science degree (excluding maths used in elective and specialist topics).

I'm not sure how useful it would be in isolation though. I don't know if I would have absorbed this information without tutorials and labs to actually use the underlying ideas. Is this a good resource for self study? I'm sorry if I sound blunt, but you do realize the document contains a table of contents?

To answer your question: looking at the contents, this book looks like it could be very useful for self-study. I'm sorry if I sound blunt, but you do realize the document contains a table of contents?

To answer your question: looking at the contents, this book looks like it could be very useful for self-study.     Is there a list of PDFs like this somewhere... I have another called A ProblemText in Advanced Calculus that was posted to /r/math or here I can't remember....  Is that all that is required as far as mathematics go at MIT for CS? Or is that the book for the introductory course?    Wonderful! Is this available in other formats? For example, EPUB and MOBI? It would be fantastic to read it on my Kindle. Wonderful! Is this available in other formats? For example, EPUB and MOBI? It would be fantastic to read it on my Kindle.  Please don't say "CC licensed", tell people *which* CC license they use. "CC licensed" can mean anything from do-whatever-you-want, public domain (CC-0) to you-can-look-but-you-can't-touch, no derivatives and no commercial use (CC-nc-nd)  To the user, CC is sufficient &#8211; it means that you are not stealing. All CC license variations pertaining to documents can be distributed unmodified for free.

The fact that this document is CC BY NC SA is only relevant if you want to redistribute or produce a derivative work. Why assume that your fellow redditor doesn't want to, say, translate parts of this to her own language? Or adapt a chapter for her own book? Users are not "just users" anymore. Then they can look up the specifics of the license themselves.

Keep things simple.  The average reader doesn't want to feel they need to understand the alphabet soup behind every license.  They just want to know whether they should feel guilty for not paying someone to read or have a copy of the thing.  so to be a programmer, all the math you ever need is there ? so to be a programmer, all the math you ever need is there ?   Wow! 800 pages of shit you should have already known by 11th grade!  
  
This is MIT level? Really? Where did you go to school, korea?  
Way to be smug... A public high school in the United States.  
  
This isn't difficult advanced material at all. Look at the actual text. Can you point to *anything* that isn't high school level? Trees, Matrices, Induction, Proofs, Sorting, Recursion, Functions etc etc etc...

The only thing I've read before college was induction and proofs in the optional philosophy class but other than that there's pretty much nothing in there that was taught in high school. If everyone should know it by 11th there would be no need to the entire first year of college dedicated to it Trees, Matrices, Induction, Proofs, Sorting, Recursion, Functions etc etc etc...

The only thing I've read before college was induction and proofs in the optional philosophy class but other than that there's pretty much nothing in there that was taught in high school. If everyone should know it by 11th there would be no need to the entire first year of college dedicated to it Wow! 800 pages of shit you should have already known by 11th grade!  
  
This is MIT level? Really?</snippet></document><document><title>Theoretical Computer Science Cheat Sheet
</title><url>http://www.tug.org/texshowcase/cheat.pdf</url><snippet>  This has been posted before - it's not really a TCS cheat sheet. It looks more like a general discrete math cheat sheet. I don't see anything about complexity classes or anything, for example. Except in the very beginning and on the second page (Master method)     There is no author or source listed in the file. Do you know if there are sheets like this for things beyond freshman level? Maybe something on topics like category theory, optimisation etc. http://www.tug.org/texshowcase/ shows the author as "Steve Seiden, died in 2002 as the result of an accident while riding his bike."</snippet></document><document><title>Free Quantum Mechanics and Computation class taught by Berkeley prof starting today</title><url>https://www.coursera.org/course/qcomp</url><snippet> </snippet></document><document><title>Help me approximate this series</title><url>http://www.reddit.com/r/compsci/comments/wpsva/help_me_approximate_this_series/</url><snippet>log(1!*2!*...*n!) is the series.

Here's all I've been able to do:

log(1!) + log(2!) + ... + log(n!) through Sterling's approximation (log n! is O(n log n) )

yields: n log n + (n-1) log n + ... + 1 log 1.

I believe this is O(n^2 log n) but I am unsure if one can do better.

I'm working on [this problem](http://maven.smith.edu/~orourke/TOPP/P41.html#Problem.41) over the summer and am in the process of learning some more computer science in the process. Any advice would be much appreciated.  This is the logarithm of the [Barnes *G*-function](http://mathworld.wolfram.com/BarnesG-Function.html).  

You can find an asymptotic expression for the log of this function (e.g., see the Wikipedia article on this function), but in fact you have the answer in front of you already.

You have two expressions -- the exact expression from more_exercise and the asymptotic expression you obtained.  Add these together and divide by two.  You'll get
(n+1){log(n)+log(n-1)+...+log(2)+log(1)}/2 = (1/2)(n+1)log(n!)

Now apply Stirling's approximation again, and you're all done.

Edit: typo fixed.  A different perspective might help:

1! \* 2! \* 3! \* ... \* n!  is 1^n \* 2^(n-1) \* 3^(n-2) \* ... \* n

log( 1^n \* 2^(n-1) \* 3^(n-2) \* ... \* n) gets you n log(1) + (n-1) log(2) + (n-2) log(3) + ... + log(n)

</snippet></document><document><title>Suggestions for a comp-sci talk</title><url>http://www.reddit.com/r/compsci/comments/wppd7/suggestions_for_a_compsci_talk/</url><snippet>A friend of mine has asked me to give a talk at his company, and I'm unfortunately low on ideas at the moment.  (I'm a PhD student doing systems research).

My audience is (supposedly) going to be reasonably technical, but not necessarily well-versed in distributed systems.  (i.e, they might use Hadoop, but they don't really know why MapReduce was an interesting paper).

Now, I could talk about my research, but my guess is that it's going to be a bit esoteric.  I'm instead leaning towards preparing a discussion on the last X (where X &amp;lt; 5) years of distributed systems research at a high level, in the hopes that I can convince some people that we're actually doing something in academia.

In short, I'm looking for some ideas -- what would you want to discuss with a researcher if one came in to hobnob?

I'll post slides, etc. if desired.  Consider what you would want to hear about from someone in a technical field that you weren't familiar with. A discussion of some of the research of the last several years could be OK, but there's a fine line between a history lesson and the "big moments" of a field. Why does distributed systems matter? What big victories has the field had, and where is the research heading? (Note that this last point is a great place to talk about your own work) And don't forget about taking the individual company into account. Is there something from the field that they could benefit from?

To give an example, I am a PhD student more in AI / Operations Research. I've only taken a couple seminars on distributed systems, but some of the really interesting things for me were reading about the problems Google and Yahoo were able to use MapReduce and Hadoop to solve (even Microsoft's Dryad had some interesting work, especially in regards to scheduling). Distributed hashing for content distribution networks is also an interesting topic where you can essentially take a look under the hood of something everyone knows about and show them how research has made a difference. Thanks for the input - it's good to have an  idea of what people outside the field find interesting/useful.  For instance, inside systems Hadoop is almost universally used as a straw-man comparison as a result of it's poor performance(perhaps unfairly).

Unfortunately, most academic projects, as I'm sure you're aware, fall far short of being very usable in the real world.  But I digress :).

I like the idea of trying to capture the motivation for distributed systems research.  It's certainly not something that's obvious just from looking at our conferences.</snippet></document><document><title>Can you recommend any good CS books for an undergraduate to read?</title><url>http://www.reddit.com/r/compsci/comments/wnjdl/can_you_recommend_any_good_cs_books_for_an/</url><snippet>**TL;DR: Any recommended books for an undergraduate considering CS as a course to get him more interested in the subject?**

Background: I have a huge interest in the world of science and maths, but I have been considering taking a degree in CS as I enjoy technology and I have learnt some minor java and python in the past. I feel I don't know the subject as well as physics and chemistry and was wondering if you guys could recommend any eye opening/fascinating books (that can maybe get me more interested in the subject)or any essential reads for a computer science undergraduate? Thank you!

EDIT: Thanks for all your feedback, never realised these books could be so expensive! Also, I apologise as I think you may get these kinds of threads a lot, sorry about that, should have read your subreddit rules before I posted.  _Algorithmics: The Spirit of Computing_ by David Harel is a great introduction to computer science that should get you more interested in the subject, and at the same time makes clear why computer science is something distinct from technology and programming.

Looking at Amazon I see that the 3rd edition is very expensive for some reason. It's fine to get the 2nd edition.
 Upvote for Harel's *Algorithmics*.  I would also add:  
  
*9 Algorithms That Changed the Future*, by John MacCormick.  This new book discusses a selection of algorithms that you deal with every day, including search engine indexing, Google's PageRank, public key cryptography, and digital signatures.  
  
*Beyond Calculation: The Next Fifty Years of Computing*, by Denning and Metcalfe (eds).  Although this book is 15 years old, its breadth and roster of famous contributors stills holds up well.  And we still have 35 years to see how far they were off :-)   Find it in your school's library.  
  
*[Computer Lib / Dream Machines](http://en.wikipedia.org/wiki/Computer_Lib)*, by Ted Nelson.  Unfortunately, you'll never find this 1974 book, but it's the one that still gives me the chills thinking of the possibilities of being a computer scientist.  It's actually two books in one -- it's *Computer Lib* from one side, but flip it over, and it's *Dream Machines*.  *Dream Machines* is/was about the up-and-coming field of computer graphics.  I'm retired now, but my career in scientific visualization was largely inspired by this book. _Algorithmics: The Spirit of Computing_ by David Harel is a great introduction to computer science that should get you more interested in the subject, and at the same time makes clear why computer science is something distinct from technology and programming.

Looking at Amazon I see that the 3rd edition is very expensive for some reason. It's fine to get the 2nd edition.
 What's with all the references to the bible in that book? What about them? I guess the author is religious.  1. [sicp](http://mitpress.mit.edu/sicp/full-text/book/book.html)
2. [List of freely available programming books](http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books) Upvote for SICP. Good introduction to most CS fundamentals, written for people with little/no coding experience. I agree that SICP is incredible but disagree that it is good for people with little to no coding experience.

If you've coded for a while, you should definitely dive into it. I did last summer and it was just plain enlightening. It very tough though. 1. [sicp](http://mitpress.mit.edu/sicp/full-text/book/book.html)
2. [List of freely available programming books](http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books)  Its hard to tell if you are looking for nitty gritty text books or more general stuff, and whether you are specifically looking to learn about programming. 

Either way though, a good book that kind of spans all of these things, and is even informative for a CS graduate is Code by Charles Petzold: http://www.charlespetzold.com/code/ 

It takes you from the most basic logic building blocks and keeps working its way through until you have a working computer. It gives you a broad overview of just how computers actually work, and what exactly happens between the plug and the keyboard. 
 Its hard to tell if you are looking for nitty gritty text books or more general stuff, and whether you are specifically looking to learn about programming. 

Either way though, a good book that kind of spans all of these things, and is even informative for a CS graduate is Code by Charles Petzold: http://www.charlespetzold.com/code/ 

It takes you from the most basic logic building blocks and keeps working its way through until you have a working computer. It gives you a broad overview of just how computers actually work, and what exactly happens between the plug and the keyboard. 
 Its hard to tell if you are looking for nitty gritty text books or more general stuff, and whether you are specifically looking to learn about programming. 

Either way though, a good book that kind of spans all of these things, and is even informative for a CS graduate is Code by Charles Petzold: http://www.charlespetzold.com/code/ 

It takes you from the most basic logic building blocks and keeps working its way through until you have a working computer. It gives you a broad overview of just how computers actually work, and what exactly happens between the plug and the keyboard. 
 This looks like a great read that puts everything together, but is it still viable? I see that it was published back in 2000. Thanks! This looks like a great read that puts everything together, but is it still viable? I see that it was published back in 2000. Thanks! Its hard to tell if you are looking for nitty gritty text books or more general stuff, and whether you are specifically looking to learn about programming. 

Either way though, a good book that kind of spans all of these things, and is even informative for a CS graduate is Code by Charles Petzold: http://www.charlespetzold.com/code/ 

It takes you from the most basic logic building blocks and keeps working its way through until you have a working computer. It gives you a broad overview of just how computers actually work, and what exactly happens between the plug and the keyboard. 
  The Little Schemer, by Freidman and Felleisen. It teaches you how to think like a programmer. It's fun, organized so it uses the Socratic method, and you learn the Scheme programming language and the functional programming paradigm along the way. It's a nice gentle introduction to some fairly deep concepts like the Y combinaor and meta-circular interpreters. 

I pimp that book so much...  *Algorithms* by Sanjoy Dasgupta, Christos Papadimitriou, and Umesh Vazirani. Not only informative, but well-written and pleasant to read.

Also, you can find pdfs online with little trouble. Although I appreciated it enough to buy a hard copy anyway. That thing will get a place on my bookshelf amid a myriad of legal tomes.  Ony of my favorite mathematics books of all time is Michael Sipser's "Introduction to the Theory of Computation." It's an introductory book into the theory of computing and includes intuitions and examples along with each proof.    *G&#246;del, Escher, Bach: An Eternal Golden Braid* by Douglas Hofstadter. It is a Pulitzer award winner, and a highly enjoyable read (in my opinion at least, and I read it in my senior year of high school). It is however, only partially a computer science book, and involves a breadth of topics including Artificial Intelligence, Cognitive Science, Neuroscience, Biology (DNA), Buddhism, amongst others.

I highly recommend it if you are someone who usually enjoys reading in general and would like to see how deeply and intricately connected computer science is to everything else around you, all in a beautifully written piece of literature.

EDIT: If my description made the book seem too general, let me assure you that you might be going over some parts several times to try and grasp the concepts that the author tries to convey. You will even find yourself jotting down formal mathematical statements at some point! Read this when a guy in my computing A-Level class recommended it to me, I found it really opened up my thinking in regards to my CS/Maths/Science/Everything. Absolutely worth reading. Read this when a guy in my computing A-Level class recommended it to me, I found it really opened up my thinking in regards to my CS/Maths/Science/Everything. Absolutely worth reading. Really? I found it just about as scientifically enlightening as the Spongebob movie.    Cryptonomicon, by Neal Stephenson. Fiction, but fits the bill for "eye opening/fascinating books (that can maybe get me more interested in the subject)"  WTF... the mass market paperback is cheaper than the Kindle edition on Amazon.   Excellent reference texts that will give you a good idea of what you are getting yourself into:  
  
+ [Discreet Structures, Logic, and Computability - Hein](http://www.amazon.com/Discrete-Structures-Logic-Computability-James/dp/0763772062)  
+ [Introduction to Algorithms - Cormen, Leiserson, Rivest and Stein](http://www.amazon.com/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844)  
+ [Artifical Intelligence: A modern Approach - Norvig](http://www.amazon.com/Artificial-Intelligence-Modern-Approach-Edition/dp/0137903952)  
  
edit - borken link  Effective Java by Joshua Bloc. While this is a great book for software developers, I think it would probably be a horrible book to stoke interest in compsci.   Effective Java teaches me a lot of what I wish they taught in school. School will only teach you the nuts and bolts. In contrast, effective java teaches you the principles necessary to build production quality code, which is why I recommend it.

If you need a Java primer, the Java tutorials already have you covered.
http://docs.oracle.com/javase/tutorial/ Effective Java teaches me a lot of what I wish they taught in school. School will only teach you the nuts and bolts. In contrast, effective java teaches you the principles necessary to build production quality code, which is why I recommend it.

If you need a Java primer, the Java tutorials already have you covered.
http://docs.oracle.com/javase/tutorial/ Indeed it does, but compsci is about more than building production quality code in Java.  Effective Java isn't a necessary book for someone that won't use Java.   Most jobs in compsci are about architecting, designing and building production quality code. Algorithms help with that, but school already teaches that.

If s/he wants to do Computer Science, I don't need to sell it. The math is always there. A lot of the essential reads aren't part of the compsci curriculum. I lead a programming team, meaning a big part of my job is hiring developers. I absolutely value having a strong theoretical framework. I also value developers who know things like

* How to use version control
* How to write SQL &amp;amp; create denormalized tables
* Fundamental design patterns
* How to write with an API
* Test driven design &amp;amp; Unit tests
* Documenting and then programming by contract

Most school don't teach the above, which is why when any student asks me the books to read, I answer with a book that I think would help them get hired and/or be a better engineer who develops better solutions with code that is more readable and maintainable. I agree with all that, but very little of that is covered by Effective Java.  It touches on the Builder, Factory and Listener patterns IIRC, but that's about it.  No VCS, no SQL, and no unit tests at all.  I don't recall anything about documentation either.  

 Most jobs in compsci are about architecting, designing and building production quality code. Algorithms help with that, but school already teaches that.

If s/he wants to do Computer Science, I don't need to sell it. The math is always there. A lot of the essential reads aren't part of the compsci curriculum. I lead a programming team, meaning a big part of my job is hiring developers. I absolutely value having a strong theoretical framework. I also value developers who know things like

* How to use version control
* How to write SQL &amp;amp; create denormalized tables
* Fundamental design patterns
* How to write with an API
* Test driven design &amp;amp; Unit tests
* Documenting and then programming by contract

Most school don't teach the above, which is why when any student asks me the books to read, I answer with a book that I think would help them get hired and/or be a better engineer who develops better solutions with code that is more readable and maintainable. Those topics aren't really related to Computer Science, but to software development. Computer Science and Software Development are two different beasts, with some minor overlaps. Unit testing, design patterns, documentation, etc. are tools used by developers to build software. These types of topics are not necessary in a Computer Science degree, and should at most be pursued in a perfunctory manner - as optional courses. 

Computer Science is a branch of mathematics that deals with computation, not software development. They are not one in the same. Effective Java by Joshua Bloc.  The best book by far is [the Art of Programming](http://www.amazon.ca/Computer-Programming-Volumes-1-4A-Boxed/dp/0321751043/ref=sr_1_1?ie=UTF8&amp;amp;qid=1342465991&amp;amp;sr=8-1).

The best textbook my university used was [Introduction to Algorithms](http://www.amazon.ca/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844/ref=pd_bxgy_b_text_c) for Algorithms I, and Algorithms II. If you can understand that textbook, you've learned some of the most important lessons in computer science. You're honestly recommending TAOP to a guy who's CONSIDERING taking CS?  Hell, most CS majors/Professional programmers have never read it because of it's length and density.  Some have it on their shelf to use as an occasional reference, but haven't read the entirety.

OP: Knuth is awesome, but honestly TAOP is an entire masters course in Computer Algorithms.  It's probably WAY more than your looking for and is likely too much for someone just out of HS thinking about taking CS classes.

EDIT: There is a reason there is the [Fokelore story about Steve Jobs meeting Donald Knuth](http://www.folklore.org/StoryView.py?project=Macintosh&amp;amp;story=Close_Encounters_of_the_Steve_Kind.txt).  Note that [Knuth refutes this story.](http://www.catonmat.net/blog/don-knuth-steve-jobs/) You're honestly recommending TAOP to a guy who's CONSIDERING taking CS?  Hell, most CS majors/Professional programmers have never read it because of it's length and density.  Some have it on their shelf to use as an occasional reference, but haven't read the entirety.

OP: Knuth is awesome, but honestly TAOP is an entire masters course in Computer Algorithms.  It's probably WAY more than your looking for and is likely too much for someone just out of HS thinking about taking CS classes.

EDIT: There is a reason there is the [Fokelore story about Steve Jobs meeting Donald Knuth](http://www.folklore.org/StoryView.py?project=Macintosh&amp;amp;story=Close_Encounters_of_the_Steve_Kind.txt).  Note that [Knuth refutes this story.](http://www.catonmat.net/blog/don-knuth-steve-jobs/) The best book by far is [the Art of Programming](http://www.amazon.ca/Computer-Programming-Volumes-1-4A-Boxed/dp/0321751043/ref=sr_1_1?ie=UTF8&amp;amp;qid=1342465991&amp;amp;sr=8-1).

The best textbook my university used was [Introduction to Algorithms](http://www.amazon.ca/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844/ref=pd_bxgy_b_text_c) for Algorithms I, and Algorithms II. If you can understand that textbook, you've learned some of the most important lessons in computer science. We use the latter textbook on a junior/senior level at my university. It may be a bit heavy for someone just trying to get into computer science. It *is* a good book, however.

Similarly, Knuth's Art of Programming is really intended to be a reference book...</snippet></document><document><title>Am I doing myself a disservice by learning Lisp more in depth?</title><url>http://www.reddit.com/r/compsci/comments/woqgm/am_i_doing_myself_a_disservice_by_learning_lisp/</url><snippet>Hi, 

Second year CS undergraduate here. I took Scheme first semester last year, and want to spend time out of class studying Lisp more. I am interested in AI and there seems to be a plethora of lisp AI books, and I really, really enjoyed programming in the language.

I know I am expected to be proficient in several languages if I want to be competitive when looking for a job, so I'm concerned that by studying lisp, I am putting myself at a disadvantage as I should be studying C++, or Java or .NET.

Am I overestimating the amount of time necessary to learn a language to proficiency, or should I be starting immediately in branching out to other languages?  You're not doing yourself a disservice by learning LISP at all. I know you're concerned about "Wasting" time learning it that you could be learning a more "employable" language, but consider this:

1. You're obviously passionate about it. Your motivation is not dollar signs, so it's from the heart. This is the best kind of motivation for moving you forward. If you spend too much time learning something you're not interested in just for the future paycheck, you may get burned out and not learn it as fast. 

2. What you learn here will help you in other languages. Others have said this, and I agree: You'll start picking up habits and ways of thinking and problem solving that will carry over to other languages like C++ or Java. That's why they push learning it in CS courses. 

3. You may actually get a job writing LISP. There are definitely paid professionals using it, though the jobs are quite few. But you never know where the skill might pop up. 

So I say go for it! Follow your passion, and you'll move forward.  Thank you very much! Quite reassuring :) To expand on his 3rd point, Clojure (a recent Lisp) runs on the JVM and is perfectly compatible with Java. You're not doing yourself a disservice by learning LISP at all. I know you're concerned about "Wasting" time learning it that you could be learning a more "employable" language, but consider this:

1. You're obviously passionate about it. Your motivation is not dollar signs, so it's from the heart. This is the best kind of motivation for moving you forward. If you spend too much time learning something you're not interested in just for the future paycheck, you may get burned out and not learn it as fast. 

2. What you learn here will help you in other languages. Others have said this, and I agree: You'll start picking up habits and ways of thinking and problem solving that will carry over to other languages like C++ or Java. That's why they push learning it in CS courses. 

3. You may actually get a job writing LISP. There are definitely paid professionals using it, though the jobs are quite few. But you never know where the skill might pop up. 

So I say go for it! Follow your passion, and you'll move forward.   I work with mostly C# for now and wish I knew more Lisp. If you're interested in it and AI, there's no reason to not study it. It's good to be familiar with the "practical" languages sure, but you only need to try to have impressive-for-a-recent-grad competency in these. And the theoretical stuff like algorithms and asymptotic analysis are gonna be more important to most employers (there are exceptions of course, like game companies require C++, no exceptions [pun unintended], and the defense contractors will want C, C++, Java, or god forbid, Ada).

Be able to write basic programs and read the API docs of C# with .NET and Java. Build one complete application in one of those, like a mobile app, a web service that fetches/sorts data, or a desktop GUI app that does something interesting or useful. These languages and frameworks are not complicated. The libraries are just large.

Might be worth dabbling in Python or Ruby too. Some kind of web app mashup thing using Rails, Sinatra, Django, Tornado, something that's popular. These tools are even less complicated, don't worry about spending too much time on them unless you are totally in love with the tech startup scene. A fun thing to play with is the [Twilio](http://www.twilio.com/) api.

Then try something low-level in C. If your curriculum already has you set to do this, then might not be needed. Play with stuff like signal handlers, sockets, and graphics (NOT NEHE). Absolutely do C before C++ (I'm assuming you haven't done much with it already, correct me if I'm wrong). At first C++ may feel like just a pickier Java, but you should treat it differently. Good design of your classes still applies, but make sure you read up on some of the C++ intricacies (don't *study* them, just be aware of things like the Rule of 3, go [here](http://www.parashift.com/c++-faq-lite/) and [here](http://yosefk.com/c++fqa/) for that info). For a C++ project, you can try more graphics stuff, make a simple game with something like SFML or cocos2dx, or try out GTK or Qt.

Then you can confidently say you have good recent graduate-level experience with the resume keyword languages and tools. These are just some suggestions to guide you in the general direction of what's useful to learn about these these languages. Spend the rest of your time on Lisp/Scheme/Haskell/Clojure/Brainfuck/whatever gives you the greatest delight.

Strive to self-manage when doing these projects. There's a lot more to being a developer than just knowing the ins and outs of a framework or language. Whether you're studying Lisp or working on one of the other projects, have concrete goals to achieve. Make time to restructure your code as you learn more, but don't drag the process out. All this might sound like a lot, but by the time you graduate you'll probably have built a way more things than this between coursework and any internships. And these projects will go by quickly if you discipline yourself. The most important thing is to always be building stuff, whether it's a small tool or a large dream project (but if you go big, break down the project into several smaller ones, otherwise it will never get done).

About me: recent graduate, music major/CS minor, work as a game developer. Wow, thank you for the lengthy reply!

Coincidentally, C# and XNA is the one other thing that I was playing around with recently.

Do you work in indie game development or AAA?

Thanks again for your post, it was exactly the kind of thing I needed. Glad it was helpful :)

I work for an indie studio of 3-ish people.

XNA is awesome! I learned a lot just trying to build robust game systems in C#/XNA. What are you trying to make/do with it? I didn't really have a concrete goal yet, I just wanted to look at C# a bit. I was working through some XNA tutorials as game design is something I wanted to explore more as well. 

Are 3D game dev endeavors viable for a solo project? I was recently linked to this [book](http://www.arcsynthesis.org/gltut/index.html), but I don't have C++ experience yet, so it doesn't seem doable in the immediate future.
 They're viable, just can take a very long time. I wouldn't start with 3D, as that's inherently more complicated on its own. Don't think of it out of reach for not having C++ experience yet, think of it as a reason to learn C++. That's also the best book I've found for starting out.

[Learn C the Hard Way](http://c.learncodethehardway.org/) is still being written, but just going through the early finished chapters can orient you to the right mindset of writing C. Alright, makes sense. A lot of vector math, I believe? I'm taking linear algebra soon anyways.

So C --&amp;gt; C++ is a wise move? Yes to both, that'll be a fun way to apply your linear algebra study. And starting with pure C gives you better understanding of memory and pointers. They're viable, just can take a very long time. I wouldn't start with 3D, as that's inherently more complicated on its own. Don't think of it out of reach for not having C++ experience yet, think of it as a reason to learn C++. That's also the best book I've found for starting out.

[Learn C the Hard Way](http://c.learncodethehardway.org/) is still being written, but just going through the early finished chapters can orient you to the right mindset of writing C. Learn C the Hard Way is a very thorough, very boring book.

You would come out knowing C rather well. I personally need something more interesting than just doing his exercises by rote. The extensions aren't really that hard, but maybe I started going crazy too soon. If not Learn C the Hard Way, then at least The C Programming Language. So many other C tutorial books and online articles run through endless examples that really aren't nearly as clearly as those presented in these works.

Maybe it's just my experience with bad books early on. I started C++ by trying to read an old edition of C++ For Dummies. Yeah, it's a slightly more lively read. Yeah, I could write a program in rhide and compile it with DJGPP. However, by the time I was reading the book these tools were (unbeknownst to me) horribly out of date and it had me writing classes and using pointers before I even really knew what the heap was.

Zed Shaw shows you precisely how the language works and gets you to deep understanding with all of those exercises. I work with mostly C# for now and wish I knew more Lisp. If you're interested in it and AI, there's no reason to not study it. It's good to be familiar with the "practical" languages sure, but you only need to try to have impressive-for-a-recent-grad competency in these. And the theoretical stuff like algorithms and asymptotic analysis are gonna be more important to most employers (there are exceptions of course, like game companies require C++, no exceptions [pun unintended], and the defense contractors will want C, C++, Java, or god forbid, Ada).

Be able to write basic programs and read the API docs of C# with .NET and Java. Build one complete application in one of those, like a mobile app, a web service that fetches/sorts data, or a desktop GUI app that does something interesting or useful. These languages and frameworks are not complicated. The libraries are just large.

Might be worth dabbling in Python or Ruby too. Some kind of web app mashup thing using Rails, Sinatra, Django, Tornado, something that's popular. These tools are even less complicated, don't worry about spending too much time on them unless you are totally in love with the tech startup scene. A fun thing to play with is the [Twilio](http://www.twilio.com/) api.

Then try something low-level in C. If your curriculum already has you set to do this, then might not be needed. Play with stuff like signal handlers, sockets, and graphics (NOT NEHE). Absolutely do C before C++ (I'm assuming you haven't done much with it already, correct me if I'm wrong). At first C++ may feel like just a pickier Java, but you should treat it differently. Good design of your classes still applies, but make sure you read up on some of the C++ intricacies (don't *study* them, just be aware of things like the Rule of 3, go [here](http://www.parashift.com/c++-faq-lite/) and [here](http://yosefk.com/c++fqa/) for that info). For a C++ project, you can try more graphics stuff, make a simple game with something like SFML or cocos2dx, or try out GTK or Qt.

Then you can confidently say you have good recent graduate-level experience with the resume keyword languages and tools. These are just some suggestions to guide you in the general direction of what's useful to learn about these these languages. Spend the rest of your time on Lisp/Scheme/Haskell/Clojure/Brainfuck/whatever gives you the greatest delight.

Strive to self-manage when doing these projects. There's a lot more to being a developer than just knowing the ins and outs of a framework or language. Whether you're studying Lisp or working on one of the other projects, have concrete goals to achieve. Make time to restructure your code as you learn more, but don't drag the process out. All this might sound like a lot, but by the time you graduate you'll probably have built a way more things than this between coursework and any internships. And these projects will go by quickly if you discipline yourself. The most important thing is to always be building stuff, whether it's a small tool or a large dream project (but if you go big, break down the project into several smaller ones, otherwise it will never get done).

About me: recent graduate, music major/CS minor, work as a game developer. Why not NEHE? Not in-depth enough? It's extremely out of date (programmable pipeline vs immediate mode) and most of the code is win32 setup and is just poorly structured. Even people who choose to the immediate mode functions over the programmable pipeline say to move off of it as quickly as possible and ditch the push/pop matrix functions and do your own math.

The updated tutorials they put up for OpenGL ES and such aren't so bad, but I had the best results with the [Arcsynthesis.org] (http://www.arcsynthesis.org/gltut/) tutorial and Joe Groff's [2.1 tutorial](http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html) (found these via /r/gamedev).       Go for it. Lisp is a nice language, and still has a bunch of stuff missing from other languages.  Exposure to good macro systems and the common lisp object system can only be a good thing.

Regarding AI, while lisp was king of the old symbolic approaches to AI, it doesn't help very much for more modern machine learning type approaches. Contemporary machine learning is heavily based around matrix and vector maths, and languages that have good out of the box support for matrixes such as matlab and numpy are more commonly used. Hey, thank you for the reply.

Unfortunately, there is very little AI / ML research occurring at my university. There is one professor, that I have not taken courses with, that lists ML as a research interest.

Would you have any suggestions on ways to move forward in the field, if I am interested in pursuing AI research? I'm more than willing to devote time outside of class, but I would like to be prepared to consider grad school, and it seems I need research experience to do that.    I will try to take a different look at this.    The number one thing here in my mind is that you are still a student, the question you should ask is will this distract you from your studies.  Seriously you have a couple of more years ahead of you and significant technology yet to learn.   I'd focus on getting the basics down first and then going back to Lisp when it fits into the work you are doing in the program.  

The second thing here is that Lisp is old and frankly could be considered to be one of the reasons AI is so backward these days.   If you really are into AI digest some lisp and then do the rest of the world a favor and come up with some new technology to base AI on.  For AI to really get anywhere it will need to leave Lisp behind.  AI is ripe for new perspectives and ideas.  

Third there is nothing wrong with enjoying a programming language, so don't take the above to be dismissal of Lisp.  I'm just not all that certain that focusing on Lisp right now is wise.  If you really expect to be employable your best bet is to develop proficiency in multiple languages and a strong understanding of the underlying principles of those languages. </snippet></document><document><title>Computer Science and Engineering - Books | InTech Open. </title><url>http://www.intechopen.com/subjects/computer-science-and-engineering/</url><snippet>       </snippet></document><document><title>When we store data we talk in bytes, but when we transfer data we talk in bits, why?</title><url>http://www.reddit.com/r/compsci/comments/wlrqh/when_we_store_data_we_talk_in_bytes_but_when_we/</url><snippet>I am a computer science major and I often find myself asking questions as such and I was wondering if you guys could explain this to me? Thanks guys!  Data is memory is not addressable by the bit, it's addressable in some larger chunk. How big the chunk is depends on your hardware architecture and the context, but in general you can consider a byte the smallest addressable piece of memory. So in the context of accessing data in memory it make sense to measure data in bytes since you can't store or access anything smaller than a byte.

Data being sent down a wire isn't like data in memory. It's being sent serially. That means that it's being sent down the pipe bit by bit. So in this context it does make sense to refer to individual bits.

Those are both oversimplifications, but the specifics will be covered in your CS curriculum. Hopefully that's enough detail to sate your curiosity for now. Actually that clears up the smoke and makes tons of sense. Thanks a ton, seriously. Just to expand a little bit, if the communication bus is paralell (I.E. more than one wire sending data at the same time) then the correct convention is to use MB for the same reasons ignacioMendez listed.

These days almost every external connection is serial.  The reason is its cheaper to make one wire super fast than it is to synchronize 8+ wires and have the signal arrive at the exact same time.  This is true even though they'd only need to be 1/8th the speed.  For examples of old paralell style busses you can look at (older) SCSI, IDE, or the trusty paralell port. Actually, parallel buses have pretty fundamental limitations when it comes to building them over a certain clock speed.  Cross-talk basically kills the signal, over a certain frequency.  This is also true for CPU buses (which at least to my knowledge are still parallel, but that might have changed in the really high density stuff).

Keeping parallel data channels in sync is pretty easy actually, if you can use a protocol which allows for syncing clocks.  Mostly it's just cross-talk and induction issues which kill parallel buses now. The methods for reducing the signal timing issues is to basically turn each wire into its own independent serial bus.  I'd argue that at that point its not a paralell bus in the historical meaning.  Anyways I think we both agree that paralell busses are very difficult to operate at high speeds and the future is serial. Doing that doesn't really eliminate the cross-talk issues at high switching rates.  There's a couple of active transmission type things you could do to boost efficiency (active cancellation of cross-talk by dedicated wires, or complimentary transmission), but it really doesn't change the fact that short wires act oddly when you start driving high freq. on them.  And, they're not bulletproof setups.

Interesting fact:  Ethernet Cat 5 &amp;amp; 6 cables are parallel conductors.  They reduce cross-talk by using twisted pairs of wires.  There's a lot of technology in those basic wires. I don't think we are disagreeing on cross talk.  Obviously that will increase regardless of whether the two channels are synchronized.  However I wouldn't count ethernet as a "paralell" bus even though there are multiple paralell wires carrying signals.  In the old fashioned paralell busses you had a single clock and all wires were synchronized to that clock.  

Each channel in Ethernet is self clocking and it doesn't have to be perfectly in sync.  Basically we are both describing different challenges with paralell technology.  If instead you count multiple serial channels bonded together (a-la Etherenet or PCIe) then your right my problem goes away and cross talk becomes the dominate issue. (So you know, it's spelled parallel.) Just to expand a little bit, if the communication bus is paralell (I.E. more than one wire sending data at the same time) then the correct convention is to use MB for the same reasons ignacioMendez listed.

These days almost every external connection is serial.  The reason is its cheaper to make one wire super fast than it is to synchronize 8+ wires and have the signal arrive at the exact same time.  This is true even though they'd only need to be 1/8th the speed.  For examples of old paralell style busses you can look at (older) SCSI, IDE, or the trusty paralell port. &amp;gt; Just to expand a little bit, if the communication bus is paralell (I.E. more than one wire sending data at the same time) then the correct convention is to use MB for the same reasons ignacioMendez listed.

That's just not true.

We call it "gigabit" ethernet. DVI is 4 Gbit/sec, etc. I'm not counting bonded serial channels as parallel.  In the old days a parallel bus was considered multiple signal wires operating at the same clock, where each channel needs to arrive at the exact same time.  The benefit was lower complexity on both the sending and receiving end. Actually that clears up the smoke and makes tons of sense. Thanks a ton, seriously. Actually that clears up the smoke and makes tons of sense. Thanks a ton, seriously. Also, the number of bits is always a higher number, so there was no reason to change it to byte at any point to make it more meaningful.

Similarly, in binary a kilobyte is 1024 bytes. A megabyte is 1024 kilobytes, a gigabyte is 1024 megabytes. But when they sell hard drives, they have small print that says "1 gigabyte = 1,000,000 bytes", so that they can sell the hard drive rated as if it had more space. [deleted] The other way round. Before SI ever got involved with digital quantities the convention in the computer world was to use the SI prefixes kilo, mega, giga etc to refer to a base 2 quantity rather than base 10. That is, kilobyte would be 1024 bytes, not 1000 bytes, mega bytes would be 2^20, rather than 10^6 bytes.

Hard disc manufacturers then sought to increase their apparent capacity by calculating megabytes using the traditional SI definition rather than the IT convention.

Eventually SI stepped in, reclaimed kilo, mega, giga and created new prefixes to be used in place of the conventional definitions. This is where the gibibytes that you are talking about comes from.

TBH noone really uses the new SI names and everyone continues to talk about having e.g. eight gigabytes of ram rather than eight gibibytes. gibi etc were afaik defined by IEC, not SI. And as AmbulatoryAndroid said, SI prefixes are old and *universal*. There is no question that using them for 2^10*x was a mistake at the time.

I have to agree that I haven't heard gibibytes in spoken language. But GiB etc are more used.  Data is memory is not addressable by the bit, it's addressable in some larger chunk. How big the chunk is depends on your hardware architecture and the context, but in general you can consider a byte the smallest addressable piece of memory. So in the context of accessing data in memory it make sense to measure data in bytes since you can't store or access anything smaller than a byte.

Data being sent down a wire isn't like data in memory. It's being sent serially. That means that it's being sent down the pipe bit by bit. So in this context it does make sense to refer to individual bits.

Those are both oversimplifications, but the specifics will be covered in your CS curriculum. Hopefully that's enough detail to sate your curiosity for now. If data is sent down bit by bit, what are packets? If data is sent down bit by bit, what are packets? If data is sent down bit by bit, what are packets? Data is memory is not addressable by the bit, it's addressable in some larger chunk. How big the chunk is depends on your hardware architecture and the context, but in general you can consider a byte the smallest addressable piece of memory. So in the context of accessing data in memory it make sense to measure data in bytes since you can't store or access anything smaller than a byte.

Data being sent down a wire isn't like data in memory. It's being sent serially. That means that it's being sent down the pipe bit by bit. So in this context it does make sense to refer to individual bits.

Those are both oversimplifications, but the specifics will be covered in your CS curriculum. Hopefully that's enough detail to sate your curiosity for now. Is it really possible to transfer a single bit over any data line such as ethernet or 802.11? Data is memory is not addressable by the bit, it's addressable in some larger chunk. How big the chunk is depends on your hardware architecture and the context, but in general you can consider a byte the smallest addressable piece of memory. So in the context of accessing data in memory it make sense to measure data in bytes since you can't store or access anything smaller than a byte.

Data being sent down a wire isn't like data in memory. It's being sent serially. That means that it's being sent down the pipe bit by bit. So in this context it does make sense to refer to individual bits.

Those are both oversimplifications, but the specifics will be covered in your CS curriculum. Hopefully that's enough detail to sate your curiosity for now. Data is memory is not addressable by the bit, it's addressable in some larger chunk. How big the chunk is depends on your hardware architecture and the context, but in general you can consider a byte the smallest addressable piece of memory. So in the context of accessing data in memory it make sense to measure data in bytes since you can't store or access anything smaller than a byte.

Data being sent down a wire isn't like data in memory. It's being sent serially. That means that it's being sent down the pipe bit by bit. So in this context it does make sense to refer to individual bits.

Those are both oversimplifications, but the specifics will be covered in your CS curriculum. Hopefully that's enough detail to sate your curiosity for now. Eh. This is intuitive but it's not completely correct.

Really, data may be sent across networks in parallel rather than bit-by-bit and when dealing with stored data it is not always byte-addressable, and indeed between non-network devices may handle stored data bit-by-bit.

tl;dr: It's just convention. &amp;gt; when dealing with stored data it is not always byte-addressable

With disk and such, block-addressable, but the smallest addressable unit is usually a byte

&amp;gt; between non-network devices may handle stored data bit-by-bit

Example? You don't address bits when dealing with network traffic, either, it's only the hardware that deals with one bit at a time, and the same thing can be said for any serial device including internal drives.

And certainly at the lowest level in any case, except for parallel-data devices (many of which are networking devices!) data is dealt with bit by bit. An individual read head on an HDD for example. Fair enough, but data is accessed by code, and code (on any practical architecture) can not address anything smaller than a byte.  One could at least make a reasonable argument for bits in networking since sending bits around is the whole point, but for storage, taking in bits would just be annoying nonsense. But you don't work at the bit level in networking either. And I think you meant "byte" in your first sentence. :) Data is memory is not addressable by the bit, it's addressable in some larger chunk. How big the chunk is depends on your hardware architecture and the context, but in general you can consider a byte the smallest addressable piece of memory. So in the context of accessing data in memory it make sense to measure data in bytes since you can't store or access anything smaller than a byte.

Data being sent down a wire isn't like data in memory. It's being sent serially. That means that it's being sent down the pipe bit by bit. So in this context it does make sense to refer to individual bits.

Those are both oversimplifications, but the specifics will be covered in your CS curriculum. Hopefully that's enough detail to sate your curiosity for now.  I don't really know, but a few possible reasons come to mind:

* Historical reasons
* A single bit isn't usually addressable, so it isn't as useful to discuss data in terms of bits
* A byte isn't always eight bits, but the physical bit rate of a network is fixed.  &amp;gt;* A byte isn't always eight bits, but the physical bit rate of a network is fixed. 

I was under the impression that a byte is always eight bits, but a word varied?

Edit: thank you for the corrections, they are appreciated :) you may stop now! &amp;gt;* A byte isn't always eight bits, but the physical bit rate of a network is fixed. 

I was under the impression that a byte is always eight bits, but a word varied?

Edit: thank you for the corrections, they are appreciated :) you may stop now! &amp;gt;* A byte isn't always eight bits, but the physical bit rate of a network is fixed. 

I was under the impression that a byte is always eight bits, but a word varied?

Edit: thank you for the corrections, they are appreciated :) you may stop now! Bytes are not necessarily 8 bits. I think the Atari had a 7-bit byte. 8 bits is more convenient though; it's a multiple of 2. not only is it even, but it's a power of two also! Bytes are not necessarily 8 bits. I think the Atari had a 7-bit byte. 8 bits is more convenient though; it's a multiple of 2. &amp;gt;* A byte isn't always eight bits, but the physical bit rate of a network is fixed. 

I was under the impression that a byte is always eight bits, but a word varied?

Edit: thank you for the corrections, they are appreciated :) you may stop now!   Ah, the young'uns in this thread are amusing me. 

Bits are not bytes, and link layer (what goes across the wire) isn't what's delivered at the application layer.  Bytes are essentially an application construct, and bits per second is what's happening at the physical layer. 

Back in the good old days of modems, you'd get 300 to 1200 baud (that's 1200 bits per second at the link layer).  Depending on the senders &amp;amp; receivers settings, you'd either have 8, 9, 10 or 11 bits per transmitted byte, and there were no other highlevel semantics like TCP/IP (unless you count x,y,and zmodem).  So, your 1200 baud connection would net you 150, 133, 120 or 109 bytes per second on your terminal.  There's a difference of 27% from the biggest to smallest, so it's hard to use "bytes per second" to measure, and just doing the standard 8 bits per byte isn't right at all, since pretty much no one used that setting because it was so unreliable.  Talking about "bytes per second" just didn't make sense, and is a measure of throughput at the application layer, not at the link layer. 

As networks got faster, they continued to measure link layer speeds in bits per second (ISDN, twisted pair ethernet, token ring, v.92, etc.) across these devices so that it was easy to compare them with each other.

Remember: Your 20Mbps (mega bits per second) cable modem can't ever transfer 2.5MBps (mega bytes per second) because of the way the data is encoded across the actual wire. Ah, the young'uns in this thread are amusing me. 

Bits are not bytes, and link layer (what goes across the wire) isn't what's delivered at the application layer.  Bytes are essentially an application construct, and bits per second is what's happening at the physical layer. 

Back in the good old days of modems, you'd get 300 to 1200 baud (that's 1200 bits per second at the link layer).  Depending on the senders &amp;amp; receivers settings, you'd either have 8, 9, 10 or 11 bits per transmitted byte, and there were no other highlevel semantics like TCP/IP (unless you count x,y,and zmodem).  So, your 1200 baud connection would net you 150, 133, 120 or 109 bytes per second on your terminal.  There's a difference of 27% from the biggest to smallest, so it's hard to use "bytes per second" to measure, and just doing the standard 8 bits per byte isn't right at all, since pretty much no one used that setting because it was so unreliable.  Talking about "bytes per second" just didn't make sense, and is a measure of throughput at the application layer, not at the link layer. 

As networks got faster, they continued to measure link layer speeds in bits per second (ISDN, twisted pair ethernet, token ring, v.92, etc.) across these devices so that it was easy to compare them with each other.

Remember: Your 20Mbps (mega bits per second) cable modem can't ever transfer 2.5MBps (mega bytes per second) because of the way the data is encoded across the actual wire.  Because 5 Megabits/s sounds better than 650 Kilobytes/s in advertisements. Doesn't really answer my question. Thanks though. Some of the above answers seem to make a lot more sense but this is actually the correct answer (sadly). It was just a way over making more impressive-sounding marketing claims. I think it started in the days of async comms protocols when the bit rate was &amp;gt;10x the rate of transferred bytes due to protocol overheads. 115kbps sounded a whole lot better than 11.5kBps.

This is the same reason a "one terabyte" hard disk may only be able to store 930GB of user data. The marketing numbers sound better. It's misleading advertising, but that's marketing for you.

Edit: woops MB-&amp;gt;GB, removed arcane reference to unformatted data size Some of the above answers seem to make a lot more sense but this is actually the correct answer (sadly). It was just a way over making more impressive-sounding marketing claims. I think it started in the days of async comms protocols when the bit rate was &amp;gt;10x the rate of transferred bytes due to protocol overheads. 115kbps sounded a whole lot better than 11.5kBps.

This is the same reason a "one terabyte" hard disk may only be able to store 930GB of user data. The marketing numbers sound better. It's misleading advertising, but that's marketing for you.

Edit: woops MB-&amp;gt;GB, removed arcane reference to unformatted data size Some of the above answers seem to make a lot more sense but this is actually the correct answer (sadly). It was just a way over making more impressive-sounding marketing claims. I think it started in the days of async comms protocols when the bit rate was &amp;gt;10x the rate of transferred bytes due to protocol overheads. 115kbps sounded a whole lot better than 11.5kBps.

This is the same reason a "one terabyte" hard disk may only be able to store 930GB of user data. The marketing numbers sound better. It's misleading advertising, but that's marketing for you.

Edit: woops MB-&amp;gt;GB, removed arcane reference to unformatted data size [deleted] Because 5 Megabits/s sounds better than 650 Kilobytes/s in advertisements. Unfortunately, the answer to many of these kinds of things is that marketing has fucked us again.  It makes me a sad panda.     Who cares? It's just a convention.</snippet></document><document><title>The "Theory" in Heuristic Optimization Theory</title><url>http://www.reddit.com/r/compsci/comments/wm9nn/the_theory_in_heuristic_optimization_theory/</url><snippet>It seems to me that the heuristic optimization field lacks some kind of mathematical theory, in the sense that the studies conducted in this area are much more empirical and its results are explained with much less mathematical rigor than they normally are in computer science, or at least in its subfields associated with *theoretical* computer science.

My understanding is that the almost all guidance experienced by those who project metaheuristic-based implementations such as Genetic Algorithms or Tabu Search, Simmulated Annealing, etc., comes from intuitive concepts such as "It is wise to avoid premature convergence at local minima".

I am aware of some analyses of metaheuristics such as Holland's schema theorem, but even wikipedia points that *"These analyses make a number of assumptions in regard to the optimizer variants and the simplicity of the optimization problems which limit their validity in real-world optimization scenarios. Performance and convergence aspects of metaheuristic optimizers are therefore often demonstrated empirically in the research literature."*

I'm still a grad student, so I could be making some enormously stupid statements. If that's the case, could you point me in the direction of mathematically rigorous approaches to this area? Thanks and sorry for my mediocre english.      [Relevant](http://en.wikipedia.org/wiki/HAL_9000)</snippet></document><document><title>Need a C++ library to fit curves to data points</title><url>http://www.reddit.com/r/compsci/comments/wlv92/need_a_c_library_to_fit_curves_to_data_points/</url><snippet>I have a program that gives data points for how long it takes to compile a program of a certain length. Some of these data points follow a log function while some follow a straight line. 

I want to be able to fit a curve to these points to be able to extrapolate how long it will take to compile longer programs without having to actually compile them.

Does anyone know of a good, simple C++ library to do this? Will I need to just write it myself if I want to use C++?  The [GNU Scientific Library](http://www.gnu.org/software/gsl/) should be able to do it.  In particular, look at its [linear regression/least-squares fitting functions](http://www.gnu.org/software/gsl/manual/html_node/Linear-regression.html).

I'm curious as to why you have to use c++ for this.  It seems like outputting csv and then running linear regression in your favorite spreadsheet app would be easier. Thanks for the help.

As for outputing to a .csv that might be a good option for debugging but the software package I'm working on has already been written and released in C++ and I'm adding a possible new feature. It's an end user software package and I want to make the program as seamless and "automatic" as possible. The [GNU Scientific Library](http://www.gnu.org/software/gsl/) should be able to do it.  In particular, look at its [linear regression/least-squares fitting functions](http://www.gnu.org/software/gsl/manual/html_node/Linear-regression.html).

I'm curious as to why you have to use c++ for this.  It seems like outputting csv and then running linear regression in your favorite spreadsheet app would be easier. I have a very similar need to the OP. GSL does look awesome. But is there an alternative that is not GPLed?

I'd have thought Boost or OpenCV would have something like this, but it seems there's just Linear Regression algorithms available.

I did find http://dynamomd.org/ has a GPLed Spline interpolation class that uses Boost, which might be handy.

Any other suggestions welcome! I have a very similar need to the OP. GSL does look awesome. But is there an alternative that is not GPLed?

I'd have thought Boost or OpenCV would have something like this, but it seems there's just Linear Regression algorithms available.

I did find http://dynamomd.org/ has a GPLed Spline interpolation class that uses Boost, which might be handy.

Any other suggestions welcome!  </snippet></document><document><title>Finding the most similar array of integers to a given one?</title><url>http://www.reddit.com/r/compsci/comments/wlkg2/finding_the_most_similar_array_of_integers_to_a/</url><snippet>I'm working on character recognition (OCR) for fun and this problem came up.

I have a dictionary of integer arrays, each having length N. (In my case, N=11). If I generate an integer array of length N, how can I find the closest match in my dictionary?

The distance between two arrays is:

    distance(array a, array b) = |a[1] - b[1]| + |a[2] - b[2]| + ... + |a[N] - b[N]|

The trivial solution is O(N * (number of elements in dictionary)), what's a better way?

**Solved here:**

http://www.reddit.com/r/compsci/comments/wlkg2/finding_the_most_similar_array_of_integers_to_a/c5edi5d  Do you want an approximate or an exact answer?

There are various things you can do:
http://en.wikipedia.org/wiki/Nearest_neighbor_search Thanks! I was sure this was a very common comp sci problem but I didn't know the name and couldn't describe it well enough.  You're looking for something very much like a [Voronoi diagram](http://en.wikipedia.org/wiki/Voronoi_diagram) in 11 dimensions with the Manhattan distance as a metric. I have no idea how complicated this is going to be to implement, though. If you the set you have to match against is pretty small (like in the example) the trivial solution may be a perfectly valid approach!

One advantage of the manhattan metric is that the boundaries of your diagram are all "relatively" simple: horizontal, vertical or at 45 degrees - but then in eleven dimensions. I.e. given by an equation of the form a_1 x_1 + a_2 x_2 + ... + a_11 x_11 = C where all the a_i are in {-1,0,1}.  Given the smallish size of my set and the complexity of solving this faster (kd-trees, etc), I'll see if the naive approach is fast enough for my needs.

Thanks for your input. Comparing it to the Voronoi diagram but in 11 dimensions gave me a good image in my mind of the problem.

Is there something about it using the Manhattan distance that I can use to make a simple but effective optimization, maybe? **Always** implement the naive and simple algorithm first.  If it isn't fast enough you have a reference implementation to test the correctness of the more complicated algorithm.  If you don't care about the magnitude of the vectors that the arrays could be said to represent then you could use the cos distance.  Correctly configured locality sensitive hashing https://secure.wikimedia.org/wikipedia/en/wiki/Locality_sensitive_hashing could solve this problem for you. Cover trees are also a really great data structure for nearest neighbor search, I've been wanting to implement them.

So d=11 (11 dimensional space), but how many data points do you have (so N=?) and are you going to be adding vectors to your dictionary? If you aren't adding vectors do your dictionary, precomputation seems like a pretty good option. d=16 now, I had to up it for my problem.

One data point for each possible character, which is 26*2+10+1 = 63 right now. I won't be adding new vectors during runtime.

I'll look into locality sensitive hashing if my current solution isn't fast enough.  [Levenshtein distance](http://en.wikipedia.org/wiki/Levenshtein_distance) maybe? [Levenshtein distance](http://en.wikipedia.org/wiki/Levenshtein_distance) maybe? I may have thrown people off by mentioning that this is for OCR. The sequences of integers are not characters, so Levenshtein distance and hamming distance won't apply. Well not without modification, but with a little bit of imagination I bet either one could be a good solution.  I've written an OCR before. And I used a Neural Network for the identifying the closest match. It was a learning exercise. :) That would work, but I'm actually reading these pixels from the screen so an ANN is a little overkill here.  Sounds like a spell checking problem.

I believe what modern spell checkers do is they rely on an observation of spelling mistakes: most errors have a hamming distance of 2 from their desired word. Thus, modern spell checkers simply GENERATE all words that have a hamming distance of 2 away from the misspelled word, and then check every word in that list to see if it's in the dictionary.

Hope that helps. Algorithmically the same, but N becomes much, much smaller. Hm, in my case, most of the generated integer arrays will not be very close to anything in the dictionary. Also, the number of "nearby" permutations would be intractable to generate.

The values of each integer in the arrays is between 0 and 12000, roughly.

edit: added an example to the post The trick with the spell checker is to find some upper bound on the distance &#8212; can you do that with your program?  I would say you'd have to use dynamic programming.</snippet></document><document><title>Function currying in Clojure and Haskell</title><url>http://www.reddit.com/r/compsci/comments/wkjfv/function_currying_in_clojure_and_haskell/</url><snippet>I'm reading through [Clojure Programming](http://www.amazon.com/Clojure-Programming-Chas-Emerick/dp/1449394701/ref=sr_1_1?ie=UTF8&amp;amp;qid=1342309022&amp;amp;sr=8-1&amp;amp;keywords=clojure+programming), which is a wonderful book, in fact one of the clearest books on a functional language I have read. SICP I felt was a more difficult read. So here is my question, and it's more directed towards clojure or other lisp people. In Haskell, a function has a type like (from [LYAHFGG](http://learnyouahaskell.com/higher-order-functions):

    multThree :: (Num a) =&amp;gt; a -&amp;gt; a -&amp;gt; a -&amp;gt; a  
    multThree x y z = x * y * z

so I can use function currying to change this function to another one,

    multTwoWithFive x y = multThree 5 x y

which yields a function of type

    Int -&amp;gt; Int -&amp;gt; Int

In Clojure, and I assume other lisps, rather than implicit currying and function types like that, you have to use partial.

    (defn mult-three [x y z]
        (* x y z))

    (def mult-two=with-five (partial mult-three 5))

Is there a less explicit way to do this? What is the reasoning for clojure to be implemented this way, and is it really that different?  Currying isn't the same as partial application, it's a technique to implement functions that take multiple arguments using only unary (i.e. that take only *one* argument) functions.

Haskell's `multThree` is really a function that takes an argument `x`, that returns a function that takes an argument `y`, that returns *another* function that takes an argument `z` that multiplies `x`, `y` and `z` together. So, `multThree 5 6` is exactly the same as `\x -&amp;gt; multThree 5 6 x` (`\x -&amp;gt; ...` denotes an anonymous function in Haskell), because a function that does nothing but pass its argument to another function `\x -&amp;gt; f x` behaves the same as that function `f`.

Also note that `(multThree 5) 6` is the same thing as `multThree 5 6`, as `multThree 5` returns a function that is then applied to `6`

Now, Clojure, Scheme, and Lisps in general actually support functions with multiple arguments, and in general don't need everything to be curried. `(mult-three 5 6)` is *not* the same as `(fn [x] (mult-three 5 6 x))` because `mult-three` actually *takes* three arguments, it does not return a function that takes the next argument and multiplies it with 5 and 6, like in Haskell. What you need here, is either to partially apply `mult-three` with 5 6 (with `(partial mult-three 5 6)`, or with an explicit anonymous procedure `(fun [x] (mult-three 5 6 x))`), or to define `mult-three` as a curried function like so:

    (defn curried-mult-three [x]
       (fn [y] (fn [z] (* x y z))))

And call it like this: `((curried-mult-three 5) 6)`, which is what Haskell does.

Why doesn't Clojure curry everything by default?
It's because curried function can't easily take arbitrarily many arguments, so having both `(* 1 2)` and `(* 1 2 3)` work are not possible (unless you do some type-level magic).

It's also somewhat easier to optimize, as you don't need to allocate a closure every call. Currying isn't the same as partial application, it's a technique to implement functions that take multiple arguments using only unary (i.e. that take only *one* argument) functions.

Haskell's `multThree` is really a function that takes an argument `x`, that returns a function that takes an argument `y`, that returns *another* function that takes an argument `z` that multiplies `x`, `y` and `z` together. So, `multThree 5 6` is exactly the same as `\x -&amp;gt; multThree 5 6 x` (`\x -&amp;gt; ...` denotes an anonymous function in Haskell), because a function that does nothing but pass its argument to another function `\x -&amp;gt; f x` behaves the same as that function `f`.

Also note that `(multThree 5) 6` is the same thing as `multThree 5 6`, as `multThree 5` returns a function that is then applied to `6`

Now, Clojure, Scheme, and Lisps in general actually support functions with multiple arguments, and in general don't need everything to be curried. `(mult-three 5 6)` is *not* the same as `(fn [x] (mult-three 5 6 x))` because `mult-three` actually *takes* three arguments, it does not return a function that takes the next argument and multiplies it with 5 and 6, like in Haskell. What you need here, is either to partially apply `mult-three` with 5 6 (with `(partial mult-three 5 6)`, or with an explicit anonymous procedure `(fun [x] (mult-three 5 6 x))`), or to define `mult-three` as a curried function like so:

    (defn curried-mult-three [x]
       (fn [y] (fn [z] (* x y z))))

And call it like this: `((curried-mult-three 5) 6)`, which is what Haskell does.

Why doesn't Clojure curry everything by default?
It's because curried function can't easily take arbitrarily many arguments, so having both `(* 1 2)` and `(* 1 2 3)` work are not possible (unless you do some type-level magic).

It's also somewhat easier to optimize, as you don't need to allocate a closure every call. Huh, that makes sense. I guess that's the trade off for explicit (static?) typing versus dynamic typing, in that it applies to functions just as much as data structures. Thanks for the explanation! It's not related to static vs. dynamic typing (at least, not in any straightforward way I can see). Haskell can be non-curried (imagine all functions taking tuples), and a dynamically typed language could be curried. Huh, that makes sense. I guess that's the trade off for explicit (static?) typing versus dynamic typing, in that it applies to functions just as much as data structures. Thanks for the explanation! Currying isn't the same as partial application, it's a technique to implement functions that take multiple arguments using only unary (i.e. that take only *one* argument) functions.

Haskell's `multThree` is really a function that takes an argument `x`, that returns a function that takes an argument `y`, that returns *another* function that takes an argument `z` that multiplies `x`, `y` and `z` together. So, `multThree 5 6` is exactly the same as `\x -&amp;gt; multThree 5 6 x` (`\x -&amp;gt; ...` denotes an anonymous function in Haskell), because a function that does nothing but pass its argument to another function `\x -&amp;gt; f x` behaves the same as that function `f`.

Also note that `(multThree 5) 6` is the same thing as `multThree 5 6`, as `multThree 5` returns a function that is then applied to `6`

Now, Clojure, Scheme, and Lisps in general actually support functions with multiple arguments, and in general don't need everything to be curried. `(mult-three 5 6)` is *not* the same as `(fn [x] (mult-three 5 6 x))` because `mult-three` actually *takes* three arguments, it does not return a function that takes the next argument and multiplies it with 5 and 6, like in Haskell. What you need here, is either to partially apply `mult-three` with 5 6 (with `(partial mult-three 5 6)`, or with an explicit anonymous procedure `(fun [x] (mult-three 5 6 x))`), or to define `mult-three` as a curried function like so:

    (defn curried-mult-three [x]
       (fn [y] (fn [z] (* x y z))))

And call it like this: `((curried-mult-three 5) 6)`, which is what Haskell does.

Why doesn't Clojure curry everything by default?
It's because curried function can't easily take arbitrarily many arguments, so having both `(* 1 2)` and `(* 1 2 3)` work are not possible (unless you do some type-level magic).

It's also somewhat easier to optimize, as you don't need to allocate a closure every call. I'm having trouble seeing why a curried function can't easily take arbitrarily many arguments. Seems like you could just keep eating an argument at a time and making another function until you hit the end. Why am I wrong? Because the type of a function that takes a single argument and returns a function on its first N applications, then returns the result on the N+1-th application, can't be defined at compile-time when N is set at run-time. Got it now, thanks. I was about to argue the function could return a function if there's an argument, and a result otherwise, then realized that would remove the benefit of currying. There's no way to tell whether (+ 1 2 3) is supposed to return a result or a curried function.

Clojure functions don't take arbitrarily many arguments unless they have rest parameters, so I think you could sorta make it work for other functions. Except you still have functions with overloaded arities.

So then I started wondering what happens when you have a function with multiple arities, and call partial on it. So I tried it and it turns out you get a partial function with multiple arities:

    (defn f1 
      ([a b] (- a b))
      ([a b c] (* a b c))
      ([a b c d] (+ a b c d)))

    (def f2 (partial f1 2 3))

    (f2)
    &amp;gt;&amp;gt; -1

    (f2 4)
    &amp;gt;&amp;gt; 24

    (f2 4 5)
    &amp;gt;&amp;gt; 14
 I'm having trouble seeing why a curried function can't easily take arbitrarily many arguments. Seems like you could just keep eating an argument at a time and making another function until you hit the end. Why am I wrong?  The lack of currying actually what turned me off of Clojure after using Haskell. I think it's a fundamental decision that functions are passed in argument lists (like in most languages) as opposed to single arguments.

Three reasons spring to mind:

* Historically, Lisps have behaved like this, and the gain in deviating is not so great as to offset the cost.
* Vararg functions become trivial, unlike in Haskell, where they're tricky.
* Having macros and functions be syntactically indistinguishable at use site means that the caller shouldn't care which the thing being called is, but currying macros is tricky (coupled with the fact that many useful macros are vararg). Vararg functions are useful for implementing useful function macros such as optional arguments, keyword-named arguments, and so on. It's much harder to get these in Haskell (without encoding).

Also, what do you mean by "currying macros"? Macros are invoked during the expansion process, so there isn't much point in producing a function during expansion instead of syntax. You can define macro-defining macros though, but that isn't currying. If forms (I use the term to refer to both macros and functions) are curried, then ((f a) b) must be the same as (f a b), as must (def (g (f a)) (g b)) (forgive me if my lisp is not exactly correct; I'm primarily a Haskeller). This can sometimes make sense for functions (e.g. via implicit partial application), ~~but it doesn't make much sense for macros.~~ but now that I've thought about it, it can make sense for macros.

Macro-defining macros _are_ currying, if you view macros as functions from syntax to syntax, which iirc is a valid view. If forms (I use the term to refer to both macros and functions) are curried, then ((f a) b) must be the same as (f a b), as must (def (g (f a)) (g b)) (forgive me if my lisp is not exactly correct; I'm primarily a Haskeller). This can sometimes make sense for functions (e.g. via implicit partial application), ~~but it doesn't make much sense for macros.~~ but now that I've thought about it, it can make sense for macros.

Macro-defining macros _are_ currying, if you view macros as functions from syntax to syntax, which iirc is a valid view. The lack of currying actually what turned me off of Clojure after using Haskell. I think it's a fundamental decision that functions are passed in argument lists (like in most languages) as opposed to single arguments.

Three reasons spring to mind:

* Historically, Lisps have behaved like this, and the gain in deviating is not so great as to offset the cost.
* Vararg functions become trivial, unlike in Haskell, where they're tricky.
* Having macros and functions be syntactically indistinguishable at use site means that the caller shouldn't care which the thing being called is, but currying macros is tricky (coupled with the fact that many useful macros are vararg). Those seem like valid reasons. Though, how are vararg functions different than pattern-matched functions? Those are orthogonal concepts. Lisp-like languages can have [advanced pattern matching](http://docs.racket-lang.org/guide/match.html) as well. The point of vararg functions is that a function can accept an arbitrary number of parameters.

For example, the `map` function in some Lisps accepts an arbitrary number of lists to map over. Elements from each list are passed in together to the function you provide to `map`.

As to your original question, some Lisps also provide an alternative definition form that creates curried functions:

    (define ((foo x) y) x)

produces a function that takes `x` and produces a function that takes `y`. There are also various macros that are meant to provide the benefits of currying, such as the `cut` macro from [SRFI-26](http://srfi.schemers.org/srfi-26/srfi-26.html) in Scheme. Ok, I was thinking of functions with multiple arities, not vararg functions.</snippet></document><document><title>Is there a compression method designed for photos from a security camera?</title><url>http://www.reddit.com/r/compsci/comments/wk9o9/is_there_a_compression_method_designed_for_photos/</url><snippet>Following on the heels of the VBR video compression question...

I have a security camera system that will take four pictures over a second when it senses movement.  I have a folder filled with similar pictures from two different cameras.  525 Mb uncompressed -&amp;gt; 513 Mb compressed with zip

But most of these pictures are of the exact same background, with a figure or object in the foreground that itself is pretty much the same over a series of pictures! The same compression method used for avi or mpeg might be very effective in this situation.

How about it, reddit: do you know of any useful compression solutions that could save me some space, or another place for me to ask this question?  There's lossless codecs (H.264/MPEG-4 AVC-lossless,^1 Dirac,^2 FFV1, others^3 ) but they will give you a [50% size reduction](http://wweb.uta.edu/ee/Dip/Courses/EE5359/dirac3.pdf), probably, at most.

The kind of compression that you have to look out for works by removing and discarding details that usually do not matter.^4 

These details are tossed away and gone forever.^5  In their place is computer guesswork that does an approximation^6 of what was there. The result is good enough that the casual human eye usually doesn't catch it.^7

But when you are dealing with people's lives,^8 different things matter.^9

What you appear to have is a high quality security system.  Those will have higher routine and maintenance costs, sure.  But I recommend you pony up and pay them.^10

It may save you when you actually need to use it.^11

------
^1 [The H.264/AVC Advanced Video Coding Standard: Overview and Introduction to the Fidelity Range Extensions](http://www.fastvdo.com/spie04/spie04-h264OverviewPaper.pdf)

^2 [Dirac Specification](http://diracvideo.org/download/specification/dirac-spec-latest.pdf)

^3 [Lossless codecs](http://en.wikipedia.org/wiki/List_of_codecs#Lossless_compression)

^4 [Transcoding losses](http://en.wikipedia.org/wiki/Digital_generation_loss#Digital_generation_loss)

^5 ...[the camera inside the store was low quality and did not capture a good image of the thief.](http://www.kcby.com/home/related/Subway-camera-didnt-capture-robber-159216355.html)

^6 [Compression artifacts](http://en.wikipedia.org/wiki/Compression_artifact)

^7 [Wavelets and human visual perception in image compression](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=547198&amp;amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D547198)

^8 ...[the clarity of the cameras was so bad that her captor could not be identified.](http://www.ndtv.com/article/cities/despite-rs-27-crore-upgrade-cctvs-offer-poor-visuals-241314)

^9 [Some systems can&#8217;t produce images clear enough to convince a jury] (http://newsok.com/home-security-cameras-help-capture-clear-picture-of-thieves/article/3685381#ixzz20e2ounpt)

^10 [It's exceptionally good video. A lot of times people will invest in a security system that is not of high quality and they get poor quality results](http://www.kctv5.com/story/18800663/police-search-for-suspects-who-stole-lampposts)

^11 [Rockaway cop to retailers: You really need better security cameras](http://www.nj.com/morris/index.ssf/2012/07/rockaway_cop_to_retailers_get.html)
 This is the correct answer. No compression method you have available to you is good enough for the trouble it causes.

Instead, invest a few hundred into a pair of 3TB drives and forget that you had an issue. Lossless compression is still fine though. He didn't specify resolution, so they might not even need as much as 3TB.    I'll take a different path from the others: why not only store the pixel info where things are different from the base image? Basic computer vision tutorials should tell you how to isolate movement. All you have to do is toss the rest and compress    </snippet></document><document><title>Calculating the probability that a point will conform to a fitted curve?</title><url>http://www.reddit.com/r/compsci/comments/wjleu/calculating_the_probability_that_a_point_will/</url><snippet>Hey Reddit! Have a situation where I am given a total ticket count, and cumulative ticket sale data as follows:

Total Tickets Available: 300

Day 1: 15 tickets sold to date

Day 2: 20 tickets sold to date

Day 3: 25 tickets sold to date

Day 4: 30 tickets sold to date

Day 5: 46 tickets sold to date

The number of tickets sold is nonlinear, and I'm asked if someone plans to buy a ticket on Day 23, what is the probability he will get a ticket?

I've been looking at quite a libraries used for curve fitting like Numpy, PyLab, Sage and they all seem to have great curve fitting libraries, but nothing to calculate the probability that a certain extrapolated point is correct. I also have data from other ticket sale locations, but the functions may not align exactly, don't know if this will be useful. Statistics is not my background and I'm not sure exactly how to approach this.   Statistics is based on probability distributions. With the data you have now you'll have to make assumptions about the distributions. Or you'll have to find more data. 

For example:
Day 1: mean is 13 tickets sold, standard deviation is 4.

Next you'll have to figure out if and how each day's sales is related to the other days.

There are lots of other things you could think about if you wanted, but I would start by figuring those parts out.

Good luck! This. You can't derive any kind of statistical conclusions from a single trial. You could, however, fit a curve and then sort of eyeball it and hope it's representative of what reality will be, but without real data to validate your model you're really just guessing. Statistics is based on probability distributions. With the data you have now you'll have to make assumptions about the distributions. Or you'll have to find more data. 

For example:
Day 1: mean is 13 tickets sold, standard deviation is 4.

Next you'll have to figure out if and how each day's sales is related to the other days.

There are lots of other things you could think about if you wanted, but I would start by figuring those parts out.

Good luck! The total data set is between a few hundred and a few thousand points total, but I was just interested in first seeing the probability that an extrapolated point on the x axis beyond the current data set (ie. Day 23), will be under the total number of tickets (300) given a linear regression from one of Python's math/statistics libraries. Thanks!   </snippet></document><document><title>n+1: The Stupidity of Computers</title><url>http://nplusonemag.com/the-stupidity-of-computers</url><snippet>  After *Hard Labor* I didn't read much more.

The exact same problems would be solved *just as poorly* by a human, if not *worse* since we have no capacity for focus. It brings to mind a quote from I, Robot:

"Can a robot write a symphony? Can a robot turn a... canvas into a beautiful masterpiece? 

Can *you*?" &amp;gt; "Can a robot write a symphony?"

Yes. Sort of. The computer needs some initial source material, but so do human composers. No one writes in a vacuum after all.

[David Cope: 'You pushed the button and out came hundreds and thousands of sonatas'](http://www.guardian.co.uk/technology/2010/jul/11/david-cope-computer-composer) I'm perfectly aware of that, I try to do it myself (to an extant, I don't know much music theory), it's just a quote which puts into perspective the meaning of what this author is implying. After *Hard Labor* I didn't read much more.

The exact same problems would be solved *just as poorly* by a human, if not *worse* since we have no capacity for focus. It brings to mind a quote from I, Robot:

"Can a robot write a symphony? Can a robot turn a... canvas into a beautiful masterpiece? 

Can *you*?" &amp;gt; After Hard Labor I didn't read much more.

Not surprising you missed the point of the article, then. The point of the article isn't that human intelligence is "magic," it's that programming intelligence into our machines is really damn hard - possibly so hard that we won't do it.

More to the point, the article says that because it's so hard we're likely to meet AIs half-way. By providing crowd-sourced intelligence and by using statistical methods and large corpuses of well-tended data we probably won't need to make "truly intelligent" machines to do a lot of information-related tasks we currently need humans for. After *Hard Labor* I didn't read much more.

The exact same problems would be solved *just as poorly* by a human, if not *worse* since we have no capacity for focus. It brings to mind a quote from I, Robot:

"Can a robot write a symphony? Can a robot turn a... canvas into a beautiful masterpiece? 

Can *you*?" After *Hard Labor* I didn't read much more.

The exact same problems would be solved *just as poorly* by a human, if not *worse* since we have no capacity for focus. It brings to mind a quote from I, Robot:

"Can a robot write a symphony? Can a robot turn a... canvas into a beautiful masterpiece? 

Can *you*?" Not sure if that was point of the article...

If you are personally asking me "Can you?", but human beings can.  &amp;gt;...computers cannot and will not &#8220;understand&#8221; us the way we understand each other...

While it is likely true that computers won't exhibit the same method of understanding, the claim that computers are incapable of human understanding has little actual support (see: sub-symbolic AI).  There would have to be some sort of atomic process that could not be represented or abstracted by a system.  It's more likely that our current lack of how we understand things is preventing us from developing such a system.  Again, it might not even be feasible to do, which is why it's more likely that we will develop a different system of understanding. The article doesn't seem to be talking about "computers in abstract," but about "computers as we know them." The author isn't claiming that human thought is uncomputable, just that we can't do it and probably won't be able to for the foreseeable future.

The article is about difficulty-in-practice, not difficulty-in-principle. As such, it talks about the shortcomings of engineering efforts, not the Church-Turing thesis or religion or philosophy of mind. It talks about historical progress and predicted social change, not fantasy. Frankly, I found the ideas in the article surprisingly well-considered given the usual focus of the magazine, and I think it's more interesting than any singularitarian diatribe I've read in a while. The article doesn't seem to be talking about "computers in abstract," but about "computers as we know them." The author isn't claiming that human thought is uncomputable, just that we can't do it and probably won't be able to for the foreseeable future.

The article is about difficulty-in-practice, not difficulty-in-principle. As such, it talks about the shortcomings of engineering efforts, not the Church-Turing thesis or religion or philosophy of mind. It talks about historical progress and predicted social change, not fantasy. Frankly, I found the ideas in the article surprisingly well-considered given the usual focus of the magazine, and I think it's more interesting than any singularitarian diatribe I've read in a while.   I'm not sure I get the main point of this article. A recurring theme is that computers are dumb because they don't think like humans do, but many of the examples given don't support this idea, so it may be trying to make some other point.

Anyway, I take issue with the idea that computers need to think like humans to be considered intelligent. Processing human language is harder for computers than humans, great, but why should that be the touchstone of intelligence? Like Russel and Norvig say, defining AI as fooling humans into thinking you're human is like defining artificial flight as fooling pigeons into thinking you're a pigeon.

&amp;gt; Could a computer have determined a blog&#8217;s political stance on its own? Unlikely. One could imagine a program that analyzed instances of known political figures with positive textual markers, but &#8220;Obama,&#8221; say, and &#8220;genius&#8221; could just as well appear on a Republican blog, where it might be sarcastic, as on a Democratic blog, where it might be sincere.

So a computer can't successfully execute the same heuristic a human would to solve this classification problem. That doesn't mean it can't solve the problem. I don't trust the author to conclude that it's "unlikely" that, for instance, a neural network would work.   This person needs to read *Structure and Interpretation*. This person needs to read *Structure and Interpretation*. Why do you say so?  The bit about Wikipedia is odd.... Natural language and ontologies and rich data are one of the most popular areas of research in using Wikipedia corpuses! I agree. *cough* dbpedia *cough* Is *cough* the new markup tag? :) I agree. *cough* dbpedia *cough*  </snippet></document><document><title>JSOL: JavaScript Object Language - functional programming language fully representable in JSON</title><url>http://jsol.org</url><snippet>   Maybe I'm missing something... if you're passing logic between programs or over a network, why would you care if it's represented in JSON? It'll require its own parser/interpreter to use sanely, so why not have that parser/interpreter generate a more compact or more readable language... like Lua or some sort of bytecode?      [deleted] Every JSOL program is a valid JSOL data structure, which means a program can easily manipulate its own source code.  Once you get used to it, this feature is extraordinarily powerful, and you'll wonder how you ever lived without it.  I can't give you a use case, though, because I'm not a Lisp programmer, so I've never really used a language that can rewrite itself on-the-fly.

The reason you and I can't see a need for a program to manipulate itself is that our programming languages can't do it, so we have trouble imagining the concept.  This is known as the "Blub Paradox".  Paul Graham wrote about it in his essay [Beating the Averages](http://www.paulgraham.com/avg.html).  You should read it, because he explains it much better than I can.  </snippet></document><document><title>Here we go again: "From this result, the conclusion that P is not NP is reached."</title><url>http://arxiv.org/abs/1207.2171</url><snippet>  I think I got it!

He defines *d* for some problem as #variables + #parameters - #invariants

He then shows that for a specific pair of problems where d != d', it is impossible to reduce one problem to the other. Therefore, it is *never* possible to reduce a problem to another problem with a different d-value.

Well, that was fun while it lasted. I'm gonna go bake myself a cookie. It had better be a huge cookie, unless you're using your oven inefficiently... [deleted] It had better be a huge cookie, unless you're using your oven inefficiently... I think I got it!

He defines *d* for some problem as #variables + #parameters - #invariants

He then shows that for a specific pair of problems where d != d', it is impossible to reduce one problem to the other. Therefore, it is *never* possible to reduce a problem to another problem with a different d-value.

Well, that was fun while it lasted. I'm gonna go bake myself a cookie. he uses ladner's theorem as if it's if-and-only-if too, which i was under the impression it wasn't...

either way, at least it's 3 pages and not 30 he uses ladner's theorem as if it's if-and-only-if too, which i was under the impression it wasn't...

either way, at least it's 3 pages and not 30 I think I got it!

He defines *d* for some problem as #variables + #parameters - #invariants

He then shows that for a specific pair of problems where d != d', it is impossible to reduce one problem to the other. Therefore, it is *never* possible to reduce a problem to another problem with a different d-value.

Well, that was fun while it lasted. I'm gonna go bake myself a cookie. I think I got it!

He defines *d* for some problem as #variables + #parameters - #invariants

He then shows that for a specific pair of problems where d != d', it is impossible to reduce one problem to the other. Therefore, it is *never* possible to reduce a problem to another problem with a different d-value.

Well, that was fun while it lasted. I'm gonna go bake myself a cookie. I think I got it!

He defines *d* for some problem as #variables + #parameters - #invariants

He then shows that for a specific pair of problems where d != d', it is impossible to reduce one problem to the other. Therefore, it is *never* possible to reduce a problem to another problem with a different d-value.

Well, that was fun while it lasted. I'm gonna go bake myself a cookie. o.O Didn't he JUST reduce 3-SAT to the diophantine equation and *explicitly state that he was reducing from a higher d-value to a lower one*, not even one page before stating that theorem?! o.O Didn't he JUST reduce 3-SAT to the diophantine equation and *explicitly state that he was reducing from a higher d-value to a lower one*, not even one page before stating that theorem?!  What I love is that he seriously thought he got it in 3 pages. Did he not get any colleagues to read it over before pushing it onto arxiv? He was probably worried they'd claim the proof for themselves. He definitely jumped the gun, though. And this is why the peer review process exists.  I'm new to this subreddit.

Do I upvote the submission for the spectacle, or downvote it to prevent said spectacle? Upvote it so we can all have a good laugh. Done and done. If you upvote twice, it doesn't count.  First to find the error gets a cookie! And by that, we mean an HTTP cookie.  Always relevant in these cases:

[Eight signs a claimen P != NP proof is wrong](http://www.technologyreview.com/view/420234/eight-signs-a-claimed-p-is-not-np-proof-is-wrong/)


(Personally, I hope that P = NP, because that would be a lot more fun) I hope so too, for the lulz.

Specifically, I hope that a proof of P=NP does not give any insight as to how to construct a polynomial algorithm to solve any given problem that is currently considered NP.

"Sooooo we know that all asymmetric encryption is broken, but we don't know how to break any of it." I hope so too, for the lulz.

Specifically, I hope that a proof of P=NP does not give any insight as to how to construct a polynomial algorithm to solve any given problem that is currently considered NP.

"Sooooo we know that all asymmetric encryption is broken, but we don't know how to break any of it." &amp;gt; "Sooooo we know that all asymmetric encryption is broken, but we don't know how to break any of it."

For some values of "broken". I'd love it if P = NP and all NP-complete problems would be e.g. &#937;( n^A(1000,1000) ), where A is the Ackermann function. I hope so too, for the lulz.

Specifically, I hope that a proof of P=NP does not give any insight as to how to construct a polynomial algorithm to solve any given problem that is currently considered NP.

"Sooooo we know that all asymmetric encryption is broken, but we don't know how to break any of it." That really would be nice, because now we have the storage and network capacity to do key distribution and derivation tasks using just symetric algorithms. It would give us the necessary nudge to start building systems that way.  It would be pretty catastrophically awful, to be honest.

P=NP means there exist polynomial known-plaintext attacks for all symmetric algorithms. It means there exist polynomial pre-image attacks for all hash functions. It means there exist polynomial attacks on all PRNG algorithms. I don't see how. There might already exist such attacks ... knowing that they exist wouldn't change that, as there are already folks working on that assumption. I don't see how either. That's why I (as well as many other people) believe P != NP. That's kind of the point, really. Always relevant in these cases:

[Eight signs a claimen P != NP proof is wrong](http://www.technologyreview.com/view/420234/eight-signs-a-claimed-p-is-not-np-proof-is-wrong/)


(Personally, I hope that P = NP, because that would be a lot more fun) Always relevant in these cases:

[Eight signs a claimen P != NP proof is wrong](http://www.technologyreview.com/view/420234/eight-signs-a-claimed-p-is-not-np-proof-is-wrong/)


(Personally, I hope that P = NP, because that would be a lot more fun) Always relevant in these cases:

[Eight signs a claimen P != NP proof is wrong](http://www.technologyreview.com/view/420234/eight-signs-a-claimed-p-is-not-np-proof-is-wrong/)


(Personally, I hope that P = NP, because that would be a lot more fun)  Physics is more my thing (new to CS and a bit below the level of most of this discussion), but this is like looking into a mirror of the whole faster than light neutrinos thing... They at least openly said they are almost sure they are wrong, but wanted other members of the scientific community to check it out, in case they were on to something.

To be fair, I don't people should be too harsh on the physicists involved with the superluminar neutrinos.

edit: lol, just realized my username is relevant I was referring more to the response from people I know in the physics community, which was essentially "That's interesting. It'll be neat if they're right, but I'm not holding my breath".  These get posted to the arxiv every week. No one cares. Please stop. Crankery is boring. I disagree for three reasons:

1. It's fun to laugh.
1. It's a good skill to practice - finding errors in proofs.
1. Someone, someday, will *probably* find an answer to this problem. So paying attention isn't a bad idea.  Per Finagle's Law, P does equal NP but no constructive proof of this is possible -- that is, while NP-complete problems can be solved in polynomial time, nobody will ever figure out how. For any polynomial time problem there (constructively) exists an algorithm that solves it in polynomial time.

This is done by enumerating all polynomial time algorithms and running them all together by dovetailing. &amp;gt; This is done by enumerating all polynomial time algorithms

Wait, you can do this? How? You pick some encoding of turing machines to naturals, so each number represents one turing machine. Then simply run them all in parallel.

For one such encoding scheme, see Goedel numbering. Oh I see, you just enumerate *all* algorithms, which includes all the poly-time algorithms. I was trying to figure out how to restrict the enumerated algorithms to poly-time ones, but you are right, you don't have to.

You still need a way to step the machines carefully such that for all i, the i'th machine takes polynomial time to complete, but I can see how that can be done. That is correct, you run machine i on imput x for |x|^i steps, reject if it does not accept in less than that time.

The trick is that if your machine is polynomial with exponent greater than i, eventually the same machine along with some "junk" will crop up for j &amp;gt; i, and so on. My idea was to step up the machines exponentially, in a diagonal fashion: run the first machine 1 step, then run the second machine 1 step and the first machine 2 steps, then the third machine 1 step the second machine 2 steps and the first machine 4 steps. etc. etc.

That way, when machine i accepts in n steps, with 2^k &amp;lt; n &amp;lt;= 2^(k+1) then you know that you have stepped at most 4 * 2^i * 2^k steps, which is less than 4 * 2^i * n steps, which is polynomial in |x| if n is polynomial in |x|.

Your way is simpler and faster, by taking advantage of the non-uniqueness of turing machines. &amp;gt; This is done by enumerating all polynomial time algorithms

Wait, you can do this? How? &amp;gt; This is done by enumerating all polynomial time algorithms

Wait, you can do this? How? ~~Consider the case of a finite number A of polynomial-time algorithms. Then, enumerating all polynomial-time algorithms would take O(A\*O(n^(k))) time (where the polynomial-time algorithms run in O(n^(k)) time).~~

~~Due to the magic of big-O, since A is a constant, O(A\*O(n^(k))) == O(n^(k)), which means that running *all* polynomial-time algorithms is polynomial.~~

**Edit:** This is apparently wrong. (I've never been good at this CS-theory stuff.)

 Is the number of polynomial-time algorithms finite? Huh? Yeah, that made perfect sense at the time, but looking back, I did the equivalent of saying 2 + 2 == "A Suffusion of Yellow". &amp;gt;2 + 2 == "A Suffusion of Yellow"

Coming soon to a weakly typed language near you! ~~Consider the case of a finite number A of polynomial-time algorithms. Then, enumerating all polynomial-time algorithms would take O(A\*O(n^(k))) time (where the polynomial-time algorithms run in O(n^(k)) time).~~

~~Due to the magic of big-O, since A is a constant, O(A\*O(n^(k))) == O(n^(k)), which means that running *all* polynomial-time algorithms is polynomial.~~

**Edit:** This is apparently wrong. (I've never been good at this CS-theory stuff.)

 Consider the case that P = EXPTIME. By the time hierarchy theorem, this implies False, so any statement can be proven. QED. &amp;gt; This is done by enumerating all polynomial time algorithms

Wait, you can do this? How? For any polynomial time problem there (constructively) exists an algorithm that solves it in polynomial time.

This is done by enumerating all polynomial time algorithms and running them all together by dovetailing.  </snippet></document><document><title>Minimum Linear Arrangement: Variants and relaxed versions?</title><url>http://www.reddit.com/r/compsci/comments/webzg/minimum_linear_arrangement_variants_and_relaxed/</url><snippet>Hi guys,
I've been looking at the Minimum Linear Arrangement problem. MinLA tries to place all nodes of a graph in a sequential order such that the distance between all connected nodes is minimized. (Source: http://tracer.lcc.uma.es/problems/minla/minla.htm )

My problem is related to MinLA, but I want to layout random graphs for optimal cache performance. For me, this means that adjacent nodes are placed in either the same cache line (or potentially an adjacent cache line). You can think of this as assigning nodes to buckets, such that the cost of two connected nodes is 0 if they are in the same bucket, and 1 otherwise.

Does anyone know of any variants of MinLA that try to solve this problem?

Thanks!

Edit: Oh, I don't care if the buckets are the same size.   I think the problem is actually the "k-Way Partition Problem" which I 
[dug up here](http://homepages.rpi.edu/~mitchj/phdtheses/xiaoyun/rpithes.pdf). </snippet></document><document><title>In proofs of correctness for algorithms, what will the loop invariants be in general?</title><url>http://www.reddit.com/r/compsci/comments/we1v6/in_proofs_of_correctness_for_algorithms_what_will/</url><snippet>I m beginning CLRS Intro to Algorithms, and this general question popped up. It seems to be that loop invariants are always data structures built up inductively.

EDIT: I thought this was going to be ignored. But I have your attention now I d like to discuss this:

I m also having trouble getting the Insertion Sort Algorithm from that book to work appropriately.

    a= [7, 2, 5, 1, 8, 10]
    print a

    for j in range(1, len(a)):    #j goes up list from 0 to 5
    key = a[j]                        #key is what is being sorted at the moment
    i = j-1
        while i &amp;gt; 0 and a[i] &amp;gt; key:  # index i goes down the list
            a[i+1] = a[i]
            i -= 1
            a[i+1] = key

    print a

What I get as output from this python code:
[7,2,5,1,8,10]
[7,1,2,5,8,10]

The first element is always unchanged. i guess that s an unwanted loop invariant!
   A loop invariant is any condition which remains constant at each evaluation of the loop. Loop invariants are boolean values, and are not necessarily tied to any particular data structure.  is a loop invariant any different from the looping condition? It seems like you are saying the loop invariant is the looping condition, which seems weird to me. Why use the term loop invariant over looping condition?
   is a loop invariant any different from the looping condition? It seems like you are saying the loop invariant is the looping condition, which seems weird to me. Why use the term loop invariant over looping condition?
   If you mean what I think you mean, then no, they aren't different. The term invariant is actually better because it emphasizes that the condition does not change on any iteration of the loop. For example, consider the following:

    while (x &amp;lt; 100)
        x++;

The condition x &amp;lt; 100 will remain true for every iteration of the loop. Once x &amp;gt;= 100, the loop will terminate (note that this is a very informal treatment of loop invariants).

Edit: look at csgordon's explanation instead. It's better than mine. is a loop invariant any different from the looping condition? It seems like you are saying the loop invariant is the looping condition, which seems weird to me. Why use the term loop invariant over looping condition?
   is a loop invariant any different from the looping condition? It seems like you are saying the loop invariant is the looping condition, which seems weird to me. Why use the term loop invariant over looping condition?
   is a loop invariant any different from the looping condition? It seems like you are saying the loop invariant is the looping condition, which seems weird to me. Why use the term loop invariant over looping condition?
     [deleted] Not sure if troll or idiot.  [deleted] [deleted] I definitely would not want to know how you think about solutions to problems using algorithms, in fear of a heart attack. [deleted] That's a lot like saying that you think about how to solve 2 + 2 with a pencil. [deleted]</snippet></document><document><title>VBR video compression (e.g. H.264) sounds great for CCTV but is dismal in practice. Just poor implementation?</title><url>http://www.reddit.com/r/compsci/comments/wdmgi/vbr_video_compression_eg_h264_sounds_great_for/</url><snippet>H.264 is now standard in video surveillance (aka CCTV) hardware. In both IP cameras and in DVRs that compress analog video. Since most cameras are stationary, a majority of their scenes are stationary when there is low human/vehicular traffic and when trees and shrubs aren't being blown about.

So is it poorly due to poor implementation that even when set to VBR recording, that bitrate during stationary scenes drops to only about half (or not even half) of the max VBR bitrate?

How low would you expect a properly implemented H.264 encoder to go on stationary scenes, while maintaining good picture quality?

I assume keyframe interval can change according to motion complexity. Why not stretch out keyframe interval to much longer, (say once every 25/30 frames) if we aren't concerned about waiting a little longer for playback-seeking?

Are many consecutive delta frames allowed to all reference the last keyframe? If this is so, then there should be no snowballing artifacts during long stretches of delta frames, right?

Or are delta frames only allowed to reference the previous frame?  I suspect it is the fact that the maximum VBR is exactly that. You've told the codec that it can use anything up to that value. It's job is to provide as accurate a possible video stream given the constraints you gave it. Since it is allowed to use up to a maximum, it will do. In 'static' scenes, the codec is trying to max out the VBR rate encoding the noise from the webcam since you've said that as long as it is using less than *x* bits per second, you want the best possible picture.

In your example, you want the VBR cap to not be a *cap*, but to the the modal bit rate. So, to do that, set the VBR rate to whatever you think the 'static' scene's bitrate should be and bump the VBR window to a minute or so to let the codec handle the rare high-bitrate events.  It is poor implementation, driven by cost, that causes the high bitrate in VBR mode on unchanging scenes. Typically, instead of using an encoder that's customised to do a good job for the CCTV setup, they use a standard off-the-shelf encoder that's designed to give reasonable real-time encoding performance on a variety of inputs (similar to the one you'd use in a mobile phone, for example).

In theory, given no change at all, a H.264 encoder feeding files could use a lot of bits to encode a 1920x1080 reference frame, then a few kilobits per second to indicate no change; in practice, this is impossible as there will be changes in the scene.

You do want to read up on H.264 to answer the rest of your questions - [Wikipedia](http://en.wikipedia.org/wiki/H.264/AVC#Features) has a reasonable summary of H.264's features, and the 06/2011 revision of the spec is freely available [from the ITU](http://www.itu.int/rec/T-REC-H.264-201106-S).

Briefly, though, you can freely choose the interval between decoder restart points - or use gradual decoder restart - and professional H.264 encoders tend to have you choose the maximum interval, reserving the right to restart earlier if they find a bitrate advantage to doing so (e.g. on scene cut). You thus say "I want a decoder to need a maximum of 0.5 seconds of data before it starts displaying picture", rather than "I want a key frame every 0.5 seconds"; the time can be tweaked appropriately to the application. This could be set very high in a CCTV application - say 60 seconds maximum time between decode start points.

H.264 permits up to 16 reference frames (32 reference fields if interlaced); P frames can refer to any past reference frame, B frames can refer to both past and future reference frames. H.264 also permits P and B frames to include I macroblocks, so you can fix up artifacting when it happens. It also permits you to apply corrections to a chosen reference - again reducing bitrate needs, by letting you encode a near-identical block as "take this reference block, and add this correction to it", instead of as a fresh block.

Note that thinking in terms of key and delta frames isn't helpful - you have I, P and B macroblocks, and you assemble those into frames in H.264. The different frame types simply restrict which macroblock types you can use. Do you think that improving it could be done with firmware update, or would they need to design and fab new chips? I suspect that improving it would need new chips; there are two pain points to deal with.

First is having enough RAM to buffer a significant number of raw frames - if you're going to use a very long GOP (60 seconds or so), you need to store both the encoded forms of the frames and their raw forms. Assuming you stick to 4:2:0 (which would be the sensible thing to do), you need 1.5 bytes per pixel for a raw frame. If you need to store 3600 frames at 720x480 pixels per frame (60 seconds at 60fps ED resolution), you need nearly 2GiB RAM just for the raw frames, all accessible to the encoder.

The second is having enough signal processing power to actually see if there's a lower bit rate encoding of the incoming picture, or if the encoding you've chosen is as good as it gets. Recall that H.264 (like most good video standards) doesn't specify the encoder at all; it simply specifies precisely what a decoder will output given an input bitstream. The job of the encoder is to analyse the input, and come up with a compressed bitstream that will make a decoder output something that looks similar to the input. This is a complexity problem - I can do less work analysing the input, and use more bits to get the decoder to give me a certain quality level, or I can do more work, and get closer to a minimal bitrate for that quality. Live applications have a second disadvantage - they are time-constrained on doing that work.

The [x264 encoder](http://www.videolan.org/developers/x264.html) is a good example to play with; if you configure it to give you output quickly, you get a lower quality at a given bit rate. If you ramp the settings up to maximum, you get much slower encodes, but a higher quality at a given bit rate.</snippet></document><document><title>Is the CS degree worth it?</title><url>http://www.reddit.com/r/compsci/comments/wbvkb/is_the_cs_degree_worth_it/</url><snippet>http://www.technewsworld.com/story/73921.html

Article related, but my question is a bit more of the "in between" case. I'm currently a Computer Science major and I do love the field, but early in my college career I didn't know what I wanted to do, so I am now going to be a Junior in my CS degree in my 5th year of college.

The loans are stacking up, and I have held a development job at a local networking lab for the last two years. I've gotten quite a bit of experience dealing with bug tracking tools like redmine, SVN version control and SQL database handling to name a few things.

I've gotten far enough in my degree that I "get it" enough that I believe I could pick up other concepts as I go, and the remaining classes I have are stuff like algorithms, compilers and assembly. I spend a lot of time on Stack Overflow and have essentially taught myself PHP and TCL on the fly.

I guess my question to you fellow compsi redditors is what is your opinion on my situation? I'm not going to base such a big life question solely on the opinions found in here, but hopefully some of you have experienced a similar thing in the past or have some kind of industry experience.

Do I keep taking the loan hits (The way I've figured it I'll have $75,000) in loans by the time I graduate, at least) or do you think I could get away with landing a job without the degree. The issue about finding reading online is that there are so many articles saying that both ways are the best.

Edit: Thanks for all the posts, it's all very informative. More than I could have hoped for.  It's getting much harder to land a job without the degree; it's used as a weed-out in most companies. What you will learn in algorithms and compilers is important information, but then again I'm biased toward education. Incidentally, very few other degrees are worth it at all, but CS will pay for itself. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. So you get employed. Great programmers/administrators are hard to find. Greatness can overcome lack of education. Now if you are just mediocre then yea you better stay in school for that degree. Yea, but that degree gets you in the door. I worked in IT for about 5 years before finishing my CS degree. Once I had finished it I doubled my salary within a year. Had anything changed about my body of knowledge? No, not really. All that changed was that dumb piece of paper. All of a sudden I would get callbacks from jobs that previously I probably wouldn't have. Sure, once I get in the door and sit down for an interview the degree is less important, but getting in that door is huge. It is not hard to get in the door when companies are begging for tech talent. Doubly so when some of the most valued experience companies look for is your own personal projects. Asking for you GitHub profile has started to become standard procedure for companies hiring in this field.

A couple of years ago I created a fun software project on the side, published it, and then had what felt like an email a day for several months from people wanting to hire me. I wasn't even looking for work. Great programmers/administrators are hard to find. Greatness can overcome lack of education. Now if you are just mediocre then yea you better stay in school for that degree. Great programmers/administrators are hard to find. Greatness can overcome lack of education. Now if you are just mediocre then yea you better stay in school for that degree. Great programmers/administrators are hard to find. Greatness can overcome lack of education. Now if you are just mediocre then yea you better stay in school for that degree. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt;And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

Because the stress of exams, dissertations and deadlines will actually make you learn things. You think you are going to actually learn computer science by just watching lectures? You need to *do* things. You need to study. You need to complete actual work. If I was interviewing someone who went through online courses with no formal supervision and he tried to tell me his education was now as good as someone with an actual degree I'd laugh him out of the room. Online courses are great, but they are no substitute for the real thing.  I was referring to algorithms and compiler design. If I wanted to learn algorithms and compiler design I wouldn't want to stress myself with years worth of other stuff in some institution I may not even be interested in just to learn those two subjects. All I wanted to learn was algorithms and compiler design after all so that I could become a better programmer that I could learn much faster online than spending years in some institution learning them, that's my point.

And yeah, if you're studying CS and you're not putting the theory into practice you're doing something terribly wrong. It's like learning C++ from a book without ever writing any code, you can't do it, it's useless and a waste of time. While some people may be able to push themselves to learn independently of a formal class with actual quizes, projects, and a final, many cannot.  I know I can't.  When I took my compiler course the class when from 30 to 6 over the course of the semester because writing a compiler even for a toy language is really hard.  If I didn't have the pressure of the having to get the compiler done I wouldn't have spent huge amounts of my spring break working on it. Then again, it depends what you want to actually do. There aren't that many people that will write compilers.

I'm currently studying CS and before I did so I actually sat down and learned stuff. I actually think I leaned more that way. Because I'm in the mood and especially, because I can better do stuff while learning, which greatly improves my understand. And because I get at my own speed.

Also a lot of classes are basically the same thing you learn in some kind of book (hey, many university professors write books that contain their class).

Of course you actually need to sit down, need a certain degree of self-discipline, but I doubt that if you don't have that you'll pass a serious university. Well, even though that may also depend on the country and university you are living it and how the professors and other people do their job.

I for myself don't think university motivates me, the opposite is kinda true, because I'm forced. It's a bit like pupils often hate reading books while people love to do so on their own free will, because they are interested, curious and just want to do something.

For me school often fails at really putting things together. When I learn something on my own, I often want to solve some kind (or a set of) problem(s) and can really follow the process of doing so, because I'm doing it on my own or via some kind of reference. In school and university you sometimes have to go to class and then think about how these things even fit together.

And when you are excited about something and start to learn and know how things work, you pretty much automatically try to make it better, learn better techniques and also raise quality, learn more abstract concepts. People often are perfectionists, so they kinda have to do so.

But hey everyone is different. I just really dislike how I have to make a degree to get more acceptance and how school and university force me to see something in the same way as some professor. People just have different minds and see things in different ways. That's why they all have different ideas, can be creative and why we can't really have machines doing all the stuff (right now).

On the other hand university also has really nice stuff to offer, like infrastructure, lots of people with the same interests, etc. I think a good university offers multiple ways of learning things. The only thing I consider bad at my university is that I have to repeat the stuff I already got over and over again in practical exercises while the things I want to understand better and actually experiment with are sometimes not done enough for me. When I learn on my own I can go exactly as deep as I want into something and spend the exact amount of time I need.

Also I agree you don't learn just from reading books, but need to practice things. And in fact that's what I think open source projects can offer in a way better way. You solve real problems and usually get good feedback from someone who has been into this for a while.

Last but not least, even people with no degree are usually able to outdo someone with degree in some parts of his fields, when he puts some effort into this. That's one of the reasons of people working together, even if they studied the absolutely same thing. I think we wouldn't be as far as we are if we didn't have a great number of people without degrees that did amazing things.. probably because they saw it in a different way.

I guess it needs both kinds of people. It's just bad if companies just see a degree, but not what someone actually can. Also, because there are huge differences between those with degrees. All excellent points.

While it is true that not many people end up writing compilers.  Knowing how they work and going though the process of building one was very enlightening, .It also provided experience working a hard project that still allowed you to objectively evaluate how you did, where if you build something new you can't compare how you did to how others have done easily.  Even though I haven't written a compiler I have worked on a project where knowing a lot about them helped me a bunch.

Universities also provide the ability to work on group projects and develop interpersonal skills.  In the work place often your interpersonal skills are as in important as you technical skills.  As for the professor making you see things his way, I feel that is a mark of a poor teacher   I was referring to algorithms and compiler design. If I wanted to learn algorithms and compiler design I wouldn't want to stress myself with years worth of other stuff in some institution I may not even be interested in just to learn those two subjects. All I wanted to learn was algorithms and compiler design after all so that I could become a better programmer that I could learn much faster online than spending years in some institution learning them, that's my point.

And yeah, if you're studying CS and you're not putting the theory into practice you're doing something terribly wrong. It's like learning C++ from a book without ever writing any code, you can't do it, it's useless and a waste of time. I was referring to algorithms and compiler design. If I wanted to learn algorithms and compiler design I wouldn't want to stress myself with years worth of other stuff in some institution I may not even be interested in just to learn those two subjects. All I wanted to learn was algorithms and compiler design after all so that I could become a better programmer that I could learn much faster online than spending years in some institution learning them, that's my point.

And yeah, if you're studying CS and you're not putting the theory into practice you're doing something terribly wrong. It's like learning C++ from a book without ever writing any code, you can't do it, it's useless and a waste of time. And my point is that you're not learning them nearly as well as someone who actually took the classes with real consequences has learned them. And my point is that you're not learning them nearly as well as someone who actually took the classes with real consequences has learned them. Professional consequences &amp;gt; educational consequences. They are completely different things and somewhat unrelated. I have seen plenty of scrub programs make it by while being unremarkable. Education gives you direct correlation to performance and that metric is important. It allows you to have specific goals and to compete in a defined way.

So basically I completely disagree with you. They are completely different things and somewhat unrelated. I have seen plenty of scrub programs make it by while being unremarkable. Education gives you direct correlation to performance and that metric is important. It allows you to have specific goals and to compete in a defined way.

So basically I completely disagree with you. Specific goals and defined competition eh? Welcome to jousting. Sport for others' enjoyment. I'm sure you'll find people who will be more than happy to offer you that stay in the Matrix. So as to get context do you or do you not have a degree? I have a bachelors and CS and I am working on a masters (which my company is paying for). 

I have found it to be a stimulating experience. Do you have a basis for this comment or is your "argument" just baseless. &amp;gt;And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

Because the stress of exams, dissertations and deadlines will actually make you learn things. You think you are going to actually learn computer science by just watching lectures? You need to *do* things. You need to study. You need to complete actual work. If I was interviewing someone who went through online courses with no formal supervision and he tried to tell me his education was now as good as someone with an actual degree I'd laugh him out of the room. Online courses are great, but they are no substitute for the real thing.  &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

Because there are very, **very** few people that are actually capable of learning it, in it's entirety, including the boring parts, without glossing over things, without being forced to. If you think you are one of these people, you are almost certainly wrong. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. If you did in fact do this course on your own, I want to interview you for a position on my team: sergeyso - at - microsoft.com. Send be a resume :-).

The problem is, vast majority of people have no discipline to do it without the structure of the formal education. So while the CAN, most peole don't actually DO. Just out of curiosity, who would you rather hire assuming the position is for a compiler team: Someone who took 1 course of compiler design at a uni as part of their CS degree or someone who never went to uni, has no CS degree and has been independently hacking compilers for 4 years, contributing to open source compilers, etc.? Do you value more the independent hacker with experience working with real life compilers or the guy with a formal education who has no experience working with real life compilers, but has proven themselves? &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. The content is almost irrelevant. The real price of education is the experience. The stress, the exams, the dissertations, the projects, and the other 99% of university life is an important experience and people want to see that in a potential employee.

In short, the only people that will agree with you are those who don't have degrees, or have crappy ones. The content is almost irrelevant. The real price of education is the experience. The stress, the exams, the dissertations, the projects, and the other 99% of university life is an important experience and people want to see that in a potential employee.

In short, the only people that will agree with you are those who don't have degrees, or have crappy ones. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. Sure wish you could save/bookmark comments. permalink? &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. A degree is proof of your claimed abilities. There always have been several ways to provide proof (certificates, work experience, etc.) or to avoid such a necessity (connections,monetary power,etc). [deleted] &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. That's great and all, but where's your degree at the end? Gonna put on your resume "well I did all these courses online... honest!"? You have no proof that you know anything, and that is what you pay for. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. &amp;gt; What you will learn in algorithms and compilers is important information

And why would you spend years in school learning this when you can learn it online today for free without the stress of exams, dissertations and deadlines?

For example, here's all the CS courses [NPTEL](http://www.youtube.com/user/nptelhrd/videos?view=1) has online for free. What's missing? What do I need to go to school for to learn? Is this not adequate enough for CS education? And by the way, I'm not even counting all the courses that MIT, Stanford, etc. have online. And all the other thousands of resources and communities and books at your fingertips full of knowledge.

As far as I can tell, you don't need to go to school to get an adequate CS education, you can do it online today for free.

edit: If anyone's wondering what they should start with, [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) is pretty much required for all the courses, so that one's good to begin your journey with.

1 lecture = 1 hour.

* [High Performance Computer Architecture](http://www.youtube.com/playlist?list=PLD8AF625E53B0691F&amp;amp;feature=plcp) - 41 lectures
* [Computer Algorithms - 2 ](http://www.youtube.com/playlist?list=PLBF0CE59A2ECA81E9&amp;amp;feature=plcp) - 41 lectures
* [Numerical Optimization](http://www.youtube.com/playlist?list=PL6EA0722B99332589&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL30FDE07FCE70BA40&amp;amp;feature=plcp) - 33 lectures
* [Natural Language Processing ](http://www.youtube.com/playlist?list=PLD392E2ACAEF0C689&amp;amp;feature=plcp) - 40 lectures
* [Computational Geometry ](http://www.youtube.com/playlist?list=PLE1010BEDB031C039&amp;amp;feature=plcp) - 40 lectures
* [Low Power VLSI Circuits &amp;amp; Systems ](http://www.youtube.com/playlist?list=PLB3F0FC99B5D89571&amp;amp;feature=plcp) - 40 lectures
* [Graph Theory ](http://www.youtube.com/playlist?list=PL612CE2AB6F38DF9A&amp;amp;feature=plcp) - 40 lectures
* [Cryptography and Network Security ](http://www.youtube.com/playlist?list=PL71FE85723FD414D7&amp;amp;feature=plcp) - 41 lectures
* [Theory of Computation ](http://www.youtube.com/playlist?list=PL85CF9F4A047C7BF7&amp;amp;feature=plcp) - 42 lectures
* [Real-Time Systems ](http://www.youtube.com/playlist?list=PLD8F7759A8F75D841&amp;amp;feature=plcp) - 40 lectures
* [High Performance Computing ](http://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C&amp;amp;feature=plcp) - 41 lectures
* [Compiler Design ](http://www.youtube.com/playlist?list=PL3690D679B876DE6A&amp;amp;feature=plcp) - 40 lectures
* [Programming and Data Structure ](http://www.youtube.com/playlist?list=PLD9781AC5EBC9FA16&amp;amp;feature=plcp) - 32 lectures
* [Electronic Design and Automation ](http://www.youtube.com/playlist?list=PLBBCE226922E31394&amp;amp;feature=plcp) - 35 lectures
* [Data Communication ](http://www.youtube.com/playlist?list=PL8BF3052396E05930&amp;amp;feature=plcp) - 41 lectures
* [Artificial Intelligence ](http://www.youtube.com/playlist?list=PLE051420C2068DCB2&amp;amp;feature=plcp) - 40 lectures
* [Software Engineering ](http://www.youtube.com/playlist?list=PL8751DA481F0F0D17&amp;amp;feature=plcp) - 39 lectures
* [Computer Architecture ](http://www.youtube.com/playlist?list=PL59E5B57A04EAE09C&amp;amp;feature=plcp) - 38 lectures
* [Data Structures and Algorithms ](http://www.youtube.com/playlist?list=PLBF3763AF2E1C572F&amp;amp;feature=plcp) - 36 lectures
* [Principles of Programming Languages ](http://www.youtube.com/playlist?list=PLF7C73918190889CE&amp;amp;feature=plcp) - 40 lectures
* [Database Management System ](http://www.youtube.com/playlist?list=PL9426FE14B809CC41&amp;amp;feature=plcp) - 43 lectures
* [Design &amp;amp; Analysis of Algorithms ](http://www.youtube.com/playlist?list=PL7DC83C6B3312DF1E&amp;amp;feature=plcp) - 34 lectures
* [Internet Technologies ](http://www.youtube.com/playlist?list=PL04D5787E247DC324&amp;amp;feature=plcp) - 40 lectures
* [Computer Organization ](http://www.youtube.com/playlist?list=PL1A5A6AE8AFC187B7&amp;amp;feature=plcp) - 33 lectures
* [Artificial Intelligence - Prof. P. Dasgupta](http://www.youtube.com/playlist?list=PL6EE0CD02910E57B8&amp;amp;feature=plcp) - 28 lectures
* [Computer Networks ](http://www.youtube.com/playlist?list=PL32DBC269EF768F74&amp;amp;feature=plcp) - 40 lectures
* [Systems Analysis and Design ](http://www.youtube.com/playlist?list=PL4F47209691234D1D&amp;amp;feature=plcp) - 40 lectures
* [Introduction To Problem Solving &amp;amp; Programming](http://www.youtube.com/playlist?list=PL94CA590D7781A9B9&amp;amp;feature=plcp) - 24 lectures
* [Introduction to Computer Graphics ](http://www.youtube.com/playlist?list=PL112A527F83F7A5E4&amp;amp;feature=plcp) - 35 lectures
* [Computer Graphics ](http://www.youtube.com/playlist?list=PL338D19C40D6D1732&amp;amp;feature=plcp) - 43 lectures
* [Discrete Mathematical Structures ](http://www.youtube.com/playlist?list=PL0862D1A947252D20&amp;amp;feature=plcp) - 40 lectures

edit: Thank you folks, I get it, you won't get a degree unless you go to a uni, it doesn't need repetition. That was not however my argument as it was meant to be solely about knowledge. And the point was meant to be: You don't need a CS degree to learn compiler design or algorithms, you have access to all the resources to learn it at home just as adequately. Including all the other CS subjects as well. If one were for example interested in compiler hacking, you don't need to spend 4 years of your life at a uni where you get 1 course worth of compiler design, it is better to stay home and start learning right now. Take a course from NPTEL, MIT, etc., buy the dragon book, join compiler hacking communities and start hacking. It is extremely arrogant to claim you need a CS degree for that as many claimed. Some even argued you can't learn a subject as well at home as in uni, like 4 years of independent compiler hacking is somehow less than 1 course of compiler design at uni? What a ridiculous thing to say. It's getting much harder to land a job without the degree; it's used as a weed-out in most companies. What you will learn in algorithms and compilers is important information, but then again I'm biased toward education. Incidentally, very few other degrees are worth it at all, but CS will pay for itself. "cstheoryphd" ;) What is your topic? (I swear I won't ask how your thesis is coming) So few people go into theoretical CS these days, it's sad. Oh no worries, it's finished! I'm a Dr. as of May. My topic was on bounding the size of multiply transitive permutation sets (not necessarily groups). &amp;gt; I'm a Dr. as of May

Congrats!!! That's quite an achievement. Thanks. So is pug rescuing :) It's getting much harder to land a job without the degree; it's used as a weed-out in most companies. What you will learn in algorithms and compilers is important information, but then again I'm biased toward education. Incidentally, very few other degrees are worth it at all, but CS will pay for itself. It's getting much harder to land a job without the degree; it's used as a weed-out in most companies. What you will learn in algorithms and compilers is important information, but then again I'm biased toward education. Incidentally, very few other degrees are worth it at all, but CS will pay for itself. &amp;gt; It's getting much harder to land a job without the degree

Unless you already have experience.  Most startups don't care if you have a degree if you've already been in the industry for a few years.

As OP says:

&amp;gt; I have held a development job at a local networking lab for the last two years

However, larger, established companies (google, oracle, etc.) probably won't even consider you if you don't have a degree. It's getting much harder to land a job without the degree; it's used as a weed-out in most companies. What you will learn in algorithms and compilers is important information, but then again I'm biased toward education. Incidentally, very few other degrees are worth it at all, but CS will pay for itself. When did it start to become more difficult? If anything, it has become less difficult. Companies like Google, that staunchly hired only top students from top universities, now hire just about anyone who can demonstrate the right talent. Many tech companies in particular are quite vocal about their hiring practices, which does not include a look in your education history. What you can do has become far more important than how you figured out how to do it, especially in the software field that is, presently, an employees market.

With that said, from a personal growth perspective, you can't go wrong getting a CS degree. It really shouldn't be a question of what jobs will come as a result, you should be doing it because you feel your life won't be complete if you do not do it. If you get a job out of it at the end, that is just gravy.

As for the OP's case, why not test the market? If you can land your dream job today, then take it. You can always return to your studies later. You can't take advantage of a hot tech market later, after it has cooled. If nothing appealing turns up, continue with what you're doing. Are the people down-voting this working in a different industry?  I've definitely noticed, at least anecdotally, that google is way more open about what backgrounds they're hiring from.  I think getting a CS degree is a fantastic idea, and if you're started you should finish, but it is certainly not necessary for having a lucrative, long and fun career in software.

I'm going to go out on a limb and guess that anyone downvoting this comment is still in undergrad, and doesn't have a lot of industry experience.   I assume it is those who have invested 16+ years of their life for a perceived workplace advantage who are now resistant to seeing it crumble.

It would be like working hard and saving dollars your entire life and then start to watch as bartering becomes the new way to trade. You are going to be rather upset at any mention of the change, and deny that it is happening.

What I don't get is why a degree cannot hold up on its own. As you say, it is a fantastic opportunity. Why does it even need to come with a better job at the end? Isn't being a better educated person reason enough? When did it start to become more difficult? If anything, it has become less difficult. Companies like Google, that staunchly hired only top students from top universities, now hire just about anyone who can demonstrate the right talent. Many tech companies in particular are quite vocal about their hiring practices, which does not include a look in your education history. What you can do has become far more important than how you figured out how to do it, especially in the software field that is, presently, an employees market.

With that said, from a personal growth perspective, you can't go wrong getting a CS degree. It really shouldn't be a question of what jobs will come as a result, you should be doing it because you feel your life won't be complete if you do not do it. If you get a job out of it at the end, that is just gravy.

As for the OP's case, why not test the market? If you can land your dream job today, then take it. You can always return to your studies later. You can't take advantage of a hot tech market later, after it has cooled. If nothing appealing turns up, continue with what you're doing. &amp;gt;  Companies like Google, that staunchly hired only top students from top universities, now hire just about anyone who can demonstrate the right talent.

Source please.  &amp;gt;  Companies like Google, that staunchly hired only top students from top universities, now hire just about anyone who can demonstrate the right talent.

Source please.  I think Google is still pretty stingy when it comes to hiring degree holders. My boss is a networking god and he said he would never work at Google because the hours are insane. I think you'd definitely need to be some kind of crazy to work there I'm interning at MTV right now, and in my observations nobody keeps "insane" hours. Most people who work here absolutely love it. I'd be interested to know what your boss experienced/was told that gave him this opinion. You're an intern? Get off of reddit, and stop eating all the food in the microkitchen!  I'm interning at MTV right now, and in my observations nobody keeps "insane" hours. Most people who work here absolutely love it. I'd be interested to know what your boss experienced/was told that gave him this opinion. I'm not sure how hours at MTV is relevant in a discussion about hours at Google...? Unless I missed something? MTV being mountainview google office. I always forget that it's not obvious outside of google what that means &amp;gt;_&amp;lt;  MTV being mountainview google office. I always forget that it's not obvious outside of google what that means &amp;gt;_&amp;lt;  I'm not sure how hours at MTV is relevant in a discussion about hours at Google...? Unless I missed something? I think Google is still pretty stingy when it comes to hiring degree holders. My boss is a networking god and he said he would never work at Google because the hours are insane. I think you'd definitely need to be some kind of crazy to work there When did it start to become more difficult? If anything, it has become less difficult. Companies like Google, that staunchly hired only top students from top universities, now hire just about anyone who can demonstrate the right talent. Many tech companies in particular are quite vocal about their hiring practices, which does not include a look in your education history. What you can do has become far more important than how you figured out how to do it, especially in the software field that is, presently, an employees market.

With that said, from a personal growth perspective, you can't go wrong getting a CS degree. It really shouldn't be a question of what jobs will come as a result, you should be doing it because you feel your life won't be complete if you do not do it. If you get a job out of it at the end, that is just gravy.

As for the OP's case, why not test the market? If you can land your dream job today, then take it. You can always return to your studies later. You can't take advantage of a hot tech market later, after it has cooled. If nothing appealing turns up, continue with what you're doing. When did it start to become more difficult? If anything, it has become less difficult. Companies like Google, that staunchly hired only top students from top universities, now hire just about anyone who can demonstrate the right talent. Many tech companies in particular are quite vocal about their hiring practices, which does not include a look in your education history. What you can do has become far more important than how you figured out how to do it, especially in the software field that is, presently, an employees market.

With that said, from a personal growth perspective, you can't go wrong getting a CS degree. It really shouldn't be a question of what jobs will come as a result, you should be doing it because you feel your life won't be complete if you do not do it. If you get a job out of it at the end, that is just gravy.

As for the OP's case, why not test the market? If you can land your dream job today, then take it. You can always return to your studies later. You can't take advantage of a hot tech market later, after it has cooled. If nothing appealing turns up, continue with what you're doing. FTA: http://economy.ocregister.com/2012/03/31/who-is-google-hiring/106711/

"A small portion have high school diplomas or tech certification". The other 96% have some sort of degree. As of March 2012. So yes, you can squeak in on merit, but it sure doesn't *hurt* to have a degree. FTA: http://economy.ocregister.com/2012/03/31/who-is-google-hiring/106711/

"A small portion have high school diplomas or tech certification". The other 96% have some sort of degree. As of March 2012. So yes, you can squeak in on merit, but it sure doesn't *hurt* to have a degree. Google doesn't just hire engineers so this statistic is a bit meaningless since that 4% could be doing practically anything. I remember going to a Google interview for a janitorial position and they asked me to explain how I would mop the floor and I showed them the algorithm to produce a z-order curve.  Look at it this way.

If you have thirty resumes in front of you and you only have room for 15 phone interviews for 3 entry position openings, which would you weed out first?

You start with college grads and start with the ones with the best GPA or most experiences. As someone that had to choose people for technical position I can tell you that choosing good people is very difficult. A college degree is a pretty standardized signal that can be easily used for hiring. Look at it this way.

If you have thirty resumes in front of you and you only have room for 15 phone interviews for 3 entry position openings, which would you weed out first?

You start with college grads and start with the ones with the best GPA or most experiences. I guess I don't know much about how busy the resume reviewers can be, but I'd look for someone with college and work experience over only one of either.  Honestly, somebody who dropped out of college would probably be looked at worse than somebody who just never went.

When you say you dropped out, we would read that as "gave up." And that's not an attractive quality.

In larger software studios you may have hiring managers who would read more into it, but as a developer who has to help with hiring things like that tend to be shortcuts we take so we can get back to doing actual work. &amp;gt; When you say you dropped out, we would read that as "gave up." And that's not an attractive quality.

For someone with no professional experience, looking for an entry level position, yes.  However, if someone dropped out of college 10 years ago, and has been in the industry ever since, successfully completing large commitments, then it doesn't really matter if they're a dropout. Very true, but I was talking more in this specific case, which I believe the OP is still a student. Honestly, somebody who dropped out of college would probably be looked at worse than somebody who just never went.

When you say you dropped out, we would read that as "gave up." And that's not an attractive quality.

In larger software studios you may have hiring managers who would read more into it, but as a developer who has to help with hiring things like that tend to be shortcuts we take so we can get back to doing actual work. I guess I don't know much about how busy the resume reviewers can be, but I'd look for someone with college and work experience over only one of either.  resume reviewers are busy and if you had some work experience vs someone with a BS from a good school and equal-ish work experience, and 14 interviews are already scheduled and i have time for 1 more, i'm going to pick the latter every single time. i have no idea who either of you are and don't know anything about your background, so from what i can see off just your resume, finishing a degree from a good program is much more impressive to me than someone who couldn't finish his degree. 

you'll be able to find a job. you will be very hard pressed to find a hard-to-get job at a top company like a google or whatever. 

 Latter? I guess I don't know much about how busy the resume reviewers can be, but I'd look for someone with college and work experience over only one of either.  Most resume reviews start with HR, someone completely outside your technical skill set.  Try to put yourself in their shoes and determine how you could select a percentage of candidates from a pile of resumes.  

Hell some larger companies have even automated a lot of this to screen resumes for key words and such and weight resumes without taking a glance at them. 

edit: Even medium-large companies are known to do this.  Most medium and large companies do this. Sorry to pick at one little point, but people could get overly optimistic reading that. I guess I don't know much about how busy the resume reviewers can be, but I'd look for someone with college and work experience over only one of either.  I guess I don't know much about how busy the resume reviewers can be, but I'd look for someone with college and work experience over only one of either.   CS has a lot of useful theory you might not all learn on your own, so yes, it is a great plus. Self-taught programmers often miss some very important concepts because nobody told them they needed to learn it.  My thoughts on the matter are this: after you've gained enough experience your degree (or lack thereof) won't matter nearly as much as that experience itself. However, having the degree or not WILL determine to some extent where you land your first job, which in turn determines the quality and type of experience you gain. Some companies won't hire someone without a degree, even if they, as you said, "get it." So the question you have to ask yourself is where you envision yourself in 5 years. If it's at a place like Google, finish the degree. Beyond the career concerns, philosophically I think that you should continue your CS studies. The theory which you'll be exposed to will benefit you either directly (via direct application of said theory) or indirectly (by enhancing your general understanding of how things work, by improving your problem solving skills, by forcing you to think more rigorously, etc.). This is especially true if you really do "love" CS; you'll miss out on a lot if you leave before junior year. 
    Now, it's easy for me to philosophize, but obviously keep your loans in mind when making your decision. What will be the difference between your debt if you quit now vs. if you finish? And also remember that in all likelihood your loans will be paid off within the decade, whereas your career will likely span the next 40+ years.   Some businesses simply will not talk to you if you don't have a degree, so you may encounter some closed doors throughout your career, regardless of how successful you are.  This could be important at a time in your life when you have a wife and kids to feed, and you find yourself laid off for whatever reason.
  I do quite a bit of programming on the job, and everything I know I've picked up "on the fly," but I got in at an entry-level job that doesn't really require it, and I just proved I could do it and grew from there. Just landing a CS job without a CS degree will probably prove difficult. Knowing how to deal with the version control and bug tracking software isn't likely to be what they care about because if you can program well they'll assume they can train you on how to deal with their version control software. They'll want to know more about how you'd approach certain problems, maybe ask for skeleton code for some things, and if they're big on a certain language they'll probably ask a couple of very detailed questions about something particular to that language. 

College degrees tend to get flogged on Reddit a lot as being over-priced and useless. I understand the frustrations, but here's my take on it:

Is it worth going to college for four years just to get the degree? Probably not. But you're not pursuing the piece of paper, you're pursuing an education. It's not the same thing. The piece of paper says you met some minimum requirements or knew how to game the system.

You're likely surrounded by knowledgable professors (though certainly not all of them are) and motivated students who are as passionate about this stuff as you are. Find them.

Use the classes as a starting point. They give you the basics, and often go well beyond that. But also get involved in projects with other passionate students who want to go above-and-beyond just meeting requirements. Get advice outside of class from the teachers who know their stuff and really want to push students succeed. Some of the best things I've learned from teachers was completely disconnected but the subject they were teaching.

The classroom gives you a good starting point, lots of great information, a pool of like-minded peers to grow with and a source of quick and frequent feedback. The degree isn't going to make you take advantage of all that, you have to. Not saying everyone who can't find a job didn't, but from personal experience of those around me, the people who did had much higher success rates getting into someplace they're happy.

The degree might not be worth it, but the education is. I do quite a bit of programming on the job, and everything I know I've picked up "on the fly," but I got in at an entry-level job that doesn't really require it, and I just proved I could do it and grew from there. Just landing a CS job without a CS degree will probably prove difficult. Knowing how to deal with the version control and bug tracking software isn't likely to be what they care about because if you can program well they'll assume they can train you on how to deal with their version control software. They'll want to know more about how you'd approach certain problems, maybe ask for skeleton code for some things, and if they're big on a certain language they'll probably ask a couple of very detailed questions about something particular to that language. 

College degrees tend to get flogged on Reddit a lot as being over-priced and useless. I understand the frustrations, but here's my take on it:

Is it worth going to college for four years just to get the degree? Probably not. But you're not pursuing the piece of paper, you're pursuing an education. It's not the same thing. The piece of paper says you met some minimum requirements or knew how to game the system.

You're likely surrounded by knowledgable professors (though certainly not all of them are) and motivated students who are as passionate about this stuff as you are. Find them.

Use the classes as a starting point. They give you the basics, and often go well beyond that. But also get involved in projects with other passionate students who want to go above-and-beyond just meeting requirements. Get advice outside of class from the teachers who know their stuff and really want to push students succeed. Some of the best things I've learned from teachers was completely disconnected but the subject they were teaching.

The classroom gives you a good starting point, lots of great information, a pool of like-minded peers to grow with and a source of quick and frequent feedback. The degree isn't going to make you take advantage of all that, you have to. Not saying everyone who can't find a job didn't, but from personal experience of those around me, the people who did had much higher success rates getting into someplace they're happy.

The degree might not be worth it, but the education is. I do quite a bit of programming on the job, and everything I know I've picked up "on the fly," but I got in at an entry-level job that doesn't really require it, and I just proved I could do it and grew from there. Just landing a CS job without a CS degree will probably prove difficult. Knowing how to deal with the version control and bug tracking software isn't likely to be what they care about because if you can program well they'll assume they can train you on how to deal with their version control software. They'll want to know more about how you'd approach certain problems, maybe ask for skeleton code for some things, and if they're big on a certain language they'll probably ask a couple of very detailed questions about something particular to that language. 

College degrees tend to get flogged on Reddit a lot as being over-priced and useless. I understand the frustrations, but here's my take on it:

Is it worth going to college for four years just to get the degree? Probably not. But you're not pursuing the piece of paper, you're pursuing an education. It's not the same thing. The piece of paper says you met some minimum requirements or knew how to game the system.

You're likely surrounded by knowledgable professors (though certainly not all of them are) and motivated students who are as passionate about this stuff as you are. Find them.

Use the classes as a starting point. They give you the basics, and often go well beyond that. But also get involved in projects with other passionate students who want to go above-and-beyond just meeting requirements. Get advice outside of class from the teachers who know their stuff and really want to push students succeed. Some of the best things I've learned from teachers was completely disconnected but the subject they were teaching.

The classroom gives you a good starting point, lots of great information, a pool of like-minded peers to grow with and a source of quick and frequent feedback. The degree isn't going to make you take advantage of all that, you have to. Not saying everyone who can't find a job didn't, but from personal experience of those around me, the people who did had much higher success rates getting into someplace they're happy.

The degree might not be worth it, but the education is.  Absolutely.

Not for jobs, not for anything else but it is pure pleasure.  Compared to what?  To a BA in Math? To no degree at all?  

Get the degree.  $75000 sure is a lot in loans.  Maybe you should work on reducing that. 

Getting a degree shows you can finish something.  Many employers are becoming more picky.  I don't have one but I knew the boss so that is the only reason I got the job I have which requires a degree. (Well I have decades of experience too.)  The first question asked on the online form was:  Do you have an 4 year degree at an accredited college or university?     

Just finish it.  Go for it.  Do it.  My kids damn sure are going to, if they have to compete with you - they have a degree, you don't, then you will lose.

 Lol, way to let the kids decide for themselves?  It seems like all of those loans would be for nothing if you don't get a degree.    &amp;gt; I've gotten far enough in my degree that I "get it" enough that I believe I could pick up other concepts as I go, and the remaining classes I have are stuff like algorithms, compilers and assembly. I spend a lot of time on Stack Overflow and have essentially taught myself PHP and TCL on the fly.

If you think Computer Science is about getting a job programming then you certainly don't "get it."

&amp;gt; Do I keep taking the loan hits (The way I've figured it I'll have $75,000) in loans by the time I graduate, at least) or do you think I could get away with landing a job without the degree. The issue about finding reading online is that there are so many articles saying that both ways are the best.

That's a tough question, and only if you're honest with yourself will you know the answer. Do you value a Computer Science education? Because there are jobs that don't require CS degrees where you develop software, program, perform IT operations, etc. Also having a CS degree will not entitle you to a job once you finish. If your education stops after you finish and you feel primed for industry, then you should probably skip the education aspect altogether, and find a job that doesn't require a degree. &amp;gt; I've gotten far enough in my degree that I "get it" enough that I believe I could pick up other concepts as I go, and the remaining classes I have are stuff like algorithms, compilers and assembly. I spend a lot of time on Stack Overflow and have essentially taught myself PHP and TCL on the fly.

If you think Computer Science is about getting a job programming then you certainly don't "get it."

&amp;gt; Do I keep taking the loan hits (The way I've figured it I'll have $75,000) in loans by the time I graduate, at least) or do you think I could get away with landing a job without the degree. The issue about finding reading online is that there are so many articles saying that both ways are the best.

That's a tough question, and only if you're honest with yourself will you know the answer. Do you value a Computer Science education? Because there are jobs that don't require CS degrees where you develop software, program, perform IT operations, etc. Also having a CS degree will not entitle you to a job once you finish. If your education stops after you finish and you feel primed for industry, then you should probably skip the education aspect altogether, and find a job that doesn't require a degree. I believe you're talking about the "get it" referring to your personal opinion of what Computer Science means to you. I'm referring to the "get it" where I think I have learned enough fundamentals to be able to learn the more advanced stuff in or out of the classroom. Sorry for the confusion. His point is that the way you are describing CS and what you want from it doesn't seem to correspond with the accepted definition of what CS actually is. It's not a degree to learn to program. It's about the science of computation. It really is only tangentially related to computers. Programming languages just happen to be an effective way of applying some of the results of computer science. I understand that, I actually read a pretty interesting article on talks about some people wanting to change the name to something less suggestive. I understand that, I actually read a pretty interesting article on talks about some people wanting to change the name to something less suggestive. Computer Science is to computers no more than astronomy is to a telescope. C'mon, attribute the quote, and quote it correctly.

&amp;gt; Computer science is no more about computers than astronomy is about telescopes.

&amp;gt; -Edsger Dijkstra His point is that the way you are describing CS and what you want from it doesn't seem to correspond with the accepted definition of what CS actually is. It's not a degree to learn to program. It's about the science of computation. It really is only tangentially related to computers. Programming languages just happen to be an effective way of applying some of the results of computer science. I believe you're talking about the "get it" referring to your personal opinion of what Computer Science means to you. I'm referring to the "get it" where I think I have learned enough fundamentals to be able to learn the more advanced stuff in or out of the classroom. Sorry for the confusion. You need to be clear if you are interested in IT or CS. Those are very different and there isn't really a debate what CS "means" to anyone. It's well defined and it's not about programming. Programming is not IT. Programming is not IT.  A CS degree is worth more than the jobs you get out of it. It teaches you how to work with others and how to manage your time. It also helps you establish a network of students and professors who may help you get bigger and better jobs further down the road.

Maybe I'm just justifying my choice to go but I do think it was worth it.

Also I didn't find too many job ads that did not list a degree as a requirement for a CS job.    As a compsci student I would just like to say that the mismatched parenthesis in the last paragraph hurt my eyes.  Other than that I hope you find everyone's comments helpful!    Hell yeah.

1 year left to go, I got an internship at $18/hour.  (more money than I had ever made)

Right out of college, I got a job at $45,000 a year starting.

7 months later, I got a job at $70,000 a year starting.

I could never have done this without the degree. Hell yeah.

1 year left to go, I got an internship at $18/hour.  (more money than I had ever made)

Right out of college, I got a job at $45,000 a year starting.

7 months later, I got a job at $70,000 a year starting.

I could never have done this without the degree. From my experience, this seems has been a pretty normal experience for a lot my CS friends who have graduated. Assuming you are in a relatively low cost-of-living area, you should be able to save a good amount of money living off that much.     You won't be able to land a high paying job without it and probably won't be taken seriously in the work place. This runs contrary to all of my experience. I'm working on a masters but I know plenty of highly paid devs without a degree at all, and if you open that up to people with a degree but not in CS/EE/etc than it's probably about 50/50, if I list my friends that are the best paid and most talented devs I know, the non-cs folk outweigh the cs ones. Heck even when I used to work in a company doing mostly pure computer science/ee research there were people with no formal background in CS working as lead researchers on DARPA projects.

Just out of curiosity what field do you work in right now, and how long have you been working in it?  I've worked on both coasts,big/small, private/state organization and never seen an environment were this was the case. Currently working in a 1500 person company working on SaaS. Also worked for a small start up doing SaaS and government doing stuff similar to SaaS.  And really none of your respected/high salary programmers are without a CS degree? I can see for some gov't work since it's not uncommon for government contracts to specify requirements which may include degrees. But even then I knew someone really respected by gov community with only a ba in music.  You won't be able to land a high paying job without it and probably won't be taken seriously in the work place. What do you consider a high paying job? I make more than 95% of the population (which kind of blew me away when I first realized it) without a degree. 

Does one need to be in the 99th percentile to be considered highly paid? I could get there too if I wanted to work 50 hours per week, but I'm happy with 30. That gives me some time to play with hobby projects and post on Reddit. For a software engineering I would consider it a job that starts at maybe 70-80 and you can work up to 6 figs eventuallys. All my opinion though. You won't be able to land a high paying job without it and probably won't be taken seriously in the work place. I *highly* disagree. While that certainly is the case in some environments, it's definitely not a universal truth. It really depends if he's looking to work at a software firm, a development agency, freelance, etc. I *highly* disagree. While that certainly is the case in some environments, it's definitely not a universal truth. It really depends if he's looking to work at a software firm, a development agency, freelance, etc. Going off this, anyone looking for a job... Stack Overflow Careers constantly has great job postings, and a lot of them start at $60k+ with "degree or 4 years college experience preferred" 60k is pretty much a joke. Too low, or too high? Too low for an experienced developer. For a straight college hire outside of a major market its fine. Yeah, if you got that in NYC you'd be getting stiffed, but if you are trying to start a programming career in Columbus Ohio then I'd say it's pretty spot on.  Too low, or too high? Too low, or too high? Depends on the market. Not really, $60k for programming is still incredibly low for butt-nowhere Idaho. Major markets are offering $90k+ Not really, $60k for programming is still incredibly low for butt-nowhere Idaho. Major markets are offering $90k+ Going off this, anyone looking for a job... Stack Overflow Careers constantly has great job postings, and a lot of them start at $60k+ with "degree or 4 years college experience preferred" You won't be able to land a high paying job without it and probably won't be taken seriously in the work place.   I'd suggest sticking with it, and taking all the mathy classes you can. Algorithms is especially important, but courses like the theory of computation, discrete mathematics, and similar all come in useful.

You're probably right about being able to pick up most of the coding on your own. So avoid those courses. They're good for easy marks if you've got any hacking experience, but they're low value.
      One thing to consider is that you might to be able to get a visa to another country without a degree; if you're in the US that could be ok due to the amount of tech companies there, but it is something to keep in mind. I actually very much would like to get a working visa to work in the UK eventually (I live in the US but would really like trying to live in the UK for a bit). That's really good to know. I actually very much would like to get a working visa to work in the UK eventually (I live in the US but would really like trying to live in the UK for a bit). That's really good to know.         </snippet></document><document><title>I need a basic data visualization tool</title><url>http://www.reddit.com/r/compsci/comments/wc6p4/i_need_a_basic_data_visualization_tool/</url><snippet>Is there a free tool I can use to plot a data series in 3D and 4D with colored points (dependent on the point value).  I need to visualize several million data points. (i.e. x,y,z,t,color) and then play a movie of it (with the option to rotate/change perspective).   fyi, there are no vectors - these are unrelated data points.

I can put the data into any format needed...

Appreciate any help - sincerely, totally clueless.    Have a look at 3d.js or raphael.js.   </snippet></document><document><title>Machine Learning Doesn't Matter?</title><url>http://quantombone.blogspot.com/2012/07/machine-learning-doesnt-matter.html</url><snippet>  &amp;gt;  Making a discipline overly academic means creating a self-contained, overly-mathematical, self-citing, and jargon-filled discipline who doesn&#8217;t care about world-impact but only cares to propagates an small community&#8217;s citations counts.

Is anyone surprised that this is the end result of contemporary academia's incentive structure? &amp;gt;  Making a discipline overly academic means creating a self-contained, overly-mathematical, self-citing, and jargon-filled discipline who doesn&#8217;t care about world-impact but only cares to propagates an small community&#8217;s citations counts.

Is anyone surprised that this is the end result of contemporary academia's incentive structure? </snippet></document><document><title>Facial recognition software for wild animals</title><url>http://www.reddit.com/r/compsci/comments/wc6oj/facial_recognition_software_for_wild_animals/</url><snippet>Hey everyone.

I've been looking into the possibility of using facial recognition software in order to identify individual animals. I'm a biologist with a pretty good knowledge of computer science, but I have still been consulting with some various experts in the field, but I thought I would cast a wider net.

The primary problem that I can see is that there is no way to standardize the lighting in the wild. Partial occlusion of the face can be prevented by manual selection of the images, so that shouldn't be too much of a problem.

Someone recommended the commercial products provided by [Neurotechnology](http://www.neurotechnology.com/), but that doesn't seem to have the capability to function well in a non-controlled environment.

I've been looking into some previous studies that seem to be relevant.

[Identification of great apes using face recognition (PDF)](http://db.tt/UlAxG27V) found that sparse representation classification had fairly good success with apes in captivity.

However, I was also interested in what [Google](http://research.google.com/pubs/pub38115.html) has done with unsupervised learning in facial recognition. They used a deep autoencoder and successfully matched 74.8% of cat faces.

This stuff is definitely pushing my conceptual knowledge, so I was wondering, before I begin pursuing this any further, if this was at all possible for use in studying sea lions in situ. If it is, am I looking even vaguely in the right direction?  ~~Why use just faces? Animals have all different head shapes, anyway, so you may not even be able to identify the face of some animals as being a face.~~

~~You should look into object recognition rather than face recognition.~~

~~Edit: Are all these animals of the same species?~~

Nevermind, they're all of the same species Yeah, they're all California sea lions. I'm hoping to use it to prevent counting an individual sea lion multiple times over a period of time. I was looking most into faces because sea lions tend to cluster in groups, so it is considerably easier to get a straight on image of a their head than it is to get an image of their entire body. That being said, using the entire body may be possible, but isn't that the same underlying technology just with different training?

I haven't found any previous example of the use of facial recognition on sea lions, but they should have enough distinctive facial features to differentiate them, shouldn't they? Ok, since they're all of the same species, you may be able to do facial recognition. For some reason I was imagining a variety of species that you wanted to identify.

Existing software will probably be tailored for human faces (trying to pick out certain human features that wouldn't exist in your images), so you'd need to get software written or tweaked specifically for sea lions. How involved would it be to train it for sea lions? I've only dealt with pretty simple networks, so I don't have much of an idea of how large the training set would have to be or any of that sort of stuff for such a complex problem. Machine learning generally isn't used for facial recognition in that sense. It's used more broadly, for example to identify whether something is a sea lion, or some other animal. In that case you'd train it on lots of pictures of sea lions, and lots of pictures of whatever else.

For the purpose of identifying many similar objects, you would probably use some other sort of pattern recognition tailored specifically for your purpose, rather than training a machine learning based tool. So it'd basically have to built from the ground up to optimize it for the different facial structure? So it'd basically have to built from the ground up to optimize it for the different facial structure?</snippet></document><document><title>Can anyone recommend some good talks or articles or classes that show, from the transistors to the user interface, the different levels of programming language, and how everything is built on something else?</title><url>http://www.reddit.com/r/compsci/comments/w9z79/can_anyone_recommend_some_good_talks_or_articles/</url><snippet>  The elements of computer systems: http://www1.idc.ac.il/tecs/ Riding on your top post coattails...

[The Elements of Computing Systems](http://www.amazon.com/Elements-Computing-Systems-Building-Principles/dp/0262640686/) and [Code by Charles Petzold](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/) are exactly what you want.

*Code* goes through number systems, basic information theory, circuits (from gates on up), memory, machine code and programming languages, all with accessible diagrams and explanations.

*TECS* has you build an actual working computer from the ground up. Riding on your top post coattails...

[The Elements of Computing Systems](http://www.amazon.com/Elements-Computing-Systems-Building-Principles/dp/0262640686/) and [Code by Charles Petzold](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/) are exactly what you want.

*Code* goes through number systems, basic information theory, circuits (from gates on up), memory, machine code and programming languages, all with accessible diagrams and explanations.

*TECS* has you build an actual working computer from the ground up. Came here to say Code by Charles Petzold as well. The elements of computer systems: http://www1.idc.ac.il/tecs/ Can you summarize the entire text in a cute, easily digestible, infographic for me? Can you summarize the entire text in a cute, easily digestible, infographic for me? http://i.imgur.com/bjIhi.png      That is kinda cool.  It would be kinda fun to make something like that, but going from the top down instead. Yeah, like something that, without going into *too* much detail, can explain the entire causal mechanism of everyday computer tasks, like saving a file, or rendering an image. Is there really nothing like this already? Yeah, like something that, without going into *too* much detail, can explain the entire causal mechanism of everyday computer tasks, like saving a file, or rendering an image. Is there really nothing like this already?  My favorite CS course at Penn was Intro to Computer Architecture. It went through each abstraction layer from transistors to C. It was very digestible and a good prerequisite to a thorough OS or computer system design course. It looks like the lecture slides are still available [here](http://www.cis.upenn.edu/~milom/cse240-Fall06/), and the textbook is [here](http://www.amazon.com/Introduction-Computing-Systems-gates-beyond/dp/0072467509). My favorite CS course at Penn was Intro to Computer Architecture. It went through each abstraction layer from transistors to C. It was very digestible and a good prerequisite to a thorough OS or computer system design course. It looks like the lecture slides are still available [here](http://www.cis.upenn.edu/~milom/cse240-Fall06/), and the textbook is [here](http://www.amazon.com/Introduction-Computing-Systems-gates-beyond/dp/0072467509). And wait you took it with milo? He taught me 371, he was great      Steve Gibson took several episodes of his Security Now! Podcast to go back to basics with a "How Computers Work" series.  I believe this covers exactly what you're asking for.  He talks about early hardware architecture and advances made to present, along with the OS and application technologies built.

You can find them at http://grc.com/sn and start with episode 233, then search "How computers work" for the remaining episodes. </snippet></document><document><title>Categories, Quantization, and Much More</title><url>http://math.ucr.edu/home/baez/categories.html</url><snippet /></document><document><title>Normalized Google distance: semantic similarity through Google hits</title><url>http://en.wikipedia.org/wiki/Normalized_Google_distance</url><snippet>  This July my colleagues and I are presenting a multidimensional fuzzy clustering technique for web mining that includes NGD as one of the dimensions. 

Now we encounter a bit of a pickle. google charges the query api that used to be somehow free (and it's not even the same) when NGD was developed. Prices are a little to high for us (our ourly rate is less than 10 dollars since we live in Argentina) and we will be switching off to bing that provides somewhat the same information for less money until we have some funding. 

[If someone is interested, there's a link to our position paper](https://sites.google.com/a/aigroup.com.ar/ofisy/papers/2012/DM2012_S_038_Gorbatik.pdf?attredirects=0&amp;amp;d=1)
 This July my colleagues and I are presenting a multidimensional fuzzy clustering technique for web mining that includes NGD as one of the dimensions. 

Now we encounter a bit of a pickle. google charges the query api that used to be somehow free (and it's not even the same) when NGD was developed. Prices are a little to high for us (our ourly rate is less than 10 dollars since we live in Argentina) and we will be switching off to bing that provides somewhat the same information for less money until we have some funding. 

[If someone is interested, there's a link to our position paper](https://sites.google.com/a/aigroup.com.ar/ofisy/papers/2012/DM2012_S_038_Gorbatik.pdf?attredirects=0&amp;amp;d=1)
 That's pretty neat. I think google already uses either Levenshtein distance or some form of it for fixing typos and misspellings. Adding another dimension or two to be able to search "approximately" would be an interesting experiment.  Interesting concept, but I don't see any applications. Natural language processing. 

If I say "I went swimming at the bank", you will know effortlessly that I mean the bank of a river, not the kind with money, partly because the word "swimming" is related to the word "bank" and primes you to think of words related to water.

If I say "I got a drink at the par" you will probably hear it as "bar", because the priming effect works on hearing as well.

In fact, without high level semantic knowledge it's impossible to do voice recognition correctly. The same sound can mean many different things depending on the context.   This is pretty interesting. It reminds of TF*IDF, almost. I wonder if it would be possible to extend this to somehow be applicable to measure the distance between terms annotated with concepts from an ontology. Instead of a term having hits, it has an instance count.  </snippet></document><document><title>Computational Balloon Twisting</title><url>http://vihart.com/papers/balloon/cccg_long.pdf</url><snippet>  Sorry, forgot the [pdf] warning in the title.     as a nerd, and a circus-skills enthusiast, I love this. Too bad kids want puppies more than they want cuboctahedrons...  This was presented at a smallish computational geometry conference in Montreal.  I happened to be there.  The talk for this paper was great! It was also fun partying with the Demaines and Vi afterwards. Did they bring lots of balloons?</snippet></document><document><title>Minecraft Redstone Computing; how to build a comparator.</title><url>http://youtu.be/XZvMq2U2Ioc</url><snippet /></document><document><title>Ask compsi: Is there a field for translating raw statistics to natural language?</title><url>http://www.reddit.com/r/compsci/comments/wb2od/ask_compsi_is_there_a_field_for_translating_raw/</url><snippet>I'm looking for keywords for semi-automatic conversion of statistical data to plain English. Is there such a field in computer science?

For instance, an age distribution like this:

        X
    X   X
    X X X X
    X X X X
    A B C D

should be translated to:

*Most of this people are either children or middle aged.*
  Yes, it is called Natural Language Generation, and it is commonly seen as a subfield of NLP/Computational Linguistics. Also, Surface Realization could fit, although that refers to the last component of the traditional NLG pipeline. Thanks! I could be wrong, but a quick search suggests that NLG was far more active in 80s and 90s. It seems there are not many modern libraries around. Has it been replaced by something else, or it is not as hot as it used to be? You are not that wrong. While there is a small but active research community in academia, there is not much software available that would work off the shelf. 

The book by Reiter and Dale "Building Natural Language Generation Systems" (Cambridge University Press, 2000) has set what is now accepted to be "the standard way" to do NLG. In a nutshell, it's a pipeline of tasks, from content selection (what to talk about) to surface realization (how to put the content into proper English/other language).

Eventually research groups tackled different modules of this pipeline, each with their own results and drawback, and no major breakthrough has happened in years. My impression is that nowadays there is more focus on the applications than on the theory/methodology.

NLG is the main topic of my PhD dissertation, so if you're still curious ask away! &amp;gt; NLG is the main topic of my PhD dissertation, so if you're still curious ask away!

Perfect!

* Like you said there are not many (or any) real-world application-ready libraries for NLG. Which libraries are the best available for now? I could only find [SimpleNLG](http://code.google.com/p/simplenlg/) (The rest of libraries isted in [aclweb](http://aclweb.org/aclwiki/index.php?title=Downloadable_NLG_systems) seem to be very ancient, too theoretical or not in active development.). The big [NLTK](http://nltk.org/) python library seems not to do much about language generation. IS there a more modern library available?

* Do you know any libraries/papers which specifically deal with translating statistical data to text? SimpleNLG is still under active development. I'm sure of it, since I met the developers some months ago. It is somehow a piece of software that has fuzzy links with the linguistic theory; something is hardwired, some layers are mixed up, etc. But when you described your goal, that was the library I first thought of, so my advice is to definitely give it a try.

I'm unfortunately not aware of general-purpose full-pipeline NLG library at the moment, and I'm personally working on revising the traditional pipeline to a certain extent (this will take years anyway).

For you second question, I'm sure there are, but I'm not at the office right now, and I had a pretty rough day, so if it's ok I would come back to you about publications tomorrow.

Feel free to PM me, so that we can continue the conversation via email. Thanks! I could be wrong, but a quick search suggests that NLG was far more active in 80s and 90s. It seems there are not many modern libraries around. Has it been replaced by something else, or it is not as hot as it used to be?  [deleted]  Well it depends on how robust you want it to be. Is it just dealing with "people" and "age?" If so, you can easily do it with any language, although the "natural language" would have to be hard coded.

I guess I don't understand why this would be difficult? Like, if you know the content of your statistics you can easily write something to do it. But if you want something that will do any data? Well I'm going to jump ship.</snippet></document><document><title>Grilling Quantum Circuits</title><url>http://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/</url><snippet /></document></searchresult>
<searchresult><compsci /><document><title>Reminder: Don't vote up or respond to off-topic posts.</title><url>http://www.reddit.com/r/compsci/comments/109ha3/reminder_dont_vote_up_or_respond_to_offtopic_posts/</url><snippet>Like most internet communities, we're always experiencing a slow but steady decline in quality. I'm sure that people who care about Computer Science will eventually leave this subreddit and migrate to /r/TrueTrueTrueCompsci. Until that dark day arrives, I'm asking everyone here to (1) restrain yourselves from upvoting or discussing posts which don't actually fit the sidebar criteria, and (2) if someone else posts something off-topic please politely direct them to a more appropriate subreddit. 

Thanks!  Has this really been a big problem?   

Also /r/TrueTrueTrueCompsci looks like fun...         &amp;gt; we're always experiencing a slow but steady decline in quality

I've never understood these statements. The entire point of Reddit is to let users determine what content is good, but these statements imply that there is some separate, objective way to determine what content is good. That would be the case if reddit were a static community, but it isn't.  New users arrive constantly, bringing with them the cultural norms of wherever they came from.  If a new user arrives at /r/compsci and sees a slew of off-topic posts on its front page, he may think that it _is_ the norm and upvote the post.  Of course it slowly becomes the norm.  The shift in content quality pushes the "old guard" out and and you are left with the meme-filled husk of the community you set out to build. But who decides what the norm *should* be? Again, the point of reddit is that the appearance is the result of a deterministic algorithm that takes as inputs the votes of all its members. It's almost purely democratic (obvious exceptions are user bans, admin moderation, and censorship). There's nothing in the philosophy of reddit that says the "old guard" is in any way more privileged than the new guard.

If you "set out to build" a community that held strictly to your idea of what is good content, reddit was the wrong place to do that. But who decides what the norm *should* be? Again, the point of reddit is that the appearance is the result of a deterministic algorithm that takes as inputs the votes of all its members. It's almost purely democratic (obvious exceptions are user bans, admin moderation, and censorship). There's nothing in the philosophy of reddit that says the "old guard" is in any way more privileged than the new guard.

If you "set out to build" a community that held strictly to your idea of what is good content, reddit was the wrong place to do that. There's nothing wrong with setting out to build a community that follows its administrators' guidelines of what good content is.  Just because there exists a mechanism to vote on the quality of content does not mean that it should be the _sole_ mechanism through which moderation is conducted.  Compare the relatively unmoderated /r/science to the heavily-moderated /r/askscience.  While their guidelines both prohibit top-level memes, you'll find many upvoted meme comments in /r/science and almost none in /r/askscience.  Objectively, the /r/askscience community does a better job following its guidelines.  Subjectively, it has a much better community. &amp;gt; There's nothing wrong with setting out to build a community that follows its administrators' guidelines of what good content is.

No, there's not, but it's foolish to use reddit to do such.

&amp;gt; Compare the relatively unmoderated /r/science to the heavily-moderated /r/askscience.

I can personally compare them and choose the one *I* prefer, but let me reiterate: the point of reddit is that the users of each community determine what appears on the community's front page.

You seem to still be supporting the idea that there is some *objective* way to determine "quality." On reddit, if a community's users enjoy memes, then that is what is *supposed* to be shown on the front page. This isn't a "problem" beyond the fact that you might not like memes. This is reddit working precisely the way it is supposed to work. &amp;gt; we're always experiencing a slow but steady decline in quality

I've never understood these statements. The entire point of Reddit is to let users determine what content is good, but these statements imply that there is some separate, objective way to determine what content is good. Same reason the history channel plays reality shows.  They base their content on what the viewers want, and now its all garbage.   You mean that their viewers want something other than what you want. I think I remember hearing the phenomenon referred to as 'network rot' or something like that.  Reality shows on TV are like memes on the internet.  They're valuable in their own right, but without moderation, it wouldn't be long before many subreddits were inundated with memes.  The difference is that most TV networks make their decisions based on what will get more viewers (and therefore more ad money/higher ratings).  Most subreddits, on the other hand, try to keep the content on topic through rules and moderation (far from a perfect solution). &amp;gt; we're always experiencing a slow but steady decline in quality

I've never understood these statements. The entire point of Reddit is to let users determine what content is good, but these statements imply that there is some separate, objective way to determine what content is good. &amp;gt; we're always experiencing a slow but steady decline in quality

I've never understood these statements. The entire point of Reddit is to let users determine what content is good, but these statements imply that there is some separate, objective way to determine what content is good.    If people don't like the posts, they'll just downvote them and they'll get buried anyway.

If people are upvoting something someone posts, there's a reason for it, and I see no reason why we should be encouraged to not upvote something that we find particularly relevant or interesting just because it bends the rules a bit. Inform the user breaking the rules, sure, but the rest of us should be allowed to upvote what we choose to upvote.</snippet></document><document><title>Implementing exceptions with escape continuations</title><url>http://matt.might.net/articles/implementing-exceptions/</url><snippet>  Required reading: [Oleg's essay on why `call/cc` as a language feature is a bad idea](http://okmij.org/ftp/continuations/against-callcc.html), including proof that `call/cc` by itself cannot be used to implement exceptions, and arguments that bolting on the other things necessary to make it so capable is detrimental to performance. Thanks! How are `call/cc`'s continuations related to `call/ec`'s?

I read through the article cringing at how inefficient this approach must be - so many functions introduced! So much code bloat! - but perhaps Scheme compilers are clever enough to eliminate all the extra lambdas if, say, the programmer doesn't call `return` within a given `try` block? Thanks! How are `call/cc`'s continuations related to `call/ec`'s?

I read through the article cringing at how inefficient this approach must be - so many functions introduced! So much code bloat! - but perhaps Scheme compilers are clever enough to eliminate all the extra lambdas if, say, the programmer doesn't call `return` within a given `try` block?  Monads seem to work for any exceptions I have. The article's about writing a compiler for a language (like Python, the example used) which has `try/catch/finally` style exceptions. Is that what you're talking about, or are you talking about using monads as an error-handling method instead of Python-style exceptions? A try/catch style can be implemented with monads. If you define how sequential operations are tied together, and have the monad store the exceptional details until it encounters a catch. So could you use that to write a Python-to-Haskell compiler, correctly handling all the interactions between try, catch, finally, return, break and continue discussed in the article? </snippet></document><document><title>Sketch of the Day: K-Minimum Values</title><url>http://blog.aggregateknowledge.com/2012/07/09/sketch-of-the-day-k-minimum-values/</url><snippet /></document><document><title>You know you are in for a good time when your book opens with this paragraph:</title><url>http://www.reddit.com/r/compsci/comments/103s9t/you_know_you_are_in_for_a_good_time_when_your/</url><snippet>"Writing the first edition of this book was a grueling task that took two and a half years and the help of many people. After the toll it took on my health and sanity, I promised that I'd never put myself through such an experience again. "

That poor, poor soul who wrote the O'reilly book "Mastering Regular Expressions" 

Why do i read this?
I am a sick sort of person.  You mean regular expressions aren't just magic strings compiled by some unknown gods that you copy and paste as needed?  Gold. I wish I could fit that on a bumper sticker. You mean regular expressions aren't just magic strings compiled by some unknown gods that you copy and paste as needed?  Not unknown gods, they're actually Russians.  s/poor/awesome/g I'm just a awesome boy from a awesome family...  "Some people, when confronted with a problem, think "I know, I'll use regular expressions." Now they have two problems." -Jamie Zawinski "Some people, when confronted with a problem, think "I know, I'll use multithreading." Now have they problems two."  [deleted] [deleted] [deleted] [deleted] "Some people, when confronted with a problem, think "I know, I'll use multithreading." Now have they problems two."  nested.. quotes.. &amp;lt;eye twitch&amp;gt; &amp;gt; "Some people, when confronted with a problem, think \"I know, I'll use multithreading.\" Now have they problems two."

Better? &amp;gt; "Some people, when confronted with a problem, think \"I know, I'll use multithreading.\" Now have they problems two."

Better? Not if you want to be grammatically and syntactically correct. I'll do it myself.

"Some people, when confronted with a problem, think 'I know, I'll use multithreading.' Now have they problems two."

Also I'm tempted to move the first period outside the quotation marks. "I just love doing that". "Some people, when confronted with a problem, think 'I know, I\'ll use multithreading.' Now have they problems two." "Some people, when confronted with a problem, think 'I know, I\'ll use multithreading.' Now have they problems two." Not if you want to be grammatically and syntactically correct. I'll do it myself.

"Some people, when confronted with a problem, think 'I know, I'll use multithreading.' Now have they problems two."

Also I'm tempted to move the first period outside the quotation marks. "I just love doing that". Not if you're British. There's no GRAMMAR concerning punctuation. There is only style. Punctuation style is not, never was and never will be grammatical.

Or syntactical.

Edit: There's a further explanation two levels below, read the clarification too! Sometimes punctuation (as understood by the english meaning of the word) can change function of words. I'm from The United States of America, One Nation Under God, and here, we get taught an ancient and arcane set of grammatical rules passed down from our elders, practicing every year from age 5 until 17/18, which includes a massive number of grammatical rules, upon which our instructors insist that our absolute unwavering adherence is of utmost importance.

In fact, had I crafted such a sentence as the above as a schoolboy, I would have been reprimanded for creating a "run-on sentence", and not having a minimum of three sentences in a paragraph.

Teachers even go as far as deducting points from homework and exams for minor grammatical errors in otherwise well thought-out works. Even in unrelated subjects like history and science. My God, I just used a sentence fragment to convey an idea. Where's my noose? I'm from The United States of America, One Nation Under God, and here, we get taught an ancient and arcane set of grammatical rules passed down from our elders, practicing every year from age 5 until 17/18, which includes a massive number of grammatical rules, upon which our instructors insist that our absolute unwavering adherence is of utmost importance.

In fact, had I crafted such a sentence as the above as a schoolboy, I would have been reprimanded for creating a "run-on sentence", and not having a minimum of three sentences in a paragraph.

Teachers even go as far as deducting points from homework and exams for minor grammatical errors in otherwise well thought-out works. Even in unrelated subjects like history and science. My God, I just used a sentence fragment to convey an idea. Where's my noose? Not if you're British. There's no GRAMMAR concerning punctuation. There is only style. Punctuation style is not, never was and never will be grammatical.

Or syntactical.

Edit: There's a further explanation two levels below, read the clarification too! Sometimes punctuation (as understood by the english meaning of the word) can change function of words. Is this serious?  I use my punctuation in English as I do in Portuguese cause I always thought they were universal grammatical rules. Punctuation in Portuguese is so absolutely grammatical and logical that I just behaved like it descended from Heaven in a golden table saying:
"Thus shalt thou punctuate thy phrases: ..."

It's really a paradigm shift if you say that there are western languages in which punctuation serve no grammatical purpose.  First of all, thanks for the reply. It made me realize I need to clarify more (because I half lied). So here it is:

Let me put it this way. Use of punctuation can change meaning, for example marking subordinate clauses etc.. That is true and almost universal.

What I ment was simply use of various quotation marks, order of symbols at the end of the sentence and stuff like that. I should have been more precise, since by "punctuation" in english, these things are also included. So I more likely ment to say "style of punctuation". Which is what OP was also referencing, but incorrectly as grammar.

The difference between: 

"And then I said, 'Oatmeal, are you crazy!?'". 

'And then I said, "Oatmeal, are you crazy!?"'. 

"And then I said, 'Oatmeal, are you crazy'!?".

is purely a matter of style, and bare no grammatical meaning whatsoever.

However, sentene like "And then, seeing her run, I run too", the commas have very precise meaning and leaving them out would change the meaning. And also sentence like "Are, you, sure, you, want, this" is not grammatical. So yes, I was partially wrong.

Having said all that, there is a western language where punctuation doesn't even exist: Latin! The mother of all western languages ;) Theydidntevenputspacesinbetweenwords:D

So it is also good to understand that you can still write perfectly syntactically correctly without punctuation.

Edit (+7hours): Just to add one more point: you don't use punctuation while you speak, but you are still understood perfectly. So you can also look at punctuation as being a replacement for intonation, stress, accent and the rest of the stuff you use to comunicate the meaning verbally. Hummm! I get what you're trying to say now. Still, it's fascinating. 

Brazilian Portuguese tend to be extremely nitpicking about grammar, so I'm not sure if all those orderings of exclamation marks and quotation marks are valid here.  I'll look it up. 

Also I think the official grammar doesn't recognize single quotes as a valid symbol (making nested quotation tremendously confuse). Is this serious?  I use my punctuation in English as I do in Portuguese cause I always thought they were universal grammatical rules. Punctuation in Portuguese is so absolutely grammatical and logical that I just behaved like it descended from Heaven in a golden table saying:
"Thus shalt thou punctuate thy phrases: ..."

It's really a paradigm shift if you say that there are western languages in which punctuation serve no grammatical purpose.  Punctuation serves grammatical and pragmatical (like "?") purposes but the way it is used is far from universal. There are well-established written languages that do not even use white spaces, like for instance Japanese.
I'd say all languages that are usually written in the Latin alphabet use comma(s|ta) nowadays. But the ways they are used differ. In the German orthography for example you would use far more comma(s|ta) than in English, Portuguese or Spanish. E.g. in German they are mandatory in front of almost every kind of subordinate clause, whereas in English they aren't.

"Sie sahen, dass das Haus aus Stein gebaut war."  
vs.  
"They saw that the house was built of stone."

In English, in contrast, they are mandatory when you begin a sentence with an adverb or certain conjunctions:

"Then, a green goblin appeared."  
vs.  
"Dann erschien ein gr&#252;ner Kobold."

You see, punctuation is far from universal! ;) Japanese uses paragraphs, doesn't it? Not if you want to be grammatically and syntactically correct. I'll do it myself.

"Some people, when confronted with a problem, think 'I know, I'll use multithreading.' Now have they problems two."

Also I'm tempted to move the first period outside the quotation marks. "I just love doing that". &amp;gt; "Some people, when confronted with a problem, think \"I know, I'll use multithreading.\" Now have they problems two."

Better? nested.. quotes.. &amp;lt;eye twitch&amp;gt; At least they're balanced Huh? Each start quote has an end quote. Oh. That isn't always the case. (And not only with quotes either. agh! my eyes! nested.. quotes.. &amp;lt;eye twitch&amp;gt; nested.. quotes.. &amp;lt;eye twitch&amp;gt; How about

**"Some people, when confronted with a problem, think "**I know, I'll use multithreading.**" Now have they problems two."** If only we could use proper quotation marks...

&amp;gt; &#8220;Some people, when confronted with a problem, think &#8220;I know, I'll use multithreading.&#8221; Now have they problems two.&#8221;

Failing that, we could alternate single and double quotes:

&amp;gt; "Some people, when confronted with a problem, think 'I know, I'll use multithreading.' Now they have problems two."

We might have trouble with the apostrophe in "I'll", though... We could use Reddit's quoting, of course:

&amp;gt;&amp;gt; Some people, when confronted with a problem, think

&amp;gt;&amp;gt;&amp;gt; I know, I'll use multithreading.

&amp;gt;&amp;gt; Now they have problems two.

Perhaps LaTeX-style?

&amp;gt; \`\`Some people, when confronted with a problem, think \`\`I know, I'll use multithreading.'' Now have they problems two.''

You might need to be careful with the backticks, though, because they could disappear into empty `code` blocks.  nested.. quotes.. &amp;lt;eye twitch&amp;gt; nested.. quotes.. &amp;lt;eye twitch&amp;gt; "Some people, when confronted with a problem, think "I know, I'll use multithreading." Now have they problems two."  "Some people, when confronted with a problem, think "I know, I'll use regular expressions." Now they have two problems." -Jamie Zawinski "Some people, when confronted with a problem, think "I know, I'll use regular expressions." Now they have two problems." -Jamie Zawinski &amp;gt;Some people, when confronted with a problem, think &#8220;I know, I&#8217;ll quote Jamie Zawinski.&#8221; Now they have two problems.

-Mark Pilgrim "Some people, when confronted with a problem, think "I know, I'll use regular expressions." Now they have two problems." -Jamie Zawinski "Some people, when confronted with a problem, think "I know, I'll use regular expressions." Now they have two problems." -Jamie Zawinski Regex's are awesome.  The key to understanding them is to build a regex compiler that creates a NFA, then a DFA.  Once you understand what they boil down to, they become really easy to use. It's not that I don't think regex can be useful, or even the right tool for a job. I just think that a lot of people use regex in a lot of places where it may be the obvious tool, it's just also the wrong one. 

I personally implemented a compiler for Python 3 and did a looooooooot of regex along the way.  &amp;gt;I personally implemented a compiler for Python 3

On your own?  Aren't compilers really difficult to make?  How clever do you have to be? No, it was part of a course in University. 

Most of the materials I used are available on the professors blog. I actually see his blog posts on this sub-reddit with regularity. [Check it out](http://matt.might.net/teaching/compilers/spring-2011/)

It's of course more difficult without the lectures and stuff, but once you realize that it's just tree transforms all the way down things get easier (though it was still the hardest thing I've programmed thus far). There is also some additional complexity added when you start trying to do register allocation etc.  Damn, I'm really kinda wishing I didn't drop out of education now.

Oh well, maybe later in life, when I have some money, or something. It's definitely never too late :)

Best of luck! Perhaps not too late, but not right now.  Prices are higher than ever, and the debt is something that would cripple me until my late thirties.

Current plan is to get a career, from that get financial stability, then adopt, then think about education again.  At some point preferably before that, I want to take maybe a year out from reality, and disappear around Japan, Iceland, other places, though I have literally no idea where that money would come from. even as an amateur and without a degree you can make $50k-60k as an entry level software engineer.  It'll be harder to get interviews, but if you can get an interview and demonstrate you know your way around a language you have a shot at a job. The market is desperate for workers really. 

^_^  "Some people, when confronted with a problem, think "I know, I'll use regular expressions." Now they have two problems." -Jamie Zawinski  It's a pretty good read. Digest the first two chapters and you'll be able to cobble together say 90% of the regex patterns you'll ever need. This is true. I read the first 2-3 chapters and regular expressions just made so much more sense. Combined with RegexBuddy and I feel as if I can almost do any regular expression necessary, except those really crazy ones like the one for full RFC email address matching. For reference: regex for full RFC 2822 email matching:


`(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t]
)+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:
\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(
?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ 
\t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\0
31]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\
](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+
(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:
(?:\r\n)?[ \t])*))*|(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z
|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)
?[ \t])*)*\&amp;lt;(?:(?:\r\n)?[ \t])*(?:@(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\
r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[
 \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)
?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t]
)*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[
 \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*
)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t]
)+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)
*:(?:(?:\r\n)?[ \t])*)?(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+
|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r
\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:
\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t
]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031
]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](
?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?
:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?
:\r\n)?[ \t])*))*\&amp;gt;(?:(?:\r\n)?[ \t])*)|(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?
:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?
[ \t]))*"(?:(?:\r\n)?[ \t])*)*:(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] 
\000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|
\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;
@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"
(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t]
)*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\
".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?
:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[
\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-
\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(
?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\&amp;lt;(?:(?:\r\n)?[ \t])*(?:@(?:[^()&amp;lt;&amp;gt;@,;
:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([
^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\"
.\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\
]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\
[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\
r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] 
\000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]
|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \0
00-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\
.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,
;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?
:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*
(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".
\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[
^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]
]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\&amp;gt;(?:(?:\r\n)?[ \t])*)(?:,\s*(
?:(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\
".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(
?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[
\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t
])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t
])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?
:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|
\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:
[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\
]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\&amp;lt;(?:(?:\r\n)
?[ \t])*(?:@(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["
()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)
?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;
@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[
 \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,
;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t]
)*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\
".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?
(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".
\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:
\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\[
"()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])
*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])
+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\
.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z
|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\&amp;gt;(?:(
?:\r\n)?[ \t])*))*)?;\s*)` Relevant, from the front page ~2 weeks ago. http://davidcelis.com/blog/2012/09/06/stop-validating-email-addresses-with-regex/ &amp;gt;But guess what? You can use pretty much any character you want if you escape it by surrounding it in quotes. For example, 
`"Look at all these spaces!"@example.com` is a valid email address. Nice.

Well damn... For reference: regex for full RFC 2822 email matching:


`(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t]
)+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:
\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(
?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ 
\t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\0
31]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\
](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+
(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:
(?:\r\n)?[ \t])*))*|(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z
|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)
?[ \t])*)*\&amp;lt;(?:(?:\r\n)?[ \t])*(?:@(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\
r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[
 \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)
?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t]
)*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[
 \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*
)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t]
)+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)
*:(?:(?:\r\n)?[ \t])*)?(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+
|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r
\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:
\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t
]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031
]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](
?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?
:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?
:\r\n)?[ \t])*))*\&amp;gt;(?:(?:\r\n)?[ \t])*)|(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?
:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?
[ \t]))*"(?:(?:\r\n)?[ \t])*)*:(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] 
\000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|
\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;
@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"
(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t]
)*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\
".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?
:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[
\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-
\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(
?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\&amp;lt;(?:(?:\r\n)?[ \t])*(?:@(?:[^()&amp;lt;&amp;gt;@,;
:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([
^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\"
.\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\
]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\
[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\
r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] 
\000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]
|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \0
00-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\
.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,
;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?
:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*
(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".
\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[
^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]
]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\&amp;gt;(?:(?:\r\n)?[ \t])*)(?:,\s*(
?:(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\
".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(
?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[
\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t
])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t
])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?
:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|
\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:
[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\
]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\&amp;lt;(?:(?:\r\n)
?[ \t])*(?:@(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["
()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)
?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;
@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[
 \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,
;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t]
)*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\
".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?
(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".
\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:
\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\[
"()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])
*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])
+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\
.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z
|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\&amp;gt;(?:(
?:\r\n)?[ \t])*))*)?;\s*)` For reference: regex for full RFC 2822 email matching:


`(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t]
)+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:
\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(
?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ 
\t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\0
31]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\
](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+
(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:
(?:\r\n)?[ \t])*))*|(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z
|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)
?[ \t])*)*\&amp;lt;(?:(?:\r\n)?[ \t])*(?:@(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\
r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[
 \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)
?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t]
)*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[
 \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*
)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t]
)+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)
*:(?:(?:\r\n)?[ \t])*)?(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+
|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r
\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:
\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t
]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031
]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](
?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?
:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?
:\r\n)?[ \t])*))*\&amp;gt;(?:(?:\r\n)?[ \t])*)|(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?
:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?
[ \t]))*"(?:(?:\r\n)?[ \t])*)*:(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] 
\000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|
\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;
@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"
(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t]
)*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\
".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?
:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[
\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-
\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(
?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\&amp;lt;(?:(?:\r\n)?[ \t])*(?:@(?:[^()&amp;lt;&amp;gt;@,;
:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([
^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\"
.\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\
]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\
[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\
r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] 
\000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]
|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \0
00-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\
.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,
;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?
:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*
(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".
\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[
^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]
]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\&amp;gt;(?:(?:\r\n)?[ \t])*)(?:,\s*(
?:(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\
".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(
?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[
\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t
])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t
])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?
:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|
\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:
[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\
]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\&amp;lt;(?:(?:\r\n)
?[ \t])*(?:@(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["
()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)
?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;
@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[
 \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,
;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t]
)*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\
".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?
(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".
\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:
\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\[
"()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])
*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])
+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\
.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z
|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\&amp;gt;(?:(
?:\r\n)?[ \t])*))*)?;\s*)` Hey do you have one for HTML too?

http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags

:P [deleted] you...you posted the same thing I did....I don't get it :( Damn, sorry dude I'm utterly tired and I didn't read. 

I will delete my post. Hey do you have one for HTML too?

http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags

:P It's reasonable to use regular expressions to parse HTML *tags* as tokens, then build a parser on top of that. Wouldn't that be more along the lines of lexing than parsing? For reference: regex for full RFC 2822 email matching:


`(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t]
)+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:
\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(
?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ 
\t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\0
31]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\
](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+
(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:
(?:\r\n)?[ \t])*))*|(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z
|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)
?[ \t])*)*\&amp;lt;(?:(?:\r\n)?[ \t])*(?:@(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\
r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[
 \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)
?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t]
)*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[
 \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*
)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t]
)+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)
*:(?:(?:\r\n)?[ \t])*)?(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+
|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r
\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:
\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t
]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031
]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](
?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?
:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?
:\r\n)?[ \t])*))*\&amp;gt;(?:(?:\r\n)?[ \t])*)|(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?
:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?
[ \t]))*"(?:(?:\r\n)?[ \t])*)*:(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] 
\000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|
\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;
@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"
(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t]
)*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\
".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?
:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[
\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-
\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(
?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\&amp;lt;(?:(?:\r\n)?[ \t])*(?:@(?:[^()&amp;lt;&amp;gt;@,;
:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([
^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\"
.\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\
]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\
[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\
r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] 
\000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]
|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \0
00-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\
.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,
;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?
:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*
(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".
\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[
^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]
]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\&amp;gt;(?:(?:\r\n)?[ \t])*)(?:,\s*(
?:(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\
".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(
?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[
\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t
])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t
])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?
:\.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|
\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:
[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\
]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\&amp;lt;(?:(?:\r\n)
?[ \t])*(?:@(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["
()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)
?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;
@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[
 \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,
;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t]
)*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\
".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?
(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".
\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:
\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\[
"()&amp;lt;&amp;gt;@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])
*))*@(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])
+|\Z|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\
.(?:(?:\r\n)?[ \t])*(?:[^()&amp;lt;&amp;gt;@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z
|(?=[\["()&amp;lt;&amp;gt;@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\&amp;gt;(?:(
?:\r\n)?[ \t])*))*)?;\s*)` Are we supposed to see a picture of a fish or something in that?  My biggest problem with reg expressions is that i use them heavy for a month then never touch them again for a few years. In between i forget what most of the characters mean :S My biggest problem with reg expressions is that i use them heavy for a month then never touch them again for a few years. In between i forget what most of the characters mean :S  and this smug little fucking lizard has no trouble with them at all:

http://regexpal.com/ I use [RegexBuddy](http://www.regexbuddy.com/). It costs $39.95 but is worth every penny... especially for the ability to convert from one language to another instantly. Just look at the cost as money you won't have to spend on alcohol while dealing with regular expressions. But&#8230;I like alcohol&#8230; 2 six packs will solve any regex problem.    
 My math says you could also get one twelve pack. My math says you could also get one twelve pack. My math says you could also get one twelve pack. I use [RegexBuddy](http://www.regexbuddy.com/). It costs $39.95 but is worth every penny... especially for the ability to convert from one language to another instantly. Just look at the cost as money you won't have to spend on alcohol while dealing with regular expressions.  Hah! This is so going to be the opening paragraph of the "acknowledgement" section of my Ph.D. Thesis! That seems like the silliest section to plagiarize Pretty sure he'd acknowledge the source. Hah! This is so going to be the opening paragraph of the "acknowledgement" section of my Ph.D. Thesis!        </snippet></document><document><title>What in computer science do you find beautiful?</title><url>http://www.reddit.com/r/compsci/comments/1049t0/what_in_computer_science_do_you_find_beautiful/</url><snippet>I'm a CS undergrad, and I want to be able to say I have seen, created, or contributed to something beautiful within my field.

So I was wondering what the people of r/compsci think. Is it algorithms? Raw code? The potential/possibilities? How do you explain/justify that to other (non-CS) people?  The Curry-Howard isomorphism (proofs-as-programs/formulae-as-types), and Martin-L&#246;f type theory and related extensions make me moist. You are aroused by type theory?

Speaking as a part-time type theorist, I think you need to see a psychiatrist. Well, not by *everything* in type theory, there are things I wouldn't touch even with a ten meters pole *cough*subtyping*cough*, but most dependent type theories are just beautiful (yes, I need to see a psychiatrist) &amp;gt;there are things I wouldn't touch even with a ten meters pole coughsubtypingcough

*Excuse me?*  That's what I've been working on for a while!  What suddenly is this? &amp;gt;there are things I wouldn't touch even with a ten meters pole coughsubtypingcough

*Excuse me?*  That's what I've been working on for a while!  What suddenly is this? In my type systems research I have avoided subtyping like the plague as well. In fact, most of us in the dependent types community aren't too happy with things like subtyping - subject reduction and strong normalization becomes a whole lot harder to prove with subtyping voodoo (confluence is still OK though). I know of no dependent type theory that includes subtyping that has actually enjoyed a realistic, implementable type checker. From my memory, there was some activity where people managed to produce a type calculus that had subtyping, overloading and a plausible-sounding subject reduction and strong normalization proof in the 90s, but there has been no further activity in the area.

One of the reasons is there is little expressiveness gained if you've already got dependent types -- Agda has instance arguments for overloading. This relies on a trivial extension to the source language without requiring any change to the core type calculus. This is a much nicer approach, in my view, than baking stuff like overloading into the type calculus. Subtype polymorphism feels similarly superfluous, when you can already expose so much type structure that good ol' sigma and pi should be all you need.  &amp;gt; aren't too happy with things like subtyping

Does that include supertyping? In my type systems research I have avoided subtyping like the plague as well. In fact, most of us in the dependent types community aren't too happy with things like subtyping - subject reduction and strong normalization becomes a whole lot harder to prove with subtyping voodoo (confluence is still OK though). I know of no dependent type theory that includes subtyping that has actually enjoyed a realistic, implementable type checker. From my memory, there was some activity where people managed to produce a type calculus that had subtyping, overloading and a plausible-sounding subject reduction and strong normalization proof in the 90s, but there has been no further activity in the area.

One of the reasons is there is little expressiveness gained if you've already got dependent types -- Agda has instance arguments for overloading. This relies on a trivial extension to the source language without requiring any change to the core type calculus. This is a much nicer approach, in my view, than baking stuff like overloading into the type calculus. Subtype polymorphism feels similarly superfluous, when you can already expose so much type structure that good ol' sigma and pi should be all you need.  Sorry for hijacking this thread...

Where can I read about how exactly implicit argument inference works in e.g. Agda?

Secondly, is there any consensus on what is the best way to do what type classes do, in a dependently typed setting? Here's a PDF of the ICFP paper: http://people.cs.kuleuven.be/~dominique.devriese/agda-instance-arguments/icfp001-Devriese.pdf In my type systems research I have avoided subtyping like the plague as well. In fact, most of us in the dependent types community aren't too happy with things like subtyping - subject reduction and strong normalization becomes a whole lot harder to prove with subtyping voodoo (confluence is still OK though). I know of no dependent type theory that includes subtyping that has actually enjoyed a realistic, implementable type checker. From my memory, there was some activity where people managed to produce a type calculus that had subtyping, overloading and a plausible-sounding subject reduction and strong normalization proof in the 90s, but there has been no further activity in the area.

One of the reasons is there is little expressiveness gained if you've already got dependent types -- Agda has instance arguments for overloading. This relies on a trivial extension to the source language without requiring any change to the core type calculus. This is a much nicer approach, in my view, than baking stuff like overloading into the type calculus. Subtype polymorphism feels similarly superfluous, when you can already expose so much type structure that good ol' sigma and pi should be all you need.  The Curry-Howard isomorphism (proofs-as-programs/formulae-as-types), and Martin-L&#246;f type theory and related extensions make me moist.   Computability proofs.  The Church-Turing thesis.  The halting problem.  The Buse Beaver problem.  Grammars and automata.  Strongly reducible semantics.  P vs NP.  The seperability of complexity classes.  Ridiculously small yet Turing-complete computing bases.  Provably optimal algorithms.  The early history of the discipline:  the search to completely axiomatise mathematics, and how that was found to be impossible.

... I see a lot of people are answering things like "being able to code stuff".  Sorry, but that's not CS. This, so much this. 

I'd like to add the Arithmetic Hierarchy to this list. The fact that there are an uncountably infinite number of *classes* of uncomputable problems blows my mind.

For those who haven't studied computability theory, a metaphor (which is of course wildly inaccurate, but lots of math/CS concepts are hard to reduce):

Suppose can ask for the solution to a single type of problem from "god" (or fsm, or those computers from I, Robot... whatever), and you can pose this problem to him an infinite number of times. There will be another class of problems, that you cannot answer despite having the answer to this problem. Suppose you get to ask him for the answer to that problem... there is yet another harder class. 

Suppose you repeat this process ad infinitum. You STILL can't solve an INFINITE number of problems...

TL;DR, problems, there are lots of them, and most of them are nowhere near solvable. &amp;gt; "god" (or fsm)

In this context, I always misread that acronym as finite state machine.  Perhaps He should be renamed the Ineffable Spaghetti Monster?

Good answer ... the Arithmetic Heirarchy butts up against some things I mentioned and is another source of wonder. Computability proofs.  The Church-Turing thesis.  The halting problem.  The Buse Beaver problem.  Grammars and automata.  Strongly reducible semantics.  P vs NP.  The seperability of complexity classes.  Ridiculously small yet Turing-complete computing bases.  Provably optimal algorithms.  The early history of the discipline:  the search to completely axiomatise mathematics, and how that was found to be impossible.

... I see a lot of people are answering things like "being able to code stuff".  Sorry, but that's not CS. What is CS? Computer science is a study of (among other things) what is and is not computable. Computers themselves are not the subject of computer science, any more than telescopes are the subject of astronomy. It was really a rhetorical question. Your "among other things" points out the blurriness of the lines that define "CS". "The study of what is and is not computable" doesn't include things like networking, security, machine learning, *complexity theory*, or say, programming languages, many other things that are researched under the title of CS. Being computer scientists we should understand the differences between two-valued ("Is CS" "Is Not CS") and many-valued logic and use the two correctly.

Sure in the mid 20th century the focus was on computability theory but it would be difficult to say that the focus of CS today is computability theory. The meanings of words (and, more importantly, fields of research) are not necessarily fixed throughout time. The term Computer Science by itself doesn't mean a lot because it doesn't, as it is generally used, exclude a lot.

Moreover, "being able to code stuff" seems like a pretty valid part of CS in terms of the thought process. For example, you may be developing a new algorithm and you might code your current thoughts to improve your understanding. Would this not be a part of CS?

Coding isn't necessarily software engineering (and you could make a case for SE being somewhat a part of CS).

Essentially the person I replied to was saying something like "I like computability theory and coding isn't good enough to be have the title CS", discrediting an extremely useful part of CS and something that a lot of Computer Scientists are very fond of. It was really a rhetorical question. Your "among other things" points out the blurriness of the lines that define "CS". "The study of what is and is not computable" doesn't include things like networking, security, machine learning, *complexity theory*, or say, programming languages, many other things that are researched under the title of CS. Being computer scientists we should understand the differences between two-valued ("Is CS" "Is Not CS") and many-valued logic and use the two correctly.

Sure in the mid 20th century the focus was on computability theory but it would be difficult to say that the focus of CS today is computability theory. The meanings of words (and, more importantly, fields of research) are not necessarily fixed throughout time. The term Computer Science by itself doesn't mean a lot because it doesn't, as it is generally used, exclude a lot.

Moreover, "being able to code stuff" seems like a pretty valid part of CS in terms of the thought process. For example, you may be developing a new algorithm and you might code your current thoughts to improve your understanding. Would this not be a part of CS?

Coding isn't necessarily software engineering (and you could make a case for SE being somewhat a part of CS).

Essentially the person I replied to was saying something like "I like computability theory and coding isn't good enough to be have the title CS", discrediting an extremely useful part of CS and something that a lot of Computer Scientists are very fond of. Computer science is a study of (among other things) what is and is not computable. Computers themselves are not the subject of computer science, any more than telescopes are the subject of astronomy. So operating systems, programming languages, databases, compilers aren't computer science? or they aren't the 'subject' of computer science?  Computer science is a study of (among other things) what is and is not computable. Computers themselves are not the subject of computer science, any more than telescopes are the subject of astronomy. Computer science is a study of (among other things) what is and is not computable. Computers themselves are not the subject of computer science, any more than telescopes are the subject of astronomy. Regarding computers not being an object of study: what about the subfield of Computer Architecture? Is it not CS? Look, I'm not here to split hairs. All throughout college we hashed over the "engineering, or science?" debate to the point where I became convinced it's meaningless. Falling back on my previous metaphor, is the design of the Hubble telescope not within the purview of astronomy?  In a way yes, in a way no.  Let's say that there is a place where engineering, physics, and science all happily overlap.   What is CS? this is the easiest way to break it down. if Computer science is Ice hockey then programming is skating. Learning how to skate/program is just the first step in ice hockey/computer science Interestingly, you don't have to program (at least in the sense of things running on physical computers)  at all in certain things that fall under the umbrella of Computer Science. A lot of CS is done on a whiteboard. What language did Turing program in? Circuit designers? Turing machine I did add a little caveat just for things like that: "at least in the sense of things running on physical computers"

The point was that learning to program isn't really the first (or even a necessary) step in Computer Science; the post that starts this thread tried to point that out, though somewhat more harshly than necessary. Interestingly, you don't have to program (at least in the sense of things running on physical computers)  at all in certain things that fall under the umbrella of Computer Science. A lot of CS is done on a whiteboard. What language did Turing program in? Circuit designers? knowing how to program helps u understand computer science better. Having a understanding of the real world implementation of computational theory is useful Computability proofs.  The Church-Turing thesis.  The halting problem.  The Buse Beaver problem.  Grammars and automata.  Strongly reducible semantics.  P vs NP.  The seperability of complexity classes.  Ridiculously small yet Turing-complete computing bases.  Provably optimal algorithms.  The early history of the discipline:  the search to completely axiomatise mathematics, and how that was found to be impossible.

... I see a lot of people are answering things like "being able to code stuff".  Sorry, but that's not CS. Computability proofs.  The Church-Turing thesis.  The halting problem.  The Buse Beaver problem.  Grammars and automata.  Strongly reducible semantics.  P vs NP.  The seperability of complexity classes.  Ridiculously small yet Turing-complete computing bases.  Provably optimal algorithms.  The early history of the discipline:  the search to completely axiomatise mathematics, and how that was found to be impossible.

... I see a lot of people are answering things like "being able to code stuff".  Sorry, but that's not CS.  When my programs compile and run as intended, it's so beautiful, I shed tears of joy.     The various problem solving paradigms, especially Dynamic Programming. That thing is magic the first time you use it, but once you get it, you'll achieve nirvana. Just like dope? Actually, yes. I've been trying to finish the dynamic programming portion of UVa Online Judge :D     The meta-circular interpreter in Scheme (as presented in SICP).     I really love scripting.  Like really basic stuff.  I love hearing my Business major friends talk about what they did all summer with their internships, working with excel, entering in data (mainly bitch work)... and knowing that I could have created something to automate everything they did. Where can I go to learn scripting? And when you say scripting, are you talking bash shell scripting basically? I have alot of experience with Java but that's pretty much it. Where can I go to learn scripting? And when you say scripting, are you talking bash shell scripting basically? I have alot of experience with Java but that's pretty much it. Gotta use the right tool for the right job. I do a lot of shell scripting but frequently at work I am given data in an excel spreadsheet that needs massaging and sometimes I will use VBA for that ([Google Refine](http://code.google.com/p/google-refine/) is also a great tool for this).         Well written perl code.  The paycheck.

I find some aspects of CS to be fun, but "beautiful" would be a bit of a stretch.</snippet></document><document><title>Fast rotation of an array in-place ...</title><url>http://www.reddit.com/r/compsci/comments/104ok1/fast_rotation_of_an_array_inplace/</url><snippet>Hey everyone, I've come across [this page](http://thomas.baudel.name/Visualisation/VisuTri/inplacestablesort.html) which includes an implementation of a fast rotation algorithm (of an array in-place) somehow using a constant sized buffer and GCD...

I have no idea why or how it works. Can anyone try to explain it to me or link me to (good) resources? I found [this](http://berenger.eu/blog/public/Inplace-merge-algorithm.pdf) but found the typos and mistakes in the codes and explanation not easy to understand... and I'm not even sure if the explanation really makes sense.


P.S.
Secondary, the same goes for the in-place merging in linear time... how does it work?
I've read some papers (Huang, etc.) but they just go over the general outline/idea without addressing the implementation.
  </snippet></document><document><title>What's new with just-in-time compilation?</title><url>http://www.reddit.com/r/compsci/comments/1044oa/whats_new_with_justintime_compilation/</url><snippet>Can anyone who keeps up with dynamic compilation research &amp;amp; projects suggest some reading? I've read about [Self](http://labs.oracle.com/self/papers/papers.html), [trace trees](http://www.ics.uci.edu/~franz/Site/pubs-pdf/ICS-TR-06-16.pdf), and [PyPy](https://bitbucket.org/pypy/extradoc/src/tip/talk/icooolps2009/bolz-tracing-jit-final.pdf)'s meta-tracing JIT. What else is interesting or influential?</snippet></document><document><title>Book Recommendations?</title><url>http://www.reddit.com/r/compsci/comments/10405o/book_recommendations/</url><snippet>I recently finished Godel, Escher, Bach (finally!) and loved it. Now I have a problem however. This was one of the first compsci related books I've read, so I've nowhere to start on interesting reads. Any recommendations on good books in this field? Anything in the genre of "If you're into compsci, this is a must read" type thing?

I hope this is an appropriate place to post this, thanks in advance. Until then, it's back to Demon-Haunted World!   With all of Hofstadter's pseudophilosophical ramblings, Godel Escher Bach has more to do with [Dianetics](http://en.wikipedia.org/wiki/Dianetics) than with the field of computer science.   
  
Anyway, why not actually get your feet wet? Good places to start are either algorithms or computability. [DPV](http://www.cs.berkeley.edu/~vazirani/algorithms.html) gives an easy introduction to the first topic, [Sipser](http://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/053494728X) is the standard for the second (you don't need the latest version). </snippet></document><document><title>Convert NFAs to DFAs Online via Finite Automata Determinizer. Headache saver for Compiler &amp;amp; Computational Complexity courses.</title><url>http://hackingoff.com/compilers/nfa-to-dfa-conversion</url><snippet>   Just read your comment in the CS beauty thread, and it made me happy in a quiet, indescribable sort of way.</snippet></document><document><title>Tiny Transactions on Computer Science - The latest research in 140 characters or less</title><url>http://tinytocs.org/</url><snippet>    These are interesting but are there not full papers for any of them?  </snippet></document><document><title>An Interview with Ken Thompson (Unix and Beyond!) from 1999 in Computer</title><url>http://genius.cat-v.org/ken-thompson/interviews/unix-and-beyond</url><snippet> </snippet></document><document><title>Do you teach CS? New subreddit for CS education (r/CSEducation)</title><url>http://www.reddit.com/r/CSEducation/</url><snippet> </snippet></document><document><title>Image Processing.</title><url>http://www.reddit.com/r/compsci/comments/101tva/image_processing/</url><snippet>Where do I start? Any active subs?  
  
Books? Elementary concepts? Tutorials? Anyone know where I can find a roadmap to studying Image processing(By image processing I mean the mathematical concepts and applications invovled with image processing)?  
  
Thanks in advance.
  
  Here's a very high up point to start from but bear in mind these are not for the freshman.
You can use them as base, by googling or checking wikipedia for any terms and theories you might be unfamiliar with - and from there on... 
This method normally works for me.

Google's research papers
http://research.google.com/search.html#q=image%20processing

EXAMPLE-BASED IMAGE COMPRESSION
http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36415.pdf

Large Scale Online Learning of Image Similarity Through Ranking
http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/35114.pdf

Other source
http://stackoverflow.com/tags/image-processing/topusers
Users normally answer to question by point out to online sources where some sort of documentation lies </snippet></document><document><title>Which parallel sorting algorithm has the best average case performance?</title><url>http://stackoverflow.com/questions/3969813/which-parallel-sorting-algorithm-has-the-best-average-case-performance</url><snippet>  &amp;gt; I'm looking to sort lists of 1 million to 100 million elements in a JVM language running on 8 to 32 cores.

So, parallelism isn't really the issue. Best best is probably to divide your list into 32 sub-lists, use the fastest single-core sorting algorithm you have on each, then mergesort them back.

I would think, if you really did have a lot of processors, mergesort would give you really close to the best possible with the least work to implement (simplicity is a nice feature). Yes, it'll be O(n) to do the merge step, but all common sorting algorithms have an O(n) step (right?) -- it seems like better would require you to read and implement something out of a research paper.

Maybe some big-data companies have solved this and found a preferred algorithm that works in practice? [deleted] Let's see your analysis. Ha, I missed the comment, am now very curious. &amp;gt; I'm looking to sort lists of 1 million to 100 million elements in a JVM language running on 8 to 32 cores.

So, parallelism isn't really the issue. Best best is probably to divide your list into 32 sub-lists, use the fastest single-core sorting algorithm you have on each, then mergesort them back.

I would think, if you really did have a lot of processors, mergesort would give you really close to the best possible with the least work to implement (simplicity is a nice feature). Yes, it'll be O(n) to do the merge step, but all common sorting algorithms have an O(n) step (right?) -- it seems like better would require you to read and implement something out of a research paper.

Maybe some big-data companies have solved this and found a preferred algorithm that works in practice? Use a divide and conquer approach to parallelize your merge step.  You can do much better than O(n).  Sleepsort :p  </snippet></document><document><title>Question regarding encryption algorithms...</title><url>http://www.reddit.com/r/compsci/comments/100js0/question_regarding_encryption_algorithms/</url><snippet>Does there exist an encryption algorithm such that given

    y=f(k,x)

where f is the algorithm, k is the key, x is the input data and y is the output, the following properties are true:

* Knowing y and x does not mean you can easily know what k is

* f(k1,g(k2,x))=g(k2,f(k1,x)) where g is the same algorithm  Unless I'm missing something obvious, this sounds like the essential properties of any public key crypto system, so yes, RSA? I knew the second one was true for public key cryptography, but I was unsure if the first one was true as well. I mean, the silent observer is never supposed to know what is actually being sent, so I didn't know if that was an essential property. Unless I'm missing something obvious, this sounds like the essential properties of any public key crypto system, so yes, RSA?  &amp;gt; Knowing y and x does not mean you can easily know what k is

This is called resistance to a "known-plaintext" attack. Anything that can reasonably be called an encryption function is expected to have it.

&amp;gt; f(k1,g(k2,x))=g(k2,f(k1,x)) where g is the same algorithm

This is sometimes called "commutative encryption". (Your notation is a bit confusing: if f=g, why use different letters for them?)

Any double encryption, even non-commutative, is vulnerable to a meet-in-the-middle attack which makes breaking it only slightly more difficult than breaking a single layer of encryption. If what you want is a system where two people are able to decrypt a text if they cooperate but neither can do it alone, check out "secret sharing". Possibly even [secure multi-party computation](http://en.wikipedia.org/wiki/Secure_multi-party_computation) &amp;gt; Knowing y and x does not mean you can easily know what k is

This is called resistance to a "known-plaintext" attack. Anything that can reasonably be called an encryption function is expected to have it.

&amp;gt; f(k1,g(k2,x))=g(k2,f(k1,x)) where g is the same algorithm

This is sometimes called "commutative encryption". (Your notation is a bit confusing: if f=g, why use different letters for them?)

Any double encryption, even non-commutative, is vulnerable to a meet-in-the-middle attack which makes breaking it only slightly more difficult than breaking a single layer of encryption.  It doesn't perfectly match your stated requirements, but threshold encryption is probably what you're after. (Strictly speaking, you seem to be asking for something deterministic.. perhaps what you really want, more generally, is Dec(k1,Dec(k2,c)) = Dec(k2, Dec(k1,c))?)

As an example (in the public key setting), suppose Alice has ElGamal public key g^a and Bob has ElGamal public key g^b. Then think of g^(a+b) as their threshold key. Someone can encrypt a message m as (g^(r), m * g^(r{a+b})). Alice can "strip away" her contribution g^(ra) from the 2nd component, leaving a normal ElGamal ciphertext under Bob's key. Or Bob can strip away g^(rb) first, leaving a normal ElGamal ciphertext under Alice's key. One party by itself cannot obtain the plaintext.    </snippet></document><document><title>Prefix Sums and Their Applications [pdf]</title><url>http://sbel.wisc.edu/Courses/ME964/Literature/prefScanBlelloch1990.pdf</url><snippet /></document><document><title>Given a polynomial-time algorithm, can we bound its running time more precisely? </title><url>http://cstheory.stackexchange.com/questions/5004/are-runtime-bounds-in-p-decidable-answer-no</url><snippet>  If by "more precisely" you mean linear, that's one of the major challenges in computer science: finding either a tighter theoretical bound on a known algorithm, or finding a variant algorithm that's so much faster that it can be classified using, for example, `O(n log n)` or `O(log log n)`, rather than polynomial time.

If, on the other hand, "more precisely" is taken to mean, for example, `O(n^2)` rather than `O(n^3)`, this is still considered a major leap, but if the difference is `O(2.8 n^2)` vs `O(3 n^2)`, this difference is largely only relevant for engineers and programmers rather than computer scientists, because the big O notation already implies that differences of a constant are included in the same class of functions. For example, `O(n^2)` is included in the `O(n^2 + n)` class of functions, because the `lim_n-&amp;gt;infinity of (n^2)/(n^2 + n) = 0`.

All of this is Algorithms 101 material; if you're at uni, ask a professor or TA. I meant: Given an *arbitrary* algorithm known to be in P, can we find a specific upper bound for its running time? Apparently (from the link), you can't-- it's undecidable.  Sorry, I wasn't trying to patronize you, I clearly misunderstood the premise. Glad you found an answer!  </snippet></document><document><title>Merge Right: A generic template for set operations</title><url>http://alaska-kamtchatka.blogspot.ca/2012/08/merge-right.html</url><snippet>  Followup: http://gallium.inria.fr/~scherer/gagallium/comments-on-merge-right/

(the original post overstates the degree to which parametricity limits the search space of functions)</snippet></document><document><title>Running Time Analysis:  How do I determine how long a process will take in multiple running times...</title><url>http://www.reddit.com/r/compsci/comments/zzzq8/running_time_analysis_how_do_i_determine_how_long/</url><snippet>The truth is, this is homework related.  I am currently taking Data Structures, and have fallen a little behind.  I am looking for some kind of kick start into figuring this out.

So, I have to assume that the running time for an algorithm with input size of 256 is 1 millisecond. (Ignoring the effect of low-order terms.)  How much time will be required to process 512 items if the running time was 1. linear, 2. linearithmic, 3. quadratic, and 4. Cubic.  How large a problem can be solved in 240 milliseconds with the same running times?

Any insight?  "linearithmic"? That's new to me. From its position in the list I'm guessing your teacher means O(n * log n) but I've never heard that name before.  "linearithmic"? That's new to me. From its position in the list I'm guessing your teacher means O(n * log n) but I've never heard that name before.  "linearithmic"? That's new to me. From its position in the list I'm guessing your teacher means O(n * log n) but I've never heard that name before.    So your four cases correspond to the following in big-Oh notation.

1) O(n)

2) O(n log n)

3) O( n^2 )

4) O( n^3 )

Let's do the easiest case. If an algorithm is of linear complexity O(n), then if 256 = 1ms then 512 = 2ms. (doubling the size of your input doubles the processing time)

For the cubic case for example, if you were to double the size of your input, your algorithm would take 8 times as long ( 2^3 ), giving you 8ms. Thank you.  This is the kick start I was looking for, although I am having trouble with the nlogn time.  I've never been good with logarithmic values.  Any hints?

How would I work backwards to solve the second part?  If I have 240ms, and 1ms can handle an input of size 256, that just means 240 * 256 is how large of an input I can handle in this time, correct?  So, for O(n^2 ), I would cut the amount of input by how much?  Ugh, my brain hurts.  If the running time is linear, that means it is a proportional relationship (ignoring low-order terms). So the basic intuition of "If you double the work, you double the time it will take" applies. You can see this using simple algebra: If f(n) = c * n, then f(2n) = c * 2n. Dividing the second term by the first, we see that the time taken to solve a problem of size 2n is twice the time taken to solve one of size n.

For the other ones, write down a simple equation for f(n), plug in 2n to f, and see how much bigger it is than f(n). That's how many times bigger f(2n) is than 1 millisecond. For example, if f(n) = x * n^2 , how much bigger is x * (2n)^2 than x * n^2 ?

Alternatively, just plot it out and check it out for yourself. The cool thing about asymptotic growth is that for sufficiently large n, it really doesn't depend on anything but the high order term, even neglecting the leading coefficient. So you can take its value numerically at N = some large value, and at 2N, and divide the results to see how many times bigger the second is without solving it algebraically. Wow, thanks for the great clarity.  This brings up a question about nlogn.  I have trouble with logarithms, and can't determine how to convey the time it would take using your method.  Also, do you have any insight on how the second part of the problem is solved (how large a problem can be solved in 240ms)?  I am trying to work backwards from the first part as in, if it took 4 times as long with quadratic time, then if you divided 240ms by 4, that would be the similar time I could multiply by the input size of 256.  Does that make sense?  So, in linear time, I can solve 240 * 256 input size.  For quadratic it would be 60 * 256 for the largest input size.

EDIT:  my asterisks made *italics*  derp &amp;gt; I have trouble with logarithms, and can't determine how to convey the time it would take using your method.

The reason for that may be that with any polynomial growth, an increase in input by some multiplier will yield an increase in output by another multiplier. (The exact ratio is the input multiplier raised to the power of k, where k is the degree of the polynomial. Try the algebra: plug in f(c*n)/f(n).) But this relationship doesn't hold for logarithms and exponentials, so it won't hold for a hybrid like n log n. That is, if f(n) = n log n, the ratio of f(2n)/f(n) isn't constant, even when lower order terms are neglected. Nonetheless, you can still plug in ((2n) log (2n)) / (n log n) with n = 256 to see what multiplier you get. There's a bit of a fib factor due to low order terms, the base of the logarithm, etc., but I think it's fair to assume that the function is exactly f(n) = n log n. (Technically, you could say it's f(n) = C * n log n where C is the dimensioned constant "1 ms / 256".)

The key growth properties of exponentials is that they have a doubling time: a constant *increment* change in input produces a constant *factor* change in output. This is fundamentally different from polynomials. Likewise, for logarithmics, a constant multiple of input produces a mere constant increment in output.

It helps to think of this in an adversarial scenario: Imagine that you're trying to solve an exponential problem, like guessing a password. If it takes you a year to try all possibilities, all the enemy needs to do is add one more character to their password, and even if the character is only either a 1 or a 0, it'll take you two years to break that password. Conversely, if the enemy is trying to break your password, all you need to do is a little more constant work (one or two more characters) whenever their computational power *multiplies*.


As for your second question, once you write down the function (perhaps with the constant C = 1ms / 256 like I had above), you should be able to plug in 240ms on the left and solve for n.  The semester just started. Did you just not go after the first day? lol, yes I have been going to class, but I have fallen behind on the required reading for the course.  We did nothing like this in class and were expected to figure it out, which I am guessing meant read and figure it out.  You *really* should be able to figure this out just based on what the professor's taught in class.</snippet></document><document><title>A paper about a computer based on surface wave propagation</title><url>http://www.reddit.com/r/compsci/comments/zyn7a/a_paper_about_a_computer_based_on_surface_wave/</url><snippet>I read a paper about the idea of a computer based on surface wave propagation. The waves activated outputs when multiple waves reached the output point at the same time. Unfortunately, the PDF was on my old computer, which was stolen, and now I can't find it. Has anyone seen it?   Do you mean something like [electronic analog computers?](http://en.wikipedia.org/wiki/Analog_computer#Electronic_analog_computers)
Or what kind of waves do you refer to? No, it was surface waves. The two examples they give are ripples in a pool of water and compression waves/vibrations/acoustic waves in some sort of homogeneous surface (although now that I think of it you could have a 3D one with compression waves in a homogeneous mass). Part of the idea is that you can have them constructively or destructively interfere to create more complex logic.  &amp;gt; Part of the idea is that you can have them constructively or destructively interfere to create more complex logic.

The problem is, wave equations are pretty much linear, usually. So all your outputs are nothing but linear combinations of inputs, and that's not interesting at all.  This is a very similar paper to what you are describing. 

[Multi-Frequency Magnonic Logic Circuits for Parallel Data Processing](http://arxiv.org/ftp/arxiv/papers/1105/1105.4671.pdf)  You may want to check out [John Bush's](http://www-math.mit.edu/~bush/) publications. 

Even if he wasn't involved in the particular work you're looking for (and I didn't go digging through all his papers), it sounds like you'll be interested in the work he does. For example, using bouncing water droplets to simulate (some aspects of) quantum mechanics: http://www-math.mit.edu/~bush/PNAS-2010-Bush.pdf   &amp;gt; We present a new method for the real-time simulation of fluid surface
waves and their interactions with floating objects. The method
is based on the new concept of wave particles, which offers a simple,
fast, and unconditionally stable approach to wave simulation.

That one? No, this was focused on computation using waves, not the study of waves themselves. No particle simulation in the paper I'm thinking of. Thanks though.</snippet></document><document><title>Transactional Synchronization Extensions - fine-grained locking in hardware on x86 [pdf]</title><url>https://intel.activeevents.com/sf12/scheduler/downloadFileCounting.do?sesfid=DF74BAFE8A519B34CE253792C24E3C3F&amp;amp;abb=B54C1E29C5FB069696558337F9217640&amp;amp;fn=2AA51E85CEB830E1D0A44648588B6247974EDFE7768607DCCBE61C8BED76E8D6</url><snippet /></document><document><title>Neural networks with biologically plausible accounts of neurogenesis</title><url>http://cogsci.stackexchange.com/q/214/29</url><snippet>  I'm pretty sure that neurogenesis only happens in a couple of places in an adult brain (as far as is known). The hippocampus and and one other place I don't remember.

I think what the person who asked this was looking for was something more along the lines of "recruitment."</snippet></document><document><title>I'm trying to remember an O(n) solution for finding the top K elements of an arbitrarily long stream. Anyone know it?</title><url>http://www.reddit.com/r/compsci/comments/zwywo/im_trying_to_remember_an_on_solution_for_finding/</url><snippet>This is the problem:

You are reading data records off a stream. You don't know how long the stream is or when it will terminate. You should treat it as a stream of infinite length with elements in random order. Your job is to maintain a current list of maximum top k records as elements are read off the stream, processed, and discarded. How can you do this in O(n) time?

I remember the solution required a buffer where:

    buffer length = 2 * k

I believe it was a variation of a divide and conquer algorithm but only goes 1 level deep, thus giving it O(n) complexity.  The result after reading 2 * k records is the right side (above k) will contain an unordered list of top k records.

You could also do this with a type of min/max heap but that would be O(n log **k**). That's not good enough. We want O(n).
 
Does anyone know the name of this algorithm and its details? I don't remember the optimal solution and it's been driving me nuts.

edit 1: Forgot, the heap method would be O(n log k). derp.  
edit 2: n is the number of elements read so far  
  The heap approach would be O(n log k).

The standard best solution to the selection problem is the [median-of-medians](http://en.wikipedia.org/wiki/Selection_algorithm#Linear_general_selection_algorithm_-_Median_of_Medians_algorithm) approach. It uses two different kinds of recursive calls, and under analysis it ends up being O(n) rather than O(n log n) or worse.

I haven't thought about how this would be done in the streaming case. But let's say that we only need to find the answer once for every b elements read off the input stream. Then it would take O(k + b) to find the new best k elements after accounting for this additional length-b input. If b is &#937;(k), then that amortizes to O(n). The heap approach would be O(n log k).

The standard best solution to the selection problem is the [median-of-medians](http://en.wikipedia.org/wiki/Selection_algorithm#Linear_general_selection_algorithm_-_Median_of_Medians_algorithm) approach. It uses two different kinds of recursive calls, and under analysis it ends up being O(n) rather than O(n log n) or worse.

I haven't thought about how this would be done in the streaming case. But let's say that we only need to find the answer once for every b elements read off the input stream. Then it would take O(k + b) to find the new best k elements after accounting for this additional length-b input. If b is &#937;(k), then that amortizes to O(n). This may be the algorithm I'm looking for, however I don't remember the 5 partition part.

Hmmm.

I was thinking it was 2 partitions and I'm positive it required having a 2*k buffer though. It may have included a quickselect to find an initial k'th element for the pivot, which would get averaged to O(0) for a sufficiently large stream. My memory of it is just too shoddy. This may be the algorithm I'm looking for, however I don't remember the 5 partition part.

Hmmm.

I was thinking it was 2 partitions and I'm positive it required having a 2*k buffer though. It may have included a quickselect to find an initial k'th element for the pivot, which would get averaged to O(0) for a sufficiently large stream. My memory of it is just too shoddy. I think the heap approach is a right answer. Because if k is fixed, then log k is a constant so that O(n log k) = O(n).

The practical problem here is that some O(n) solutions are worse than O(n) for a stream. An algorithm might be O(n) for the first n elements of the stream but you'd want to be able to update the result very quickly upon seeing the n+1st element, rather than having to run the O(n) algorithm over again on a list of size n+1. That gives something like O(n^(2)) behavior. As long as k is fixed updating the heap is O(1).

Selection is also a solution for similar reasons: Some implementations of selections when selecting for the top k-th element will leave the top k elements grouped together in the array. You can pull these off and use them as your answer. Afterward, updating here is also O(1) because you're calling select on a fixed number (k+1) of elements. In practice this is going to be slower though than the heap solution for large enough k because that's O(k) instead of O(lg k).

What you might be thinking of is reading elements k at a time and using a version of median-of-medians which leaves the top-k elements grouped. Then you keep a buffer of size 2k, when you have read a multiple of k elements call median-of-medians and keep just the top k -- that's your answer. Update by reading the next k into the other side of the buffer and rerunning.  What is *n* if this is a potentially unbounded stream? Do you want to keep the top k as you stream in, in time linear in the number of elements you've read in so far?

If you have the whole list you can do it in O(n) with a variation on quickselect, but that would not generalize to having a stream where you want to keep the top k you've seen so far. The heap solution is the only one I can think of in that case. Although that would be O(n log k), not O(n log n), you can keep a bounded max heap of size k.

EDIT: I think you may be thinking of the quickselect variation, since you mentioned divide and conquer. It's not clear to me how this could be applied to the stream version of this problem.  Are you really certain there even is a O(n) algorithm for this scenario?

Besides, the heap approach is only O(n log k) if all items need to be added to the heap, i.e. if they're in sorted order and every new item you get is larger than the smallest in the heap. The best case is obviously if the first k elements also happen to be the top k. Than, it would become O(n + k log k).

If you expect the data to be (mostly) sorted, you can probably come up with a heap/storage-variant that will have O(1) properties for adding a new largest and removing the smallest element (maybe something with a ring-buffer of size k, replacing the smallest with a new largest would be just a single array-store and adjusting the offset).
Something like that may be possible for any ordering of the elements, which would allow you to make it O(n) with a bit more buffersize than 1*k, and what you're looking for.

If the data is mostly random, I wouldn't expect the overhead for that special behavior to be worth it though. Especially for small k, the amount of actual heap-insertions would be fairly low, since most data will be discarded by the initial check with the current kth value.
So the constants involved with any optimization, to make keeping track of the current top k come closer to O(1), could easily outweigh just using a basic minheap with O(log k)... At least for smallish k's.

Btw, there is obviously a O(n) solution: just keep track of all elements in a random-access data structure and apply the median-of-medians to it ;) But that would require O(n) space, rather than O(k).  The heap solution would have O(n log k) complexity assuming you keep a min/max heap of k elements and the unbounded stream has an upper bound of n elements. What do you mean by n here? Are you rather looking for an O(k) solution?

EDIT: O(k) doesn't actually make sense since you have to at least read the input once, which would be O(n) so I guess I figured what you meant by it. Guess it is bed time for me, sorry.   [deleted]  i'm surprised at the complexity of most of the responses...

computing the max is O(n). computing the top k maxes is just O(k)*O(n) = O(n) since k is constant. you just, instead of maintaining a max, maintain an array of k maxes. then for each new record you read from the stream, you compare it to each record in the array. if it larger than any of them (k operations), you search the list for the smallest record (k operations) and insert the new record in it's place.

so it's 2 * k operations per record where k is constant. so O(n) for the whole shebang.

or am i missing something? So you want to consider k a constant? Sure, consider this; using k = 100 and n = 10000000.

You'd have 100*10000000 comparisons (well, sligtly less because you can skip the first 100), and after that the search for the smallest value again, which is on average 100/2 comparisons. Increasing k to 200 will also double the amount of comparisons required. So that's why its called O(kn), the amount of computations increase both with a larger k or a larger n.
For small k, this may be efficient enough. But for larger k, it may help to only have to do log(k) operations or less. Even if the amount of work per actual operation is more.

So keeping track of the smallest value already reduces the required work quite heavily, the choice for the actual data structure to keep track of the k elements is somewhat arbitrary, but it helps if it can do insertions, removals and finding the (new) smallest element fairly efficient. So you want to consider k a constant? Sure, consider this; using k = 100 and n = 10000000.

You'd have 100*10000000 comparisons (well, sligtly less because you can skip the first 100), and after that the search for the smallest value again, which is on average 100/2 comparisons. Increasing k to 200 will also double the amount of comparisons required. So that's why its called O(kn), the amount of computations increase both with a larger k or a larger n.
For small k, this may be efficient enough. But for larger k, it may help to only have to do log(k) operations or less. Even if the amount of work per actual operation is more.

So keeping track of the smallest value already reduces the required work quite heavily, the choice for the actual data structure to keep track of the k elements is somewhat arbitrary, but it helps if it can do insertions, removals and finding the (new) smallest element fairly efficient. i'm surprised at the complexity of most of the responses...

computing the max is O(n). computing the top k maxes is just O(k)*O(n) = O(n) since k is constant. you just, instead of maintaining a max, maintain an array of k maxes. then for each new record you read from the stream, you compare it to each record in the array. if it larger than any of them (k operations), you search the list for the smallest record (k operations) and insert the new record in it's place.

so it's 2 * k operations per record where k is constant. so O(n) for the whole shebang.

or am i missing something? k is not a constant.  If record length (number of bits to represent a single record) is bounded, we can construct a monotonic minimal perfect hash function over the range of record values which will remain totally sorted in O(n). Equivalent entries are overwritten. Space is disregarded. Hmm. That could work, as long as the # of bits can fit into RAM. We wouldn't be able to do it with 64 bit integers. It's definitely possible with 32 bit integers and maybe up to 2^35 to 2^37 depending on hardware available.

We lose duplicates though. On the other hand, we could store that info as a separate smaller structure for storing counts and timestamps. It would only need to hash k * m duplicate records and sort once by timestamp. Hmm. That could work, as long as the # of bits can fit into RAM. We wouldn't be able to do it with 64 bit integers. It's definitely possible with 32 bit integers and maybe up to 2^35 to 2^37 depending on hardware available.

We lose duplicates though. On the other hand, we could store that info as a separate smaller structure for storing counts and timestamps. It would only need to hash k * m duplicate records and sort once by timestamp.   Hmmm....

So, suppose you have a buffer of k elements, then add k more from the input stream, then find the median in O(k) (using quickselect + median of medians) and drop the lower half of the buffer in O(k) again. So you shrink it back to k elements in O(k), and only need to do it once every k elements, so the amortized complexity is O(1) \* O(n).

EDIT: that's basically what [Workaphobia said](http://www.reddit.com/r/compsci/comments/zwywo/im_trying_to_remember_an_on_solution_for_finding/c68fs7j). But you'd have to repeat that O(k+b)-quickselect a total of O(n/b) times, repeatedly re-checking the same k elements. Is that really still O(n)? I'd say it works out to O((k+b) * (n/b)). And since b is a constant, that would become O(kn), wouldn't it?

Edit: With b = k, that would work out to O(2n), which basically is still O(n). But that will be the best case, so the constants involved in that best case may easily be much worse than just a simple heap-based  select. It would obviously heavily depend on the size of k and the ordering of the input data.        The really important part is how much time it takes to evaluate one additional element, given that you are looking for the top k elements. O(n) isn't the important part, but O(k), so that you can scale it to arbitrarily large lists of the top k elements without bogging down. I don't understand your comment. We are processing an infinite supply of records so n stands for the number of records seen so far at the current time.

There is no way not to read n elements.   Heap method is the correct approach.The algorithm has to be worse than linear since:

-&amp;gt; You have to look @ every element in the stream (so linear at least)
-&amp;gt; The result isn't 1 element, it is a bunch of them. Just because the result isn't 1 element doesn't mean you can't still have an O(n) solution.  For example, count sort can sort n single digit numbers in O(n). OP gave us no information to believe that we are allowed to do more than pairwise comparison. Hence, count-sort and such are not applicable here.</snippet></document><document><title>Threads Cannot be Implemented as a Library [PDF]</title><url>http://www.hpl.hp.com/techreports/2004/HPL-2004-209.pdf</url><snippet /></document><document><title>Learn To Code, Get A Job: Treehouse Offers Free Courses To 2,500 College Students</title><url>http://techcrunch.com/2012/09/14/learn-to-code-get-a-job-treehouse-offers-free-courses-to-2500-college-students/</url><snippet>   This is bullcrap. There are NO jobs for programmers and most of the junior programmer that apply, don't even know how to program a simple buzzfuzz program, even the ones who has a master in computer science. They are selling you a broken dream like kids hoping one day he'll be a rap star, pop star or a famous actor. I gonna make a guess that you are unemployed and you happen to be one of those 'idiots' you speak of I have two jobs. Your guess is wrong. You're** False. His grammar was correct. Unless it originally said "your wrong" and he edited it to make your correction seem erroneous. I actually doubt he changed it. Around a month ago, I went around reddit and was incorrectly editing things. So if you had said "I went there once", I would have said "they're**". It's called screwing with people. 

Also, it's grammer, not grammar.  Touche. ;)

I heartily approve of this trolling.  Yes because I actually give a flying fuck what you think of me you nigger bastard. 

Also, it's motherfucking touch&#233;. You forgot the accent aigu&#235;. Oui oui fils de pute.  You misunderestimate the gravity of the "chocons" monsieur "Pepatalude".

Reconsider.

Best,
Papa</snippet></document><document><title>How to inplace merge 2 arrays in linear time ?</title><url>http://www.reddit.com/r/compsci/comments/zvozv/how_to_inplace_merge_2_arrays_in_linear_time/</url><snippet>Given two sorted arrays, one of size n, and the other of size m+n whose first m elements are sorted and it has n extra space, how to merge them inplace in the second array ? 

Or, another variation could be (which the C++ STL provides as inplace_merge), given a single array with size n+m [which are given as well] (first n sorted elements of the first array, next m elements of the second array), how do I sort it in-place without using external temporary array ?

Could someone explain the basic idea behind a linear time inplace merging algorithm simply or provide some hints for it ?

Thanks;  This sounds like a homework question the way you ask it.

Off the top of my head you need have three pointers / index variables.

One starts at position m+n of array m+n.

One starts at position m of array m+n.

One starts at the end of array n.

for each iteration, you compare the item at the m and n pointers, see which one goes last, then put that at the m+n pointer and decrement the appropriate two pointers. and repeat until all pointers are at 0 (the last two will hit zero at the same time).

this is guaranteed to work because if all elements in n came after m, then there is room for all of n at the end of the array. And if all elements of m came before any elements of n, well you would have moved them all out of the way before you needed to put any from n in. nop, not for college atleast, but it has been asked in an interview to a friend and I couldn't immediately figure out a solution.

If it was for homework it would have been much easier for me to search for a ready-made implementation, a research paper somewhere and get it done in ~30 minutes.

Thanks a lot for your reply :) I think I got it, you do like the standard merge but from the opposite direction so we can keep it linear time instead of having to move the array.

I'll try to implement now, see if I could make it work correctly, thanks a lot frankster. Yep, the trick is filling it from the back instead of the front. Done :)

My implementation w/ testing: http://pastebin.com/AM3re8wh

Thank you very much :D [deleted] tabs ftw ! BSOD to the infidels using whitespace !

Netbeans\Code::Blocks default options used to convert tab to whitespace, and then everytime I have to press 4+ backspaces to remove the whitespaces when I need to modify a loop or restructure some block :/ [deleted] ah, I wasn't much sure also about what you meant :)

Yup you're right, I used to/still do care about those and variable names/comments when coding for uni assigments or projects for other people, but for most of the summer I've been coding for myself only or for online judges like topcoder, codeforces etc where the only important things are correctness and typing speed (for the easy problems), with very little chance to ever look at the code again once its submitted. This sounds like a homework question the way you ask it.

Off the top of my head you need have three pointers / index variables.

One starts at position m+n of array m+n.

One starts at position m of array m+n.

One starts at the end of array n.

for each iteration, you compare the item at the m and n pointers, see which one goes last, then put that at the m+n pointer and decrement the appropriate two pointers. and repeat until all pointers are at 0 (the last two will hit zero at the same time).

this is guaranteed to work because if all elements in n came after m, then there is room for all of n at the end of the array. And if all elements of m came before any elements of n, well you would have moved them all out of the way before you needed to put any from n in.  I'm not a compsci student; I work more on HCI rather than algorithms, so I may be wrong on some of these.

For both of them, I see some sort of swapping thing happening.  i.e.:

    function swap(a, b)
        var tmp;
        temp = a;
        a = b;
        b = a;

Use appropriate pointers/references as needed, pass array_a[i] and array_b[j] to swap them. 

What you'd then do is (for either one of them) probably something like iterate through each index of the bigger array, and check if swapping is  required against each index of the smaller array. When null is found, swap as well.

i.e. (VERY poorly written and unoptimized code):

    for (var i = 0; i &amp;lt; larger.length; i++)
    {
          for (var j = 0; j &amp;lt; smaller.length; j++)
          {
                if ((larger[i] &amp;lt; smaller[j]) OR larger[i] == null) 
               { 
                   swap(larger[i], smaller[j]); 
                   // break out for loop if possible, since it is sorted smaller[j] will not be bigger than smaller[j+1] 
               }
          }
     }

There are probably ways to optimize it, but they're not popping to mind right away (9am is too early).  But I think that's how I'd approach this problem.  If you need it in linear time, then it will be more difficult... For your swap, if you want to impress with esoteric bitfiddling knowledge and not using extra memory for a temp var, you can use the following:

http://en.wikipedia.org/wiki/XOR_swap_algorithm

    left = left XOR right;
    right = left XOR right;
    left = left XOR right;

Presto!  In-place bitwise swap. that would work only if the array is represented by single integer elements btw. Though, technically, if all the objects are the same size. You can do it that XOR as well. YMMV. I think so, but you would have to overload the default xor operator ?! I tried it and doesn't look like C++ accepts to xor structs. XOR's just a bitwise operation. I don't think you would have to, but I'm not exactly sure now. In theory at least, there isn't a problem if all the objects are the same size.

Edit: working on implementing it in c, but I'm not exactly sure how to treat a struct as a region of memory... a struct is contigious region of memory much like an array. the problem is that the xor operator is overloaded only for integers in C\C++. But yup, theoretically it should just work.

I think the easiest way would be using the sizeof operator to know how many bytes it has, then xor it byte by byte after casting it to char*. For your swap, if you want to impress with esoteric bitfiddling knowledge and not using extra memory for a temp var, you can use the following:

http://en.wikipedia.org/wiki/XOR_swap_algorithm

    left = left XOR right;
    right = left XOR right;
    left = left XOR right;

Presto!  In-place bitwise swap. Or, you know, [the xchg instruction](http://www.fermi.mn.it/linux/quarta/x86/xchg.htm).   frankster's solution works well for your first question, but I don't believe there is a linear time algorithm for merging in the variation you give. The C++ STL has one, or so its claimed, http://www.cplusplus.com/reference/algorithm/inplace_merge/

Scroll down for the "Complexity" part.

But yup, my main concern was about the first question, the second one I think would be too difficult to be asked in a 1hr interview or short quiz. The C++ STL has one, or so its claimed, http://www.cplusplus.com/reference/algorithm/inplace_merge/

Scroll down for the "Complexity" part.

But yup, my main concern was about the first question, the second one I think would be too difficult to be asked in a 1hr interview or short quiz. There are standard in place sorting algorithms. So the second question just devolves to "have you read/been taught any in-place sorting algorithms?" So its easy if you recall, hard if you have to work it out from first principles. I wouldn't consider it an amazing interview question (unless they specifically wanted to test computer theoretical knowledge).

BTW if you're interested in algorithms, the coursera algorithms course is quite good. It discusses several algorithms from a fairly rigorously mathematical point of view. I found it a little challenging sometimes but rewarding. https://www.coursera.org/course/algo   Sounds like you are trying to do the merge, from merge sort http://en.wikipedia.org/wiki/Merge_sort     </snippet></document><document><title>DFA that accepts all binary strings divisible by 5.</title><url>http://www.reddit.com/r/compsci/comments/zv6sg/dfa_that_accepts_all_binary_strings_divisible_by_5/</url><snippet>The problem is as follows:

Model a DFA such that it accepts all binary strings that begin with a 1, and are divisible by 5, reading right to left. 

i.e. 101 is an acceptable answer but 0101 is not. 

I spent an exorbitant amount of time on this problem until I reached what I thought was a good solution to it:

I constructed 5 machines (0%5, 1%5, 2%5, 3%5, 4%5) that when reached would step into another of the 5 machines depending on a 'clock' built on each of these machines. This corresponded to the number being added since for every bit it can be said to add either 2,4,8, or 6 in that order - repeated.

Regardless, my approach *works*. What I can't get my head around is how this could be done in DFA with only 5 states - which is apparently what the answer is. I'm awaiting my professor to post his solution to it but at the moment all I can think of is that his solution is wrong or he actually composed one that examines the data left to right which is an entirely more simplistic problem. 

If anyone has any insight into this, I would love to hear how this is solved in 5 states.

11001000 (200) is a good number to test if your DFA works as it is not a number divisible by 5 if read the other way and goes into the case where a simple method of solving the problem starts to unravel.

  **Edit**: I had misread OP's problem as reading from left-to-right instead of right-to-left.  I answer the right-to-left problem in a grandchild of this post.

Let's start by leaving aside this bizarre constraint that the number should start with a 1.  Then you can test divisibility by 5 using a 5-state DFA as follows: the 5 states are labeled by the 5 possible congruence classes of the number (as read so far) mod 5, i.e., 0, 1, 2, 3, 4, with 0 being both the initial and the only accepting (final) state, and state k has a transition to state 2k that is labeled 0 and to 2k+1 that is labeled 1 (here 2k and 2k+1 are computed mod 5): e.g., state 3 has a transition to state 1 that is labeled 0 and to state 2 that is labeled 1 because 2&#215;3&#8801;1 mod 5 and 2&#215;3+1&#8801;2 mod 5.

This DFA computes the congruence mod 5 of the number as you read it from left to right: adding a binary digit on the right (i.e., reading a new digit) multiplies the number by 2 and adds 0 or 1 according to that digit, so the DFA computes the congruence as it goes along.

Now you have this bizarre constraint that the number should start with a 1.  For this you need to alter the previous machine a bit: create a new state, let's call it V (for "void"), which is the initial state instead of 0, and V only has a transition to state 1 labeled 1, no transition labeled 0 (well, depending on your definition of a DFA, it may or may not be permitted to have no outgoing transition: if it is not, add yet another state F, for "fail", with a transition from V to F labeled 0, and transitions from F to F labeled 0 and 1; the state F is non-accepting, of course).  This uses 6 (or 7) states.  It is not hard to show that you can't do better, because there are indeed 7 different classes of prefixes (witnessed by the empy string (for V), "0" (for F), "1" (for 1), "10" (for 2), "11" (for 3), "100" (for 4) and "101" (for 5)) which lead to different behaviors on future strings, so they *must* correspond to different states: so 6/7 states are required and the automaton I described is minimal.

PS: Just for fun, [this regexp](http://www.madore.org/~david/weblog/2011-04.html#d.2011-04-25.1871) matches the multiples of 7 written in base 10. This is still just for left to right though, isn't it? The problem lies in reading right to left Oh, I missed that bit, sorry.  But it doesn't change much:

If you omit the bizarre condition about zeroes, you still need only five state, except now instead of representing congruence mod 5 of the number read so far they represent congruence mod 5 of "what has to be found at the left so as to get a multiple of 5".  The DFA for that is exactly the one I described, but reverting all the arrows (for obvious reasons): so now state k has an arrow labeled 0 leading to &#189;&#183;k mod 5, which is 3&#183;k mod 5 (because 3 is the inverse of 2 mod 5) and an arrow labeled 1 leading to 3k+2.

Now we have to add the bizarre condition again.  This time the cure for it is to duplicate state 0 in order to remember whether "the last (i.e., leftmost) digit read was a zero": so we have a state 0-just-read-a-zero (not accepting), a state 0-just-read-a-one-or-just-started (initial, and accepting), and states 1 through 4.  Both states 0 have a transition labeled 0 to 0-just-read-a-zero (and 1 to state 2), state 1 has a transition labeled 1 to 0-just-read-a-one (and 0 to state 3); the rest is as previously.  That 6 states are needed is proved by the fact that the 10 suffixes "0", "101", "01", "1", "11" and "011", leading to the 6 states in the order I described (when read from right to left) match 6 different sets of strings as prefixes.  (**Edit**: I had initially written 10 states, duplicating all, but only the 0 state needs to be duplicated.)

Also, if you want to reject the empty string, you need to ever-so-slightly change the "0" states, but you still use exactly 6 states. &amp;gt; so now state k has an arrow labeled 0 leading to &#189;&#183;k mod 5, which is 3&#183;k mod 5 (because 3 is the inverse of 2 mod 5) and an arrow labeled 1 leading to 3k+2.

Arrow labelled 1 leading to 3k-1.

&amp;gt; This time the cure for it is to duplicate every state in order to remember whether "the last (i.e., leftmost) digit read was a zero":

Do we need to do that for all states ? Surely, duplicating for the 0 state is enough. So in your original DFA, with state k -1-&amp;gt; 3k+2, and k-0-&amp;gt;3k, you simply split the 0 state into 0 (which is the accepting state) and 0'. 0 -0-&amp;gt; 0', and 0 -1-&amp;gt; 2. 0' -0-&amp;gt; 0' and 0'-1-&amp;gt;2. So 6/7 states needed. 

Suffix wise, you have some duplication going on there. The suffix '01' and '111' are in the same equivalence class. eg; 101 (5) and 1111 (15) . &amp;gt; Arrow labelled 1 leading to 3k-1.

No, 3k+2 is correct, because 2(3k+2)+1 is k (mod 5).

&amp;gt; Do we need to do that for all states ?

Ah, I don't know why I said that.  I'll edit my post to fix this. **Edit**: I had misread OP's problem as reading from left-to-right instead of right-to-left.  I answer the right-to-left problem in a grandchild of this post.

Let's start by leaving aside this bizarre constraint that the number should start with a 1.  Then you can test divisibility by 5 using a 5-state DFA as follows: the 5 states are labeled by the 5 possible congruence classes of the number (as read so far) mod 5, i.e., 0, 1, 2, 3, 4, with 0 being both the initial and the only accepting (final) state, and state k has a transition to state 2k that is labeled 0 and to 2k+1 that is labeled 1 (here 2k and 2k+1 are computed mod 5): e.g., state 3 has a transition to state 1 that is labeled 0 and to state 2 that is labeled 1 because 2&#215;3&#8801;1 mod 5 and 2&#215;3+1&#8801;2 mod 5.

This DFA computes the congruence mod 5 of the number as you read it from left to right: adding a binary digit on the right (i.e., reading a new digit) multiplies the number by 2 and adds 0 or 1 according to that digit, so the DFA computes the congruence as it goes along.

Now you have this bizarre constraint that the number should start with a 1.  For this you need to alter the previous machine a bit: create a new state, let's call it V (for "void"), which is the initial state instead of 0, and V only has a transition to state 1 labeled 1, no transition labeled 0 (well, depending on your definition of a DFA, it may or may not be permitted to have no outgoing transition: if it is not, add yet another state F, for "fail", with a transition from V to F labeled 0, and transitions from F to F labeled 0 and 1; the state F is non-accepting, of course).  This uses 6 (or 7) states.  It is not hard to show that you can't do better, because there are indeed 7 different classes of prefixes (witnessed by the empy string (for V), "0" (for F), "1" (for 1), "10" (for 2), "11" (for 3), "100" (for 4) and "101" (for 5)) which lead to different behaviors on future strings, so they *must* correspond to different states: so 6/7 states are required and the automaton I described is minimal.

PS: Just for fun, [this regexp](http://www.madore.org/~david/weblog/2011-04.html#d.2011-04-25.1871) matches the multiples of 7 written in base 10. **Edit**: I had misread OP's problem as reading from left-to-right instead of right-to-left.  I answer the right-to-left problem in a grandchild of this post.

Let's start by leaving aside this bizarre constraint that the number should start with a 1.  Then you can test divisibility by 5 using a 5-state DFA as follows: the 5 states are labeled by the 5 possible congruence classes of the number (as read so far) mod 5, i.e., 0, 1, 2, 3, 4, with 0 being both the initial and the only accepting (final) state, and state k has a transition to state 2k that is labeled 0 and to 2k+1 that is labeled 1 (here 2k and 2k+1 are computed mod 5): e.g., state 3 has a transition to state 1 that is labeled 0 and to state 2 that is labeled 1 because 2&#215;3&#8801;1 mod 5 and 2&#215;3+1&#8801;2 mod 5.

This DFA computes the congruence mod 5 of the number as you read it from left to right: adding a binary digit on the right (i.e., reading a new digit) multiplies the number by 2 and adds 0 or 1 according to that digit, so the DFA computes the congruence as it goes along.

Now you have this bizarre constraint that the number should start with a 1.  For this you need to alter the previous machine a bit: create a new state, let's call it V (for "void"), which is the initial state instead of 0, and V only has a transition to state 1 labeled 1, no transition labeled 0 (well, depending on your definition of a DFA, it may or may not be permitted to have no outgoing transition: if it is not, add yet another state F, for "fail", with a transition from V to F labeled 0, and transitions from F to F labeled 0 and 1; the state F is non-accepting, of course).  This uses 6 (or 7) states.  It is not hard to show that you can't do better, because there are indeed 7 different classes of prefixes (witnessed by the empy string (for V), "0" (for F), "1" (for 1), "10" (for 2), "11" (for 3), "100" (for 4) and "101" (for 5)) which lead to different behaviors on future strings, so they *must* correspond to different states: so 6/7 states are required and the automaton I described is minimal.

PS: Just for fun, [this regexp](http://www.madore.org/~david/weblog/2011-04.html#d.2011-04-25.1871) matches the multiples of 7 written in base 10. Not sure why this is so upvoted considering it doesnt answer the op... The problem was to do it right to left. This solution is left to right.  A six state automata which does the trick : http://i.imgur.com/ay6lR.png .

Note: It accepts an empty string. What did you use to generate this diagram?  dot, perhaps? A six state automata which does the trick : http://i.imgur.com/ay6lR.png .

Note: It accepts an empty string. Welp, this does seem to solve it. Thank you for this, I honestly couldn't even fathom an answer as simple as this for the problem. If you don't mind, how did you come up with this? I don't follow how you came up with the various jumps - i.e. 1(0*) going to Mod_2, this seemed like it should link up to Mod_1 to me.  I can confirm that he (or maybe you?) is mixing up rights and lefts. Left to right can be done in 5 states pretty easily; right to left takes a minimum of 20 (which it sounds like your approach manages).

There should be a more succinct reason why, but at the very least you could apply a [DFA minimization algorithm](http://en.wikipedia.org/wiki/DFA_minimization) to your DFA and find that it is, in fact, the smallest DFA which accepts that language.

EDIT: Just realized that the restriction that the first digit can't be a 0 probably puts you in at more than 20 states... probably 24? It is entirely possible that I am reading the question wrong and he actually did want left to right, but I am happy that you are in agreement that 5 states cannot be managed with right to left. Thanks for your response :). I can confirm that he (or maybe you?) is mixing up rights and lefts. Left to right can be done in 5 states pretty easily; right to left takes a minimum of 20 (which it sounds like your approach manages).

There should be a more succinct reason why, but at the very least you could apply a [DFA minimization algorithm](http://en.wikipedia.org/wiki/DFA_minimization) to your DFA and find that it is, in fact, the smallest DFA which accepts that language.

EDIT: Just realized that the restriction that the first digit can't be a 0 probably puts you in at more than 20 states... probably 24?  ~~When you can't do your homework on your own then at least learn to google...~~

OK sorry I was an ass. ~~When you can't do your homework on your own then at least learn to google...~~

OK sorry I was an ass. This is a left to right DFA. If you had taken the time to actually read my post I mentioned the specifics of the problem. Next time, try not to be an ass and maybe actually read the post - or better yet, don't post. I even gave you a test case that would fail under the DFA you supplied. The solution to left to right is obvious, not so much right to left. Not to mention the fact I already turned it in. I just wanted to know if there actually existed a 5 state solution to right to left division by 5. Regular languages are closed under reverse. For minimality, use Myhill-Nerode to get minimum number of states.

And he is right - you shouldn't casually post homework questions. Specially when you haven't turned the answer in yet.  Please read the original post, I had turned in the answer 8 hours previous. Uggh. Open mouth - insert foot. Apologies.

Anyways, the rest of my post is still hopefully useful. You can just flip the transitions of a DFA to get a DFA for a reverse regular language. And the Myhill-Nerode is the formalization of what the top rated comment is saying - classes of prefixes. 
 haha, it's all good. I only posted this problem due to it being probably the longest I've ever worked on a single problem for a class. I was a bit irked when I was told it could be done in 5 states so I wanted clarification :). 

I originally tried reversing the left to right but I couldn't figure out how to transform the non deterministic nodes in any way that would make sense for the problem.  Do you go to ucf?
 Indeed I do hah  nice. how you liking the class so far? 
     I don't understand a lot about your problem, but it seems to me that working right to left should be just a simple as working left to right (except how do you know that you've reached the left hand end of the number).  Basically in both cases, each digit's contribution to the remainder is determined by its position (mod 5). The problem is this. I'll give you a 1 to start, now I'll start adding 0s. A completely random amount, but eventually i stop and give you a 1. From left to right, 10000000...1. Each 0 simply increased the original number by 2x. On the other hand for right to left we have 1...000000001 read right to left.  The 0s in this case do nothing to the number until that 1 is given. Then we have the issue of how much was added which without memory, we have no idea how many 0s occurred. This is what the 2,4,8,6 clock keeps track of on my machine - essentially what to add when that 1 is hit. Am I misreading your problem, or aren't there just 2 cases for the rightmost 3 bits where the number is divisible by 5? 000 and 101 binary (0 and 5 decimal). &amp;gt; the rightmost 3 bits where the number is divisible by 5? 000 and 101 binary (0 and 5 decimal)

Eh... no:

     5 = 00000 101
    10 = 00001 010
    15 = 00001 111
    20 = 00010 100
    25 = 00011 001
    30 = 00011 110
    35 = 00100 011
    40 = 00101 000

I.O.W. power 10 heuristics cannot be used for the power 2 math.
 Am I misreading your problem, or aren't there just 2 cases for the rightmost 3 bits where the number is divisible by 5? 000 and 101 binary (0 and 5 decimal).  What is a dfa? Deterministic Finite Automaton What is a dfa? What is a dfa? What is a dfa? Didn't downvote for the question, downvoted for the hissy fit. Seriously, grow up.  I just asked two people who have masters degrees in CS what a dfa is and neither one knew. Both knew what an Automaton
is and both knew what a Finite  Automaton is and both knew what a Deterministic Finite Automaton is. 

So the apparent problem is the use of a too obscure TLA. (Three Letter Acronym) with the requisite expansion of same to establish clarity in the posting. 

Most writing style manuals and guidelines specify that all acronyms be expanded next to the acronym once, near the beginning of the article, the first time the acronym is used.  I just asked two people who have masters degrees in CS what a dfa is and neither one knew. Both knew what an Automaton
is and both knew what a Finite  Automaton is and both knew what a Deterministic Finite Automaton is. 

So the apparent problem is the use of a too obscure TLA. (Three Letter Acronym) with the requisite expansion of same to establish clarity in the posting. 

Most writing style manuals and guidelines specify that all acronyms be expanded next to the acronym once, near the beginning of the article, the first time the acronym is used.  I found that kind of troubling actually. DFA is a pretty common TLA, and the context should make it apparent immediately if there were some confusion. In the courses I took, DFA was never used. It was always spelled out.  as with all language use, what is common in one geographic area, may have no such commonality in other areas. Did you take a models of computation or complexity theory course?</snippet></document><document><title>Best book to learn operating system design?</title><url>http://www.reddit.com/r/compsci/comments/zu7rl/best_book_to_learn_operating_system_design/</url><snippet>I'm looking into learning OS design, do you guys have any books that would be practical to learn with? Preferably with ideas explaining different kernel types and having you compile your own kernel, etc from the beginning(So I'm looking for one with exercises if possible)  Start with Tannenbaum's "Modern Operating System"

Then move on to here: http://www.osdever.net Start with Tannenbaum's "Modern Operating System"

Then move on to here: http://www.osdever.net Start with Tannenbaum's "Modern Operating System"

Then move on to here: http://www.osdever.net Start with Tannenbaum's "Modern Operating System"

Then move on to here: http://www.osdever.net Start with Tannenbaum's "Modern Operating System"

Then move on to here: http://www.osdever.net  The dinosaur book! http://www.amazon.com/Operating-System-Concepts-Abraham-Silberschatz/dp/1118112733 The dinosaur book! http://www.amazon.com/Operating-System-Concepts-Abraham-Silberschatz/dp/1118112733 I'm just now taking an OS class with this as the required textbook. I think it's pretty good! The dinosaur book! http://www.amazon.com/Operating-System-Concepts-Abraham-Silberschatz/dp/1118112733   I learned it in college. It is a little bit difficult for me to imagine how most people without huge dedication could learn OS design and write an OS by himself. So I would suggest to take an university course. An online course might also work. I learned it in college. It is a little bit difficult for me to imagine how most people without huge dedication could learn OS design and write an OS by himself. So I would suggest to take an university course. An online course might also work. It's actually not bad.  I wanted to learn OS design, so I decided to simply write my own OS, and figure out things as I go.  That was May, and I now have a fairly fully featured real time operating system for ARM Cortex-M4.  https://github.com/prattmic/F4OS

Obviously, it is far less complex than something like Linux, but it has all of the major features of memory management, task scheduling, IO abstraction, etc. What were the primary resources, book wise, you used to get started on something like this?       I think learning OS development using a Raspberry Pi is a wonderful approach. I highly suggest The University of Cambridge tutorial computer lab's tutorial, "Baking Pi - Operating Systems Development" http://www.cl.cam.ac.uk/freshers/raspberrypi/tutorials/os/  </snippet></document><document><title>Checking that the name of a person is valid based on n-gram distributions?</title><url>http://www.reddit.com/r/compsci/comments/ztx7g/checking_that_the_name_of_a_person_is_valid_based/</url><snippet>Here is the problem. I have a large database of first and last names (in the order of millions of entries). When a new name comes in, I want to check to see if this name is "valid" and not a bunch of spam, such as someone typing in non-sense (mashing the keyword) or stringing together a bunch of curse words.

I am thinking the best way to do this is to analyze the database and build a distribution of n-grams. When a new name comes in, I can compute the n-grams of the new name and compare it to the distribution of the database. If the distribution deviates, perhaps the name is invalid.

I'm just thinking aloud right now. Any help in pointing me in the right direction or past experience would be awesome.

  I can't weigh in on the technical discussion, but do have a question.  When you detect an invalid name, how do you plan on responding?  Google got into a fair bit of hot water when it decided it was only going to allow "real names" on Google+... people who had renamed themselves (legally) to "3ric" or "Dead" or "Scud", or people from other cultures who only had a 'first' name to begin with, or shared a name with a celebrity, or otherwise had strange names, found themselves banned from the network without warning.  Of course you want to detect spammers, just keep in mind that there are many edge cases and acting too quickly on false positives risks angering your users. Yep. Sounds like a recipe for false negatives. That being said, if someone is mashing the keyboard: "sdgjnrsklgj" - you might check for too many consonants in a row in such a way that doesn't make sense. Yep. Sounds like a recipe for false negatives. That being said, if someone is mashing the keyboard: "sdgjnrsklgj" - you might check for too many consonants in a row in such a way that doesn't make sense. I can't weigh in on the technical discussion, but do have a question.  When you detect an invalid name, how do you plan on responding?  Google got into a fair bit of hot water when it decided it was only going to allow "real names" on Google+... people who had renamed themselves (legally) to "3ric" or "Dead" or "Scud", or people from other cultures who only had a 'first' name to begin with, or shared a name with a celebrity, or otherwise had strange names, found themselves banned from the network without warning.  Of course you want to detect spammers, just keep in mind that there are many edge cases and acting too quickly on false positives risks angering your users. Good point. I will certainly keep that in mind.  This should be [required reading](http://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/). Good read, especially insightful on why name validation is hard. Basically I'm getting the impression that it might not be worth trying to build such a system? Good read, especially insightful on why name validation is hard. Basically I'm getting the impression that it might not be worth trying to build such a system?  Using n-grams seems like a decent approach, but once you have them what do you plan to do with them? How are you thinking of distinguishing the "valid" and "invalid" names?

Do you have a set of names you are confident of their validity? Are we talking real given and surnames, or internet handles and such?
If you don't have a high-confidence ground-truth set, then the problem you are looking at is known alternately as "outlier detection", "[anomaly detection](http://en.wikipedia.org/wiki/Anomaly_detection)," or "one-class classification". You can use any standard approach to this problem (I'm most familiar with one-class SVM, but there are many others).


btw, you might want to try asking at /r/MachineLearning. Once I have the distributions of n-grams I was hoping I could apply outlier detection using a one-class SVM as you suggested. Currently, that is the only reasonable idea I have. I can't run anything supervised because I only have examples of valid names, not invalid names.

The names are given names, not internet handles. And yes, I have very high-confidence in my ground-truth dataset.   I have two fairly large lists of male and female names (mostly common American names) which I compiled for a data mining project.  If you'd like to add those to your list let me know.  

What techniques were you thinking of using to do the analysis?  For quickly prototyping data mining techniques, I would suggest the [rapidminer software](http://rapid-i.com/). I normally use Python and sklearn to quickly jump into the analysis. The list of names that you have compiled, are they first names? Last names? Both?  You might be in trouble unless you're 100% sure all your names come from the same language and culture.  A lot of people have mentioned problems with people with unusual or foreign names, but what about someone just making names up from real names? Between [this site](http://www.fakenamegenerator.com/) and my brain I've used quite a few pseudonyms in the past, none of which your database would detect. Names like John Smith and Homer Simpson are frequently used as fake names and also are the names of real people. I feel like this would be a pretty lame approach overall and you would get a lot of false positives and false negatives from people with unusual names or using random "real" names. I could maybe see it being potentially useful if it were part of a holistic attempt to gauge the likelihood of someone being a spammer, if you also took into account the user's activity, word selection, etc. or somehow figured out a way to "fingerprint" spam accounts and use heuristics to compare them to other accounts, but just analyzing names would never work.   Unfortunately, n-grams only apply to a specific language (English in your case) and names can be foreign and not follow English rules. Not saying this is better, but if I was doing something along these lines, I would fuzzy-compare the name to a list (Fuzzy because names tend to have slight variations that can't always be caught by your list.) I think that with several thousand names, you should be able to isolate the invalid entries reasonably well. And I guess while we're talking about fuzzy comparisons, you could make exception rules like accept all last names ending with "wski", "son", etc. But if you end up using n-grams, I'd like to know how it turned out! Thinking about something entirely different, now. I remember reading that there are (fairly simple) algorithms to generate dummy text for web templates and that it's not completely random. If it was random, you likely wouldn't be able to pronounce the words. Maybe an easier way to weed out keyboard slammers would be to check against the rules of those algorithms? Not sure where to find any, though. Good luck! I did this.  It works against keyboard slammers.

http://stackoverflow.com/questions/6297991/is-there-any-way-to-detect-strings-like-putjbtghguhjjjanika

 Thinking about something entirely different, now. I remember reading that there are (fairly simple) algorithms to generate dummy text for web templates and that it's not completely random. If it was random, you likely wouldn't be able to pronounce the words. Maybe an easier way to weed out keyboard slammers would be to check against the rules of those algorithms? Not sure where to find any, though. Good luck! Well if you took a Markov network and trained it using a bunch of text, sure, you could make it spit something out that would look like reasonable english (or whatever language you choose). I'm not sure how that is relevant to detecting if a name is invalid though.</snippet></document><document><title>Can someone help explain to me what protege is?</title><url>http://www.reddit.com/r/compsci/comments/zuc4t/can_someone_help_explain_to_me_what_protege_is/</url><snippet>Hi,

I'm currently taking a grad course on distributed databases. I know I'll probably get flamed for being dumb, but I have yet to have a database class before, so all this is new to me (although I have had some sql experience)

Our professor asked us to use protege and I just can't wrap my head around it.

I'm not even sure 100% what it does.  

Is it a DBMS? Whats all this noise about ontologies?

If someone could explain this to me like the idiot I am, I would greatly appreciate it.

(link: http://protege.stanford.edu/)

Thanks in advanced. </snippet></document><document><title>Computational Complexity: What happened to the Parallel Random Access Machine?</title><url>http://blog.computationalcomplexity.org/2005/04/what-happened-to-pram.html</url><snippet>  It's an older and amusingly written right before the explosion of massively parallel general purpose computing we're seeing today. Still, I wanted to open up some discussion about formal models of parallel algorithmic complexity. Have CUDA/OpenCL brought PRAM back to everyone's good graces? Or is some more "realistic"/restrictive model more popular? Are there many people even doing analysis of parallel algorithms these days? I took a graduate course in parallel algorithms that was all based on PRAM concepts.  Interesting mathematical tools, but they don't really translate all that well to any of the actual machinery that we have available to us.  PRAMs are completely theoretical devices that have idealized memory characteristics so that the math becomes provable.  

If you attempted to implement the types of algorithms that you can prove with a PRAM, you would quickly find that the results are very non-deterministic.  Which might be fine in practice, or it might not, you'll never know until you profile. I took a graduate course in parallel algorithms that was all based on PRAM concepts.  Interesting mathematical tools, but they don't really translate all that well to any of the actual machinery that we have available to us.  PRAMs are completely theoretical devices that have idealized memory characteristics so that the math becomes provable.  

If you attempted to implement the types of algorithms that you can prove with a PRAM, you would quickly find that the results are very non-deterministic.  Which might be fine in practice, or it might not, you'll never know until you profile. It's an older and amusingly written right before the explosion of massively parallel general purpose computing we're seeing today. Still, I wanted to open up some discussion about formal models of parallel algorithmic complexity. Have CUDA/OpenCL brought PRAM back to everyone's good graces? Or is some more "realistic"/restrictive model more popular? Are there many people even doing analysis of parallel algorithms these days? Unrelated. How did you get the Omega lambda expression beside your name? It's an older and amusingly written right before the explosion of massively parallel general purpose computing we're seeing today. Still, I wanted to open up some discussion about formal models of parallel algorithmic complexity. Have CUDA/OpenCL brought PRAM back to everyone's good graces? Or is some more "realistic"/restrictive model more popular? Are there many people even doing analysis of parallel algorithms these days? &amp;gt; Have CUDA/OpenCL brought PRAM back to everyone's good graces?

Can you please clarify why you consider the programming models provided by CUDA/OpenCL similar to the PRAM model?

&amp;gt; Or is some more "realistic"/restrictive model more popular? Are there many people even doing analysis of parallel algorithms these days?

I think the following quote answers your question (more in the pdf, of course):

&amp;gt; As a result, concurrent data structure designers compare their designs empirically by running them using micro-benchmarks on real machines and simulated architectures [9, 52, 97, 103]. In the remainder of this chapter, we generally qualify data structures based on their empirically observed behavior and use simple complexity arguments only to aid intuition. -- [1.1.4 Complexity Measures](http://www.cs.tau.ac.il/~shanir/concurrent-data-structures.pdf)

As a last note, maybe it's worth mentioning PRAM realization projects such as [REPLICA](http://www.vtt.fi/sites/replica/index.jsp) and [XMT](http://www.umiacs.umd.edu/~vishkin/XMT/index.shtml) (actually, I think that is an exhaustive list -- on still active projects that is). &amp;gt;Can you please clarify why you consider the programming models provided by CUDA/OpenCL similar to the PRAM model?

It's not a perfect mapping, but if you strip away the details of CUDA (SIMT, memory access times, blocks, etc..) and also imagine away the finiteness of GPU memory and register files, you still have a large number of processors executing identical code with common access to CRCW shared memory. 

You can view the space of complexity of a CUDA as the amount of shared memory needed for a given input of size n and the time complexity as either the number of global synchronizations/kernel launches or (more fine-grained) the number of operations executed on a particular processor across all kernel launches. 

 But CUDA is asynchronous, while PRAMs are totally synchronous because all processors (threads) execute in lockstep. This makes any comparison between them very difficult. PRAMs also provide strict memory consistency (stronger than sequential consistency), while CUDA doesn't (and afaik, strict consistency doesn't even make sense for an asynchronous architecture).

And to consider "memory access times" (cache effects, transferring data between different units in the system (because of the heterogeneity of the system), and so on) a detail is not entirely honest, because memory access times cannot be ignored in any real CUDA program (while for PRAMs, we can ignore it).

I also don't see why you consider "common access to CRCW shared memory" such a big deal, isn't this the situation for any common shared-memory programming model such as pthreads and so on as well? And, again afaict, CRCW only makes sense in systems with lockstep execution because how can we otherwise guarantee that processors execute some instruction in the same "step"?

The fixed number of registers and memory space is not a problem however, because these must be fixed for any PRAM realization (also, Brent's theorem).</snippet></document><document><title>Technical introduction to quantum query complexity</title><url>http://cstheory.blogoverflow.com/2011/07/quantum-query-complexity/</url><snippet /></document><document><title>Magic: the Gathering is Turing Complete</title><url>http://www.reddit.com/r/compsci/comments/zp7c1/magic_the_gathering_is_turing_complete/</url><snippet>[Magic: the Gathering is Turing Complete](http://www.toothycat.net/~hologram/Turing/)

A little while ago, someone asked ["Is Magic Turing-complete?"](http://draw3cards.com/questions/2851/is-magic-turing-complete) over on Draw3Cards. I decided to answer the question by actually assembling a universal Turing machine out of Magic cards such that the sequence of triggered abilities cause all the reads, writes, state changes etc. (That is, the players of the game don't need to make any decisions to be part of the Turing machine - it's all encoded in the game state.)

I kept meaning to do a bit more with the site before posting it to Reddit and places, but never got around to it. Eventually someone by the name of fjdkslan posted it over on the [Magic the Gathering subreddit](http://www.reddit.com/r/magicTCG/comments/zoojk/magic_is_apparently_turing_complete/). JayneIsAGirlsName suggested we repost it over here on /compsci, so... [here you go](http://www.toothycat.net/~hologram/Turing/) :)   One note, which I also posted on the /r/magicTCG thread: I understand that there are some issues with the Turing machine I'm using, Alex Smith's 2,3 machine: basically it's controversial whether it counts as a truly universal Turing machine or not.

I do have a more complicated version of the Magic Turing Machine which uses the smallest uncontroversial universal Turing machine, which has 18 colours rather than 3. You have to use a heck of a lot more Rotlung Reanimators to make that work, and it's a lot harder to understand, so I've never got around to making a website explaining that one. (Basically, you switch the roles played by Teysa and colours with the roles played by Rotlung Reanimator and creature types, and have loads more hacked Dralnu's Crusades so everything has about five creature types.) The Teysa version is just about simple enough to be comprehensible, which is why that's the version still on the site. From the page:
&amp;gt; This isn't the most common way of demonstrating Turing completeness, but it is one of the more understandable

What are the more common ways, out of interest? A common one is to implement some NP-hard algorithm\^Wproblem. That wouldn't demonstrate Turing Completeness at all. In order to be Turing Complete, you must be able to compute all of the computable functions. To show that a system can do this, you produce a Universal Turing Machine. A UTM takes as input the specification for another Turing Machine and simulates that machine's computation. Being able to decide some NP-Hard problem doesn't imply that you can compute all of the computable functions. 

Also, I will nitpick about your word choice. There are no NP-Hard algorithms. There are only NP-Hard problems (or languages if you want to be really precise). 

---

You might be mixing up proofs of Turing Completeness with proofs of NP-Completeness. To prove that a *problem* is NP-Complete you can transform it into an NP-Hard problem, show that the transformation takes polynomial time, and show that the problem is in NP. In the case of Turing Completeness, we prove that a computational model is Turing Complete by showing that we can construct a Universal Turing Machine using that computational model.  &amp;gt; Also, I will nitpick about your word choice.

With pleasure, as I was still sleeping and you're completely right.

However, note that a SAT-solving Turing machine _is_ a UTM, since the input machine can be described as a formula (Cook's proof shows how) and this formula can be fed to the SAT solver.  Hence, if you implement a SAT solver in Minesweeper, you have indeed implemented a universal Turing machine and hence proved Turing-completeness of Minesweeper. But it would be a slightly strange way to see it, at least IMO; just saying "I implemented a solver for an NP-hard problem in Minesweeper" removes an indirection.

The same holds for any other NP-hard problem (even one that is not in NP), since you can use it to solve SAT. No, this can't be right.

Looking [at the proof sketch](http://en.wikipedia.org/wiki/Cook&#8211;Levin_theorem#Idea) in the Wikipedia, this:

&amp;gt; since the input machine can be described as a formula (Cook's proof shows how)

is true only for TMs that terminate in a polynomial number of steps. Look, the number of boolean variables in the formula is explicitly defined as being proportional to p(n)^2 , there's a bunch of variables for every relevant cell at every relevant point in time.

Being able to simulate machines that are guaranteed to stop after a polynomial number of steps is really different from having an UTM. Uhm, need to look more at it. Thanks! By the way, as I understand it, this can't be right because Turing-completeness and its set of undecidable problems can't possibly intersect with any complexity theory problems.

Also by the way, I'm kind of hyped from reading and understanding most of the awesome ["On the (Im)possibility of Obfuscating Programs"](http://www.iacr.org/archive/crypto2001/21390001.pdf) (pdf warning), which employs Halting-problem-like approach to prove the impossibility of obfuscation running in polynomial time (and producing a polynomially inflated/slowed-down program), so I just wanted to share that with you!  &amp;gt; Also, I will nitpick about your word choice.

With pleasure, as I was still sleeping and you're completely right.

However, note that a SAT-solving Turing machine _is_ a UTM, since the input machine can be described as a formula (Cook's proof shows how) and this formula can be fed to the SAT solver.  Hence, if you implement a SAT solver in Minesweeper, you have indeed implemented a universal Turing machine and hence proved Turing-completeness of Minesweeper. But it would be a slightly strange way to see it, at least IMO; just saying "I implemented a solver for an NP-hard problem in Minesweeper" removes an indirection.

The same holds for any other NP-hard problem (even one that is not in NP), since you can use it to solve SAT.  There is [another proof of completeness](http://cstheory.stackexchange.com/a/2057/1037) (requiring the cooperation of the players) over on the theoretical computer science research Q&amp;amp;A. Heh. Indeed, it was that post on CSTheory that prompted me to come up with mine. The difference is that in Joe's answer on CSTheory, the *players* have to play the role of the Turing machine: "A looks at the state of B's mana, represents the state of A's mana on the previous turn. A applies the transition rule according to the universal (24,2) machine to B's state to obtain his new state." In my Turing machine, it's *the rules of Magic* that are playing the role of the Turing machine. No cooperation by the players is required except always choosing to say "yes" whenever the game offers them a "you may" option.  That's! Freaking! Awesome!  I'm curious more about what class of problem the game itself falls into, and where it fits into the continuum of games in terms of their computational difficulty (for example, chess is an easier game, computationally, than is go, which is essentially impossible to play well on a computer because for each move, there are typically between 100 and 200 options). Of course, it would be necessary to decide whether "the game" includes deck-building or not. Ultimately, what I want to know is, will there ever be a computer better at magic than the best human players? &amp;gt;than is go, which is essentially impossible to play well on a computer

This is a popular misconception, it used to be true, but computer go has come a long long way. Part of the advance is just based off of how much faster computers have become, but there have also been a number of advances in game play AI (largely Monte Carlo Tree Search as far as I understand). I could speak more on the topic, but I will conclude with my source:

http://en.wikipedia.org/wiki/Computer_Go#Performance

On the other hand I really am interested in your question. I would argue that the state space of mtg can easily exceed the state space go, however it all completely depends on what decks are being played. If two RDWs decks pay each other the state space and the branching factor is extremely, extremely low. However if there is someone playing control vs combo the decision making that has to happen is extremely large and 'solving' mtg in all cases like that, especially making a constructed deck in a given metagame would require huge amounts of new research. Most game playing AI research is done on games of perfect information and with deterministic rules. Both of these are thrown out with MTG. Not to mention that when looking at games like chess or go each turn has a relatively standard branching factor and are homogeneous in nature. With MTG each 'turn' consists of many different phases and each phase, specifically the main phase could easily have a branching factor of 50 or greater, in a storm deck or some of the more complex combo decks the branching factor could easily be much much greater. (for instance I play a deck in t2 that has a four card inf mana combo that requires about 5 to 6 different steps to generate one mana).

I could go on, but class is going to start soon. If anyone would like to discuss this more in depth leave a comment. There are a bunch of AI playing Magic Rules Engines out there. Some of the more skilled ones do use MinMax trees to figure out the best plays. The smartest ones also don't have as much Card Support due to AI playing complexity. I'd be quite impressed to see an AI successfully Pilot some of the more complex Combo decks in magic history (ProsBloom, Storm, etc). Building a specific AI for a combo deck would be easy. Building a generic AI that given a combo deck could find (and use) the combo would be significantly harder. There are a bunch of AI playing Magic Rules Engines out there. Some of the more skilled ones do use MinMax trees to figure out the best plays. The smartest ones also don't have as much Card Support due to AI playing complexity. I'd be quite impressed to see an AI successfully Pilot some of the more complex Combo decks in magic history (ProsBloom, Storm, etc). Do you happen to have any links?

Also on second thought I suppose that playing storm would be relatively easy, as all you do is calculate the probability of being able to do 20 dmg on any given turn. The real challenge is figuring out this stuff without it being hard coded. &amp;gt;than is go, which is essentially impossible to play well on a computer

This is a popular misconception, it used to be true, but computer go has come a long long way. Part of the advance is just based off of how much faster computers have become, but there have also been a number of advances in game play AI (largely Monte Carlo Tree Search as far as I understand). I could speak more on the topic, but I will conclude with my source:

http://en.wikipedia.org/wiki/Computer_Go#Performance

On the other hand I really am interested in your question. I would argue that the state space of mtg can easily exceed the state space go, however it all completely depends on what decks are being played. If two RDWs decks pay each other the state space and the branching factor is extremely, extremely low. However if there is someone playing control vs combo the decision making that has to happen is extremely large and 'solving' mtg in all cases like that, especially making a constructed deck in a given metagame would require huge amounts of new research. Most game playing AI research is done on games of perfect information and with deterministic rules. Both of these are thrown out with MTG. Not to mention that when looking at games like chess or go each turn has a relatively standard branching factor and are homogeneous in nature. With MTG each 'turn' consists of many different phases and each phase, specifically the main phase could easily have a branching factor of 50 or greater, in a storm deck or some of the more complex combo decks the branching factor could easily be much much greater. (for instance I play a deck in t2 that has a four card inf mana combo that requires about 5 to 6 different steps to generate one mana).

I could go on, but class is going to start soon. If anyone would like to discuss this more in depth leave a comment. Well, Go is known to be PSPACE-hard without ko rules, and is at least EXPSPACE-hard with basic ko rules and may be beyond EXPSPACE with superko (American/Chinese ko rule).

So it's fair to say that Go is difficult for computers. Chess may be as well, but is not easily parametrizable.

EDIT: Understated the complexity. Do you mean actually playing the game well or simulating the rules?

Because if its simulating the rules doesn't creating a turning machine in something make it exist in all of the problem spaces as it is a general computation machine, meaning that you can program any problem in it.

Also if you are just talking about simulating all of the rules that does not mean that searching the game space of the current meta-game is also in the complexity level. Or example there could be a card (or a combination of cards) that requires you to do a traveling salesmen problem, but that card costs 20045 mana and loses you the game. No one would play that card if they are trying to win. Of course this is a trivial example of what I am talking about, but I hope you understand my point in a more general sense. I mean answering the question "From the current board state, is there a line of play that is guaranteed to result in a win." I mean answering the question "From the current board state, is there a line of play that is guaranteed to result in a win." That question's pretty much answered by my Magic Turing machine combo (either the (2,3) one or the (2,18) one)): the game will terminate in a win for player A if the Turing machine it's simulating halts, and never terminate if the Turing machine doesn't. So no, it's not determinable in the general case. As sl236 [said on the other thread](http://www.reddit.com/r/magicTCG/comments/zoojk/magic_is_apparently_turing_complete/c66od99), you could set up the Turing machine to compute something unknown - something cryptographic, or a nice [unsolved problem](http://en.wikipedia.org/wiki/Unsolved_problems_in_mathematics) - and only halt on one of the possible answers; and I could easily set it up so that, say, Player C could have an action that would only let them win once the machine has halted. So no, I believe that question is proven unanswerable by my machine. That question's pretty much answered by my Magic Turing machine combo (either the (2,3) one or the (2,18) one)): the game will terminate in a win for player A if the Turing machine it's simulating halts, and never terminate if the Turing machine doesn't. So no, it's not determinable in the general case. As sl236 [said on the other thread](http://www.reddit.com/r/magicTCG/comments/zoojk/magic_is_apparently_turing_complete/c66od99), you could set up the Turing machine to compute something unknown - something cryptographic, or a nice [unsolved problem](http://en.wikipedia.org/wiki/Unsolved_problems_in_mathematics) - and only halt on one of the possible answers; and I could easily set it up so that, say, Player C could have an action that would only let them win once the machine has halted. So no, I believe that question is proven unanswerable by my machine. Also, your Turing machine isn't actually using Magic rules, since the players have actions that aren't mandated by game rules but are mandated to maintain the functioning of the machine. Heh. I've tried pretty hard to eliminate all such, but there's still the matter of Kazuul Warlord giving Player A a "you may" option which A has to always choose to take up. (I believe there are a few ways to eliminate the equivalent "you may" option on Skirk Drill Sergeant / False Dawn.) Players B and C don't have any options at all and I could easily arrange for them to only get an option when the machine terminates. But you're right, it's not 100% forced on Player A's part. Making token Rage Forgers off a trigger can't be very hard. To say nothing of the possibilities if you made different players control the positive and negative ends of the tape; then Cathar's Crusade would make that half easy. I'm curious more about what class of problem the game itself falls into, and where it fits into the continuum of games in terms of their computational difficulty (for example, chess is an easier game, computationally, than is go, which is essentially impossible to play well on a computer because for each move, there are typically between 100 and 200 options). Of course, it would be necessary to decide whether "the game" includes deck-building or not. Ultimately, what I want to know is, will there ever be a computer better at magic than the best human players?  This is amazing. My only question is, how is it a Turing Machine when the switching of states (activating Skirk Drill Sergeant to drop Chancellor of the Spires and play Time and Tide) is done according to Denzil's choice? Is this to simulate the arbitrary tape input? It's not done according to Denzil's choice. Skirk Drill Sergeant has a *triggered* ability, which only triggers when a Sphinx dies. (It's not an activated ability, despite the requirement for a mana payment.) The machine does sadly have to assume/require that whenever a player is presented with a "you may" option, the player always chooses to do it. Skirk Drill Sergeant's triggered ability falls into this category.

I'd love to avoid all "you may" effects, but sadly there aren't printed cards that'd let me even construct the tape this way under that restriction, because Kazuul Warlord is a "you may" effect. As a next best option, I designed the machine such that any time a player gets a "you may" option, that player is always able to do the action being offered, and always only in precisely one way.     Haha, this is amazing. Can't wait for next week to meet with my buddies and study the shit out of this :D Things like this always amazed me... "Let's take whatever and make a turing machine out of it" is possibly the coolest idea ever. Just shows how little it really takes to be an universal computer. &amp;gt; "Let's take whatever and make a turing machine out of it"

Is this a thing? Is there a site for this? Rule 34.5 - If it exists and could possibly be conceived of as turing complete, there is a turing machine of it. I kind of want to see rule 34 on rule 34.5...

Sex as Turing complete... Haha, I like it. If I knew enough about turing machines, I'd do it. &amp;gt; "Let's take whatever and make a turing machine out of it"

Is this a thing? Is there a site for this? Dunno, but if you just google any random thing that possibly could be TM, there probably is an article on it. Minesweeper is one of my favourite :P

Sed, html+css, minecraft... random things that pop to mind. "is turing complete" or something similar is a good google search term. I've mused a little bit about what computer games you can create Turing machines in. Crafting games like Minecraft, Terraria, Dwarf Fortress and Little Big Planet are obvious. Anything with a cellular automata nature, such as anything that includes the Game of Life or Wireworld, is also possible. Beyond that? I'm not sure.

I find it worth distinguishing between game systems that are deliberately designed to allow computation, and those where it's an accident. Minecraft, for example, includes redstone pretty much purely so that people can create wires and logic. Similarly Terraria's wires. On the other hand, the Turing completeness of the Game of Life was definitely accidental - Conway didn't realise that simple ruleset was *that* complex when he came up with it!

So there might be a couple of board/card games that let you deliberately create logic consequences, the kind of edutainment games that are designed to teach schoolkids about electronics or programming or that kind of thing. But I'm not aware of any other board/card games apart from Magic: the Gathering that are anywhere near complex enough to support a Turing machine. Which is kinda cool. :) You can build one in Transport tycoon too. Trains act as "electrons" and signals (red/green on track) act as 0/1. People made adders or even full ALUs, isn't that difficult. Full blown UTM would take some time but it's definitely possible.

We often use "logic gates" to automatically inject trains to the network if some conditions are met (for example, lack of supply trains etc.) All made without any specific game support. I've mused a little bit about what computer games you can create Turing machines in. Crafting games like Minecraft, Terraria, Dwarf Fortress and Little Big Planet are obvious. Anything with a cellular automata nature, such as anything that includes the Game of Life or Wireworld, is also possible. Beyond that? I'm not sure.

I find it worth distinguishing between game systems that are deliberately designed to allow computation, and those where it's an accident. Minecraft, for example, includes redstone pretty much purely so that people can create wires and logic. Similarly Terraria's wires. On the other hand, the Turing completeness of the Game of Life was definitely accidental - Conway didn't realise that simple ruleset was *that* complex when he came up with it!

So there might be a couple of board/card games that let you deliberately create logic consequences, the kind of edutainment games that are designed to teach schoolkids about electronics or programming or that kind of thing. But I'm not aware of any other board/card games apart from Magic: the Gathering that are anywhere near complex enough to support a Turing machine. Which is kinda cool. :)   [deleted] [deleted] [deleted] &amp;gt; Well, he's willing to take liberties with the cards themselves and the number of colors in the game

Huh?  No he's not.  The Turing Machine he's built operates completely within the rules of the game, no liberties taken.

&amp;gt; assuming you keep track of where cards are placed relative to one another is a much smaller leap

Where the cards are placed on the physical playing field is not part of the game, and this represents taking substantially larger liberties than using the text of the cards as written. [deleted] The text of the cards as written, only modified in the very specific ways permitted by the printed cards [Artificial Evolution](http://gatherer.wizards.com/Pages/Card/Details.aspx?multiverseid=39923) and [Mind Bend](http://gatherer.wizards.com/Pages/Card/Details.aspx?multiverseid=129644), being cast repeatedly by [Djinn Illuminatus](http://gatherer.wizards.com/Pages/Card/Details.aspx?multiverseid=292730). It's possible to accomplish precisely the modifications in question in a normal 4-player game of Magic following the rules of Magic (admittedly with decks that are fairly unusual, but completely legal). [deleted] [deleted]  This is hardly a surprising result. Check out [misdirection](http://gatherer.wizards.com/Pages/Card/Details.aspx?multiverseid=19681). Can you misdirect someone's misdirection to itself?

 No, spells cannot target themselves.

But funnily enough, you can misdirect their misdirect to your misdirect. But. But. But then they change your misdirection. And... does that unchange their misdirection? I've played magic somewhat casually for years, but I've got not the faintest idea how this would get resolved. 1. Abel casts a spell (it doesn't mattter which, but it doesn't have Split Second). 

2. Natasha casts Misdirection on it. 

3. Abel Misdirections Natasha's Misdirection.

4. Abel's Misdirection resolves, and changes Natasha's Misdirection to Abel's Misdirection. It is then removed from the stack because it has resolved.

5. Natasha's Misdirection sees that its target has left the stack, and it has no legal targets left. It is countered by game rules ("fizzles").

6. Abel's first spell resolves as intended.

(This also works if Natasha cast Counterspell rather than Misdirection.) Oh. Duh. I for some reason mentally had Natasha's Misdirection somehow, um, being put back on top of the stack by the misdirection and.... this is embarrassing.   How does Denzil get WWW in order to pay for Skirk Drill Sergeant? It looks like only two creatures came into play under his control, the Dragon/Dryad/Drake and the zombie token. An extra Carnival of Souls solves this problem.

Also, congrats on this! Its actually a pretty elegant system once you get past the mess of creature type changes and P/T altering effects.  There are three creatures entering the battlefield per state change: the Chancellor of the Spires, the Dragon/Dryad/Drake and the Ally/Zombie, so that provides the 3 mana needed. 
Thank you! It's undeniably fiddly, but it does have its own odd kind of beauty :) Hmm, wait. The Chancellor comes into play because of Skirk Drill Sergeant's trigger. This means you get the mana after you would have paid 2R for the ability. After the first time the Chancellor comes into play you have enough mana for the next cycle but the very first time you need to pay for the trigger you only have WW in your mana pool since no Chancellor has ever come into play at this point.   Another creature is created before that to account for that. Which one? According to his writeup, none of the creatures were necessarily cast that turn. Its obviously an easy fix with a zillion options, but it is technically a misstep in his specification.  I'm pretty sure he mentions it in the detailed explanation. Otherwise lands. He already had to have some. The only thing that he says must have been done this turn is cast Gather Specimens, False Dawn, Wheel of Sun and Moon, and clear the graveyards. He doesn't say anything about any of the creatures having come into play that turn. 

Its an incredibly minor quip. There are zillions of solutions to the issue. He could just specify that the player starts with W in his mana pool.  There are three creatures entering the battlefield per state change: the Chancellor of the Spires, the Dragon/Dryad/Drake and the Ally/Zombie, so that provides the 3 mana needed. 
Thank you! It's undeniably fiddly, but it does have its own odd kind of beauty :)  </snippet></document><document><title>Alternatives to Stack Machines?</title><url>http://www.reddit.com/r/compsci/comments/zpln9/alternatives_to_stack_machines/</url><snippet>Stack machines are an abstract computing paradigm that are interesting in their own right. They are particularly useful to me because they can programmed with a FORTH-like language , and those programs are easily encoded as a genotype for genetic algorithms. (They are a linear block of instructions, basically).

However, recently I need an alternative to this paradigm I need an equally simplistic model of programming that does not operate on stacks -- but instead operates on "sets". The elements of the "set" are unordered and all unique. I need list-like functionality that is capable of "chunking" elements of a set to make a new set. Ideally I would like a paradigm that is also straightforward to evolve in a genetic algorithm.

( "Chunking" here means handling data types that are themselves built up out of set elements. So let I=&amp;lt;3,11,18&amp;gt; and another set A=&amp;lt;"fox","boar","hedgehog"&amp;gt;. A chunking operator could produce, for example this set N=&amp;lt;"boar",11&amp;gt;. ) Other primitive operations should be in the manner of "find all the sets in this list which contain a string", "create the intersection of these two sets", and so on.

Does anyone know what this kind of computing paradigm is called? Is it called List Processing (LISP?), or something similar? What other literature could help me find something like this? Thanks.
   I don't think you need a whole new paradigm, but some of the things you are saying *very vaguely* resemble Prolog. Aha.  So maybe Prolog with a context free grammar... hmm... [Prolog has  Definite Clause Grammar built-in.](http://www.cs.sunysb.edu/~warren/xsbbook/node10.html)    The problem with sets is that, being unordered and unique, you can not work out which operation to perform in which order, and you can only have each operation present once. 

Since the order of the operations passed to a computation is critical - unless you can guarantee that every operation is commutative - you would need to control the order of the operations somehow, thereby sacrificing one of the primary characteristics of set-hood. I don't see how you can get away with the single instance per operation either (short of nasty contrived lookup table encodings that would undermine your evolutionary computing model).

As far as I can see, once you relax those constraints, you're back at lists, stacks or trees. ;-) [Post canonical systems](http://en.wikipedia.org/wiki/Post_canonical_system) are a historically-significant computational model that works on sets (of strings).  (See also semi-Thue.)   If a stack language consists of functions with signatures Stack -&amp;gt; Stack, then your (totally unexplored) family of set languages would consist of functions with signature Set -&amp;gt; Set. 

What can you do with these functions? Well, off the top of my head, I imagine we can pick an arbitrary element out, filter the set on some predictate, apply a function to the whole set, and maybe even perform a combinatorial "all-pairs of elements" applied to a given operator. 

    SINGLETON {a,b,c} = {a} or {b} or {c}
    MAP f {a,b,c,} = {f a, f b, f c} 
    FILTER f {a,b,c} = {elt in {a,b,c} s.t. f(elt) = True} 
    ALLPAIRS f {a,b,c} {d, e, f} = {f(a,d), f(a, e), ... } 

But, importantly, can programs in such a languages themselves be sets? I suspect not, since some of these operators are obviously noncommutative. For example: 
  
    FILTER (even?, MAP(double, {1})) == {2} 
    MAP(double, FILTER (even? {1})) == {}

So you couldn't just treat either program as {FILTER even?, MAP double}.  &amp;gt; then your (totally unexplored) family of set languages would consist of functions with signature Set -&amp;gt; Set. 

Yes. The purpose of this thread was to find out if this type of "paradigm" already exists in the literature or not. According to you, this is is totally new.  Related question: Which operations do we know of that are guaranteed to commute? For example, mapping and indexing commute since only mapping actually affects the values of a collection. 
   
    x = [25, 100]
    (map sqrt x)[0] == (map sqrt x[0])

Are there any more interesting combinations of operations which commute? </snippet></document><document><title>Evolutionary Game Theory: How is it approached computationally?</title><url>http://biology.stackexchange.com/q/3487/500</url><snippet /></document><document><title>CS Professor says "Learn by copying"</title><url>http://aims.muohio.edu/2012/09/10/data-structures-all-the-way-open/</url><snippet>  I'm not so sure this is a good idea.

I took my data structures course relatively early, I believe it was my second real programming course. I'm glad we had straightforward narrow parameters to meet and did mundane things like implementing linked lists.

I learned a lot from that course, it helped build the fundamental knowledge I needed to learn to program effectively. There was plenty of time to be creative later in my degree program, and I did so using the mental tools I developed in the earlier, more boring courses.

Who knows, there's certainly more than one way to learn. Maybe teaching first graders how to spell by making them write poetry would be effective, too. I regret not paying as much attention as I could for my structures course.  My teacher was a very nice and patient woman but a horrible teacher (with no disrespect).  She had 100s of slides for one lecture and we went through each one in a monotonous manner.  Unbeknownst to her, all of the students found her source code hidden in her website.  No one did the work nor cared but we all passed because of this.

I still grasped a ton of knowledge just by sitting there and listening.  I wish I could take it again but I know I'll be implementing those structures when I have spare time.  They are crucial as I am now recognizing. If you're a CS professor and leave your homework solutions wide open on a public server, you deserve to have cheating students. Yeah, you're absolutely correct.  It doesn't benefit the students (myself included) though.  It actually hurts us. I'm not so sure this is a good idea.

I took my data structures course relatively early, I believe it was my second real programming course. I'm glad we had straightforward narrow parameters to meet and did mundane things like implementing linked lists.

I learned a lot from that course, it helped build the fundamental knowledge I needed to learn to program effectively. There was plenty of time to be creative later in my degree program, and I did so using the mental tools I developed in the earlier, more boring courses.

Who knows, there's certainly more than one way to learn. Maybe teaching first graders how to spell by making them write poetry would be effective, too. My hope is that it isn't going to be either/or. In the assignment the students are working on this week they are, indeed, going to build a linked list. But then they will use it to draw stacked shapes, like in a window manager or a vector-based drawing program. My hope is that it isn't going to be either/or. In the assignment the students are working on this week they are, indeed, going to build a linked list. But then they will use it to draw stacked shapes, like in a window manager or a vector-based drawing program.  Not a bad idea... but "pleasant grading"? It might be pleasant but it will be a lot harder to be fair, to set grading criteria etc. "more pleasant" grading. Probably more difficult, from experience.  In principle I think this is a great idea to encourage students who are motivated and have new ideas they want to try. However, I predict this approach won't work well with less motivated students who simply want to do the work that is asked of them. Designing a project is a completely different skill set than being able to implement something according to a design document. It requires a much higher level of understanding that may not be suitable for some first-year students.

It seems Bo Brinkman has discovered a great way of teaching to students that are similar to himself. However I question the impact this will have on the other, non-overachieving, students. This is my biggest concern too. In the past I used a lot of competitions in class, and I definitely think it worked best with students like myself.

So far, with this approach, I've been pretty pleased. The median level of achievement appears to be very high compared to previous semesters. We will see if it continues that way the rest of the semester. Great! On a more positive note, I strongly support the use of github in the classroom. All instructors that incorporate the use of github, or bitbucket, deserve a high-five.

You should write a SIGCSE case study paper and submit it in a year. The [SIGCSE 2013](http://sigcse.org/sigcse2013/) deadline was last Friday. This is my biggest concern too. In the past I used a lot of competitions in class, and I definitely think it worked best with students like myself.

So far, with this approach, I've been pretty pleased. The median level of achievement appears to be very high compared to previous semesters. We will see if it continues that way the rest of the semester. About the no plagiarism rule - do you allow people to blatantly copy as long as they credit properly?

I had a prof who, as far as I can tell, would happily mark you if you submitted stuff that was entirely someone else's work and credited it as such.  But that was a first year course that didn't count towards the final degree. Students are allowed to copy, as long as they cite/credit the source, but they don't receive points for the copied code unless they significantly modify it.

So, for example, one student could write a "drawCircle" method and another could write a "drawRectangle" method, and then they could swap code. So their final products end up looking more impressive than they would if they had to write 100% of the code. In principle I think this is a great idea to encourage students who are motivated and have new ideas they want to try. However, I predict this approach won't work well with less motivated students who simply want to do the work that is asked of them. Designing a project is a completely different skill set than being able to implement something according to a design document. It requires a much higher level of understanding that may not be suitable for some first-year students.

It seems Bo Brinkman has discovered a great way of teaching to students that are similar to himself. However I question the impact this will have on the other, non-overachieving, students. In principle I think this is a great idea to encourage students who are motivated and have new ideas they want to try. However, I predict this approach won't work well with less motivated students who simply want to do the work that is asked of them. Designing a project is a completely different skill set than being able to implement something according to a design document. It requires a much higher level of understanding that may not be suitable for some first-year students.

It seems Bo Brinkman has discovered a great way of teaching to students that are similar to himself. However I question the impact this will have on the other, non-overachieving, students. I've found that out in my job as well. I was fortunate to be able to work on a project (and later lead it) which matched my proclivities. But it takes a very special kind of mind set to be able to take (as the article put it) "aesthetic" goals and turn that into beautiful code. Most software developers, especially in the early stages, need concrete goals and criteria for success. My architecturally oriented project however, was often filled with goals that started out with "we need a better way to do *some task*...". We would have to break down the problem domain and weigh the tradeoffs of different designs and implementations. Because we were the framework project, we actually had to pay attention to fundamental OO concepts. It's annoying to create code around a conceptual error in an application and then have to work around it; in a base library or framework, making the same type of mistake and having dozens of other projects use that as a base can easily become painful to the point that the other projects will just roll their own competing, conflicting, and half-assed solutions anyway. Often by copy/replacing upwards of a dozen framework classes. Just to fix one tiny little problem. And some times we would have to take a step back from this exploratory codethulu baby, realize that we missed the mark in some fundamental aspect, scrap it and start over. Or sometimes we have to realize that this is as good as it is getting right now, come to terms with the limitations, and move on.

When I started coding I thought anyone could be a coder. When I became a professional coder, I realized that not everyone can model problems and manipulate them through well defined algorithms. When I became a framework guy, I realized that not every coder can think in the levels of abstraction necessary for modeling and manipulating the problem domain in terms general enough to form the base of solid reusable code. And across all levels I find that not everyone has the internal impetus to look back over what they have done, both recently and in the past, and ask "Should I have done that better? What were the problems that came out of that approach, and how should I solve something like that in the future? What have others done in this problem space, and why did they choose that route?" In principle I think this is a great idea to encourage students who are motivated and have new ideas they want to try. However, I predict this approach won't work well with less motivated students who simply want to do the work that is asked of them. Designing a project is a completely different skill set than being able to implement something according to a design document. It requires a much higher level of understanding that may not be suitable for some first-year students.

It seems Bo Brinkman has discovered a great way of teaching to students that are similar to himself. However I question the impact this will have on the other, non-overachieving, students.   I wish my data structures course was taught by this guy  Bo Brinkman was my data structures professor, and I wish he had this attitude when I was there.    I think both kinds of learning have their place. Learning algorithms is much more engineering-ish than making a game, which is much more art-ish. I am designing a simple turn based galcon style game (think risk in space with planets) and the part I am looking forward to is the server side turn generation. Algorithms will probably become needed in a big way after the simple part is in the past. man, I remember playing kde's take on that ages ago. that was quite the pastime. good luck! :)  IAMA Student currently taking Dr. Brinkmans Data Structures class, AMA. Do you think your classmates will be too lazy to follow-through with the work?   I like this. That's what the up arrow is for.  His GitHub link just redirects me to my homepage. Anyone else having that issue?

Got around it [here](https://github.com/MiamiOH-CSE274).     Doesn't every compSci/programming professor recommend this? "Look/copy/rewrite the example code for this weeks assignment" sort of thing. Maybe last millenium people didn't learn coding by copying, but there sure as hell do now. Somebody didn't read any of the article...</snippet></document><document><title>Sizing a LRU cache</title><url>http://www.reddit.com/r/compsci/comments/zpl5v/sizing_a_lru_cache/</url><snippet>I have a set of data where each point consists of a unique identifier and a time stamp.  Each point represents a point in time where the user represented by the identifier loads some user specific data.  The data is sorted by timestamp in ascending order.  E.g.,

(1234, 01:30), (1234, 01:31), (2345, 02:22), (3456, 02:30), (4567, 03:00), (3456, 03:20), (4567, 04:00)

Loading the user data can take a while so I&#8217;ve implemented a LRU cache to improve performance.   I&#8217;m looking for a way to size the cache to guarantee a certain hit rate given a large set of historical data.  

In the above data, there are 4 users, 1 of which (2345) loads the resource once so it&#8217;s obvious to me that a cache size of 3 will guarantee the highest possible hit rate for that data (miss/add, hit, miss/add, miss/add, miss/remove/add, hit, hit, yielding hit rate of 43%).  What&#8217;s not obvious to me is how to find out the size of the cache required for, say, a 25% hit rate without running multiple simulations on the data.  Thoughts anybody?
   &amp;gt; What&#8217;s not obvious to me is how to find out the size of the cache required for, say, a 25% hit rate without running multiple simulations on the data.

You do one simulation with sort of an "infinitely large" LRU cache: use a stack-like structure, and on each access, see how far down the stack the accessed item is ("reuse distance"), then pull it to the top. A reuse distance greater than actual cache size corresponds to a cache miss. A histogram of reuse distances from your simulation will show you how large a cache is needed for a given miss rate.

If you're dealing with a large amount of data, you may need a [more clever data structure](http://www.cs.rochester.edu/~cding/Documents/Publications/pldi03.pdf) to make the simulation run fast enough.</snippet></document><document><title>Undergrad question: how do I get an algorithm that's in my head into actual code?</title><url>http://www.reddit.com/r/compsci/comments/zoad6/undergrad_question_how_do_i_get_an_algorithm/</url><snippet>I think this is common beginner problem. Is there a methodical way to this? My profs always show me some stuff from software engineering like stepwise programming, but I'm still boggled when I have to use even simplest algorithms since I have a hard time turning them into code.

For example, a selection sort is totally easy to imagine and draw out. Unfortunately, I have to spend *at least* an hour just to get it work right on an array in actual code. This feels unacceptable. When I go work in a production environment, how would I be able to get anything done if it takes an hour to get such a simple algorithm done?

I finish all assignments and generally enjoy CS, but I absolutely hate getting a simple sounding problem and then having to spend 12 hours of my time finishing it, 9 of which are spent feeling hopeless and incompetent and constantly swapping bad code for new bad code :(     Try top-down design. You start out with a broad idea of the algorithm. Then you turn that into pseudo-code (in practise you can write out the pseudo-code in comments). Then you can start changing the general structure of the pseudo-code into real code (e.g. start with loops). At this point finishing the code is not far away. When you get practise you can start writing code directly when you have a rough sketch of the algorithm in your head.   Breaking the problem down into manageable chunks is very important. Most simple problems are simple only because they are made up of other simple problems. Object Oriented programming in C++/Java is actually really helpful to new programmers because it encourages you to break the problem down into bite sized chunks to work on.

Writing a complex function as a set of smaller, simpler functions changes the mindset from focusing on one big problem to 3 or 4 smaller simpler problems. Figuring out how to split the function up comes with practice. Even if your not going to re-use a piece of code (which is always the aim in efficient programming) its sometimes helpful to put it in its own function and give it a name so that your mind can process it easier.         making correct software is hard. you've got, what, not even 4 years of barely part time experience writing software?

even after an hour, i doubt your sort is correct.

http://googleresearch.blogspot.com/2006/06/extra-extra-read-all-about-it-nearly.html

keep practicing. you'll get better. ... this is at least two levels of irrelevant.

One, he's writing selection sort. It's an O(n*n) comparison sort which has no binary averaging step. Two, it has nothing to do with the question he's asking ("how do I go from process to code?"). </snippet></document><document><title>Transactional Hardware on x86</title><url>http://jeremymanson.blogspot.com/2012/02/transactional-hardware-on-x86.html</url><snippet>  We use x86 at work for our EEG data. Cool post!</snippet></document><document><title>"Richard Feynman Computer Heuristics Lecture"</title><url>http://youtu.be/EKWGGDXe5MA</url><snippet>   </snippet></document><document><title>On the Feasibility of Side-Channel Attacks with Brain-Computer Interfaces</title><url>https://www.usenix.org/system/files/conference/usenixsecurity12/sec12-final56.pdf</url><snippet>  </snippet></document><document><title>Are there any significant benefits to learning data structures in one language or another?</title><url>http://www.reddit.com/r/compsci/comments/zj1dr/are_there_any_significant_benefits_to_learning/</url><snippet>This semester I'll be taking a data structures class which is taught using Java as the primary language. I don't have a whole lot of exposure to the language (I [wrote a Minecraft plugin once](http://forums.bukkit.org/threads/inactive-fun-youreit-v0-4-a-game-of-tag-670-677.12634/)) but that's about it (I have a lot of experience in other languages). I'm wondering if anybody has opinions on whether or not there are any significant benefits to learning data structures in say Java, versus learning them in C.

My gut feeling is 'no.' On a conceptual level I do think that any language would suffice, but I am interested in other and differing opinions.  I would go one step further and argue that *learning* data structures ought to be language agnostic. Data structures are abstractions that shouldn't be tied to the language you first implement them in. 

I would also argue that being able to implement any given data structure in both C (because it has very little sugar) and a functional language like Haskell or Scheme could be useful in deepening your understanding of any one data-structure. 

The paradigm shift from C-&amp;gt;OOP(Java, C++) is pretty small; 'make a pointer to a struct and call it an object'. But the shift from OOP-&amp;gt;functional is not small. Being able to do both would be an excellent learning exercise.  Pretty much. I teach data structures class and we don't even use any concrete language. We present one imperative and one functional "pseudocode" (mostly "C-like" and "Haskell-like") and that's it. But we have it set up such that students don't even program at all, it's much more theoretical course. There are other that they can enroll to practice implementation. And I think this is a good way to do it. Optimally, you should be able to implement the structure/algo in anything if you *really* understand what's going on. Alright, do you think there is any value in learning from actually implementing data structures ? I have personally found value in implementing data structures, because that just makes everything just so concrete and precise that you see things very clearly and are then able to extract the underlying idea of the data structure and use it in other places as well. I agree. I can't think of a single thing that we did pseudocode for in class, then never had to implement. I know there were a *lot* of concepts like that....but I don't remember them. On the other hand, I can remember building a heap, and the code behind doing it. I have a good picture of what using that data structure means; not because I memorized runtime complexity stats, but because I remember the algorithms involved well enough to figure out the complexity on my own. Alright, do you think there is any value in learning from actually implementing data structures ? Honestly, no. Because most of the time, you won't implement it. I think it is much more valuable to know the properties and usage, and a general idea on how it works inside.

Actually implementing it comes handy when you need some functionality not already present and when you have to create something new. That's what we have "Data structures II" course for. In the first "basic one", it's mostly about cleverly using existing stuff, learning about usage and properties.

Well, obviously this does not concern algorithms like BFS, DFS and those common ones that you use all the time. You should definitely know how to implement that. Not so much for max-flow algorithms or R/B-trees (never in my life I've implemented R/B-tree).

That being said, some people really learn better if they implement it themselves. I have no objections against that, if you want or need to, go ahead. But then never use it in production, because it will very likely be wrongly implemented and slower than library implementation. (using impersonal you) I think that a lot of design can be learned from implementing Data Structures. Especially for someone who has never had to design a program before having to make choices about a Data Structure can be *really* hard. The up-side to a data structure is it has an extremely well defined behavior that you must be able to replicate no matter how you go about implementing it. Ok, but knowing the implementation won't provide you with any more information than knowing the properties of the structure and using a library one.

If I need a priority queue, I don't need to know how to implement binary heap, I need to know it has logn insertion and extraction and that it's better than sorted array or linked list or something else.

We actually discuss the algorithms for heapifying an array, insertion, deletion or how the hash table works inside, so it's not like they are completely shielded from that. I simply believe that *understanding* the structure is far better than implementing it in one particular language. Especially if you've never programmed before, you might get the feeling that it works the same way in any language, which obviously is not true. Ok, but knowing the implementation won't provide you with any more information than knowing the properties of the structure and using a library one.

If I need a priority queue, I don't need to know how to implement binary heap, I need to know it has logn insertion and extraction and that it's better than sorted array or linked list or something else.

We actually discuss the algorithms for heapifying an array, insertion, deletion or how the hash table works inside, so it's not like they are completely shielded from that. I simply believe that *understanding* the structure is far better than implementing it in one particular language. Especially if you've never programmed before, you might get the feeling that it works the same way in any language, which obviously is not true. I am just starting a data structures course now, officially being on in Java, but we are allowed to use other languages (C++ is the one I would use). 

I can appreciate the value of implementing them because it gives you a certain understanding that goes beyond just remembering. I am especially terrible at remembering, which is why I do not often take humanities.

How does the method you use to teach go beyond simple memorization? Can one work with data structures in a theoretical way and do proofs and other things on them?

I do appreciate the theoretical, if I wasn't I wouldn't be in CS. Being able to prove something is much more satisfying then stuff I did in physics. I'm not sure if I understand you, but implementing a data structure isn't the same as using it. Implementing it means writnig a code for the actual binary search tree or whatever, using it in program means using the functionality. We don't do the former, we expect the latter.

You can definitely prove stuff about them, and we also have some excercises on that (correctness/convergence, time and space requirements).

As per memorization, you could probably pass the course with the lowest grade if you learn the whole textbook verbatim. But in many questions you are required to actually *understand* the structure, what it does and what it doesn't. If you only remember the main idea of a heap, you can probably figure out that it can be used as a priority queue if I ever ask you to make one. Honestly, no. Because most of the time, you won't implement it. I think it is much more valuable to know the properties and usage, and a general idea on how it works inside.

Actually implementing it comes handy when you need some functionality not already present and when you have to create something new. That's what we have "Data structures II" course for. In the first "basic one", it's mostly about cleverly using existing stuff, learning about usage and properties.

Well, obviously this does not concern algorithms like BFS, DFS and those common ones that you use all the time. You should definitely know how to implement that. Not so much for max-flow algorithms or R/B-trees (never in my life I've implemented R/B-tree).

That being said, some people really learn better if they implement it themselves. I have no objections against that, if you want or need to, go ahead. But then never use it in production, because it will very likely be wrongly implemented and slower than library implementation. (using impersonal you) I think you're wrong with your comment here. I think implementation details reveal a lot about the underlying complexity of the data structure. For example, you likely teach that hash maps are constant time lookup. And yes, you probably glaze over the fact that there might be conflicts, etc. But, not until you actually build a hash map do you understand the complexities involved as well as the cost of using such a data structure. In a lot of cases, lists have faster lookup times than hashmaps because of the conflict issue.

Now, granted, if you're teaching a grad level algorithms course, then implementation is not necessary. Everyone's done their years of programming and build a thousand implementations of linked lists. But, if you're teaching beginners, you're doing them a disservice by not making them understand what goes on under the hood. We discuss implementation pseudocodes as I said above, we just don't make them implement the stuff. I'm not the author of the syllabus, but I strongly believe that implementing red-black tree or hash-table will teach you exactly nothing relevant to the course. It's a good programming excercise, but if you *understand* what and how linked list works, you should not have any conceptual problems implemting it, other than being not familiar with the language.

We also discuss conflicts in hashmaps and some strategies on how to manage that (resizing/rehashing etc.). And never in my life I've seen serious hashtable implementation that'd be slower than a list. In fact, IT IS PLAIN IMPOSSIBLE! Because even if all the items went into one bucket, you'd end up with list lookup complexity :)

It's a second semester course for undergrads. There are also parallel courses of either C or Python or (I think) Pascal where they do implement this stuff. It is not mandatory to enroll those however. The fact that you think it is plain impossible indicates a need to better understand hash maps. Perhaps a course that asked you to write an implementation of one would be good :-) Show me then. Consider an awful hashing function

def bucket(against, N)

  if (isChar(against[0]):
     return 0
  ... 

Everything that begins with a letter gets mapped to the first bucket of your hash map.  Then, you have to resort to chaining, or double hashing, etc. The bottom line, hash maps are only O(1) lookup if you're using a good hash function. Otherwise you're hosed
 Honestly, no. Because most of the time, you won't implement it. I think it is much more valuable to know the properties and usage, and a general idea on how it works inside.

Actually implementing it comes handy when you need some functionality not already present and when you have to create something new. That's what we have "Data structures II" course for. In the first "basic one", it's mostly about cleverly using existing stuff, learning about usage and properties.

Well, obviously this does not concern algorithms like BFS, DFS and those common ones that you use all the time. You should definitely know how to implement that. Not so much for max-flow algorithms or R/B-trees (never in my life I've implemented R/B-tree).

That being said, some people really learn better if they implement it themselves. I have no objections against that, if you want or need to, go ahead. But then never use it in production, because it will very likely be wrongly implemented and slower than library implementation. (using impersonal you) No question about using a well tested and proven library implementation in production.  Why roll your own unless you absolutely must?  But when I did learn data structures ages ago, we did implement some of the simpler ones, or did naive implementations of the more complex ones.  For me the most eye opening part of the class and the implementation was how to recognize in the structure and in the implementation what sort of big-O behavior particular operations would have.  Perhaps I was a little more thick headed than my classmates who would understand it at first sight (at least they claimed they got it).  Getting my hands dirty like that was far more hard work than what the class required but for me at least I was more confident in my new found knowledge.  I remember that heaps and self-balancing trees seemed like really heavy (and heady) concepts at the time.  Now they are just tools in the toolkit. Alright, do you think there is any value in learning from actually implementing data structures ? I think there is definitely value in learning to implement them in some language, it doesn't really matter which. 

When I took my data structures class I studied for exams by implementing data structures / algorithms in Python. 

Doing so gave me the chance to play around with some sample data and get a feel for how efficient they were, etc.  I would go one step further and argue that *learning* data structures ought to be language agnostic. Data structures are abstractions that shouldn't be tied to the language you first implement them in. 

I would also argue that being able to implement any given data structure in both C (because it has very little sugar) and a functional language like Haskell or Scheme could be useful in deepening your understanding of any one data-structure. 

The paradigm shift from C-&amp;gt;OOP(Java, C++) is pretty small; 'make a pointer to a struct and call it an object'. But the shift from OOP-&amp;gt;functional is not small. Being able to do both would be an excellent learning exercise.  I would go one step further and argue that *learning* data structures ought to be language agnostic. Data structures are abstractions that shouldn't be tied to the language you first implement them in. 

I would also argue that being able to implement any given data structure in both C (because it has very little sugar) and a functional language like Haskell or Scheme could be useful in deepening your understanding of any one data-structure. 

The paradigm shift from C-&amp;gt;OOP(Java, C++) is pretty small; 'make a pointer to a struct and call it an object'. But the shift from OOP-&amp;gt;functional is not small. Being able to do both would be an excellent learning exercise.  I would go one step further and argue that *learning* data structures ought to be language agnostic. Data structures are abstractions that shouldn't be tied to the language you first implement them in. 

I would also argue that being able to implement any given data structure in both C (because it has very little sugar) and a functional language like Haskell or Scheme could be useful in deepening your understanding of any one data-structure. 

The paradigm shift from C-&amp;gt;OOP(Java, C++) is pretty small; 'make a pointer to a struct and call it an object'. But the shift from OOP-&amp;gt;functional is not small. Being able to do both would be an excellent learning exercise.  Learning ~~data structures~~ abstract data types with a programming language agnostic approach can be done, using a specification language. Learning data structures on the other hand is dependent on the programming language of choice and the constructs it provides to build such structures. 

  I would go one step further and argue that *learning* data structures ought to be language agnostic. Data structures are abstractions that shouldn't be tied to the language you first implement them in. 

I would also argue that being able to implement any given data structure in both C (because it has very little sugar) and a functional language like Haskell or Scheme could be useful in deepening your understanding of any one data-structure. 

The paradigm shift from C-&amp;gt;OOP(Java, C++) is pretty small; 'make a pointer to a struct and call it an object'. But the shift from OOP-&amp;gt;functional is not small. Being able to do both would be an excellent learning exercise.   As far as *data structures* are concerned, not that much; however, a ds class is used also to make you a better programmer; I think it is a good idea to learn how to deal with pointers and explicit allocation/deallocation of memory, so C (or even better, C++) would be a better language, IMHO.                 I'd argue for Python. Other languages like C and Java require more overhead in lines of code to express even very simple things, Actually, for certain things, Python is not great. C is very good if you want very low-level control over how things are laid out in memory, for example. I imagine learning about cache blocking and register blocking is far easier in C than in Python :P.

I think this is going to be true of a bunch of data structures--their exact layout is very important, and C gives you much more direct access to it than Python since Python has a bunch of extra indirection. 

While this knowledge is not terribly important in *most* programming, the times it *is* important, it is *very* important. You can sometimes speed your code up very significantly just by being moderately clever with your memory access patterns.  I would say that the best language would be C++, since it's Object-Oriented (which is quite nice to understand some structures) and it's pretty easy to know when you are using pointers (Java kind of hides that).

Nevertheless, it doesn't really matter, since you only need to grasp the concept behind the structure, and the theory stays (almost) the same for all languages.

Also, if you already know the difference between a linked list and an array, you won't need explicit pointers in your language to understand what's going on. For learning data structures, I'd argue that C++ is one of the worse choices. Wrangling the object-oriented bits of the language can get in the way of learning the actual algorithms behind a given data structure. The language requires you to write a whole bunch of extra code just to have the objects actually work, but the objects are largely irrelevant when considering the data structure itself. Nevermind that templates (which you would probably feel inclined to use if you were trying to learn data structures) are a pain to debug, especially if you're not entirely sure what you're doing in the first place (which is what learning implies). 

I'd argue that a higher level language makes a better choice: it allows you to focus on the algorithms behind a given structure, without obstructing it with additional layers of crap.  For learning data structures, I'd argue that C++ is one of the worse choices. Wrangling the object-oriented bits of the language can get in the way of learning the actual algorithms behind a given data structure. The language requires you to write a whole bunch of extra code just to have the objects actually work, but the objects are largely irrelevant when considering the data structure itself. Nevermind that templates (which you would probably feel inclined to use if you were trying to learn data structures) are a pain to debug, especially if you're not entirely sure what you're doing in the first place (which is what learning implies). 

I'd argue that a higher level language makes a better choice: it allows you to focus on the algorithms behind a given structure, without obstructing it with additional layers of crap.   You can't learn data structures in languages without pointer arithmetic. This should be self-evident since you can't create data structures without pointer arithmetic. Data structures seem to work just fine in plenty of languages without needing pointers (prime examples are functional and dynamically typed languages, but there's plenty other examples). I don't think you could argue references are the same as pointers since there is no explicit handling of addresses in references. sure they work fine, because the languages provide them, you can't implement them, and therefore you can't learn how they work. All you can do is link the data structures together like lego.

And yes, before someone stupid suggests it, you can emulate memory with arrays, and therefore implement any data structure in emulation, but at that point you might as well be using C or Algol (which are both great languages, though Algol is a little lacking in modern implementations). </snippet></document><document><title>notation in papers (specifically, type and category theory)</title><url>http://www.reddit.com/r/compsci/comments/zid8j/notation_in_papers_specifically_type_and_category/</url><snippet>So, i have tried to read a decent number of cs papers, but about half way through, most of them start throwing around horizontal lines(implication? bijection?), sideways Ts(adjoint functors in category theory, i have no idea what in type theory), and other symbols that i can't search for with google.

Are there any good resources on what these symbols mean? I'm guessing a general resource doesn't exist, but are there good notation reference books/websites for specific subjects? like, can i find a book with 10 pages that just has a list of symbols and the various meanings people use them for in type theory?

or does everyone just pick this up the hard way?       Most people pick this up the easy way: by having it explained to them, preferably in a class :). There happens to be a general construct for the horizontal line, it was described by Peter Aczel (a great logician, as he happened to be well versed in category theory and computer science!). The reference is:

P. Aczel. An introduction to inductive definitions. In *Handbook of Mathematical Logic" chapter C.7, pages 739-782.

In general though, the only thing to understand about the horizontal line is this: if what is above the line is true/provable/an element of the set/a morphism then what is below the line is true/provable/an element of that set/a morphism.

The turnstyle (&#8866;) is a bit more complex. In logic: G &#8866; P usually means: under the assumption G, P can be shown to hold. In category theory it usually means there is a morphism between objects G and P. I agree with everything you said but the last sentence: In category theory the turnstile means an adjunction between two functors.  </snippet></document><document><title>I Have a Microsoft Internship Interview in Redmond Coming Up In Two Weeks -- Any Good Practice Problems To Work On?</title><url>http://www.reddit.com/r/compsci/comments/zhalr/i_have_a_microsoft_internship_interview_in/</url><snippet>I'll be interviewing for the Software Development Engineer In Test position as an Intern at Microsoft. I'm mostly nervous for the tricky coding questions. Are there any problems you have faced that you think would be good to practice? Thanks guys!  Microsoft interviewer here :-).

(1) We want to know how you think, not what the solution is (that we already know). If you get stuck, don't panic. Talk about what your brain is going through as you are approaching the problem. The interviewer will help you.

(2) If you blow one interview, do not panic and do NOT let this define your day. Brain freezes happen. This is the reason we have multiple interviewers.

(3) Do not touch the glass!

(4) You will need basic familiarity with computer science: algorithms (trees, heaps, sorting, graph algorithms, time complexity, space complexity, linear programming), and operating systems concepts.

(5) You will not generally need to know "knowledge" stuff - what API X does, or what language feature Y does.

(6) You will need to be able to code on the whiteboard. Extra points for C and ability to use pointers (but only when it actually makes sense).

(7) Humility! Try to appear super amazing, and interviewer will take it as a challenge. Then you are screwed. Instead, lower the expectations, then beat them.

(8) Math: be handy with combinatorics and probabilities. Geometry for extra credit. Math illiteracy has no place in computer science.

(9) Do not forget to not touch the glass.

(10) Have genuine interest in a computer science topic, and know a lot about the area of interest. Be able to intelligently discuss it, with pros, contras, and interesting challenges.

(11) Know why you want to be a programmer or tester. 

(12) Be specific: generic answers anger interviewers. Try to forget everything that your college advisor (humanities major) taught you about communicating during the interview. General statements - bad. Specific answers, backed up by stories from your experience - good.

(13) Keep to the left. Do not litter. Do not loiter. Do not touch the glass.


Good luck with your interview and hopefully see you on campus :-)! What's with the glass? What's with the glass? What's with the glass? What's with the glass? Microsoft interviewer here :-).

(1) We want to know how you think, not what the solution is (that we already know). If you get stuck, don't panic. Talk about what your brain is going through as you are approaching the problem. The interviewer will help you.

(2) If you blow one interview, do not panic and do NOT let this define your day. Brain freezes happen. This is the reason we have multiple interviewers.

(3) Do not touch the glass!

(4) You will need basic familiarity with computer science: algorithms (trees, heaps, sorting, graph algorithms, time complexity, space complexity, linear programming), and operating systems concepts.

(5) You will not generally need to know "knowledge" stuff - what API X does, or what language feature Y does.

(6) You will need to be able to code on the whiteboard. Extra points for C and ability to use pointers (but only when it actually makes sense).

(7) Humility! Try to appear super amazing, and interviewer will take it as a challenge. Then you are screwed. Instead, lower the expectations, then beat them.

(8) Math: be handy with combinatorics and probabilities. Geometry for extra credit. Math illiteracy has no place in computer science.

(9) Do not forget to not touch the glass.

(10) Have genuine interest in a computer science topic, and know a lot about the area of interest. Be able to intelligently discuss it, with pros, contras, and interesting challenges.

(11) Know why you want to be a programmer or tester. 

(12) Be specific: generic answers anger interviewers. Try to forget everything that your college advisor (humanities major) taught you about communicating during the interview. General statements - bad. Specific answers, backed up by stories from your experience - good.

(13) Keep to the left. Do not litter. Do not loiter. Do not touch the glass.


Good luck with your interview and hopefully see you on campus :-)! Can you comment on what else might be important if one was interviewing for an internship at Microsoft Research instead of the development side? Can you comment on what else might be important if one was interviewing for an internship at Microsoft Research instead of the development side? By my understanding, the interviews would be pretty different at MSR. I think it would be more geared towards talking about actual research stuff in the area (i.e. tell me about one of your research projects, what do you think about doing research on topic X). In the interviews my friends had, they were never asked a question where there would be a right or wrong answer, just more general perspective questions. I think to even get to the interview stage for MSR, they need to have a pretty good sense that you will be worth it and at the interview, they are just trying to judge if you're a right research fit.  Microsoft interviewer here :-).

(1) We want to know how you think, not what the solution is (that we already know). If you get stuck, don't panic. Talk about what your brain is going through as you are approaching the problem. The interviewer will help you.

(2) If you blow one interview, do not panic and do NOT let this define your day. Brain freezes happen. This is the reason we have multiple interviewers.

(3) Do not touch the glass!

(4) You will need basic familiarity with computer science: algorithms (trees, heaps, sorting, graph algorithms, time complexity, space complexity, linear programming), and operating systems concepts.

(5) You will not generally need to know "knowledge" stuff - what API X does, or what language feature Y does.

(6) You will need to be able to code on the whiteboard. Extra points for C and ability to use pointers (but only when it actually makes sense).

(7) Humility! Try to appear super amazing, and interviewer will take it as a challenge. Then you are screwed. Instead, lower the expectations, then beat them.

(8) Math: be handy with combinatorics and probabilities. Geometry for extra credit. Math illiteracy has no place in computer science.

(9) Do not forget to not touch the glass.

(10) Have genuine interest in a computer science topic, and know a lot about the area of interest. Be able to intelligently discuss it, with pros, contras, and interesting challenges.

(11) Know why you want to be a programmer or tester. 

(12) Be specific: generic answers anger interviewers. Try to forget everything that your college advisor (humanities major) taught you about communicating during the interview. General statements - bad. Specific answers, backed up by stories from your experience - good.

(13) Keep to the left. Do not litter. Do not loiter. Do not touch the glass.


Good luck with your interview and hopefully see you on campus :-)! Great advice -- except for the slur in point 12.  In all my years in academia, I have never heard anyone, from any field (I'm a scientist, and I work with folks across all sorts of departments), encourage generalities over specific statements -- whether in interviews, applications for nationally competitive fellowships, or college assignments.  (And why you would assume someone's advisor would be outside that person's area of study is a bit of a mystery to me as well.) Microsoft interviewer here :-).

(1) We want to know how you think, not what the solution is (that we already know). If you get stuck, don't panic. Talk about what your brain is going through as you are approaching the problem. The interviewer will help you.

(2) If you blow one interview, do not panic and do NOT let this define your day. Brain freezes happen. This is the reason we have multiple interviewers.

(3) Do not touch the glass!

(4) You will need basic familiarity with computer science: algorithms (trees, heaps, sorting, graph algorithms, time complexity, space complexity, linear programming), and operating systems concepts.

(5) You will not generally need to know "knowledge" stuff - what API X does, or what language feature Y does.

(6) You will need to be able to code on the whiteboard. Extra points for C and ability to use pointers (but only when it actually makes sense).

(7) Humility! Try to appear super amazing, and interviewer will take it as a challenge. Then you are screwed. Instead, lower the expectations, then beat them.

(8) Math: be handy with combinatorics and probabilities. Geometry for extra credit. Math illiteracy has no place in computer science.

(9) Do not forget to not touch the glass.

(10) Have genuine interest in a computer science topic, and know a lot about the area of interest. Be able to intelligently discuss it, with pros, contras, and interesting challenges.

(11) Know why you want to be a programmer or tester. 

(12) Be specific: generic answers anger interviewers. Try to forget everything that your college advisor (humanities major) taught you about communicating during the interview. General statements - bad. Specific answers, backed up by stories from your experience - good.

(13) Keep to the left. Do not litter. Do not loiter. Do not touch the glass.


Good luck with your interview and hopefully see you on campus :-)! Mighty nice of you to post an inside look at the Microsoft interview process.

As an undergrad CS/Math double major, working at a big tech company like Microsoft or Google is something of a dream of mine. 

Out of curiosity, how many new college grads do you guys tend to hire?

For that matter, how does one go about applying for an internship? haha :D Mighty nice of you to post an inside look at the Microsoft interview process.

As an undergrad CS/Math double major, working at a big tech company like Microsoft or Google is something of a dream of mine. 

Out of curiosity, how many new college grads do you guys tend to hire?

For that matter, how does one go about applying for an internship? haha :D Microsoft interviewer here :-).

(1) We want to know how you think, not what the solution is (that we already know). If you get stuck, don't panic. Talk about what your brain is going through as you are approaching the problem. The interviewer will help you.

(2) If you blow one interview, do not panic and do NOT let this define your day. Brain freezes happen. This is the reason we have multiple interviewers.

(3) Do not touch the glass!

(4) You will need basic familiarity with computer science: algorithms (trees, heaps, sorting, graph algorithms, time complexity, space complexity, linear programming), and operating systems concepts.

(5) You will not generally need to know "knowledge" stuff - what API X does, or what language feature Y does.

(6) You will need to be able to code on the whiteboard. Extra points for C and ability to use pointers (but only when it actually makes sense).

(7) Humility! Try to appear super amazing, and interviewer will take it as a challenge. Then you are screwed. Instead, lower the expectations, then beat them.

(8) Math: be handy with combinatorics and probabilities. Geometry for extra credit. Math illiteracy has no place in computer science.

(9) Do not forget to not touch the glass.

(10) Have genuine interest in a computer science topic, and know a lot about the area of interest. Be able to intelligently discuss it, with pros, contras, and interesting challenges.

(11) Know why you want to be a programmer or tester. 

(12) Be specific: generic answers anger interviewers. Try to forget everything that your college advisor (humanities major) taught you about communicating during the interview. General statements - bad. Specific answers, backed up by stories from your experience - good.

(13) Keep to the left. Do not litter. Do not loiter. Do not touch the glass.


Good luck with your interview and hopefully see you on campus :-)!  I'd recommend buying one (or both) of these books and reading through them:

- [Cracking the Coding Interview](http://www.amazon.com/Cracking-Coding-Interview-Programming-Questions/dp/098478280X/)
- [Programming Interviews Exposed](http://www.amazon.com/Programming-Interviews-Exposed-Secrets-Programmer/dp/047012167X/)

Each of those books has a wide range of programming problems that are commonly asked during an interview. I highly recommend going one question at a time and actually trying to solve them yourself before looking at the solution. This way you won't simply have a dictionary of interview questions--you'll also be able to develop the skills to figure your way through similar problems.     I had a real interview (college grad not intern) with them, didn't get it, but they asked me to completely make a red black tree from memory. Started with insert, delete, and I didn't know how to rotate off the top of my head. Another interviewer asked me to compress a bitmap, I did Huffman encoding, but he really wanted run length encoding. The other questions were more theoretical. Not sure if I have this one right, but it was find three different ways to rotate a globe in two moves and have the north pole remain in the same place. Oh yeah another was to create a class definition for a library that does not give out handles due to the requirement of being thread safe. I'd recommend reviewing basic algorithms from CLRS and reading code complete (more like skimming) cause they really like that book. ( I actually bought it before they emailed it out in PDF format to me for free)

Edit: I was doing regular software dev, test may be more tester related. From what I could tell they follow a very strict lifecycle. Software engineer makes a spec, developer implements, and tester tests. Someone from MS may want to clarify, my friend got a job there, but I'm too lazy to ask him for ya I had a real interview (college grad not intern) with them, didn't get it, but they asked me to completely make a red black tree from memory. Started with insert, delete, and I didn't know how to rotate off the top of my head. Another interviewer asked me to compress a bitmap, I did Huffman encoding, but he really wanted run length encoding. The other questions were more theoretical. Not sure if I have this one right, but it was find three different ways to rotate a globe in two moves and have the north pole remain in the same place. Oh yeah another was to create a class definition for a library that does not give out handles due to the requirement of being thread safe. I'd recommend reviewing basic algorithms from CLRS and reading code complete (more like skimming) cause they really like that book. ( I actually bought it before they emailed it out in PDF format to me for free)

Edit: I was doing regular software dev, test may be more tester related. From what I could tell they follow a very strict lifecycle. Software engineer makes a spec, developer implements, and tester tests. Someone from MS may want to clarify, my friend got a job there, but I'm too lazy to ask him for ya    Right as they are beginning to explain how the interview will go, stand up, look at your phone and say

&amp;gt;I have important updates ready to be read!!

Soon, the look of confusion and shock will give way to a trepidatious continuation of the interview.  When it does, jump up and turn off the lights.  Explain that the interview had to be reset to allow the updates to be fully integrated into your consciousness.  Explain that you *would* have given them the opportunity to prevent it, but they seemed busy.  

Also, bring a blue paint bomb and set it off at some point.

You might think it's crazy behavior, but they spend entire careers making this type of thing a reality for millions of people across the world.  They will appreciate the return gesture.   The biggest problem will be fining a life in the murky black hole that is Redmond. And the solution is to live in Seattle. And die a slow death on one of the bridges.
In a few years or more there will be a train perhaps. The biggest problem will be fining a life in the murky black hole that is Redmond.  </snippet></document><document><title>Which recently developed algorithms do you think are interesting/noteworthy?</title><url>http://www.reddit.com/r/compsci/comments/zfpj4/which_recently_developed_algorithms_do_you_think/</url><snippet>I saw this [quora question](http://www.quora.com/What-are-the-most-important-algorithms-developed-in-2000%E2%80%932010) but I think the answers are incomplete. What else is "interesting" in recent computer science research? Which new algorithms will have a long-lasting impact? 
  I'm a cryptographer so I'm heavily biased here. But as far as I'm concerned, the most important development in computer science in 2000-2010 is Gentry's [fully homomorphic encryption](http://crypto.stanford.edu/craig/). I'm a cryptographer so I'm heavily biased here. But as far as I'm concerned, the most important development in computer science in 2000-2010 is Gentry's [fully homomorphic encryption](http://crypto.stanford.edu/craig/). Wait, full homomorphic cryptography?! I thought that was still in development  I don't know about any new recently developed general algorithms (from what I've seen, most new algorithms nowadays tend to be fairly domain-specific, like for machine learning or crypto), but there have been some new data structures developed recently that are very interesting:

* [Distributed hash tables](http://en.wikipedia.org/wiki/Distributed_hash_table) (2001)

It's basically a hash table with key/value pairs distributed over several nodes.  It's used for services like torrenting and distributed file systems, and the first few ones came out in 2001, but there's still on-going research in this field.  There's a [neat visualization of the BitTorrent protocol here](http://mg8.org/processing/bt.html), which uses a DHT called [Kademlia](http://en.wikipedia.org/wiki/Kademlia).

* [Tango tree](http://en.wikipedia.org/wiki/Tango_tree) (2004)

A tree data-structure that is search-only, but is able to do searches in O*(log log n)* time.

* [Quotient filter](http://en.wikipedia.org/wiki/Quotient_filter) (2007)

A data structure that's similar to a bloom filter (tests whether an element is in a set or not, with a small probabilistic chance of a false positive).  It uses up more space, but only requires evaluating one hash function, so it can be significantly faster. &amp;gt; Tango tree (2004)
&amp;gt;
&amp;gt; A tree data-structure that is search-only, but is able to do searches in O(log log n) time.

The Wikipedia page has it wrong. The tango tree is O(lg lg n) *competitive*, which means something different.

It is true that some searches may finish quickly, but that is also true of many other binary search trees. For instance, the element at the root of a red-black tree can be found very quickly.

The advantage of a tango tree is that it is no more than lg lg n times worse than the best offline binary search tree on the same input.

Finally, there is a lower bound on general predecessor search in the comparison model: lg n. Even in the RAM model, predecessor has an \Omega(\sqrt{lg n/lg lg n}) lower bound (assuming linear space), which is higher than lg lg n. &amp;gt; Tango tree (2004)
&amp;gt;
&amp;gt; A tree data-structure that is search-only, but is able to do searches in O(log log n) time.

The Wikipedia page has it wrong. The tango tree is O(lg lg n) *competitive*, which means something different.

It is true that some searches may finish quickly, but that is also true of many other binary search trees. For instance, the element at the root of a red-black tree can be found very quickly.

The advantage of a tango tree is that it is no more than lg lg n times worse than the best offline binary search tree on the same input.

Finally, there is a lower bound on general predecessor search in the comparison model: lg n. Even in the RAM model, predecessor has an \Omega(\sqrt{lg n/lg lg n}) lower bound (assuming linear space), which is higher than lg lg n.  [Cuckoo Hashing](http://en.wikipedia.org/wiki/Cuckoo_hashing).  [timsort](http://en.wikipedia.org/wiki/Timsort) - it's rare there's an advancement in general comparison based sorting algorithms Timsort is significant. It is the default sort in the Python libraries and will be in Java as well. And is a relatively recent algorithm.          I haven't read the quora question, but to me the most, uh, recent interesting thing was sha-1.  It was created based on an unknown (to the civilian community) weakness in sha, and even with the fix published, it was still years before the civilian community understood what made sha weak.

I don't really know why even seeing what the fix was it took so long to discover the cause, but it gave an indication of where military crypto technology was vs. civilian crypto technology.   PCP seems to me so counterintuitive that it's mind blowing. I don't pretend to understand it but I'd like to. PCP? PCP? Probabilistically checkable proofs and the PCP theorem.
http://en.wikipedia.org/wiki/PCP_theorem PCP seems to me so counterintuitive that it's mind blowing. I don't pretend to understand it but I'd like to. PCPs are important to the development of algorithms, but you'd be hardpressed to either call it an algorithm or a recent discovery (the PCP theorem was originally proven ~20 years ago, which is a long time by computer science standards).   What do you define as "recent"?  </snippet></document><document><title>Help with Undergrad Research</title><url>http://www.reddit.com/r/compsci/comments/zh4p3/help_with_undergrad_research/</url><snippet>Hello r/compsci

I am a 3rd year undergraduate Math/CompSci major and am looking into applying for a research grant for the coming summer.  I know it's early but I want to get a start on it.  The problem is I have no idea what I want to do my research in!  If you were me what areas would you be interested in, keeping in mind that the grant is only for 4 months.  Also, kind of important, I love algorithms and optimization and would like to have something in that area.  I know that is a HUGE area and I'm not being specific at all but a push in a direction would be greatly appreciated.

Thanks for your help!   I'll let you in on a little secret about grad school. The research you do is what your professor is interested in. So go find a professor who's doing work that interests you and go ask him/her.  Applying to an REU (Research Experience for Undergrads) program is also a valid option. You work with professors and grad students at other schools on research projects. You get paid and are given a stipend for housing and food. Would highly recommend. Big list of sites here: http://www.nsf.gov/crssprgm/reu/list_result.cfm?unitid=5049 Thanks, I am in Canada but I am sure that I can find something similar up here in the Great White North.  Time to get Googling!</snippet></document><document><title>Large scale databases without indexes?</title><url>http://www.reddit.com/r/compsci/comments/zfrqo/large_scale_databases_without_indexes/</url><snippet>I work at a very large company that deals with a huge volume of data.  However, the company as a whole avoids using virtually any indexes on the data(backend- Services provide speedy access for frontend).  We have the data partitioned on the date and the region that the data is tied to, but that is it.

This makes querying the data from the databases extremely, extremely slow, like 40 minutes to 2 hours.

The reasoning given by the DBA's is that for our case, we don't maintain indexes because it slows down the data insertion, and it is "faster" to just do a full table scan.

Is there any truth to this?  It seems very wrong to me that properly created indexes wouldn't at least speed up enough queries to be worth their insertion costs.  No indexing at all would only seem useful if you're constantly adding data to the db but almost never retrieving it or doing lookups, basically just hoarding data to hoard data. Is your company building a db to datamine later? You are right that in general indexing to speed up lookups is worth the minor insertion hit, but from what you've told us it seems like your db wasn't designed with lookups being a priority. With the right data structures you can get insertions to be very fast and still have fast lookups.  I don't know if they are hoarding data- I do know that they have a huge rate of insertions, but it seems logical to me that at least a few of our teams are hitting the full table at least once a week like mine.

The DB definitely wasn't designed for lookups to be a priority- I'm just confused as to why.  I know that for indexes using a B tree or binary tree that it will be log N insertion overhead... but these DBA's are much more specialized than me- surely they have thought of this?  :/ &amp;gt; but these DBA's are much more specialized than me- surely they have thought of this? :/

I would not trust too much in that preconception. I've often been surprised at how "professionals" were inept in various ways.

The only case I can think of that might explain not having indexes is if the selection query is in some way complex making it impossible or impractical to use them. For example if you're doing complex joins/groups/orders, or are extracting the whole database at once, or some other such reason. But there are only very, very rare cases where you can't benefit overall greately from using indexes.    Unfortunately, if you are using a classical relational database, it may be difficult to implement an indexing system that is truly adapted to your needs. What DBMS are you using?

You could, on the other hand, asynchronously queue the insertions/updates to an external system. If I were you, I'd model this system after Datomic's indexing: have an in-memory index that is periodically flushed to disk to keep the in-memory index small. When querying, merge the indexes (or the index results) to get a result that encompasses all results. The annoying part about this is that your query would have to be filtered through this system first before going to the database.

Then, there are also papers like this that have some interesting ideas: https://docs.google.com/viewer?a=v&amp;amp;q=cache:U3xNAjyiBssJ:www.mit.edu/~eugenewu/files/papers/shinobi-icde11.pdf+&amp;amp;hl=en&amp;amp;gl=us&amp;amp;pid=bl&amp;amp;srcid=ADGEESjb-payNdYo0AQQidG7hiJyooFzGLYtR657Armu7SHBTNeoOthKL_Mw2CQkmUpgltmXQZbTB8-GDbkQ3kjRf2_-9Ww6OBW559zClY3e-uaO36NyWoLjE8-gs5gsh7egzcISnQTg&amp;amp;sig=AHIEtbT5RcqVNssm53eKHwYQwtOYTQ1YDQ

In general, numbers are your friends. If you can prove that a technique improves the numbers for querying with an acceptable impact on the insert/update performance, you will have something concrete to talk about with the DBAs. Plus, it will allow you to answer your own question: "is there any truth to this?" All on Oracle.

I would think even if we needed to make 50 indexes or more, it would still be a gain... But maybe not?  I unfortunately don't have data on how many queries are run versus how many inserts are done.  It Depends.

As pack170 said, if there is a huge rate of insertions *(or if the DBMS is partiularly sucky and/or poorly administered)* then having no indexes makes some sense.  Although in that case you probably also have no relations and the fact that you're using an RDBMS is questionable in any case.

But you can't argue with what's there.  I'd suggest having a think about the sort of queries that are being done, and asking (yourself) questions like:  Are there any intermediate data structures ("summary tables" or "digest tables" some might call them) that could be created from the raw data to speed up queries?  Can such pseudo-indexing be done on a batched basis (eg daily - no realtime up-to-the-minute data will be available but that may not be neccessary if fast analytics emerge)?  Can the size of the query dataset be reduced - perhaps only the last 24 hours?  How can such operations be moved offline, so the main (insert-heavy) server(s) isn't impacted by queries/analytics being run on another machine?

If you want to attack this, you need to be extremely mindful of the criticality of the database server and that *nobody* is going to want to risk overloading or crashing it.  If you can confidently suggest a scenario to shift some data onto another server for better-performing analytics *without risk to the current server*, you might have a shot at making a difference.  One example might be to parse events from a textual log to shoot them off to another server rather than querying the (already heavily loaded) RDBMS tables.  Be aware that you will need to carefully consider worst-case performance and code defensively so that if anything goes wrong, your process doesn't bring the server down. We have some relations, and the insert rate is huge... but the query rate seems to me to be huge as well.  To handle the load we are scaling out copies of the table to many, many machines and distributing the query requests to those. We have some relations, and the insert rate is huge... but the query rate seems to me to be huge as well.  To handle the load we are scaling out copies of the table to many, many machines and distributing the query requests to those.  So many questions...    
What database management system are they using?    
Can you characterize the usage?  Strictly insert and read? Is it ever updated?  Does old data get archived?    
Is it used for a data warehouse or data mart?    
  The only reason I can think of for this is if it's for data warehousing.  If you're doing any transaction processing from it, it'll be a nightmare.  If you only do occasional queries for reporting it's probably okay. It is data warehousing, but I'm not sure that the queries are "occasional."  Our team at least is doing a weekly query that queries all of the last 7 days, and there are a lot of teams that use this data. It is data warehousing, but I'm not sure that the queries are "occasional."  Our team at least is doing a weekly query that queries all of the last 7 days, and there are a lot of teams that use this data. Well, for starters a typical data warehouse may not use many or any indexes.  The Insert once, but Select many times in a typical database would give credence to having index(es) as the cost on an Insert happens once while the benefit on Selects is realized multiple times.  You say a once weekly query that gets all data for the last 7 days.  Depending on your partitioning strategy this may well be faster as a full table read.  Consider this.  If all the data you read (7 days worth) happens to be 1/4 of a Monthly partition, then if an index were used it would likely involve reading the index (multiple reads here) to get the block and row ids of each row meeting your criteria, then reading the data blocks from the table.  If there are joins, cached blocks could be thrown out incurring more physical reads from disk. And to read 1/4 of a table via indexed reads will almost always be slower than reading the whole table. And the optimizer might not use the indexes anyway if it determines the cost is higher with them.  Sometimes the fortune you seek is in another cookie.  Can the query be scheduled to run 2 hours before any users need it?      </snippet></document><document><title>I have to make an uneducated decision - can you help me? (Machine Intelligence or Real Time Software)</title><url>http://www.reddit.com/r/compsci/comments/zgezj/i_have_to_make_an_uneducated_decision_can_you/</url><snippet>Hey guys,


I have to, as part of this semester's computer science, make a decision on a class I will go to (I cannot attend both, just one of them). I have been talking to professors, reading wiki and talking to other students. Ultimately, I think I grasp the fundamental difference between the two classes, where Machine Learning is about constructing agents that can automatize a given task - like seeing a pattern in huge data sheets, and RTS involving programming that revolves around the wall-clock. My brother has given me some examples on what this could be.


However, what I want to talk to you about, is the more theoretical aspect of things. When I think about AI's and machine intelligence, I think about trees, I think about Dijkstra's pathfinding algorithms (for robots on wheels!).. I also think about DFA's or NFA's - anything involving states and transitioning between them in that way... But that is pretty much it. Basically, I have no clue what "theoretical" aspects of Computer Science would be touched on if I chose MI or RTS. I find mutual exclusion incredibly interesting (threads and parallelism - throughput is hot!!!) - but I cannot make a choice yet until I know more about what theoretical topics would be probably gone over in either of the courses.


So if you have any ideas on a good ol' guess what my professors would want me to learn - can you give me a head's up? It would make it tentimes easier for me, I think, to get a better idea of what interest me the most.  ML is going to cover a lot of statistics.  In terms of theory, you'll see algorithms and maybe some graph theory, but it probably won't cover any sort of "core" CS theory.

RTS will probably discuss issues with scheduling, locking, I/O, etc.  It will definitely require a deeper knowledge of systems theory.  If you like threads and parallelism, then RTS might be more interesting to you since you'll have to think about how you design programs and how the OS schedules your programs. So one could perhaps even say that RTS will benefit me more as a programmer in the long run given I know more about the OS, scheduling, mutual exclusion and all that? Whereas MI is much more specific, to just ONE field?


Do you have any idea how you tell an algorithm what it has done before? I assume that is what AI's are all about. So one could perhaps even say that RTS will benefit me more as a programmer in the long run given I know more about the OS, scheduling, mutual exclusion and all that? Whereas MI is much more specific, to just ONE field?


Do you have any idea how you tell an algorithm what it has done before? I assume that is what AI's are all about. Yes, RTS will be more beneficial to you if your interested is programming.

ML has applications to a number of problems and fields.  It's very useful.  A strong foundation in ML can help you get a number of good jobs as it's very important to recommendation systems, determining what sorts of ads to show users, or really anything where analysis of large data sets is of relevance.

AI is not about telling an algorithm about what it has done before.  AI has a number of different subfields.  Some are involved with representation of knowledge and trying to develop mechanisms for computers to make decisions based on data rather than explicit algorithms.  Some are about allowing computers to understand human languages rather than converting everything to a computer language -- I'm thinking of natural language processing here.  Some involve methods for mimicking the behaviors of natural systems such as evolution to solve optimization problems.  AI is very broad.

Although ML may be classifiers as AI, I think its purpose differs from traditional AI. Traditionally, AI was concerned with endowing computers with the ability to reason likes humans -- that was by and large a failure.  ML is more about applying computers to analyze big data sets and make decisions based on statistical analysis of data that can change with the data rather than predetermined algorithms that would have to be reprogrammed every time a new situation arises that the programmers did not consider.  Does your school not provide a detailed syllabus?     

In ours you could see exactly what a subject was about, how it was graded, and what the required reading material was long before you signed up to it (before you even attended the program).    

This would include a list of exactly what points it would hit with a brief description.  </snippet></document><document><title>Winning isn't everything: Evolutionary stability of Zero Determinant
  strategies</title><url>http://arxiv.org/abs/1208.2666</url><snippet> </snippet></document><document><title>What makes a particular architecture friendly to caching?</title><url>http://www.reddit.com/r/compsci/comments/zdy6e/what_makes_a_particular_architecture_friendly_to/</url><snippet>Purely with relation to von Neumann based architectures.

For example why would there be differences in how friendly accumulator, stack and architectures based on register files were to caching. They're all retrieving instructions and data from memory, so I don't see the specifics have any bearing on how friendly each individual architecture is to caching?

Thank you!  Do you have a source for this? I agree with you, I can't see a reason. The memory hierarchy is relatively detached, and it essentially just puts data in your registers. Well, it's worth remembering that L1 cache is basically a single cycle still, pipelined, or a a few cycles if you have a hazard. You can ignore the growing divide between processor speed and memory if you can fit in a few dozen KB :-)  I see - thanks! This is really helpful. Do you have a source for this? I agree with you, I can't see a reason. The memory hierarchy is relatively detached, and it essentially just puts data in your registers.</snippet></document></searchresult>
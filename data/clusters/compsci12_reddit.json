{
  "processing-time-total" : 2347,
  "clusters" : [ {
    "id" : 0,
    "size" : 9,
    "score" : 115.14319099144967,
    "phrases" : [ "Regular Expressions" ],
    "documents" : [ 0, 4, 7, 12, 24, 25, 36, 48, 49 ],
    "attributes" : {
      "score" : 115.14319099144967
    }
  }, {
    "id" : 1,
    "size" : 8,
    "score" : 52.02591648040949,
    "phrases" : [ "Computer Scientist" ],
    "documents" : [ 0, 15, 18, 33, 39, 41, 44, 46 ],
    "attributes" : {
      "score" : 52.02591648040949
    }
  }, {
    "id" : 2,
    "size" : 6,
    "score" : 62.75516516188467,
    "phrases" : [ "Graph" ],
    "documents" : [ 0, 10, 12, 14, 32, 38 ],
    "attributes" : {
      "score" : 62.75516516188467
    }
  }, {
    "id" : 3,
    "size" : 5,
    "score" : 81.07314144026064,
    "phrases" : [ "Google Research" ],
    "documents" : [ 4, 12, 20, 35, 43 ],
    "attributes" : {
      "score" : 81.07314144026064
    }
  }, {
    "id" : 4,
    "size" : 5,
    "score" : 87.41156246160334,
    "phrases" : [ "Software Engineering" ],
    "documents" : [ 0, 1, 4, 25, 33 ],
    "attributes" : {
      "score" : 87.41156246160334
    }
  }, {
    "id" : 5,
    "size" : 4,
    "score" : 142.09813150908133,
    "phrases" : [ "Alan Turing" ],
    "documents" : [ 31, 36, 40, 41 ],
    "attributes" : {
      "score" : 142.09813150908133
    }
  }, {
    "id" : 6,
    "size" : 3,
    "score" : 49.374194384408916,
    "phrases" : [ "GPU" ],
    "documents" : [ 6, 9, 26 ],
    "attributes" : {
      "score" : 49.374194384408916
    }
  }, {
    "id" : 7,
    "size" : 3,
    "score" : 104.946909737104,
    "phrases" : [ "Neural Network" ],
    "documents" : [ 0, 12, 20 ],
    "attributes" : {
      "score" : 104.946909737104
    }
  }, {
    "id" : 8,
    "size" : 3,
    "score" : 57.202960695328464,
    "phrases" : [ "Rules" ],
    "documents" : [ 1, 22, 30 ],
    "attributes" : {
      "score" : 57.202960695328464
    }
  }, {
    "id" : 9,
    "size" : 3,
    "score" : 59.000098188585135,
    "phrases" : [ "Strategy" ],
    "documents" : [ 10, 45, 46 ],
    "attributes" : {
      "score" : 59.000098188585135
    }
  }, {
    "id" : 10,
    "size" : 3,
    "score" : 48.68160639898757,
    "phrases" : [ "Textbooks" ],
    "documents" : [ 0, 2, 25 ],
    "attributes" : {
      "score" : 48.68160639898757
    }
  }, {
    "id" : 11,
    "size" : 2,
    "score" : 91.12375048992381,
    "phrases" : [ "Design a User Interface which is Part" ],
    "documents" : [ 1, 29 ],
    "attributes" : {
      "score" : 91.12375048992381
    }
  }, {
    "id" : 12,
    "size" : 2,
    "score" : 52.30024255117971,
    "phrases" : [ "Email" ],
    "documents" : [ 6, 16 ],
    "attributes" : {
      "score" : 52.30024255117971
    }
  }, {
    "id" : 13,
    "size" : 2,
    "score" : 26.71625709419835,
    "phrases" : [ "Intel" ],
    "documents" : [ 6, 21 ],
    "attributes" : {
      "score" : 26.71625709419835
    }
  }, {
    "id" : 14,
    "size" : 2,
    "score" : 61.87078403857949,
    "phrases" : [ "Logo" ],
    "documents" : [ 31, 34 ],
    "attributes" : {
      "score" : 61.87078403857949
    }
  }, {
    "id" : 15,
    "size" : 2,
    "score" : 55.00948932914388,
    "phrases" : [ "Methods Declared Const" ],
    "documents" : [ 23, 42 ],
    "attributes" : {
      "score" : 55.00948932914388
    }
  }, {
    "id" : 16,
    "size" : 2,
    "score" : 59.48997509910031,
    "phrases" : [ "Speed" ],
    "documents" : [ 6, 17 ],
    "attributes" : {
      "score" : 59.48997509910031
    }
  }, {
    "id" : 17,
    "size" : 2,
    "score" : 74.46970696240615,
    "phrases" : [ "Turing Award Winners" ],
    "documents" : [ 27, 31 ],
    "attributes" : {
      "score" : 74.46970696240615
    }
  }, {
    "id" : 18,
    "size" : 9,
    "score" : 0.0,
    "phrases" : [ "Other Topics" ],
    "documents" : [ 3, 5, 8, 11, 13, 19, 28, 37, 47 ],
    "attributes" : {
      "other-topics" : true,
      "score" : 0.0
    }
  } ],
  "processing-time-algorithm" : 2275,
  "documents" : [ {
    "id" : 0,
    "title" : "What are some tips and/or tricks you wish you would have known going into a CS degree",
    "snippet" : "I am starting college soon for CS and would really like to know some tips you have picked up along the way and would have liked to have known when you started your degree. \n\nI took a programming class in HS learning VB.NET and am currently taking CS101 on Udacity. Anything I should know coming in that I probably don't already? \n\nThanks!  You need to do stuff outside of class. Not just programming. Practice the math stuff too, especially the stuff related to theory (although, I've heard from a lot of people that the theoretical side isn't stressed as much at many schools like it is at mine). Yep, I went into CS with a sporadic math background and only took what was needed. I regret this entirely, I'm now doing a PhD in CS and there are so many topics I wish I spent more attention or took classes in during my undergrad such as group/ring/finite field theory, number theory, probability &amp; stats, etc. Although I do dabble in each, just having a better math background is good for your CS hygiene. I would actually rephrase that a bit and say one of the most important skills for a computer scientist to have it the ability to learn a given area of mathematics quickly.  \n\nThere are so many areas that a computer scientist might need to know that it covers more ground than what a math major would cover in 4 years of undergrad.  But it is essential that when confronted with mathematics he/she does not have a background in a computer scientist has the confidence to say \"oh well, better get pull out a textbook\".\n\nI came to an MS in CS from English BA, and so had very little formal math background. At first I felt like this was a major failing until I realized that this really forced me to learn math on demand or avoid anything at all interesting in CS.   \n\nI consider this the lazy evaluation approach to mathematics ;) honestly I think CS programs need a course called \"How to learn math fast, and skim equations in papers\" Out of curiosity, how complicated was it to get into a cs masters program with only a BA? The requirements for a lot of programs now seem like they'd make it impossible to get in without a BS or at least a minor in cs. Yep, I went into CS with a sporadic math background and only took what was needed. I regret this entirely, I'm now doing a PhD in CS and there are so many topics I wish I spent more attention or took classes in during my undergrad such as group/ring/finite field theory, number theory, probability &amp; stats, etc. Although I do dabble in each, just having a better math background is good for your CS hygiene. Just out of curiosity, where does ring theory pop up? I'm primarily a mathematics student interested in algebra and I haven't seen rings applied to other fields nearly as much as groups are. The only thing I can think of is maybe Gröbner bases. Off the top of my head, polynomial rings are used for error correcting codes. Also cryptography (at least that's where I learned the notion of rings in the first place). Very interesting! Thanks a lot! Rings coming up in error correcting codes is completely new to me (primarily because I know almost nothing about error correcting codes) and although I know finite fields are pretty big in cryptography, I had no idea that rings in general were useful. I don't know to how great an extent rings that are not finite fields are used by themselves for cryptography, but seeing as how fields are special cases of rings, ring theory is real important. Plus also if you want to construct a finite field of order p^n where n &gt; 1, you'd do so by first constructing the polynomial ring over Z/pZ and then modulating it by an irreducible polynomial of order n over Z/pZ. &gt;modulating\n\nUsually we (mathematicians) would say modding out by.  Just out of curiosity, where does ring theory pop up? I'm primarily a mathematics student interested in algebra and I haven't seen rings applied to other fields nearly as much as groups are. The only thing I can think of is maybe Gröbner bases. You need to do stuff outside of class. Not just programming. Practice the math stuff too, especially the stuff related to theory (although, I've heard from a lot of people that the theoretical side isn't stressed as much at many schools like it is at mine). Er, just study and do your assignments and you'll be fine. It's misleading to say \"You need to do stuff outside of class!\". Of course you do, it's called homework and studying.\n\nDoing extra stuff is good, but you can get 4.0 grades (if that's your goal) without doing more than studying assigned-material etc. Hmm. I think it depends on what you want to do post-graduation. If you want a job programming, it's best to do some while in school beyond coursework, such as an open source project. You learn the ins and outs of working with others and you can build a portfolio. I agree, but mepcotterll's post is talking about doing math etc outside the normal coursework. You don't **need** to do anything outside the assigned  coursework, even if you want to get perfect grades. Er, just study and do your assignments and you'll be fine. It's misleading to say \"You need to do stuff outside of class!\". Of course you do, it's called homework and studying.\n\nDoing extra stuff is good, but you can get 4.0 grades (if that's your goal) without doing more than studying assigned-material etc. This is the answer I came here for.  Are you starting a computer science program? If so, best of luck to you! I recommend you do things outside of class that you find *fun*, most of all. Passion for some aspect of computing and computer science will be a major asset. I am in my first quarter, yes. And thanks! I'm actually loving it. I have never been good at math. However, there's a big picture I focus on when learning various mathematical concepts such as how it applies to computation and data storing/retrieval and that has sparked a new found curiosity and interest in math that has never existed. Suddenly, math has become a tool, a companion, rather than some grueling form of frustration.   Here's some things I've noticed both with myself and other classmates, and then in the transition to the real world:\n\n * Curiosity is your friend.  The more you dig in and comprehend WHY something works, the better off you'll be.  But sometimes, because of the nature of school, you'll have to settle for just enough understanding to pass the test.\n\n * The biggest thing you need to learn in Computer Science is that CS is all about a different way to think.  Developing your critical thinking and problem solving skills is key.  That was the major difference between the students that were able to suffer through the curriculum and those that excelled at it.\n\n * If you find yourself dreading the programming assignments, you need to change majors.  Even in the hardest classes I had to take, there was a certain amount of \"fun\" and \"curiosity\" that went along with most of the programming assignments.  Even for the profs that assigned insanely time-consuming projects, they were somewhat enjoyable (but even then there were times that I wanted to throw the machine out the window).\n\n * Don't give in to the \"oh crap, this is over my head\" that will inevitably hit.  Stick with it, and eventually it will click.\n\n * Knowing how to write code without a good understanding of how the computer works is a bad idea.  I remember fellow students who were in class with me trying to learn things like data structures, but didn't have the slightest idea where they were actually storing their files when they hit \"Save\" in the development environment.  And yes, I've even had to work with people like this in the real world.\n\n * The first step before coding ANYTHING is to fully understand the problem, from the point of view of the person WITH the problem. This becomes more important in the real world.  The more you can put yourself in the shoes of the average non-techie user, the more usable your software will be.  And the better overall understanding of the problem you have, the less re-working you'll have to do.\n\n * Resist the temptation to look up a solution online (or get the solution from a fellow student).  You'll end up being a better programmer for sticking it out and figuring it out yourself.  (And yes, in the real world, you WILL end up searching for solutions online, but you'll be finding a small piece of the much larger problem, NOT the entire solution to the problem)  office hours! always hit up the profs for things you need more explanation with. most students usually don't bother with office hours.   Go to all career fairs every single year.  Apply to companies that aren't at your career fair as well.  Apply for internships at reach companies in the fall.  If you get an internship, great!  If not, apply to all kinds of companies in the spring and try to apply to your reach companies again if possible.   I have a good tip: if you hate math, change the degree NOW, before it's too late. Can you clarify this a bit? My CS program required some math prerequisites (Calc 1 &amp; 2, Linear algebra), but the CS program itself really wasn't math intensive as I remember it. There weren't many times where I actually had to use knowledge I gained from those prereqs. Perhaps CS programs vary in the amount of math application, but I don't think a dislike of math automatically disqualifies you for CS.  When people think of \"math\" they think of what they experienced in high school: algebra, calculus, maybe some statistics. \"Math\" in the CS context is broader than that and covers \"mathematical thinking.\" Can you take a problem, express it quantitatively, abstract away extraneous details, perform the appropriate analysis to produce a solution, and convert it back to a less quantitative human-approachable form? Oh god when you describe math like that is sounds 100x better. I wish more people would do so. When people think of \"math\" they think of what they experienced in high school: algebra, calculus, maybe some statistics. \"Math\" in the CS context is broader than that and covers \"mathematical thinking.\" Can you take a problem, express it quantitatively, abstract away extraneous details, perform the appropriate analysis to produce a solution, and convert it back to a less quantitative human-approachable form? Ya, I took Discrete Math I last semester. That was one of the most interesting classes I've ever had, however hard it was.  Ya, I took Discrete Math I last semester. That was one of the most interesting classes I've ever had, however hard it was.  When people think of \"math\" they think of what they experienced in high school: algebra, calculus, maybe some statistics. \"Math\" in the CS context is broader than that and covers \"mathematical thinking.\" Can you take a problem, express it quantitatively, abstract away extraneous details, perform the appropriate analysis to produce a solution, and convert it back to a less quantitative human-approachable form? Computer scientists who disregard math as a major part of their trade are mostly just bad computer scientists.\n\nMath is crucial to good cs. Recursion, tree/set theory, search algorithms, computational complexity, memory management, best and worse case run times...the list goes on and on.\n\nAnd no, it's not traditional math per say, but it's harder and more interesting math. Your first discrete math class will be total hell, but it's all reallllly interesting and fun to think about. &gt;tree/set theory,\n\nWhat?  When people think of \"math\" they think of what they experienced in high school: algebra, calculus, maybe some statistics. \"Math\" in the CS context is broader than that and covers \"mathematical thinking.\" Can you take a problem, express it quantitatively, abstract away extraneous details, perform the appropriate analysis to produce a solution, and convert it back to a less quantitative human-approachable form? You're speaking the truth right there. Paul Lockhart talks about this in a very popular essay of his, [A Mathematician's Lament](http://www.maa.org/devlin/LockhartsLament.pdf). It's worth reading for the first two illustrations alone. When people think of \"math\" they think of what they experienced in high school: algebra, calculus, maybe some statistics. \"Math\" in the CS context is broader than that and covers \"mathematical thinking.\" Can you take a problem, express it quantitatively, abstract away extraneous details, perform the appropriate analysis to produce a solution, and convert it back to a less quantitative human-approachable form? Not all colleges don't make you do \"real math\". Here's the mathematics requirement for my CS degree:\n\n- Precalculus\n- Calculus (yes, yes, I know up to here is High School stuff)\n\n- Foundations of Mathematics (I hear it's dreadful and much more difficult than the title suggests)\n\n- Calculus 2\n\n- 3 \"advanced\" 300/400 level extremely theoretical classes\n\n\nHating math and loving programming is the worst combination possible for a CS degree. Shit sucks. I might have change majors. I have no idea. If you are concerned about the math in CS but still want to be a programmer, a major like info science/systems or software engineering might be better for you. Less emphasis on theory, more focus on real world application of it. You're probably right. It's a shame I'd have to switch schools and leave all of my friends behind, though. My school doesn't offer Software Engineering. I might just suck it up and work my way through math. You're probably right. It's a shame I'd have to switch schools and leave all of my friends behind, though. My school doesn't offer Software Engineering. I might just suck it up and work my way through math. Can you clarify this a bit? My CS program required some math prerequisites (Calc 1 &amp; 2, Linear algebra), but the CS program itself really wasn't math intensive as I remember it. There weren't many times where I actually had to use knowledge I gained from those prereqs. Perhaps CS programs vary in the amount of math application, but I don't think a dislike of math automatically disqualifies you for CS.  Depending on what you want to do. If you simply want to learn a bit of programming and go work for a bank or similar company and work on CRUD applications, then you really don't need it all that much. But then CS degree is a waste of time and money.\n\n\"Hot topics\" in CS right now (not all, but let's say some I take interest in)\n\n* Computer graphics - without intimate knowledge of linear algebra and some calculus, you'll be unable to do much beyond simple rotating cubes. BY COMPUTER GRAPHICS I DO NOT MEAN: modelling in Cinema 4D/3Dmax, photoshop or any such nonsense. I mean stuff like writing Id Tech 5 engine, where you actually **have to** programm all those things you get in the modelling software. Just go check the very simplest model of lightning and reflection (lambert) on wiki, and think how much more difficult the \"real stuff\" is.\n\n* AI, including: machine learning, image/optical recognition, speech recognition, data mining - all of these heavily rely on lingebra and calculus and statistics. (for example, see how neural networks work)\n\n* AI, including: autonomous robot guidance, automatic behaviour, navigation and stuff like that. Heavily uses logic and concepts from logic\n\n* AI, including: language processing, language recognition, automatic translation... again, statistics, some topics from logic, language theory\n\n* Theoretical COMPSCI - the list would be too long, but pretty much all of \"higher math\" including algebra, category theory, set theory, graph theory, logic, order theory... \n\nThere's many more I'm sure, this is what I study or take interest in. If you actually want to have some results and do something for the world/field, without very good math background you are completely useless.\n\nAgain, if you want to learn to program, then don't waste time with uni, pick a book and learn it yourself and get a certificate somewhere. Employers don't give a shit about what degree you have as long as you can produce working code.  Another cool aspect of computer graphics is simulation. There's the classical fluid and deformable (squishy) objects simulations, but even rendering is a form of simulating light bouncing around the scene in a physically accurate way. These applications require knowledge of calculus to formulate, and once they are discretized in order to be tractable, they can be formulated in terms of linear algebra. Take for example the two projects I for my graphics class: a [Deformable Object Simulation](http://inst.eecs.berkeley.edu/~avik/cs283/proj1/) and a [Radiosity Solver](http://inst.eecs.berkeley.edu/~avik/cs283/proj2/).\n\nIn my experience, linear algebra is to CS what calculus is to physics; it shows up *everywhere* in the field. Depending on what you want to do. If you simply want to learn a bit of programming and go work for a bank or similar company and work on CRUD applications, then you really don't need it all that much. But then CS degree is a waste of time and money.\n\n\"Hot topics\" in CS right now (not all, but let's say some I take interest in)\n\n* Computer graphics - without intimate knowledge of linear algebra and some calculus, you'll be unable to do much beyond simple rotating cubes. BY COMPUTER GRAPHICS I DO NOT MEAN: modelling in Cinema 4D/3Dmax, photoshop or any such nonsense. I mean stuff like writing Id Tech 5 engine, where you actually **have to** programm all those things you get in the modelling software. Just go check the very simplest model of lightning and reflection (lambert) on wiki, and think how much more difficult the \"real stuff\" is.\n\n* AI, including: machine learning, image/optical recognition, speech recognition, data mining - all of these heavily rely on lingebra and calculus and statistics. (for example, see how neural networks work)\n\n* AI, including: autonomous robot guidance, automatic behaviour, navigation and stuff like that. Heavily uses logic and concepts from logic\n\n* AI, including: language processing, language recognition, automatic translation... again, statistics, some topics from logic, language theory\n\n* Theoretical COMPSCI - the list would be too long, but pretty much all of \"higher math\" including algebra, category theory, set theory, graph theory, logic, order theory... \n\nThere's many more I'm sure, this is what I study or take interest in. If you actually want to have some results and do something for the world/field, without very good math background you are completely useless.\n\nAgain, if you want to learn to program, then don't waste time with uni, pick a book and learn it yourself and get a certificate somewhere. Employers don't give a shit about what degree you have as long as you can produce working code.  Can you clarify this a bit? My CS program required some math prerequisites (Calc 1 &amp; 2, Linear algebra), but the CS program itself really wasn't math intensive as I remember it. There weren't many times where I actually had to use knowledge I gained from those prereqs. Perhaps CS programs vary in the amount of math application, but I don't think a dislike of math automatically disqualifies you for CS.  You don't have to love it, but you have to do it. From my experience, math courses are the courses, that cause the most people to fail (before other courses would). Why? Not because they're especially hard (most people should be able to master them), but because (unlike in school) you have to actually work for them on a regular basis. Some more, some less of course. Can you clarify this a bit? My CS program required some math prerequisites (Calc 1 &amp; 2, Linear algebra), but the CS program itself really wasn't math intensive as I remember it. There weren't many times where I actually had to use knowledge I gained from those prereqs. Perhaps CS programs vary in the amount of math application, but I don't think a dislike of math automatically disqualifies you for CS.  Can you clarify this a bit? My CS program required some math prerequisites (Calc 1 &amp; 2, Linear algebra), but the CS program itself really wasn't math intensive as I remember it. There weren't many times where I actually had to use knowledge I gained from those prereqs. Perhaps CS programs vary in the amount of math application, but I don't think a dislike of math automatically disqualifies you for CS.  I have a good tip: if you hate math, change the degree NOW, before it's too late. I disagree. If you hate math but love programming, you are doing it wrong. Maybe you don't prefer some types of math, but you'll be a far stronger programmer in the end if you suck it up and learn a little discrete math. Once you're in the job, you'll find that the math you learned are necessary tools you use but most of your time you are dealing with actual code (actually most of the time you'll be thinking about design). Isn't that pretty much what I said? How is that disagreeing? :d I'm saying that people who hate math can still suck it up and learn to program. Also, the math that people dislike personally may be very different from the maths required to become a great programmer. Calculus almost never comes into play for business programming, for example. You don't need university to become good or even great programmer. That's my point. The best guys in the industry don't have degrees. That's misleading. Going to uni means you spend 4 years being forced to learn difficult concepts in programming presented by people who monitor industry trends and know what topics are fundamental to becoming an effective programmer/computer scientist. You'll be surrounded by peers who will no doubt be important connections after college.\n\nThe alternative is to take jobs doing very non-exciting things in the industry, desperately trying to work your way up past QA or support. Very non-exciting things like launching space ships and programming high-performance simulation engines. John Carmack would like a word with you.\n\nProgramming is not the point of computer science major and it is not something you need university education for. If you believe that 1 year of 2 hours per week + some stupid homework in java or C++ will make you into a programmer, you're just being silly :P\n\nStuff covered in most of the \"real world courses\" as people like to call them (operating systems, file systems, databases and similar crap) can be learned on your own with a cheap book and 5 days of time. University prepares you for a research and developement position. If you want to do anything else but that, you're wasting your time and money. If you need forcing to learn, you're wasting your time and money too.\n\nDisclaimer: using impersonal you. I have a good tip: if you hate math, change the degree NOW, before it's too late. I disagree.  I hate math, but I can do it, and I suffered through it to get my CS degree.  \n\nActually, let me make a slight correction... I hate doing math for the sake of doing math.  The times that the math is being used to do something really cool on the computer, it's cool.\n\nBut honestly, even if you don't like math, if you can stick it out for the degree, you can still do a lot with the degree with minimum real-world math usage.\n\nBut the one thing I wish I would've known before starting on my CS degree is that at my school, it was actually possible to get a math minor without any extra coursework.  There were math courses (I THINK linear programming was one of them) where they were offered by both the CS department and the Math department.  Either one was OK to use for the CS degree, but if you took the math ones, you'd have enough math credits for a math minor. yes. The worst thing I remember, this happened a few times.\n\nDoing something like the mechanics of an eigenvector (nobody in college ever showed how an eigenvector wasn't just some fanciful arbitrary set of operations) I would mistakenly have a dash of pen on my paper in front of a 1, making it a \"-1\" and then carrying it through the mechanics coming up with a wrong answer.\n\n\"0%?!?!?!?!? Go Fuck Yourself\" I'd think.  It's not like I drew a teddy bear or a house on the paper. If they want to teach a bunch of mindless mechanical computationalists, then clearly, except for a mishap in penmanship, I did the right incantations.\n\nNever mattered.  I even would fold the paper, and say \"OK, but from here on out is it correct.\" and then \"ok, from here on up is it correct.\"  The TA would be some picky asshole and be like \"duhrrrrr, yes, but duhhrrr doesn't matter\".\n\nAbsofuckinglutely ridiculous. If these things were attached to something that weren't just grueling arbitrary computationalism, getting a -1 instead of like a 500 would have been obviously wrong.\n\nThis is probably why physics was so easy. I've never had a problem with math. I've had a problem keeping track of, and organizing, a page full of numbers that have no grounding or basis.  I am not OCD like that. I have a good tip: if you hate math, change the degree NOW, before it's too late. I have a good tip: if you hate math, change the degree NOW, before it's too late. I wouldn't go this far. You might not be a good contender for a high end research job if you hate math. But there is plenty of room in software and web development for people lacking in math. If you hate math but can put up with it and do it because you love other aspects of computer science then I say go for it.\n\nI say this as someone who had a very tough time with math in college but enjoys his software development job very much and feels he is pretty good at it.  GET INTO AN OPEN SOURCE PROJECT NOW.  I can't stress this enough.  \n\nIt's for a few reason:\n\n1) You need to learn how to work in a group.  You will have group work to do.\n\n2) Coding in the real world is different from coding in an academic setting.  Sure you can write a PC speaker driver, but what does it really mean?  You also need to see how good and bad code is written.\n\n3) Most of CS is writing code, but also understanding how others code.  A lot of people do things in strange ways and you'll need to understand that in industry.  \n\n4) Being on a project may make you learn other languages...the lightbulb will come on and you'll see the similiaries across many different languages. \n\nMake sure you like math.  I couldn't hack math minor, but I could hack the math.  A lot of people can't.  If you don't like induction proofs, algorithmic analysis, and the like, you might want to find a different path.\n\nAs a final note: Don't freak out if you don't get it the first time.  Nobody gets everything the first time (no matter what they say).  CS is pretty tough in some areas and some areas you might not be strong in...that's ok.  \n\nI took to OS stuff like a duck to water, but when it came to compilers, I was pretty lost (I know...I know).  It was just too much of a disconnect for my poor little brain to handle.  \n\n GET INTO AN OPEN SOURCE PROJECT NOW.  I can't stress this enough.  \n\nIt's for a few reason:\n\n1) You need to learn how to work in a group.  You will have group work to do.\n\n2) Coding in the real world is different from coding in an academic setting.  Sure you can write a PC speaker driver, but what does it really mean?  You also need to see how good and bad code is written.\n\n3) Most of CS is writing code, but also understanding how others code.  A lot of people do things in strange ways and you'll need to understand that in industry.  \n\n4) Being on a project may make you learn other languages...the lightbulb will come on and you'll see the similiaries across many different languages. \n\nMake sure you like math.  I couldn't hack math minor, but I could hack the math.  A lot of people can't.  If you don't like induction proofs, algorithmic analysis, and the like, you might want to find a different path.\n\nAs a final note: Don't freak out if you don't get it the first time.  Nobody gets everything the first time (no matter what they say).  CS is pretty tough in some areas and some areas you might not be strong in...that's ok.  \n\nI took to OS stuff like a duck to water, but when it came to compilers, I was pretty lost (I know...I know).  It was just too much of a disconnect for my poor little brain to handle.  \n\n I know this sounds silly but what are some guidelines for actually picking a good open source project to work on as well as finding them. I know this sounds silly but what are some guidelines for actually picking a good open source project to work on as well as finding them.  Version control is awesome.  Learn how to use it.  Even better if you can get friends to use it on group projects. Can't stress this enough. Far too many graduates come out of uni having never heard of source control. I recommend putting all your code on github.com. it has really good tutorials for setting up git on your machine and how to use it.  Can't stress this enough. Far too many graduates come out of uni having never heard of source control. I recommend putting all your code on github.com. it has really good tutorials for setting up git on your machine and how to use it.  Version control is awesome.  Learn how to use it.  Even better if you can get friends to use it on group projects. Make sure that the people you are working with understand the tools.\nI've had an assignment that we almost failed because I fixed a bug but my partner merged it in from his branch. I didn't notice it because it was a corner case, and after a month I forgot about it and submitted it with the bug. And of course the corner case was met 5 times in the test. Version control is awesome.  Learn how to use it.  Even better if you can get friends to use it on group projects. Version control is awesome.  Learn how to use it.  Even better if you can get friends to use it on group projects. Version control is awesome.  Learn how to use it.  Even better if you can get friends to use it on group projects. I concur.  In addition, version control will pay for itself in saved time by your 3rd semester.  Got a bit of the thing working?  Commit it with a short log message about what works.  Broke it and don't know why?  Look at the differences!  You can always go back!\n    Man I don't know if i could get into an open source project. I don't feel that i have enough coding experience. But i'll try Man I don't know if i could get into an open source project. I don't feel that i have enough coding experience. But i'll try   It's not super complicated but it takes **a lot** of work. And remember, CS was made by humans. It's not something other fields like Physics and Biology have, they have to learn what nature created. Here you are learning what humans have created.  If you are stuck on a problem and can't get past it with more than 4 hours of brainstorming / work / banging your head on the keyboard, just stop working and call it a day.  I've had so many problems I stressed over trying to fix, only to quit and wake up the next morning and fix it in 5 minutes.  Welcome to Computer Science, where most resumes are made up and degrees don't matter.\n\nI have worked at large companies that you know about. I hire programmers all the time and I see 2 trends in the industry:\n\n1. We have to really validate resumes become people lie on them all the time; Especially people who are what we as Americans consider \"offshore\".  Its a dirty little secret of HR that If you are not white, be ready to have to deal with that bias even if it is unfair.  Its not that employers don't trust you, its that they don't trust anybody.  If something on your resume is even close to being possibly untrue, REMOVE IT.  You may be tempted to stretch the truth a little to get your start; DO NOT DO THIS as it will poison you and make you toxic down the road.\n\n2. Dirty little secret #1 is that it doesn't matter if the guy actually has a CS degree or not.  If you have a Math degree or a hard science degree, its really all the same.  Now this doesn't work if you have a liberal arts degree or a degree in \"woman's studies\" because all we care about is if the person is smart.  But having the degree in science, tech, engineering, math (STEM), helps prove that.  The rest is just paperwork and experience because either you are smart enough or you are not.\n\n3. Your education does not stop after university/collage.  If you think so, don't even bother going into CS.  You will spend thousands of dollars and countless sleepless hours every year to maintain and RE GET your computer science education every year after you graduate.  If you do not, you will lose out and eventually not be somebody anybody will want to hire.  You will also need to learn new languages languages all the time,  bare minimal 1 every 2 years and that is pushing it on how slow you are being at reeducating yourself.\n\n4. Unlike some professions where people think that \"if I just get this degree, then things will be ok\" CS is a constant re-education experience and having the degree doesn't not magically grant you anything at all.  If your only reason for going into CS is the money you think you will make do not bother.  Employers want people who do it for teh love and after a few years your resume will show if you love CS or not, so do not waste all that money if you do not love it.\n\nIf you really want to be a good programmer, work hard to un-learn everything VB thought you.  VB is a great language for non-programmers to get their feet wet, but a real programming software craftsman knows to use the best tool for the job, and thee are few if any cases to use VB over C# as things like \"on error resume next\" are more trouble than they are worth and teach bad habits you need to grow up and leave behind as you grow more experiences.\n\nLearn as many other languages and platforms as you can to get the experience, and do a project in all of them that you can open source and put on your resume. For example the first time I was hired by MS to be a software guy there, I was hired coming from a Linux/Gentoo background and I had code in many very public open source projects.\n\nIf you want to be a \"real programmer\" you need to learn the things that other experienced craftsman know and can teach. look up \"Software Craftsmanship\" and if you can not hold yourself to the same standard, don't bother with CS as it is not for you. &gt; You will spend thousands of dollars and countless sleepless hours every year to maintain and RE GET your computer science education every year after you graduate.\n\nWhat do you mean by spending thousands of dollars? Books, training courses, certifications, software to help you learn to code better software, etc * I don't spend more than $1000 per year on books and I'm a full time college student, buying extremely expensive textbooks.\n* Tutorials and documentation are free on the internet; if you're paying money for training courses then you're probably doing it wrong.\n* Don't employers typically pay for certification tests?\n* What software are you buying that's costing you so much per year? * I don't spend more than $1000 per year on books and I'm a full time college student, buying extremely expensive textbooks.\n* Tutorials and documentation are free on the internet; if you're paying money for training courses then you're probably doing it wrong.\n* Don't employers typically pay for certification tests?\n* What software are you buying that's costing you so much per year? My comment seems to have gotten lost but here is a recap:\n\n1. The textbooks you are buying as a student are cheap compared to some of the books you will be expected to have read even after your first year as a software developer.  And then, you will not be getting the discounts you are getting now.\n\n2. Look up oracle training books and classes sometimes.  Or agile/scrum training certifications, or any number of certs that HR requires to allow you to get even hired. Or the cost of Visual Studio or a full MSDN license just for yourself.  Or the cost of OS installs for VM's, or the cost of home server hardware, or the cost for your own VM hosting system.  Hardware and devices. Oh what? Your using your companies license keys?  Bad bad bad.. you should be professional and buy your own as otherwise you risk liability.\n\n3. Not everything is available for free on the internet; in fact some manuals cost hundreds of dollars as you can not just download for free some things.  And if you pirate them, you risk your job as its a legal liability - if you agree or not doesn't matter - if pirated stuff gets used in the corporate network.\n\n4. Companies do NOT always pay for certs.  Some may be willing to give you the time off without it hurting your vacation days earned, but companies that actually pay for everything and invest in you are a rarity these days because its cheaper for them to hire people who already know away from companies that used to do that and wised up and realized that when they did so they just created employees that had the power to take that extra training to demand more pay because of it, or go get hired somewhere else because they now had more skills and were worth more.  Companies will NOT invest in you and that is why you have to invest in yourself.\n\nAnd this is in no way all the costs.. Please give me an example of a book you're expected to buy and read on your own which is more expensive than a $150 textbook. And why in god's name would you even consider buying a personal VS or MSDN license? VS has express editions and for your home network there's always piracy and free-software alternatives. Piracy is NOT an option for a company work machine.  HR at every company I have seen has STRICT INSTRUCTIONS to FIRE any developer who does such a thing.  Also, as a developer you have to be ethical in all that you do.  You have the income, so why not buy it? When you are making over 100k/year what is your excuse not to pay?\n\nAs for open source, legal complications make it not an option for many companies.  In addition, the paid versions of things have features open source stuff either doesn't, or doesn't support very well.  For example the Ultimate version of VS has features the express version clearly doesn't. &gt; company work machine\n\n&gt; As for open source, legal complications make it not an option for many companies\n\n*The whole point* is that we're talking about home machines. Obviously your employer will pay for 100% of the software on their systems.\n\n&gt; For example the Ultimate version of VS has features the express version clearly doesn't.\n\nYou really need Team Foundation Server and application lifecycle management for your home network?...\n\n&gt; You have the income, so why not buy it? When you are making over 100k/year what is your excuse not to pay?\n\nSo that you can retire.\n\n&gt; Also, as a developer you have to be ethical in all that you do.\n\nDevelopers aren't licensed engineers. * I don't spend more than $1000 per year on books and I'm a full time college student, buying extremely expensive textbooks.\n* Tutorials and documentation are free on the internet; if you're paying money for training courses then you're probably doing it wrong.\n* Don't employers typically pay for certification tests?\n* What software are you buying that's costing you so much per year? Look up oracle training courses. Or Scrum/Agile Training Certifications.  Some companies require them to even get hired and you cant get tehm on the internet for free and still meet HR requirements. ;)\n\nA lot of \"easy\" books are very cheap; I'm talking about the stuff that is larger then textbooks. Each book will run you at least 100$, unless you can get them used and then its more like 50.. but still. If you think the books your reading (textbooks) are hard, you are in for a surprise later when you get into the things most people don't add to curriculum because its considered too hard to teach and not profitable enough.. but you should read them anyway because its sort of expected professionally.\n\nWork does not always pay for certification tests; Why should they when you love what you do? Or at least that is what they think.\n\nAnd what about home systems/computers?  A new home server to play on?  A laptop? That new ipad? In the end you spend thousands and don't even realize it. &gt; A lot of \"easy\" books are very cheap; I'm talking about the stuff that is larger then textbooks. Each book will run you at least 100$...\n&gt; If you think the books your reading (textbooks) are hard, you are in for a surprise later when you get into the things most people don't add to curriculum because its considered too hard to teach and not profitable enough.\n\nWhat the fuck are you talking about? I guarantee you that the textbooks I have to buy aren't easy, and they're not nearly as cheap as $100. Also, the graduate-level texts in math and CS are cheaper, not more expensive, than the undergraduate texts I've been buying. So what the fuck are you even talking about? What is \"larger then\" textbooks?\n\n&gt; Work does not always pay for certification tests; Why should they when you love what you do?\n\nIf they're requiring it, they should pay for it. If they're not requiring it, why are you doing it?\n\n&gt; And what about home systems/computers? A new home server to play on? A laptop? That new ipad? In the end you spend thousands and don't even realize it.\n\n.... toys for myself don't count as study or training. You may think they are not easy.. but they are beginner stuff.  You are just starting to learn after all.  As you learn more and more you will understand what hard really means.\n\nWhy don't you tell me what books you are buying and how much, so I do not have to make assumptions are comparisons to my own education?\n\nThey require it because they can.  They don't have to make it a hard requirement, but they can fire you if you do not know it and they give you tasks to do that require you know it to do the work.  Happens all the time.  Once again, your lack of experience is showing and I am simply trying to help you.\n\nToys as you put it may not count, but the skills you gain from having them help you professionally, and so its expected that you have them.  After all how are you supposed to effectivly dev something for the latest mobile device if you have never used one as a customer? OK..\n\nhttp://www.amazon.com/Contemporary-Abstract-Algebra-Joseph-Gallian/dp/0547165099/\n\nhttp://www.amazon.com/Algorithm-Design-Jon-Kleinberg/dp/0321295358/\n\nhttp://www.amazon.com/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844/\n\nhttp://www.amazon.com/Introductory-Combinatorics-5th-Richard-Brualdi/dp/0136020402/\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/\n\nAnd I guarantee you those nice amazon prices aren't that steeply discounted around the start of the semester, and certainly not in any brick-and-mortar bookstore. Like I said.. the books are expensive.  You just proved my point better than i could have done so myself as the books are clearly even more expensive than I had thought.       \nDo not expect you will be give much instruction about how to program in a CS school. Other than some basics, your professors will likely focus on concepts and leave implementation to be a self directed activity. If this bothers you, you are may not be cut out for CS.\n\nFocus on taking the classes where you will learn the most. Never ever take classes based on how easy it is to get a grade in them. It is much more important to learn a lot than to worry about grades. The ideal scenario here is to take a class where you push yourself to learn, and still get great grades. It is very doable. Don't take short cuts in learning. ( e.g. See: Which is more beneficial to learn as a first programming language, Java or C++? Why? ).\n\nAlways try to build a unified view of computer science various different classes that you take. Your Theory of Computation class can teach you something valuable for your Compilers class. Your algorithms class is related to your graphics class. Your graphics class is related to your linear algebra class. As and when there is an overlap of knowledge in different classes, use that overlap to your advantage to continually reinforce your learning. Pre conditions and post conditions you learn about in correctness proofs of programs map directly to asserts. Keep on collecting pieces of the puzzle and keep putting them together.\n\nDo not sacrifice theoretical learning for implementation centric learning, and vice versa. They are both duals of each other, and not at odds with each other. If you sacrifice theory, it will deprive you of a much needed analytical framework to rigorously scrutinize the complexity of algorithms. If you sacrifice implementation, you might as well be a math major specializing in discrete math. Likewise, do not sacrifice depth for breadth or vice versa.\n\nDon't fret about ignoring your non CS classes, particularly if you want to work in the industry. I didn't pay attention to many of them, and had B's bogging down my GPA but whatever.\n\nYour algorithms, OS and compiler classes will likely be your most important classes. Don't just stop at what your teacher tells you, be a sponge and absorb anything and everything you can learn about those topics. They will stand you in good stead for a long, long time. ( See: Programmers: What skills do self-taught programmers commonly lack? )\n\nChoose your collaborators and study partners well. They can really inculcate the right mindset for learning in you.\n\nIt helps to develop good relationships with your professors, on multiple fronts. It generally feels good to look back and reflect on how you didn't think of people that you learnt from with a sense of hostility. This is very important if you want to apply to graduate school, want to do research with a professor or have them help you publicize jobs for your company when you are later working in industry.\n\nNot generic to CS, though it goes without saying that you should establish good friendships. Your college friendships will last a long time, and will be a social network as well as a professional network for a lifetime.\nTry to get your programming projects done well in advance of deadlines, ideally have them ready for submission a week in advance so that you are always debugging and programming with a relaxed mindset. Make backups of your work out of paranoia.\n\nParticipate in programming competitions if you can. You have nothing to lose, and pushing yourself to learn will be helpful. ( See: Does ACM-ICPC or IOI success correlate with industry success? )\n\nYour first job or other pursuit after school will be very formative. Be sure to invest appropriately in ensuring you choose well. Naively picking the job that simply maximizes your paycheck may well be a sub optimal decision.\nSpend time going the extra mile to learn about your OS ( Linux: What are some time-saving tips that every Linux user should know? ), your debugger ( eg: How can one become as efficient at using GDB as in using a visual debugger? ) and your editor ( e.g. Vim: What are some favorite Vim commands? ). They can be very useful for improving your productivity and what you learn you get to keep.\n\nAt the end of the day, you should realize that a large part of your education is teaching you how to learn to learn rather than specific coursework.\n\nIts tempting at times to write programs that satisfy your project requirement, and could get past TA grading, but won't meet the cut for a real program. e.g. its easy to leak memory in your assignments and get full credit for it, though try to push yourself to do a good job here and try to be a perfectionist in whatever code you do control.\n\nMake good use of opportunities to listen to lectures and talks that aren't a part of official coursework. In the case of particularly prolific speakers, one good talk can influence your thinking for a lifetime. Some Dijkstra lectures I attended still echo in my head\n\n  \n&gt; by Kartik Ayyar,from Quora.com  Swap two variables without the use of a third by xor-ing them twice. You are welcome.  That's a (well) known trick. You'd be surprised how many \"senior\" software developers I've told that two that were blown away by it.  But it is usually covered in introductory programming texts. Yeah, I'm probably posting that in the wrong reddit. I'm not comp sci, but I program. But isn't programming part of comp sci (CS)? I think of /compsci as not a programming reddit, but an acedemic reddit about computer science.  For example, I've never once ever considered the  P vs NP problem as a \"programmer\"     Find an editor you like. My first semester was spent almost entirely in BlueJ and it was terrifically annoying.\n\nScript ALL the things. Any chance you have to write yourself a little script or tiny program to automate or help you with something you do a lot feels (to me, anyway) very rewarding. As it turns out, there's a lot of stuff you can do *before* you get your degree and work your way up the food chain to become a software engineer.\n\nYour own career path may vary, but the point remains that making stuff for yourself gives you a tangible and immediate reward which makes the whole thing very fulfilling and keeps you practicing and honing your skills and coming up with ways to apply the lessons you learn in class. &gt; Find an editor you like.\n\nSecond. In my first programming class, my first quarter of college, they taught us to use xedit. xedit has this terrible design by which you enter the filename then click either load or save. If you load instead of save, it'll overwrite your file without prompting. If you save instead of load, it'll overwrite your file on disk with an empty buffer. Many hours were lost redoing work because I didn't know any better at the time. To this day, I'm convinced that the TA just hated undergrads and did it to spite us.\n Xedit is horrific. At least BlueJ allowed us to get used to OO programming by drawing its own dependency charts/UML like stuff and let us create instances of objects and call functions on them without having to write test drivers and the like. Xedit had no redeeming features I ever found.\n\nI found it hilarious when my classmates and I all started branching out to our own editors. In particular, hearing the sporadic beeps (from the computer) and swears (from the user) around the room was a good way to pick out which of us were learning vi or Vim. Oh man, that beep from old Solaris machines when you hit backspace once too many. No sound is more frustrating.\n\nAnd the time freshman year when we figured out how to disable terminal beeps.   don't try to make it perfect.  make things that compile and mostly work.  your professors and TAs don't care if you fixed a little bug if it segfaults when you run it, so make sure when you're doing your work, you get the big things out of the ways first.\n\nalso along the same lines, if something's taking too long to implement or figure out, leave some comment notes for yourself (or TAs reading your code), comment out stuff that causes errors and move on.\n\ni'd also really recommend learning to use good tools.  it may seem \"uppity\" to tell you to use a \"real editor\" or \"real compiler\" or debugger or whatnot, but i'd really recommend spending time while you're just going in learning something like vim or emacs, or even just getting really good at notepad++ or scite or something in general you like.  using windows notepad or nano for hours and hours gets really old really fast.  for debuggers, find ones that are well documented and do the job well, like gdb, valgrind, or dr. memory (i mainly program in c/c++ for my classes).  i wish i'd learned to use gdb earlier.  same thing with compilers, i personally like gcc and g++ but one of my friends swears by clang (i can't get it to work though).\n\nalso consider what your professors and classmates/friends know.  if your TAs know how to use gcc, don't use visual studio.  that's where a lot of my classmates run into trouble.\n\nand, if you can't tell from the rest of my post, i'd recommend being comfortable on linux.  having windows as well is a good thing, especially if you have to use windows languages (like .net and vbs) but knowing how to use linux, and having the option there (i'd recommend a dual boot as opposed to something like cygwin) is nice in case you run into trouble.  if you're on mac, i have no idea.  i know you have a bash-like terminal, and it's unix based, so you shouldn't need a dual boot.\n\n**tl;dr** get the big things done first, don't make things perfect, find good tools that you like and can get help with and learn them, set up a dual boot and learn linux &gt;your professors and TAs don't care if you fixed a little bug if it segfaults when you run it,\n\nThe good ones do. My best professor uses a script he made to test for all the cases he can think of. So your code to concatenate two files fails if one of the files contains a line over 5000 characters long? That's ten points. It's not as ridiculous as it sounds since normally he will give good instructions and is always willing to help with assignments(which aren't too big a portion of your grade). Really helps you to think more thoroughly about what exactly you are doing and how it could fail.        ",
    "url" : "http://www.reddit.com/r/compsci/comments/w83g4/what_are_some_tips_andor_tricks_you_wish_you/"
  }, {
    "id" : 1,
    "title" : "Art in software engineering. ",
    "snippet" : "I've been programming for nearly 11 years and I have been studying Computer Science for the last 2. In all those years I realized, that my ability to create a nice looking graphic layout has not improved. Every program I design looks ugly. What are some tips I can use to make the GUI in my programs look better?   I hear you man, I ended up getting a graphic artist to deal with all the colours, and look and feel. Here are the basic rules we use. I don't know what language or platform you are using so this is going to be generic.\n\n* Define your interface flow base upon the culture you are writing for. North Americans read left to right top to bottom. Thus the upper left corner will be the first place they look so put information about where they are at there. They will return to this point when they are lost. in Japan it is the same for horizontal writing left to right then up to down, but for vertical writing it is up to down then right to left. Thus the upper right corner would be a better place for a navigation system.\n\n* Style EVERYTHING. use CSS on all objects, when we start web interfaces we create a generic class for all form objects. When we need a more specific type we create another class for it. ALWAYS set the class when you create the object. Thus when your client decides to change your colour scheme it is easy.\n\n* Don't use default colours. It is easy to get great colour pallets. Here this program gives you a colour pallet based upon an image. http://www.degraeve.com/color-palette/ It is common for me to give the client pictures and say which one tickles your fancy for colour. This one is more technical http://colorschemedesigner.com/ We use this one when we are dealing with colour tone.\n\n* Use borders, and panels to collect ideas into a similar location. Use colour to denote similar information on different forms.\n\n* Strive for consistency. Make sure if you put your OK, Cancel in the lower right that they stay there AND never become Cancel, OK.\n\n* Use  Shortcuts. People when they get started on an app love it when the app helps them get to the end point. They get frustrated after the Nth time they have to do the same operation via multiple screens or a wizard. Allow your users to grow with your app making it easier for them to shortcut to the end point. This may mean you have to have another page that consolidates multiple screens into a single screen but your are writing N-Tier anyways are you not ;) Keyboard shortcuts for the web are done with the onkeypress-DOM and JQUERY has a great method for implementing this.\n\n* Make sure you give feedback early and often. For every operator action, there should be some system feedback. For frequent and minor actions, the response can be modest, while for infrequent and major actions, the response should be more substantial.\n\n* Look at other interfaces and mimic them. Your client wants the app to look like Outlook. Make it look like outlook. \n\n* Look deeply at interfaces and define what you like and don't like about it and use what you like.\n\nThere is a lot more that we could go into, but the cardinal rule of interface design is \"A user interface is well-designed when the program behaves exactly how the user thought it would.\" This goes for the application look and feel as well So get users testing your interface early and often. Prototype interfaces and let the users see it. Let people give you examples of programs they like and study them.\n\nI hope this helps. lets get a discussion going about this.\n\n &gt; Style EVERYTHING\n\n&gt;Don't use default colours\n\nPlease, use default colors and styles if you are not a designer. I'd rather have boring gray application than something that looks like it has escaped from crayola factory. The problem with default colors is that they're often too bright. Using system UI elements is okay, but if you're going custom you better use a proper palette. &gt; Style EVERYTHING\n\n&gt;Don't use default colours\n\nPlease, use default colors and styles if you are not a designer. I'd rather have boring gray application than something that looks like it has escaped from crayola factory. &gt; Style EVERYTHING\n\n&gt;Don't use default colours\n\nPlease, use default colors and styles if you are not a designer. I'd rather have boring gray application than something that looks like it has escaped from crayola factory. I don't want crazy colours either but I do want to be able to supply what the client is looking for. Putting a style on an element doesn't prevent you from leaving as many defaults in place as possible. &gt; Style EVERYTHING\n\n&gt;Don't use default colours\n\nPlease, use default colors and styles if you are not a designer. I'd rather have boring gray application than something that looks like it has escaped from crayola factory. &gt; Style EVERYTHING\n\n&gt;Don't use default colours\n\nPlease, use default colors and styles if you are not a designer. I'd rather have boring gray application than something that looks like it has escaped from crayola factory. I hear you man, I ended up getting a graphic artist to deal with all the colours, and look and feel. Here are the basic rules we use. I don't know what language or platform you are using so this is going to be generic.\n\n* Define your interface flow base upon the culture you are writing for. North Americans read left to right top to bottom. Thus the upper left corner will be the first place they look so put information about where they are at there. They will return to this point when they are lost. in Japan it is the same for horizontal writing left to right then up to down, but for vertical writing it is up to down then right to left. Thus the upper right corner would be a better place for a navigation system.\n\n* Style EVERYTHING. use CSS on all objects, when we start web interfaces we create a generic class for all form objects. When we need a more specific type we create another class for it. ALWAYS set the class when you create the object. Thus when your client decides to change your colour scheme it is easy.\n\n* Don't use default colours. It is easy to get great colour pallets. Here this program gives you a colour pallet based upon an image. http://www.degraeve.com/color-palette/ It is common for me to give the client pictures and say which one tickles your fancy for colour. This one is more technical http://colorschemedesigner.com/ We use this one when we are dealing with colour tone.\n\n* Use borders, and panels to collect ideas into a similar location. Use colour to denote similar information on different forms.\n\n* Strive for consistency. Make sure if you put your OK, Cancel in the lower right that they stay there AND never become Cancel, OK.\n\n* Use  Shortcuts. People when they get started on an app love it when the app helps them get to the end point. They get frustrated after the Nth time they have to do the same operation via multiple screens or a wizard. Allow your users to grow with your app making it easier for them to shortcut to the end point. This may mean you have to have another page that consolidates multiple screens into a single screen but your are writing N-Tier anyways are you not ;) Keyboard shortcuts for the web are done with the onkeypress-DOM and JQUERY has a great method for implementing this.\n\n* Make sure you give feedback early and often. For every operator action, there should be some system feedback. For frequent and minor actions, the response can be modest, while for infrequent and major actions, the response should be more substantial.\n\n* Look at other interfaces and mimic them. Your client wants the app to look like Outlook. Make it look like outlook. \n\n* Look deeply at interfaces and define what you like and don't like about it and use what you like.\n\nThere is a lot more that we could go into, but the cardinal rule of interface design is \"A user interface is well-designed when the program behaves exactly how the user thought it would.\" This goes for the application look and feel as well So get users testing your interface early and often. Prototype interfaces and let the users see it. Let people give you examples of programs they like and study them.\n\nI hope this helps. lets get a discussion going about this.\n\n Some of these seem more applicable for web apps. There's usually no reason to change the color scheme of a desktop app—stick with the user's or operating system's theme.\n\nYou could read Apple's [Human Interface Guidelines](https://developer.apple.com/library/mac/#documentation/userexperience/Conceptual/AppleHIGuidelines/Intro/Intro.html). Yes, it's written for OS X developers, but a lot of it can be used to make good-looking UIs for any platform: how to capitalize text consistently, how to decide spacing between window elements, etc. Some of these seem more applicable for web apps. There's usually no reason to change the color scheme of a desktop app—stick with the user's or operating system's theme.\n\nYou could read Apple's [Human Interface Guidelines](https://developer.apple.com/library/mac/#documentation/userexperience/Conceptual/AppleHIGuidelines/Intro/Intro.html). Yes, it's written for OS X developers, but a lot of it can be used to make good-looking UIs for any platform: how to capitalize text consistently, how to decide spacing between window elements, etc.    Hire someone to do it.\n\nWould you let your designer write code? No, probably not. So why would you let your software engineer design?\n\nTwo completely orthogonal professional disciplines. A good designer is worth the money.    You will need to learn some basic principles of graphic design. Although it's not directly about UI design, this book lays out the important principles in a very easy to understand way:\n\n[The Non-Designer's Design Book, Robin Williams](http://www.amazon.com/Non-Designers-Design-Book-Robin-Williams/dp/0321534042) (no, not that Robin Williams).\n\nI honestly can't recommend this book enough. As a non-artist, it completely changed my way of thinking about screen layout, page layout, text, and visual relationship.\n\nAlso check out [Robin Williams Design Workshop](http://www.amazon.com/Robin-Williams-Design-Workshop-Edition/dp/0321441761)\n\nI also would consider reading a book about usability, such as Alan Cooper's [About Face](http://www.amazon.com/About-Face-Essentials-Interaction-Design/dp/0470084111). This won't help you with design, but it will give you a clearer picture of why some things work better than others. It will be a good complement to what you learn from the graphic design books. Someone else mentioned Don Norman's [Design of Every Day Things](http://www.amazon.com/Design-Everyday-Things-Donald-Norman/dp/1452634122). That's also a good book and shorter than the Cooper book, although neither really gets you thinking visually -- which is what's so great about Robin Williams.         Take an introductory art course. It will help you learn to observe objectively. Art is all about observation and critical honesty.\n\nYou'll learn to see things in terms of line, shape, color, contrast, gradient, etc and learn to recognize good composition. That isn't enough to start designing though.  A basic art class doesn't get into the differences between, say, a painting and a UI. A UI has a specific purpose to communicate, allow interaction and then give feedback. There are many ways to get this communication wrong that a 101 art class is not going to discuss. \n\nFor example, that class wouldn't go over what fonts to use, how many to use on one page, and when to use them. What colors can you use that colorblind people will see, or how the elderly or disabled would view this application. How would someone from a different part of the world view your color choices?  What buttons should map to which functions in a way that is fastest to learn, or consistent with their experiences (eg,  F1 for the help menu, ctrl-z for undo)? \n\nLikewise, just because an artist is good at making something aesthetically pleasing does not mean that it is very usuable. [This website](http://www.realbusiness.com/), for example, looks like it was made by artists who knew what they were doing, but, it's got real long loading times, and it's not practical for gathering information from it? \n\nBasically what I was trying to say that design is not something that can just be picked up by an engineer. It's another discipline that even other artists don't understand if they're not trained in it. The other lesson here is to watch what kind of designer you hire and make sure they actually know what's usuable and what's superfluous flashy photoshop work. \n\n That isn't enough to start designing though.  A basic art class doesn't get into the differences between, say, a painting and a UI. A UI has a specific purpose to communicate, allow interaction and then give feedback. There are many ways to get this communication wrong that a 101 art class is not going to discuss. \n\nFor example, that class wouldn't go over what fonts to use, how many to use on one page, and when to use them. What colors can you use that colorblind people will see, or how the elderly or disabled would view this application. How would someone from a different part of the world view your color choices?  What buttons should map to which functions in a way that is fastest to learn, or consistent with their experiences (eg,  F1 for the help menu, ctrl-z for undo)? \n\nLikewise, just because an artist is good at making something aesthetically pleasing does not mean that it is very usuable. [This website](http://www.realbusiness.com/), for example, looks like it was made by artists who knew what they were doing, but, it's got real long loading times, and it's not practical for gathering information from it? \n\nBasically what I was trying to say that design is not something that can just be picked up by an engineer. It's another discipline that even other artists don't understand if they're not trained in it. The other lesson here is to watch what kind of designer you hire and make sure they actually know what's usuable and what's superfluous flashy photoshop work. \n\n Oh fuck off.\n\nThe dude doesn't have time to take another 20 courses to get a 2nd degree. Idiot.",
    "url" : "http://www.reddit.com/r/compsci/comments/w6lp0/art_in_software_engineering/"
  }, {
    "id" : 2,
    "title" : "From Nand to Tetris - The Elements of Computing Systems: a course and a textbook.",
    "snippet" : " ",
    "url" : "http://www1.idc.ac.il/tecs/"
  }, {
    "id" : 3,
    "title" : "The architecture of Berkeley DB.",
    "snippet" : "   Excellent resource, documenting architectures for lot of the cool stuff out there in the wild - HDFS, LLVM, Bash, Git, GDB, nginx to name a few. Thanks a bunch for posting this.\n\nHow did you find this? (If you don't mind my asking)",
    "url" : "http://www.aosabook.org/en/bdb.html"
  }, {
    "id" : 4,
    "title" : "What was computer science like in college?",
    "snippet" : "I'm going to be a freshman at Arizona State in August majoring in Computer Science (Software Engineering). Though having the degree is going to be extremely helpful, I'm curious about the content in which CS students learn. It almost seems like depending on your goals you may not be ready for a career when you get out. Is this true?  If you expect to only do course work and come out and expert programmer then it won't happen. They will teach you about computer science or software engineering, you need to learn how to program on your own.\n\nIf you spend you summers doing internships or design games and webpages for the hell of it you will realize the difference between someone who can program and someone who can't when you do group projects in your final years. There is a BIG difference. If you expect to only do course work and come out and expert programmer then it won't happen. They will teach you about computer science or software engineering, you need to learn how to program on your own.\n\nIf you spend you summers doing internships or design games and webpages for the hell of it you will realize the difference between someone who can program and someone who can't when you do group projects in your final years. There is a BIG difference. I'm a rising sophomore, though I've already taken what most compsci students at my school take by the end of sophomore year, and I disagree with this to some extent. I've definitely learned a lot about programming from my classes. I have a few classes that are actually, genuinely \"Learning to program\" classes, and they're not all beginner level, either. There's a 300 level course that teaches you to program in C++ without learning too many new concepts.\n\nAnd sup, bro?\n\nEDIT: I feel the need to clarify that I also agree that those who program outside of class certainly have a better understanding of application than anyone else, and there are differences even in the first couple of classes between those who like programming and those who don't care about good practice and things like that. That said, I think this is true of *any* skill. How much programming outside of class have you done, though?\n\nYou've learned a lot about programming, I'm sure. It feels like a lot, at least. But even in the 'learning to program' classes, it feels like a lot more than you're really learning. I thought I was a big baller until I started trying to do things outside of class and realized exactly how little I knew.\n\nThe professors and classes usually just don't have the time to teach the depth of knowledge that comes with really trying things on your own. Sure, you know what a linked list is. But can you use that linked list to actually *do* things, other than the project you worked on for class? I'm a rising sophomore, though I've already taken what most compsci students at my school take by the end of sophomore year, and I disagree with this to some extent. I've definitely learned a lot about programming from my classes. I have a few classes that are actually, genuinely \"Learning to program\" classes, and they're not all beginner level, either. There's a 300 level course that teaches you to program in C++ without learning too many new concepts.\n\nAnd sup, bro?\n\nEDIT: I feel the need to clarify that I also agree that those who program outside of class certainly have a better understanding of application than anyone else, and there are differences even in the first couple of classes between those who like programming and those who don't care about good practice and things like that. That said, I think this is true of *any* skill. I’ve been programming since high school, and I’m about 2 semesters away from getting my BS in CSc.  At my university, many of the *programming* classes encourage more of a hack-job than anything else.  Most of the time, my professors don’t care how the code looks as long as it performs the task it’s supposed to.  Team projects through school, along with my job, have shown me that most people don’t consider the future.  It’s not uncommon that I’m asked to improve features/repair software that is complete spaghetti code--unreadable and un-scalable.  If you hand your source off to your peers and they react by giving you a look like they just ate a bad burrito from taco bell, you’re probably not as good of a programmer as you think you are.\n\nEdit: Getting \"A's\" in all of your programming classes does not mean you are a good programmer. I definitely have room for improvement (I've only been coding for two years), but I'd say I'm pretty good, though I don't typically like working with others because they tend to not be good.\n\nMy school is quite good about good practice. Particularly the TAs are all over it. I had a TA who is notorious around the school for being ridiculous about virtually everything. It's not really evident in the grading scheme, but there's a lot of bad rep that you get amongst the higher ups for coding with bad practice, and I think it affects the quality of code that gets produced significantly. I’m glad to hear your dept seems to be on top of things early on, but the fact that you’re reluctant to work with others because of their poor quality of work doesn’t really help your argument.  Advice from one CS major to another: Try to work with as many different people as you can early on.  If you’re not doing team projects, ask them to show you some of their code from recent assignments to gain perspective of how they tackled the problem (after you’ve turned in the assignments yourself, of course).  Not only will you become a better programmer by seeing other people’s work, you will be able to **weed out the people who are lousy workers.**  You’ll thank me when it comes time to do your senior project. I'm a rising sophomore, though I've already taken what most compsci students at my school take by the end of sophomore year, and I disagree with this to some extent. I've definitely learned a lot about programming from my classes. I have a few classes that are actually, genuinely \"Learning to program\" classes, and they're not all beginner level, either. There's a 300 level course that teaches you to program in C++ without learning too many new concepts.\n\nAnd sup, bro?\n\nEDIT: I feel the need to clarify that I also agree that those who program outside of class certainly have a better understanding of application than anyone else, and there are differences even in the first couple of classes between those who like programming and those who don't care about good practice and things like that. That said, I think this is true of *any* skill. I am a rising junior. I believe everyone's comments about out of class work making the difference. But the education and foundation you start with will make a difference. Not to down talk others styles but I had a teacher who prior to settling at our school would travel from company to company correcting bad programming habits. Needless to say her demands of clean code where rigorous,  but I feel that demand for a level of perfection has made a huge difference. Also I want to comment on just how much your curiosity can make a difference. Just looking at your life and the thinking how things are programmed or how you could program something will get that problem solving aspect of your brain going.  I'm a rising sophomore, though I've already taken what most compsci students at my school take by the end of sophomore year, and I disagree with this to some extent. I've definitely learned a lot about programming from my classes. I have a few classes that are actually, genuinely \"Learning to program\" classes, and they're not all beginner level, either. There's a 300 level course that teaches you to program in C++ without learning too many new concepts.\n\nAnd sup, bro?\n\nEDIT: I feel the need to clarify that I also agree that those who program outside of class certainly have a better understanding of application than anyone else, and there are differences even in the first couple of classes between those who like programming and those who don't care about good practice and things like that. That said, I think this is true of *any* skill. You don't learn C++ until the 300-levels? Seriously?\n\nI think you're going to look back on this attitude in two or three years and laugh. Nothing personal, almost everyone does. Most of the skills are transferable. *shrugs*\n\nMost of the classes in 100-200 are perfectly introductory (introduction to computer circuits, introduction to Java, intro to web design, etc.) while 300+ is actually doing interesting stuff (games, real websites, etc.) or continuations (advanced algorithms etc.).\n\nConsidering all of that, why is learning C++ later so silly? I should note that there is not a large emphasis on learning different languages within class at my school. We're very Java-heavy. Understanding caches, memory allocators, and other low-level systems stuff cannot be taught in Java and are not transferable from higher-level languages. You either learn them in C, C++, or some asm, or you don't learn them at all.  I agree that, if you know you're going to continue after your first language, C++ is the better language, but it's a lot harder to start with.  Most of the skills are transferable. *shrugs*\n\nMost of the classes in 100-200 are perfectly introductory (introduction to computer circuits, introduction to Java, intro to web design, etc.) while 300+ is actually doing interesting stuff (games, real websites, etc.) or continuations (advanced algorithms etc.).\n\nConsidering all of that, why is learning C++ later so silly? I should note that there is not a large emphasis on learning different languages within class at my school. We're very Java-heavy. C++ was just what I learned first, so it surprised me another school was so different. It seems more useful to learn data structures and simple algorithms freshman year instead of Java, which I don't see used too often outside of universities. Java classes also seem to focus on graphics instead of actual computer science.\n\nSorry if the last post came off as arrogant, most people in most majors will consider their freshman selves to be naive. I thought I was so smart for figuring out how pointers worked while the rest of the class struggled and getting my work done on time, but now as a senior I spend about 5 hours on my own projects for every 1 hour in homework. You can learn a lot about programming by just sticking to projects (like you said). The kids I know who do that have better grades than me, but I have a better internship than them ;) Over the past three or so years, C++ is being replaced as the beginner's programming language by Java. It was replaced on the AP exams and colleges tend to follow suit, though it was sort of a concurrent decision.\n\nWe barely touched graphics with Java. The stuff I do with C++ is basically the exact same stuff I did with Java. Command line, mostly. The only difference is that there's a lot more built in support for GUIs in Java, which we did do plenty of. It's all just a way to visualize the logic, though.\n\nAlgorithms usually comes in the first semester of the second year at my school. okay, so that's not as strange as I thought. My high school didn't have any programming so I can't comment on the AP tests. What's the point of switching from C++ to Java if you're doing the same things though? Java always felt messy to me, and then it's more difficult to jump to memory management in C (which we did sophomore year). &gt;What's the point of switching from C++ to Java\n\nJava abstracts away some aspects that are mostly distractions while people are still learning to program, while still being a widely used language in the real world. Although Java introduces it's own distractions by enforcing object oriented style. Personally I feel something like Python would be better for teaching, but I guess it does rely on being able to teach a more widely used language later.\n\n&gt;it's more difficult to jump to memory management in C\n\nYou don't avoid this by doing C++ first though, you just have to learn memory management up front while you're still learning to program, which I think is non-ideal.  okay, so that's not as strange as I thought. My high school didn't have any programming so I can't comment on the AP tests. What's the point of switching from C++ to Java if you're doing the same things though? Java always felt messy to me, and then it's more difficult to jump to memory management in C (which we did sophomore year). Java is way easier and more fun to teach. It's about getting kids interested in the first place, not retaining them.\n\nAs someone who started in Java and is now learning C++, C++ is about a million times messier. I agree that any given person should start with C++ if they plan on continuing after their first language, but schools are more concerned with getting kids to understand the basics of OOP before understanding all the little messy stuff. Eh, I guess we just like different things :P All the library calls in java frustrated me to no end; I like using C where all I need is scanf, printf, malloc, and free.  I didn't go to Arizona State, so I can't speak for their curriculum. (Which is listed here: http://cidse.engineering.asu.edu/undergraduate/majors/computer-science-bs/degree-requirements-bscs/)\n\nYou're going to learn a lot of fundamentals. A lot of it you'll never use again, but it exposes you to ideas, gives you knowledge of where you can look something up later if needed. More importantly, it'll teach you a way of thinking.\n\nBut you'll probably go through multivariate calculus, maybe even DiffEqs. You'll probably have a discreet math course, a linear algebra, and a basic stat course. You'll take CS 101, 102, 103, with the last one probably being a strong introduction to algorithms and data structures. You might take an assembly course. You'll almost definitely take an operating systems course. After that, you'll probably figure out what's most interesting to you, and take your senior electives in that. Do a capstone, take a lit course or two, and you've got a degree.\n\nBest of luck in your endeavors. &gt; discreet math\n\n:| i deserve a full blown ಠ_ಠ. I didn't go to Arizona State, so I can't speak for their curriculum. (Which is listed here: http://cidse.engineering.asu.edu/undergraduate/majors/computer-science-bs/degree-requirements-bscs/)\n\nYou're going to learn a lot of fundamentals. A lot of it you'll never use again, but it exposes you to ideas, gives you knowledge of where you can look something up later if needed. More importantly, it'll teach you a way of thinking.\n\nBut you'll probably go through multivariate calculus, maybe even DiffEqs. You'll probably have a discreet math course, a linear algebra, and a basic stat course. You'll take CS 101, 102, 103, with the last one probably being a strong introduction to algorithms and data structures. You might take an assembly course. You'll almost definitely take an operating systems course. After that, you'll probably figure out what's most interesting to you, and take your senior electives in that. Do a capstone, take a lit course or two, and you've got a degree.\n\nBest of luck in your endeavors. Thanks I had planned to use these fundamentals to learn many languages, frameworks and other things. What do you think of that curriculum? ASU is a well known for their engineering program, especially within universities in Arizona.  Thanks I had planned to use these fundamentals to learn many languages, frameworks and other things. What do you think of that curriculum? ASU is a well known for their engineering program, especially within universities in Arizona.  &gt; to learn many languages, frameworks and other things\n\nMy advice is not to think of it in terms of \"I know C, C++, objective-C, C#, Java, ..., I know jQuery, mooTools, prototype.js, ..., etc.\" because as you might already know, once you understand one OO procedural language, it's very straightforward to learn another, and the same goes for frameworks addressing the same basic problem.\n\nInstead, try to learn the fundamentals of CS, and in your spare time or work time, develop mad skillz in one or more implementations.\n\nAlgorithms, Data-structures, functional programming, database design?, computer architecture...these are the things you'll be glad you got a strong grasp on when you are asked to learn HyperLanguage++#8.1 for a job. Thanks will do, and I'll do as much learning as possible before I head off. It would be nice to be able to make something that would even go ahead and help my studies. Thanks will do, and I'll do as much learning as possible before I head off. It would be nice to be able to make something that would even go ahead and help my studies. Thanks I had planned to use these fundamentals to learn many languages, frameworks and other things. What do you think of that curriculum? ASU is a well known for their engineering program, especially within universities in Arizona.  It seems very standard; you'd get a very similar education in almost any CS department. It'll largely be what you make of it. Work hard, and it will be rewarding. Some things will suck (there's always that class), but that's life.\n\nI assume you're pretty much fresh from highschool. If that's the case, remember there's a lot more to college than the classes you take. Don't party so hard you fail every class. But don't pass an opportunity to go to a party just because it's Tuesday and you have class in the morning.\n\nSign up for an early literature or history course or something. Go out of your way to make friends who are NOT engineers. Be social at their parties. It'll help you keep your soul (easy to lose as a CS major) and keep you regularly exposed to girls (or guys; I don't know you).\n\nLike I said, don't fail your classes. Keep a decent GPA. Then you can worry about your career senior year. Jobs aren't hard to come by in this area.\n\nEdit: Work (paid) internships during the summer. You'll make good beer money and your resume will rock. You'll learn practical tools, frameworks and languages here too. Thanks I had planned to use these fundamentals to learn many languages, frameworks and other things. What do you think of that curriculum? ASU is a well known for their engineering program, especially within universities in Arizona.  In my experience, you'll learn more Doing than Watching. I'd HIGHLY recommend working your ass off and always having an internship/job. Don't take summers off and have a 20 hour a week programming job during the school year. If your university has a co-op program take advantage of it. Sure, I missed out on a little bit of the college experience but not too much when I spent a year co-oping instead of going to school.\n\nPlaces I'd recommend looking for a 10-20 hour a week programming job are with your professors or perhaps the Psychology Department (That's where I worked). This is invaluable experience that you won't get elsewhere simply because it teaches you the whole software lifecycle and ways to manage it. \n\nWhen working for the Deptartment of Psych I had to start at design (from often poorly constructed requirements), go to implementation and finally maintenance pretty quickly. It taught me how to deal with Product Management, Time Management and a host of other pretty valuable industry things that you won't really get in the classroom.\n\nI know ASU is a fun party school but really try to limit your partying to the weekends. Treat college like a job and you will be extremely prepared when you graduate. Also, all of those people you met while working and doing your co-op will be there when you get out waiting to hire you.\n\nGood luck and remember that Software Engineering is something that you can do well and pass and graduate and not be able to write a stich of code. I interview those people all the time and they never get the job. \n\n I'm not a big partier anyways, so it shouldn't be to difficult, and I don't plan on doing greek life, they're chapters are for the most part pulled, and a bit too wild for my tastes. And thanks, I plan to use the knowledge from software engineering to help me learn code. I'm not a big partier anyways, so it shouldn't be to difficult, and I don't plan on doing greek life, they're chapters are for the most part pulled, and a bit too wild for my tastes. And thanks, I plan to use the knowledge from software engineering to help me learn code. In my experience, you'll learn more Doing than Watching. I'd HIGHLY recommend working your ass off and always having an internship/job. Don't take summers off and have a 20 hour a week programming job during the school year. If your university has a co-op program take advantage of it. Sure, I missed out on a little bit of the college experience but not too much when I spent a year co-oping instead of going to school.\n\nPlaces I'd recommend looking for a 10-20 hour a week programming job are with your professors or perhaps the Psychology Department (That's where I worked). This is invaluable experience that you won't get elsewhere simply because it teaches you the whole software lifecycle and ways to manage it. \n\nWhen working for the Deptartment of Psych I had to start at design (from often poorly constructed requirements), go to implementation and finally maintenance pretty quickly. It taught me how to deal with Product Management, Time Management and a host of other pretty valuable industry things that you won't really get in the classroom.\n\nI know ASU is a fun party school but really try to limit your partying to the weekends. Treat college like a job and you will be extremely prepared when you graduate. Also, all of those people you met while working and doing your co-op will be there when you get out waiting to hire you.\n\nGood luck and remember that Software Engineering is something that you can do well and pass and graduate and not be able to write a stich of code. I interview those people all the time and they never get the job. \n\n &gt; have a 20 hour a week programming job during the school year\n\nWhat school did you go to where this was feasible? &amp;#3232;\\_&amp;#3232; Thanks I had planned to use these fundamentals to learn many languages, frameworks and other things. What do you think of that curriculum? ASU is a well known for their engineering program, especially within universities in Arizona.   Keep in mind that CS and SE are two different fields.  CS is theoretical - data structures, algorithms, that sort of thing.  SE is designing and writing programs.  They overlap in a way that's rare in other fields, but they are two different things.\n\nSort of like the relationship between physics and mechanical engineering. As much as people *love* to throw around that Dijkstra quote, I don't think the distinction between Computer Science and Software Engineering is clear anymore. CS is some weird combination of mathematics, science, and engineering. Depending on precisely what you are doing you might spend a lot or little amount of time doing any one part. \n\nI'll give an example. Right now, power usage is a serious problem in our devices; perhaps more so than time or space complexity. However, we don't have a rigorous way of evaluating the \"power complexity\" of an algorithm. Coming up with something like this is absolutely in the realm of CS, but relies on physical implementation details like chip layout. Granted, this doesn't really have much in the way of Software Engineering, but it isn't hard to think of problems that are serious CS problems that require a sizable engineering effort.  Keep in mind that CS and SE are two different fields.  CS is theoretical - data structures, algorithms, that sort of thing.  SE is designing and writing programs.  They overlap in a way that's rare in other fields, but they are two different things.\n\nSort of like the relationship between physics and mechanical engineering.  College is a place where you have basically reserved 4 years of your life for learning.\n\nHow much of that time you actually devote to learning is up to you. You can do the minimum and barely pass, or heck even work hard and pass with good grades, and not actually *learn* anything.\n\nThere is a difference between studying and learning. Studying passes tests, learning means you understand the material.\n\nThat means when your professor goes and mentions that \"CPUs are pipelined\", you can remember the fact and write it down on a test, or you can go out and learn what pipelining really means and what impacts it has in the real world.\n\nYour choice.\n\nWhat choices you make, what topics you choose to dive into outside of class (beer pong is a topic and so is functional programming) determine what you will get out of college.\n\nAs an aside, I recommend you read [*Code* by Charles Petzold](http://www.charlespetzold.com/code/) it will seriously make your first few years of CS go a lot easier, and you will end up understanding a lot of things that most students just learn about. \n\nAlso, as other people have said already, get internships. They are both good job experience, and they also will put into perspective what you are doing at school and make your courses a lot easier. Applying concepts learned in class to real problems (and not just one assignment after another) makes things stick a lot better. I'll make sure to give it a read, thanks!      Looks like you're a Java school. I'd recommend learning some c++ (learn how to manage memory), LAMP and Windows web stacks for almost certain employment after college (including web languages like ruby, python, php, etc), and C if you're like me and have/had dreams of grandeur with embedded systems programming. If you want to be a windows guy, start getting good with c# and .net.\n\nYou said its an SE program, which leads me to believe that you're a realistic person, and don't want to waste time on a PHD. So the best advice that I can give you at this point is get very good at teaching yourself how to learn.\n\nIf you intend to go into CS research, this all changes. Oh, and learn Haskell if you go this route. You recommend a lot of C-like languages. I think it wouldn't be bad to try out .Net and Python too, as they are very different. Looks like you're a Java school. I'd recommend learning some c++ (learn how to manage memory), LAMP and Windows web stacks for almost certain employment after college (including web languages like ruby, python, php, etc), and C if you're like me and have/had dreams of grandeur with embedded systems programming. If you want to be a windows guy, start getting good with c# and .net.\n\nYou said its an SE program, which leads me to believe that you're a realistic person, and don't want to waste time on a PHD. So the best advice that I can give you at this point is get very good at teaching yourself how to learn.\n\nIf you intend to go into CS research, this all changes. Oh, and learn Haskell if you go this route. At my school it is \"Computer Science (Software Engineering)\" would you recommend going just standard \"Computer Science\"?  I'm a masters of computer science student at Georgia tech, and I just want to throw in there that learning your data structures and algorithms is key to the interview process.  Also, internships; do them.    welcome to ASU! Got my bachelors there and I'm starting grad school there in the fall. PM me if you have any campus/program questions     Must. Resist. Arizona State joke. Must. Resist. Arizona State joke.   I'm lucky, I go to a really great school where being a major equals 3 things:\n\n1) You take interesting classes that will give you a solid foundation. Even just doing the minimal amount of work to do well in class, anything deeper you try to learn will be vastly easier after theory, data structures, scheme, architecture et al.\n2) You are constantly encouraged to go the extra mile by your professors - try to prove something extra or add another feature, improve your run time or make a whole big thing during j-term. You can really capitalize on their support and knowledge to get more out of the major as a whole than any series of classes could give you\n3) CS Parties. These are awesome. Obie?      Then again... with all that experience it has to be extremely easy to pick up new technologies. For me learning concepts with Python expedited learning Ruby and JS immensely.  The university won't have classes in how to program in a specific language, except for 101 (probably with Java). That is a good thing. Do all the theoretical book-learnin' now, because you probably won't bother later. It will be helpful as you gain experience and start solving more complex problems. No (sane) employer is going to make you implement a binary tree, however you should know when *using* a binary tree is appropriate, and the best way is to study this stuff.\n\nYou should be worried about being \"career ready\", but it's not about what languages you've learned in class. Learn lots of languages. Get a work study job. Do some side projects, put them on the web. It's just about having some experience. If you do that, you'll be set.\n Sweet thanks a lot! I was getting slightly worried as to whether I had chosen the right major. So is it normal for someone to be a CS major with specialization in web app development both front and back end? When I think computer science my brain pictures outdated hardware and low level languages.  If your CS program is doing it right then the things you learn should (largely) transcend hardware and flavor-of-the-month languages. \n\nHonestly, hearing other people's experience with their CS programs is unlikely to be of much value. Every CS program is different and the scope and focus of CS programs vary more widely than most other disciplines. At top-tier schools the CS curriculum tends to be somewhere between an applied-math and engineering degree, mixing core CS theory with practical real-world applications. At some other schools it's more like a \"this is how you program\" vocational degree, which IMO is a sure-fire sign of a lousy program. \n\nNo, it's not normal for a person to specialize in \"web app development\" in a (decent) CS program period. Those are the kinds of things you would explore outside of class on your own time, or they might offer an elective or two that you could take. Your core classes would likely be more abstract, but would give you the foundations to explore more niche software topics in depth.\n\nTalk to an adviser at your university.\n I have to disagree. At my University, CS is an entire dept, with 3 separate but overlapping major course paths: CS (programming focus), Information Technology (Web Development / Sys Admin focus), &amp; Information Systems (Business &amp; Management focus). Each path has its own focus, but everyone learns a bit about the other. Everyone learns how to program, but CS majors learn more in-depth concepts, &amp; multiple languages. IT majors learn a lot more about web development - both client and server side. IS majors get experience working with SAP. Each path also has free electives that can cross disciplines. This allows students to choose their own path. If you want to focus on Web Development, you are free to do so and there is a whole set of courses available for just that. Despite being able to specialize, you are will also be required to take courses that give you a broad base knowledge. I firmly believe any CS dept worth their salt will afford you a similar experience.\n\nThe point I'm getting at is- your degree is what you make of it. As one of my favorite teachers once said \"In the final evaluation, each student is responsible for their own education.\" If you aren't learning what you want, take the steps to educate yourself or find a school that provides the curriculum you want. That being said, you shouldn't sweat it too much. Once you graduate, you will realize your degree means jack. Its a piece of paper that says you can learn. It says nothing about what you know. Most entry-level positions have this in mind. Your real specialized training typically comes on the job after college, or on your own. Honestly, that sounds like the type of program I would avoid. \n\nObviously I have my own personal biases for what a CS program should or shouldn't be. I went to a university that emphasized a math/engineering style approach to CS. Gaining programming experience and proficiency is something you were expected to do largely on your own time, at internships, etc. It works out great if you're self-directed and have actual interest in the field, less so otherwise. \n\nIMO the types of programs you're talking about (IS/IT/MIS) are better left to trade schools and don't really belong in a university setting (or at least not in the CS department). The university programs are simply degreeifications of fields that were already being served just fine (and more affordably I might add) by more traditional vocational routes. In the same way I feel that engine-repair/maintenance does not belong in a university mechanical engineering department I really don't think that IS/IT belongs in a university CS department. I think that if your program is spending the majority (or even a significant amount) of its time teaching you how to write web apps, or use frameworks, or any other tasks that might fall under the heading \"just programming\" you're not really getting your money's worth.  \n\nMy experience in interviewing people from these types of vocationally-focused programs is that they fail hard at the more theoretical/technical sections, as the stuff that I deal with requires somewhat more math proficiency than your typical software job. Of course this sword cuts both ways and I've seen just as many people with great theory/math skills that can't program their way out of a paper bag. However, I think it's much more practical to pick up programming experience on the job than math and theory.\n\nI don't doubt that these types of programs can give you a perfectly adequate (possibly better out of the gate) education for a typical entry-level programming or sysadmin position. However, if you decide you'd rather go to a top-flight grad school after your undergrad I fear you might have a rather difficult time compared to someone coming from a more \"traditional\" CS program. To me, the best CS programs are the ones that keep the most doors open after graduation. \n\nThis argument, of course, is almost as old as the CS programs themselves and the reality is that for the vast majority of software jobs out there it just doesn't matter.\n\n Sweet thanks a lot! I was getting slightly worried as to whether I had chosen the right major. So is it normal for someone to be a CS major with specialization in web app development both front and back end? When I think computer science my brain pictures outdated hardware and low level languages.  CS is not programming class. You'll have assignments that require programming, to implement things for your algorithm classes and such, but you're not there to learn how to program, you're there to learn computer science.  Programming is something you'll pick up along the way, and the ones who graduate as great programmers are the ones who spend their spare time working on their own projects.\n\n&gt;outdated hardware and low level languages.\n\n\nOutdated hardware? You'll do 99% of your work on your own laptop, most likely. Low level languages?  I took a single intro to C first year, and even at that it was more just to teach the basics of how the OS manages memory for processes. The ammount of C code I actually wrote for that class was a couple hundred lines, and no more. I spent my own time really learning C, because I like it.\n\n\nedit: I guess I didnt answer your question. No, you won't 'specialize' in anything, most likely. You'll have a solid foundation the hows and whys of algorithms, and such,  and whatever you 'specialize' in (if you insist on calling it that) will be whatever you choose to work on in your spare time. Sweet thanks a lot! I was getting slightly worried as to whether I had chosen the right major. So is it normal for someone to be a CS major with specialization in web app development both front and back end? When I think computer science my brain pictures outdated hardware and low level languages.  There is nothing wrong with web development, but it's not something you should \"specialize\" in at the college level. You'll be better of learning it on your own. Your professors likely won't be at the cutting edge of web development anyway, since it is constantly changing and industry-focused. \n\nHowever, they are at the cutting edge of theoretical computer science. Take this opportunity to learn from them/specialize in something like that.\n\nI interview a lot of people / screen a lot of resumes. Let's compare two resumes I might come across:\n\n**Person 1**\n\n * B.S. in Computer Science with Specialization in Robotic Systems\n * Link to web app built independently\n\n**Person 2**\n\n* B.S. in Computer Science with Specialization in Web Application Development\n* Link to web app built for a class assignment\n\nMy gut response to Person 1 is \"Boy, that must be a smart, motivated guy.\" My gut response to Person 2 is \"Hrm, this guy seems like he's just doing the minimum to get by.\"\n\n\"Web application development\" is a small, small piece of the cool stuff on the web. Google.com is a \"web app\", but that's only the front door. There's a ton of stuff going on behind the scenes, and that's where the computer science comes into play. Don't pigeonhole yourself. Sweet thanks a lot! I was getting slightly worried as to whether I had chosen the right major. So is it normal for someone to be a CS major with specialization in web app development both front and back end? When I think computer science my brain pictures outdated hardware and low level languages.  Don't do that. Don't worry about specific technologies or web nonsense. That's the way you become stuck in a dead end. You're building a toolbox for your future career and the more exposure and comfort you have with a wide variety of topics the better you'll be. You have a couple of incredible resources available to you that you will most likely never have again: Time to study and access to people who know unbelievable amounts more than you.\n\nAlso math, do as much math as you can. If you ever think \"when will I use this\" immediately punch yourself in the face. Just do it, math is problem solving which is the essence of programming. You have constraints and problems, the solution has to fit within the constraints.  Then again... with all that experience it has to be extremely easy to pick up new technologies. For me learning concepts with Python expedited learning Ruby and JS immensely.  &gt; learning concepts with Python expedited learning Ruby and JS immensely. \n\nAs someone who just took the time to learn JS beyond the basic \"google jQuery recipe, insert\", I'm not sure that learning ANY language could expedite learning prototypical programming.  What was Netscape thinking!?  &lt;/semi-sarcasm&gt; I think languages that have less-obvious 'gotchas' than Javascript such as Self, Io, or even Lua could definitely strengthen one's understanding of prototypes.  ",
    "url" : "http://www.reddit.com/r/compsci/comments/w4gn5/what_was_computer_science_like_in_college/"
  }, {
    "id" : 5,
    "title" : "Bresenham's Line Drawing Algorithm Implemented in Minecraft",
    "snippet" : "  How do they do this? Do they use some tool for generating the logic circuits and exporting them to Minecraft? Minecraft features an on/off system called redstone that facilitates the implementation of Boolean logic. When I started playing last winter I knew nothing about binary, but with a little bit of research I was able to learn quite a bit.\n\nI've also built a half-precision floating-point adder, a few types of multipliers, a divider using conditional subtraction (and I've begun work on an SRT divider), a square root extractor, numerous ALUs, and a Kogge-Stone Adder. At some point I'll build a CPU, but I've focused mostly on algorithm implementation because I find it fascinating.\n\nIf people find this interesting I'd be happy to share more videos as a make them.\n\nEdit: Also, to answer your question - I looked up Bresenham's Line Algorithm on wikipedia and built circuitry that would run the algorithm. I mean, how do you build the structures from redstone - do you actually design it on paper/elsewhere and then walk around in the game and build it brick by brick, or do you have a map generator program that generates it all automatically from a higher-level description? Minecraft features an on/off system called redstone that facilitates the implementation of Boolean logic. When I started playing last winter I knew nothing about binary, but with a little bit of research I was able to learn quite a bit.\n\nI've also built a half-precision floating-point adder, a few types of multipliers, a divider using conditional subtraction (and I've begun work on an SRT divider), a square root extractor, numerous ALUs, and a Kogge-Stone Adder. At some point I'll build a CPU, but I've focused mostly on algorithm implementation because I find it fascinating.\n\nIf people find this interesting I'd be happy to share more videos as a make them.\n\nEdit: Also, to answer your question - I looked up Bresenham's Line Algorithm on wikipedia and built circuitry that would run the algorithm. Sounds like you really enjoy it, you should learn VHDL/Verilog and get paid to do this. I do! It's easily my favorite hobby and I will absolutely look into that; I've been meaning to delve into real world applications. I'm a molecular biologist and from what I've observed, learning some computer science would be invaluable.  How do they do this? Do they use some tool for generating the logic circuits and exporting them to Minecraft?  This is so impressive.  I took CG 1 last quarter in college and I play Minecraft a lot. I had to write Bresenham's myself and I found it confusing at times, doing it in Minecraft without writing any code just boggles my mind.\n\n Though I rarely work with redstone in Minecraft, these large builds are EXTREMELY impressive.  The redstone in the game at times does not behave like you would expect it to, making large builds very time consuming. Thanks! And yeah, some of the quirks of redstone can make things pretty interesting, though in my builds I try to avoid exploiting those quirks because of how difficult to predict they can be (I'm thinking of a some frustrating hashset bugs in particular).\n\nAlso, even if you don't do much redstone you should consider stopping by our server (mc.therdf.net). As someone who understands logic and computing I think you might enjoy looking around!  Would you have preferred to make this in another game/system besides Minecraft? What is the ideal program you can think/imagine of for making such creations? I'd agree with Shrogg about Minecraft as a fantastic logic simulator and add that it's an excellent tool for teaching/learning. He and I are involved in administrating a server that focuses on redstone and I'm continually impressed by what some of our members come up with - and the majority of them are high school students with little or no computer science background.\n\nThat said, I'm always interested in learning about other systems and trying new things, so if you have any suggestions I'd love to check them out. I don't know of a better alternative, but I was thinking. What about a game that is designed around making circuits, and making that process fun?\nIf you have any ideas of your own about a game that can be used to learn let me know. Would you have preferred to make this in another game/system besides Minecraft? What is the ideal program you can think/imagine of for making such creations? ",
    "url" : "http://www.youtube.com/watch?v=H7uclAqUmDw"
  }, {
    "id" : 6,
    "title" : "Dark silicon and the end of multicore scaling [PDF]",
    "snippet" : "  tl;dr ? Current multicore architectures don't scale because too much transistors are just sitting there doing fuckall.\n\nCopy pasting sequential processors make for bad parallel processors, news at 11. that was wonderful...\nalso, whats the solution suggested in the paper?  It's been suggested enough in the past and present that we need to fundamentally rethink software and hardware. The future is parallel, there's a consensus about that, but there are some fundamental limitations in our current (multicore) approach.\n\nI'm betting in my PhD on a more dataflow-oriented approach, it just seems to tick all the hardware and software requirement boxes.\n\nFurther reading:\n\n* http://herbsutter.com/welcome-to-the-jungle/\n* S. Adve and H.-J. Boehm, “Memory models: a case for rethinking parallel languages and hardware,” Commun. ACM, vol. 53, no. 8, Aug. 2010.\n* Arvind and R. A. Iannucci, “A critique of multiprocessing von Neumann style,” presented at the ISCA '83: Proceedings of the 10th annual international symposium on Computer architecture, 1983.\n Dataflow? But transputer wasn't much of a success.  I believe Erlang is an example of the dataflow approach, as is data that's piped between a chain of Unix commands. Actors are the epitome of this, but seem to be conceptually hard for people to grasp. I believe Erlang is an example of the dataflow approach, as is data that's piped between a chain of Unix commands. Actors are the epitome of this, but seem to be conceptually hard for people to grasp. Dataflow? But transputer wasn't much of a success.  Transputer failed because it was invented in the 1980s during a lull in the one core improvement curve - but that quickly picked back up speed and lasted until about 2004. Right solution, wrong decade.  Dataflow? But transputer wasn't much of a success.  The Transputer wasn't dataflow.  It used the CSP computational model: control-driven, private memory.\n\nStreaming processors and map-reduce are examples of datadriven execution; a statement in your program is enabled for executed when its dependent resources become available.  Even tracing JITs are dataflow and so is the OoO execution core of about all mainstream processors since the intel P5.  GPUs are _massive_ dataflow machines by the same stretch.\n\nMy reasoning is basically: if processors are internally dataflow driven, compilers use dataflow analysis to optimize stuff and big warehouses are running dataflow frameworks, why the hell aren't we programming in a dataflow language.\n\nThe SISAL language over at LLNL was getting pretty good and very fast, but the entire parallel research domain simply got shafted around the early 90's.   Aren't you confusing data-parallel (SIMD, GPUs, MapReduce) with dataflow (modeling the program as a DAG; arcs are buffered communication channels; when buffer size=0 it degenerates to CSP)? Dataflow? But transputer wasn't much of a success.  For sure the Transputer wasn't dataflow itself. It did come out during a lot of hype/research oriented around pure dataflow machines in the early 1980s so perhaps a few of those were constructed from Transputers.\n\nI like the work on dataflow but I wouldn't think it's the definitive solution to future of computing. One of the authors of that paper, Doug Burger, did some really interesting work into the EDGE ISA which is block-oriented statically assigned dataflow. It has some really nice properties although is still subjected to the main obstacles in computer architecture right now. It's been suggested enough in the past and present that we need to fundamentally rethink software and hardware. The future is parallel, there's a consensus about that, but there are some fundamental limitations in our current (multicore) approach.\n\nI'm betting in my PhD on a more dataflow-oriented approach, it just seems to tick all the hardware and software requirement boxes.\n\nFurther reading:\n\n* http://herbsutter.com/welcome-to-the-jungle/\n* S. Adve and H.-J. Boehm, “Memory models: a case for rethinking parallel languages and hardware,” Commun. ACM, vol. 53, no. 8, Aug. 2010.\n* Arvind and R. A. Iannucci, “A critique of multiprocessing von Neumann style,” presented at the ISCA '83: Proceedings of the 10th annual international symposium on Computer architecture, 1983.\n Another big issue is the synchronous clock based architectures, where it's only as fast as the slowest path. Yes. I think another big development will be implementing at low level data structures that allow fully out or order asynchronous execution from centralized buffers in the CPU. I-structures and M-structures were already a step in that direction. M-structures ended up as Mvars in Haskell, I'm not sure what to compare I-structures with today. Scoreboarding?\n\nI-structures: data structures for parallel computing\nhttp://dl.acm.org/citation.cfm?id=69562 Yay, they used array comprehensions two decades before they appeared in Python! :-)\n\nIt's sad to see so much good research \"rot\" in the libraries just because it's old:(  I-structures and M-structures were already a step in that direction. M-structures ended up as Mvars in Haskell, I'm not sure what to compare I-structures with today. Scoreboarding?\n\nI-structures: data structures for parallel computing\nhttp://dl.acm.org/citation.cfm?id=69562 I was more thinking of using those algorithms in hardware. Instead of instructions flowing through a pipeline in sync with a clock, multiple groups of differently clocked hardware operate in parallel. I was thinking less about computing in parallel and more about redesigning the CPU so that more of its functionality can be used at any time.\n\nI also think this approach is inherently more extensible. If the CPU does not rely on rigid pipelining, than if additional hardware is added, as long as that hardware stores its results correctly, there should be no disruption to the rest of the CPU if additional execution units are removing data from the central pool. You might be interested in reading a bit more in-depth about modern pipelines, scoreboarding and hardware speculation pretty much aim to do this. Essentially, modern processors follow some limited form of data-flow execution at their very core. \n\nThe only non-clock architecture that I have ever heard of is Moore's http://www.greenarraychips.com/.  Moore as in the Forth guy. It's been suggested enough in the past and present that we need to fundamentally rethink software and hardware. The future is parallel, there's a consensus about that, but there are some fundamental limitations in our current (multicore) approach.\n\nI'm betting in my PhD on a more dataflow-oriented approach, it just seems to tick all the hardware and software requirement boxes.\n\nFurther reading:\n\n* http://herbsutter.com/welcome-to-the-jungle/\n* S. Adve and H.-J. Boehm, “Memory models: a case for rethinking parallel languages and hardware,” Commun. ACM, vol. 53, no. 8, Aug. 2010.\n* Arvind and R. A. Iannucci, “A critique of multiprocessing von Neumann style,” presented at the ISCA '83: Proceedings of the 10th annual international symposium on Computer architecture, 1983.\n In terms of mainstream consumer technology and the common man's computer, we've JUST gotten into the portable \"post-PC\" (yes it's bullshit Apple propaganda but remember I'm talking about the average Joe who needs only email and Safari) ARM-powered age. How are we supposed to magically change how we think about processors if people are still stuck with 2 specific instruction sets (ARM and x86_64)? Unless this article was meant for specific purpose computers like server racks and mainframes, in which case most of the processors in those have pretty good SMP and multithreading. Correct me if I'm wrong. Yep, that's [this talk](http://view.eecs.berkeley.edu/w/images/3/31/Micro-keynote-hwu-12-11-2006_.pdf)\n\n\nYou could see the problem as us currently sitting on a local minima. There is no easy, traditional way to crawl out of this valley in which we have invested so much time and effort. All signs point out that the market is up for one hell of a disruption. \n\n\nSpeaking of disruptions, notice how you included ARM. You probably wouldn't have ~2 years ago. Yep, that's [this talk](http://view.eecs.berkeley.edu/w/images/3/31/Micro-keynote-hwu-12-11-2006_.pdf)\n\n\nYou could see the problem as us currently sitting on a local minima. There is no easy, traditional way to crawl out of this valley in which we have invested so much time and effort. All signs point out that the market is up for one hell of a disruption. \n\n\nSpeaking of disruptions, notice how you included ARM. You probably wouldn't have ~2 years ago. It's been suggested enough in the past and present that we need to fundamentally rethink software and hardware. The future is parallel, there's a consensus about that, but there are some fundamental limitations in our current (multicore) approach.\n\nI'm betting in my PhD on a more dataflow-oriented approach, it just seems to tick all the hardware and software requirement boxes.\n\nFurther reading:\n\n* http://herbsutter.com/welcome-to-the-jungle/\n* S. Adve and H.-J. Boehm, “Memory models: a case for rethinking parallel languages and hardware,” Commun. ACM, vol. 53, no. 8, Aug. 2010.\n* Arvind and R. A. Iannucci, “A critique of multiprocessing von Neumann style,” presented at the ISCA '83: Proceedings of the 10th annual international symposium on Computer architecture, 1983.\n that was wonderful...\nalso, whats the solution suggested in the paper?  dark silicon? Disclaimer: I haven't read the linked paper, although I was at ISCA so I might've heard the talk.  Also, one of my professors at UVA (Kevin Skadron) talked about dark silicon in one of his architecture seminars.\n\nThe basic idea of dark silicon is not necessarily just that you have transistors/cores not doing anything, but that they are dark in the sense that they have no power coming to them.  Sure, after a point you may have trouble feeding them with data for general purpose computation, but GPUs have shown that there are classes of problems where we can keep large numbers of cores fairly busy.  So some of the idea is just for power management, if you can't keep them busy you may as well turn them off.\n\nBut the bigger issue is after a point, you just can't practically keep them powered on.  IIRC this is basically an issue of how much power you can bring in, although with heat dissipation issues.  There's also essentially an upper bound on the number of cores you can reasonably stamp on a die, because with a given package size you only have so many pins to feed the cores with power and data, and increasing the die size decreases production output.  I don't mean this to be a cheap shot at an otherwise excellent paper, but are the authors confusing Moore's law with David House's assertion that IC performance would double every 18 months, or have I misunderstood? Moore's law just hit the wall. Not the feature size, but the feature price wall. Strange times.   http link?  Can't access FTP at work.\n\nI assume this is saying what we've known for a long time, that multi-core is a stopgap solution to the real problem which is single-threaded performance.  Clock speed increase scale performance linearly, while core count increase don't and will afford no speedup at all unless the application is designed for multi-core.  Since not all algorithms are parallelizable, there's a point of diminishing returns. http link?  Can't access FTP at work.\n\nI assume this is saying what we've known for a long time, that multi-core is a stopgap solution to the real problem which is single-threaded performance.  Clock speed increase scale performance linearly, while core count increase don't and will afford no speedup at all unless the application is designed for multi-core.  Since not all algorithms are parallelizable, there's a point of diminishing returns. Can you name a few algorithms that are not parallelizable? That was an oversimplification.  In reality, it's more about Amdahl's law, which relates to the degree of parallelization and the effort required to parallelize it.  You get the most speedup from so-called \"embarrassingly parallel\" problems, such as rendering.\n\nOther algorithms, take sort for example, have dependencies on themselves.  If you consider quicksort, you could create a new thread for each nested call, but eventually sorting in each range is worth parallelizing - at some point the ranges will be so small that creating threads will take longer than sorting single threaded.  You could of course create a more parallelizable sort algorithm, but you would hit diminishing returns very quickly. RE parallelizable sort, you should look at Duane Merrill's work on GPU sorting: https://sites.google.com/site/duanemerrill/awards-publications You'll notice that maximum speedup, despite having thousands of cores, is just 3.8x, and up to 4.2 times faster than an i7.  That's Amdahl's law at work.  *Technically* you can parallelize it, but it ties up your GPU and wastes a ton of cycles.  Extremely low efficiency, you'd never use this in anything other than a GPGPU program. Low-efficiency if you are comparing speedup/cores. But it's comparing 'fat' i7 cores with relatively 'slim' GPU cores.  4.2 times the i7 still puts it slightly on top of it in flops/watt.\n\nAlthough, that's just nitpicking. Only HPC would consider paying the development cost.",
    "url" : "ftp://ftp.cs.utexas.edu/pub/dburger/papers/ISCA11.pdf"
  }, {
    "id" : 7,
    "title" : "Wanted to know the etymology of Unix Commands...",
    "snippet" : "for example, \"ls\" sounds like a list and lists directory contents... but some commands are very obscure... e.g. pwd gives the current directory... I wanted to know the origin of this command and any command in unix, of which you know the origin, which doesn't seem obvious from it's name.\n\nEDIT: Thanks for all the help you guys. I found this too: http://stackoverflow.com/questions/258509/etymology-of-linux-commands\n\n  Grep = global (regular expression) print. :g/re/p is a vi command meaning \"show all lines matching the regular expression re\".  \nvi = visual. Before vi editors didn't show you the text as you edited it.  \nAwk is from the initials of the authors: Aho, Weinberger and Kernighan.  \nSed = Stream Editor. Thanks, didnt know this... that text wasnt visible while typing  \"cat\" is short for concatenate. Counter to the way it's usually used, you can pass it a list of files (not just one), it will concatenate them, printing one right after the other.\n\n\"cd\" = change directory\n\nYou now already know \"pwd\". I think the rest are pretty self explanatory once you know what they do. Any others you're curious about? &gt; \"cat\" is short for concatenate.\n\nActually \"catenate\", according to srb. The man pages agree with me. *shrugs*\n\nMy source is pages 12 and 232 of *The UNIX System* by [S. R. Bourne](http://en.wikipedia.org/wiki/Stephen_R._Bourne) of Bell Labs, published 1983. I'll take srb's word over that of rms, who wasn't around when the first version of `cat` was written. Also, if you're looking at the same man page as me, it doesn't actually say that `cat` is short for \"concatenate\", it just says that that's what the program does (\"catenate\" and \"concatenate\" are synonyms). And I note that the program is not called \"con\" :-) Now thats really getting to the root of the matter   ls stands for \"list segments\" -- a vestige of [Multics](http://en.wikipedia.org/wiki/Multics), where files were called segments.  How is this Computer Science?  [Let me Google that for you.](http://www.faqs.org/faqs/unix-faq/faq/part1/section-3.html) More like http://lmgtfy.com/?q=pwd+unix+command+abbreviation  pwd = present working directory. (or print working directory. I've heard both.)\n\nsometimes you'll see cwd for the same thing. (current working directory)     The manual page of each command is general informative.  Just type:\n\n    man pwd\n    man ls\n\netc.\n most important, man man    See if your library has \"Unix Unbound,\" by Harley Hahn. He's an excellent writer, and he gives the history of Unix in some brief stories about the people who actually invented its parts and pieces.   You can probably figure out a lot of the origins from the man pages. \"pwd\" is \"print working directory\", for what it's worth. Present working directory, I believe",
    "url" : "http://www.reddit.com/r/compsci/comments/w2fnx/wanted_to_know_the_etymology_of_unix_commands/"
  }, {
    "id" : 8,
    "title" : "A Gentle Introduction to Algorithm Complexity Analysis (x-post from r/programming)",
    "url" : "http://discrete.gr/complexity"
  }, {
    "id" : 9,
    "title" : "Liquid Metal",
    "snippet" : "  It's interesting work. The linked papers are well worth checking out.\n\n&gt; so maybe, finally, we are beginning to see rise of the FPGA for general purpose computing\n\nI doubt that this is the case. Sure, FPGAs are extremely useful for a some types of processing applications, but in all the places I have seen them used for computation in the wild they are used as very highly specialized coprocessors. The tradeoffs of FPGAs means, I suspect, that they will never be as widely used for general-purpose computing as even GPUs are now.\n\nFPGAs use in heterogeneous computing seems likely to be restricted to applications which hit their sweet spot of computational complexity. The application has to be compute-heavy enough that getting data to the box isn't the bottleneck, but simple enough that it can be ported to the FPGA using the (somewhat crude) tools and (expensive) development process available today.\n\nEventually somebody is going to come along with a really great hardware and software package which makes FPGAs more widely useful, but I don't think we've seen that day yet. Research like this is very interesting indeed.\n &gt; Eventually somebody is going to come along with a really great hardware and software package which makes FPGAs more widely useful\n\nI think hardware will have to lead software. No one just has spare FPGAs lying around to say, \"Huh, guess I'll turn this guy into a computational node.\" It's interesting work. The linked papers are well worth checking out.\n\n&gt; so maybe, finally, we are beginning to see rise of the FPGA for general purpose computing\n\nI doubt that this is the case. Sure, FPGAs are extremely useful for a some types of processing applications, but in all the places I have seen them used for computation in the wild they are used as very highly specialized coprocessors. The tradeoffs of FPGAs means, I suspect, that they will never be as widely used for general-purpose computing as even GPUs are now.\n\nFPGAs use in heterogeneous computing seems likely to be restricted to applications which hit their sweet spot of computational complexity. The application has to be compute-heavy enough that getting data to the box isn't the bottleneck, but simple enough that it can be ported to the FPGA using the (somewhat crude) tools and (expensive) development process available today.\n\nEventually somebody is going to come along with a really great hardware and software package which makes FPGAs more widely useful, but I don't think we've seen that day yet. Research like this is very interesting indeed.\n Gpus have limitations as well.  I believe effective bandwidth for shuttling data to the gpu and back is relatively poor.\n\nAnecdotally, im not sure consumer gpus are up to computing workloads either.  I ran ihash on my nvidia 520 overnight as an experiment, and woke up to a nonfunctional GPU (im guessing the stock cooler wasn't really cut out for long periods of 100% utilization).  Had to RMA the card.\n\nAll this to say, I think the vague dream of completely abstracting away where code runs, and magically gpu accelerating arbitrary general purpose consumer software is unlikely to happen soon.  Only specific workloads are really suitable for gpu acceleration, and I worry that bargain consumer gpus aren't up to the task.\n\nOf course, I don't think the article was making either of these claims.\n\nOn that note, with the recent password hashing scandals, it was mentioned that there isn't yet a widely distributed gpu implementation of bcrypt.  Anyone know why that is?",
    "url" : "http://whiley.org/2012/07/04/the-liquid-metal-project/"
  }, {
    "id" : 10,
    "title" : "PageRank Algorithm Reveals Soccer Teams' Strategies",
    "snippet" : "  This is genius. This theory predicts that past winners of major tournaments are past winners of major tournaments because they were better teams.\n\nSo...How do I use the theory for games that will happen in the future. What? You can't?\n\nHindsight is 20/20. I wonder if this is also true. Did we read the same article? It's like you got to the second paragraph and stopped. \n\nPerhaps you read the paper and the article simply goes into more advanced graph analysis than the paper itself?\n\nOr perhaps you're unimpressed with the way they determined which was the better team?  ",
    "url" : "http://www.technologyreview.com/view/428399/pagerank-algorithm-reveals-soccer-teams/"
  }, {
    "id" : 11,
    "title" : "Book/resource suggestions for learning about networks",
    "snippet" : "Hey all,\n\nI was just reading [this thread](http://www.reddit.com/r/compsci/comments/vwhwf/what_is_the_most_useful_thing_you_learned_from/) about the most useful things people learned in their degree that they still use in their job today. Networking seemed to be pretty high on the list.\n\nI have just completed my degree however the networking modules I took during my studies were not up to scratch and I feel there are large gaps in my knowledge.\n\nSo yeah any suggestions?\n\nEdit: Thanks for all the great suggestions guys! Think I'm gonna get me a copy of TCP/IP illustrated for starters and check out some of those online resources too.  The [TCP/IP Illustrated](http://www.amazon.com/TCP-Illustrated-Vol-Addison-Wesley-Professional/dp/0201633469) series is very good, if you have some basic knowledge. If you are just starting out, then I'd recommend Tanenbaum's [Computer Networks](http://www.amazon.com/Computer-Networks-Edition-Andrew-Tanenbaum/dp/0132126958). Whatever book you get, make sure it's one that focusses on Ethernet and TCP/IP, for the most part. You can specialize into other networks later as you need to, but those two are the most widely used in industry by far.\n\nAnother way to learn about networking is through practice. Set up a home network, write a basic client and server using sockets, play with tcpdump, etc.  Read something like [this](http://www.amazon.com/Computer-Networking-Top-Down-Approach-Edition/dp/0132856204/ref=sr_1_3?ie=UTF8&amp;qid=1341282416&amp;sr=8-3&amp;keywords=computer+networks+a+top+down+approach) to get the ideas first, then go to TCP/IP Illustrated for the details. The TCP/IP Illustrated book is almost like an encyclopedia. If you want practical knowledge quick:\n\nhttp://www.linuxhomenetworking.com/wiki/\n\nand also play around with virtual machines. You can establish networks between your virtual machines. The networking certification exams have study guides too, although they have a poor reputation (I don't really know why). \n\nI also don't really know why I'm getting downvoted.       ",
    "url" : "http://www.reddit.com/r/compsci/comments/vyaxx/bookresource_suggestions_for_learning_about/"
  }, {
    "id" : 12,
    "title" : "What is the most useful thing you learned from your B.S that you still use today at your Job? ",
    "snippet" : "As the title states, What is the most useful subject from school in C.S that you still use to this day? \n\ni.e sorting, searching, Operating systems, threading.. such and such.   Surprisingly, a deeper understanding of Networking has proven to be incredibly useful. \n\nOf course the fundamentals you mentioned are also critical, but networking is the one that surprises me as being more important than it was played up to be. Soooo.... out of curiosity, what was the course number for Networking at your college? CSCE-463 so NOT cs2818?  Missed a golden opportunity there, you did. Soooo.... out of curiosity, what was the course number for Networking at your college? Surprisingly, a deeper understanding of Networking has proven to be incredibly useful. \n\nOf course the fundamentals you mentioned are also critical, but networking is the one that surprises me as being more important than it was played up to be. Unfortunately my college had a really really shitty Networking course (ironically was taught online, which also made it more shitty due to the way the professor 'taught' it). Do you have any recommended books for networking? I really think this is one of my weakest fundamental points that I should actually know since I do web development for a living. I've always used [Computer Networking A Top-Down Approach](http://www.amazon.com/Computer-Networking-Top-Down-Approach-5th/dp/0136079679) by Kurose and Ross. (it's also fairly easy to find a PDF of it). It's a good read for understanding the fundamentals and the bigger picture.   How to work 10 hours on a problem without making any progress until you find a solution. Seriously. I've always said the main benefit of a technical education is it teaches you how to *grind* through problems. Before college, I looked at very challenging technical things and said \"this is beyond my level of ability.\" Now I say, \"I'm gonna stare at you, problem, until I *own* you and you like it.\" My favorite is when you work for a day, get nowhere then wake up the next morning and suddenly have a better understanding than you did before.\n\nHappened to me just a few days ago, I literally started narrating my thoughts so that I would hang on to them. Woke up from a dream the night before an exam. Turns out I had been mentally been working through a problem and realised that I didn't know the solution. Looked it up. Got &gt; 80%. My favorite is when you work for a day, get nowhere then wake up the next morning and suddenly have a better understanding than you did before.\n\nHappened to me just a few days ago, I literally started narrating my thoughts so that I would hang on to them. [deleted] My favorite is when you work for a day, get nowhere then wake up the next morning and suddenly have a better understanding than you did before.\n\nHappened to me just a few days ago, I literally started narrating my thoughts so that I would hang on to them. [deleted] you actually haven't.\n\nbut here's a token How to work 10 hours on a problem without making any progress until you find a solution. How to work 10 hours on a problem without making any progress until you find a solution. Perseverance can never be undervalued. How to work 10 hours on a problem without making any progress until you find a solution. How to work 10 hours on a problem without making any progress until you find a solution. How to work 10 hours on a problem without making any progress until you find a solution. How to work 10 hours on a problem without making any progress until you find a solution.  That meaningful comments make the difference between understandable and opaque for complex programs. (Disclaimer: two out of the four years I programmed exclusively in one assembler language or another, and then continued in nothing but assembler for another 10 years or so. Still do it anyway.)\n\n You would be amazed at how shitty some programmers actually comment their code.\n Let alone the ones that insist that they are *so good* that comments aren't necessary. lol\n\nThis happened in my Intro classes...they weren't happy with their grades.  The professor I learned assembler programming from was quite the stickler on comments.\n\n* Every line will be commented. (Given one instruction per line, a macro language, and a complex instruction set, this was pretty reasonable.)\n* The comment will say what the line accomplishes, not what it does (i.e., \"move to the next item\", not \"add 4 to register 1\").\n\nAny line not meeting these specs lost 5 points, and given that a typical assembler program was a minimum of 50-60 lines, there was a lot of opportunity to lose points. Sounds like over commenting to me. Sounds like over commenting to me. Sounds like over commenting to me. You would be amazed at how shitty some programmers actually comment their code.\n You would be amazed at how shitty some programmers actually comment their code.\n You would be amazed at how shitty some programmers actually comment their code.\n Especially those who read these tutorials - [Tips for writing readable and eye catching code](http://www.c4learn.com/tips-for-writing-readable-and-eye.html) I hope the author isn't serious That meaningful comments make the difference between understandable and opaque for complex programs. (Disclaimer: two out of the four years I programmed exclusively in one assembler language or another, and then continued in nothing but assembler for another 10 years or so. Still do it anyway.)\n\n How long ago did you learn to program?  I'm not trying to make fun, I'm honestly curious.  The assembly language section of my college degree, which I took last year, covered two weeks and was in an apparently useless language(y86, with some extra stuff added for IO). I started in 1977. :) I did a lot of systems programming and related stuff on IBM mainframes up until the 1990's. Almost all of that was in assembler language, with the primary debugging tool being core dumps. By the time I finished, I could disassemble the code in my head while reading the hexadecimal. Not much call for that now...\n\n(Edit: date obscured by formatting.)\n I started in 1977. :) I did a lot of systems programming and related stuff on IBM mainframes up until the 1990's. Almost all of that was in assembler language, with the primary debugging tool being core dumps. By the time I finished, I could disassemble the code in my head while reading the hexadecimal. Not much call for that now...\n\n(Edit: date obscured by formatting.)\n Once you memorize the opcodes that doesn't seem so hard.  How long ago did you learn to program?  I'm not trying to make fun, I'm honestly curious.  The assembly language section of my college degree, which I took last year, covered two weeks and was in an apparently useless language(y86, with some extra stuff added for IO). How long ago did you learn to program?  I'm not trying to make fun, I'm honestly curious.  The assembly language section of my college degree, which I took last year, covered two weeks and was in an apparently useless language(y86, with some extra stuff added for IO).  Easily estimating the running time of algorithms, ie big O.   This is definitely one of the big ones. A few months after I started at my current company, I was able to identify a common function that operated in O(n^2 ) time, and managed to restructure it to an O(n) operation. This reduced some report runtimes from hours to minutes, and others from minutes to subsecond. It's definitely important to be able to identify algorithm run times. This is definitely one of the big ones. A few months after I started at my current company, I was able to identify a common function that operated in O(n^2 ) time, and managed to restructure it to an O(n) operation. This reduced some report runtimes from hours to minutes, and others from minutes to subsecond. It's definitely important to be able to identify algorithm run times. Easily estimating the running time of algorithms, ie big O.   Related - using the right data structure for the job.   Easily estimating the running time of algorithms, ie big O.   So far the biggest use I've had for Big O in any formal sense is passing job interviews.  But it's useful to keep in the back of your head. Big O isn't something one \"keeps in the back of your head\" when programming. It's *literally* involved in every decision you make about the representation of your program. (i.e., algorithmic design and choice of data structures.) At my job I've rarely needed to do more than think \"logarithmic\" or \"quadratic\" when thinking about runtimes.  Most of the algorithms I use are well known in complexity so I don't need to work it out. From my experiences, I've never needed to work out a recurrence equation for an algorithm (and runtime of iterative algorithms is usually trivial to figure), but its handy to look at code, think \"oh, this algorithm is taking quadratic time... wait, isn't there an n*log(n) algorithm for this\", and then improve things. So far the biggest use I've had for Big O in any formal sense is passing job interviews.  But it's useful to keep in the back of your head. big O is more about communicating time complexity to other humans, isn't it? you don't need it to be formal if you're just optimizing, as far as I know. Easily estimating the running time of algorithms, ie big O.   [deleted] [Masters Theorem](http://en.wikipedia.org/wiki/Master_theorem) will generalize divide and conquer  Stuff I learned in classes:\n\n- Object Oriented Design\n\n- basic graph algorithms (I've used a lot of tree structures in previous work, and my next project has a lot to do with a massive digraph)\n\n- Understanding basic algorithmic efficiency\n\n- Understanding how to understand advanced algorithmic efficiency for when the basics don't cut it\n\nOther things aren't really taught, but you must pick them up.  These include things like\n\n- working with a customer (often a professor fits into a similar role in class)\n\n- how to compensate for group members that lag behind\n\n- the customer isn't always right, but he pays for you so you act like he is and you stay in the job because the work is really interesting, important, and fulfilling, but the customer has no clue what they really want but still have the desire to be in full control of the project\n\n- that great little place just down the street starts happy hour at five &gt; how to compensate for group members that lag behind\n\nHow _do_ you compensate for group members that lag behind? What if they just don't do anything? &gt; how to compensate for group members that lag behind\n\nHow _do_ you compensate for group members that lag behind? What if they just don't do anything?   SQL.  Every business has a database they want reports out of.\n\nOOP / OOD.  Every line of code you write is one you have to maintain.  The better designed it is, the easier it is to maintain.  Class projects you never see again once you get a grade don't prepare you for that.\n\n+1 on networking (thanks cs2818) OOD is a must.\n\nI find that no matter what level of a programmer you are, or how far you are in your career, Anyone can benefit from it. \n\nand I'm realizing that SQL and Databases are almost a must nowadays. Where I work we do queries and database calls all the time and not knowing what exactly is happening sucks.  OOD is a must.\n\nI find that no matter what level of a programmer you are, or how far you are in your career, Anyone can benefit from it. \n\nand I'm realizing that SQL and Databases are almost a must nowadays. Where I work we do queries and database calls all the time and not knowing what exactly is happening sucks.  SQL.  Every business has a database they want reports out of.\n\nOOP / OOD.  Every line of code you write is one you have to maintain.  The better designed it is, the easier it is to maintain.  Class projects you never see again once you get a grade don't prepare you for that.\n\n+1 on networking (thanks cs2818) SQL.  Every business has a database they want reports out of.\n\nOOP / OOD.  Every line of code you write is one you have to maintain.  The better designed it is, the easier it is to maintain.  Class projects you never see again once you get a grade don't prepare you for that.\n\n+1 on networking (thanks cs2818) SQL.  Every business has a database they want reports out of.\n\nOOP / OOD.  Every line of code you write is one you have to maintain.  The better designed it is, the easier it is to maintain.  Class projects you never see again once you get a grade don't prepare you for that.\n\n+1 on networking (thanks cs2818) SQL.  Every business has a database they want reports out of.\n\nOOP / OOD.  Every line of code you write is one you have to maintain.  The better designed it is, the easier it is to maintain.  Class projects you never see again once you get a grade don't prepare you for that.\n\n+1 on networking (thanks cs2818)  Algorithms and Theory of Computation, no doubt. Very little of what I do is particularly algorithmically interesting, but knowing the lower bounds of things is critical in deciding an approach. I very often find myself doing something where the naive approach would be to attempt to do something equivalent to solving the Halting Problem or an NP-complete problem, at which point it becomes clear that we're better off with an approximation or with something that's less general and portable, but computationally practical. What's your job if you don't mind me asking? I productionize, test, and maintain stuff that is mostly written by other people. A lot of the more practical resource management concerns end up being equivalent to the halting problem, scheduling (the general SAT-like form), or the knapsack problem. If we can force things to behave like tractable subsets of the knapsack and scheduling problems, we can avoid situations that end up looking like the halting problem. Sometimes we can't, and we have to do application-specific things to resolve it. An important part of the job is knowing when to invest in a general solution to a problem that comes up repeatedly, and when to stick to application-specific solutions, because a general solution is infeasible or completely impossible. I've always read that real-world instances of SAT tend to be much more tractable than random instances of SAT. Have you found this not to be the case?  I shit you not, Public Speaking.  Might not get a chance to read this because it's going to be far enough down, but if you do, this one's really important.\n\nI learned how *I learn*. I'm realized I'm not a person that sits in a classroom and learns anything. I learned that I learn from reading, and from trying things out and failing. The computer science stuff was nowhere near as important to me - the fact is I knew most of it going into my program. I'd been programming all my life, I learned how to do things through example and repetition.\n\nBut learning that I can't spend more than four hours a day on any single project or I get bored and distracted, or learning that if I don't write the report *ahead* of the more interesting stuff, that it won't get written... those are the most important things I learned. I learned to split up my work into increments, diligence on keeping notes WITH the project and not separately, and to make sure I pick teammates on more than the basis of friendship. Might not get a chance to read this because it's going to be far enough down, but if you do, this one's really important.\n\nI learned how *I learn*. I'm realized I'm not a person that sits in a classroom and learns anything. I learned that I learn from reading, and from trying things out and failing. The computer science stuff was nowhere near as important to me - the fact is I knew most of it going into my program. I'd been programming all my life, I learned how to do things through example and repetition.\n\nBut learning that I can't spend more than four hours a day on any single project or I get bored and distracted, or learning that if I don't write the report *ahead* of the more interesting stuff, that it won't get written... those are the most important things I learned. I learned to split up my work into increments, diligence on keeping notes WITH the project and not separately, and to make sure I pick teammates on more than the basis of friendship.     Using Google to solve my own problems. \n\nIf you can't research a problem and find the solution, your goanna have a bad time. &gt; [goanna](http://members.iinet.net.au/~bush/panoptes.jpg) have a bad time. That's alot of bad time.  recursion recursion That's itteration.  See above That's itteration.  recursion recursion [Recursion!](http://www.reddit.com/r/compsci/comments/vwhwf/what_is_the_most_useful_thing_you_learned_from/c58d985)  The challenge is picking out some individual lesson, or even subject, that is intrinsic to my knowledge set. All I can say is that if I didn't study what I studied (CompSci with a heavy focus in Distributed Computing) there would be no way I could do what I do now (high reliability computing) I think it's the \"heavy focus\" that's the most important. A lot of people seem to just get a general CS education without finding a particular niche that they're interested in; they're the same ones talking about how little they use from school on the job.\n\nHowever, if you actually find some particular subject (like distributed systems), your education will be far more useful in that niche. I think just getting a general CS education and actually specializing in your favorite field are two very different processes--I personally like the latter far more. \n\nI'm interested in programming languages more than distributed systems, despite the fact that my university--and, consequently, my friends and fellow students--seems disproportionately focused on exactly that.       Formal and informal grammars, pumping lemmas, and back-propagating neural nets.\n\nI write a shit-ton of regular expressions, so this stuff was immensely helpful when graphing out a complex parse tree.  The neural network part was helpful with feedback response loops like PIDs and recursive pattern matching.\n\n &gt;Formal and informal grammars, pumping lemmas, and back-propagating neural nets.\n\nWhat kind of employer do you have? I hated Syntax and Semantics.  I never went to ~~school~~university, and I learned programming just by doing it, and occasionally reading blogs, stackoverflow and foss source code.\n\nNow, I've read every single comment in this thread and found nothing which indicates I missed anything by not attending university. Am I wrong, or ..? The stuff you missed by not going to university wasn't the knowledge, but the people.\n\nI found out early on back in uni that I could've taught myself most of the course material (in fact I knew a decent portion of the stuff going into the program). For the stuff I didn't know, it probably would've taken me years to even discover that the fields/concepts even existed. But even with that, yes, you could teach yourself all that stuff without ever stepping foot in a university.\n\nIt wasn't until I met the right people that I continued being delusional in thinking that I was so much better than most others (and that my uni education was worthless) since I effortlessly went through my first two years (my delusion being fueled by the fact that I had taught myself ~70% of the material while I was still in high school).\n\nWhen I met a group of upperclassmen, that all changed. These guys were just a year older than me but I quickly realized they knew a lot more than most of the seniors I had met. I found out that while I was just cruising through my first two years, these guys used that time to study even more stuff and get to know a bunch of professors and work with them. I felt like a novice all over again. When I went into my 3rd year I got to take some courses with these guys and the stuff I learned working with them was invaluable.\n\nIf it weren't for these guys I highly doubt I would have the job I have right now (well known industry giant). Wasn't just me either but everyone in our group of friends that studied with us is doing really well right now.\n\nSo while I did learn quite a bit in a short period of time, I'd say the more important thing was the networking and people I met and worked with. Sure you can meet the best of the best online in a forum, but in a school environment you get to work with these people for years, making collaboration and sharing of ideas much easier, and hell they might even help you get a job like in my situation.\n\nTL;DR I used to have the same mindset as you, but now when someone asks me if university is worth it, I say definitely. IMHO, I'd say you missed out on a lot of opportunities and experience. I never went to ~~school~~university, and I learned programming just by doing it, and occasionally reading blogs, stackoverflow and foss source code.\n\nNow, I've read every single comment in this thread and found nothing which indicates I missed anything by not attending university. Am I wrong, or ..? One liners are great, but not a substitute for a deep understanding of a varied number of topics. What or who are you referring to with the term _one liners_? There is a tendency online to refer to many pieces of knowledge with a single one liner. While it is easy to understand and repeat the one liners themselves, the underlying knowledge often goes much deeper.\n\nAs an example, in the context of software development when the phrase \"Keep it simple stupid\" is used, it is expected that you should understand what it means. Software development in itself is not a simple field. KISS can refer to a proper separation of concerns, not repeating yourself (another great one liner on its own) and other unnecessary complexity, just to name a few. \n\nIn that sense, knowing the one liner is not a substitute for having the actual understanding of the topic.       MVC architecture, networking, SOLID principles, test driving, pair programming, refactoring, version control. You went to a vocational school?         I imagine few will agree with me on this one, but what I learned is that language choice matters. \n\nI took a \"theory of computer languages\" course, and the instructor was a big fan of several programming languages like Lisp, Prolog and Pascal. He believed that the reason why everyone hadn't moved into Lisp programming is just old fashioned inertia, and that Lisp programming is much more productive. That stuck with me, he also taught the dreaded undergrad compilers course at my college, so I had a lot of respect for him.\n\nAnyway, in school, despite this languages course, I only really learned Java. Early in my career though, I got the opprotunity to learn Perl on the job. I found that it had many of the features of Lisp (anonymous subroutines, closures, higher order functions) and adopted it as my primary \"go to\" language. You can do functional programming in it, and while FP in Perl isn't FP in Haskell or Clojure, its a hell of a lot more natural than it would be in something like Java. \n\nI honestly believe its done a lot to help me be successful in my career. With Perl I can whip out applications quickly in a few thousand lines of code that might have been monsters in Java. If I work at a Java shop, its generally pretty easy to stand out from the crowd by doing RAD in Perl. I don't claim to be able to write stuff faster than any Java programmer (only an idiot would make such a claim, there are plenty of amazing Java guys out there), but I think on average, Perl really has given me an advantage in getting things done quickly.\n\nDownsides to it is that Perl carries a heavy stigma. If you just do Perl people assume you're only good for smallish tasks (and the lack of other \"Modern Perl\" programmers at the places I've worked adds to this). There are few \"Modern Perl\" programmers at the jobs I've had, so I rarely get to really work on a team. I've had trouble switching over to something like Ruby or Python, I think because Perl has such a shitty reputation as a write once language, I think people offering Python jobs don't realize how similar that language is to Perl.\n\nThat said, I really think that everyone should learn to code in the most powerful, feature rich language that they can find work in. It makes you happier to be able to get things done quickly. \n\n I think you're missing that Perl's implementation is interpretive and is dynamically typed. These two things make it extremely easy to develop small to mediumish applications quickly. But you pay a price: performance, more bugs and loads of runtime errors.\n\nAnd a lot of people hate Perl probably for the same reason I won't touch it with a ten foot pole: every Perl program I've had to work with has been an absolute nightmare. It seems many share in this experience.\n\n&gt; I think people offering Python jobs don't realize how similar that language is to Perl.\n\nThey are only similar in the sense that their popular implementations are interpretive and that their typing systems are dynamic. Otherwise, they are completely at odds with each other---right down to the philosophy driving language development. &gt;I think you're missing that Perl's implementation is interpretive and is dynamically typed. These two things make it extremely easy to develop small to mediumish applications quickly. But you pay a price: performance, more bugs and loads of runtime errors.\n\nNot missing it, I just don't care. I've never had a serious issue with performance in Perl. Generally your applications will bottleneck elsewhere, such as in the database. If you do bottleneck in code, its probably solvable by changing your algorithm. \n\nThere are apps where performance matters, but most jobs aren't asking for guys to write an OS kernal or video transcoder. \n\nThe bugs thing is mitigated by a strict adherance to functional programming and unit testing. I'd trust a Perl program written using FP practices ( avoiding the use of mutating state, not just using map and grep ) over an OO Java program anyday, static typing be dammed. \n\n&gt;And a lot of people hate Perl probably for the same reason I won't touch it with a ten foot pole: every Perl program I've had to work with has been an absolute nightmare. It seems many share in this experience.\n\nThis is, IMO, Perl's biggest problem. I've had that experience as well. There's a ton of older Perl code out there, and its nearly all a mess. Newer Perl written by competent people using FP practices or Moose are much better. \n\n&gt;They are only similar in the sense that their popular implementations are interpretive and that their typing systems are dynamic. Otherwise, they are completely at odds with each other---right down to the philosophy driving language development.\n\nThey are the same with regard to their feature sets, or at least the subset of their features that are really useful. Python gets list comprehensions, which I'd love to work with, but other than that, there's not much difference.  &gt; Not missing it, I just don't care. I've never had a serious issue with performance in Perl. Generally your applications will bottleneck elsewhere, such as in the database. If you do bottleneck in code, its probably solvable by changing your algorithm.\n&gt; There are apps where performance matters, but most jobs aren't asking for guys to write an OS kernal or video transcoder.\n\nYeah, right on, the only time performance matters is in OS kernels and video transcoders. /s\n\n&gt; The bugs thing is mitigated by a strict adherance to functional programming\n\nFirst of all, Perl doesn't have referential transparency. Second of all, if your tools aren't forcing this \"strict adherence,\" then forgive me that I don't take your word for it. (Only because you are a human.)\n\n&gt; and unit testing. I'd trust a Perl program written using FP practices ( avoiding the use of mutating state, not just using map and grep ) over an OO Java program anyday, static typing be dammed.\n\nWhether you believe it or not, static typing catches an entire class of type-related bugs at compile time that dynamically typed languages cannot compete with. And don't forget about strong vs. weak typing.\n\nYour problem is your trust. You're telling other developers that your code is better because you adhere to some principle of programming. But without the tools to check you, your claims of correctness don't mean much.\n\nUnit testing helps, but it isn't going to help dynamic environments any more than it helps static environments.\n\n&gt; This is, IMO, Perl's biggest problem. I've had that experience as well. There's a ton of older Perl code out there, and its nearly all a mess. Newer Perl written by competent people using FP practices or Moose are much better.\n\nPretty much all code written by competent people is OK to work with. But that is irrelevant :-)\n\n&gt; They are the same with regard to their feature sets, or at least the subset of their features that are really useful. Python gets list comprehensions, which I'd love to work with, but other than that, there's not much difference.\n\nIf you only want to evaluate language differences by \"feature lists\", then you are probably right (but I don't keep a running tally). &gt;Yeah, right on, the only time performance matters is in OS kernels and video transcoders. /s\n\nAre you really one of those guys that doesn't acknowlegde that performance, in general, isn't that important anymore?\n\nIf you're doing some kind of hard core numerical processing, Perl has C based libraries for that. If you're just writing glue code, which I think is what most of us do nowadays, there's no need for the language to be lightning fast. \n\nBeyond that, Perl isn't that slow when compared to other commonly used languages. \n\n&gt;First of all, Perl doesn't have referential transparency. Second of all, if your tools aren't forcing this \"strict adherence,\" then forgive me that I don't take your word for it. (Only because you are a human.)\n\nThe point stands. Sure, I'd rather code in Clojure than Perl. Sure, I'd love language level tools which keep my code from producing harmful side effects. That said, writing code in that style will produce fewer bugs than using an OO style. It allows for easier testing as well. You *could* do this in Java by writing a bunch of static functions, and I've heard that some people do, but its not as natural. \n\n&gt;Whether you believe it or not, static typing catches an entire class of type-related bugs at compile time that dynamically typed languages cannot compete with. And don't forget about strong vs. weak typing.\n\n&gt;Your problem is your trust. You're telling other developers that your code is better because you adhere to some principle of programming. But without the tools to check you, your claims of correctness don't mean much.\n\n&gt;Unit testing helps, but it isn't going to help dynamic environments any more than it helps static environments.\n\nI don't expect anyone here to trust me. The people in this reddit are not spring chickens by and large. Many of them understand the difference, and I imagine that most don't see the claim that adhering to functional programming practices reduces bugs in your programs as controversial. \n\nAnd I don't see strict typing as valueless, but I don't have the unreasonable level of regard for it that some do. People manage to write correct, working programs without automatic type checking all the time. The fact that the Catalyst Web Framework isn't written in a strictly typed language doesn't mean that it is bug riddled. The fact that DBIx::Class isn't written in a strictly typed language doesn't mean that it inserts bad data into your database. Both tools work and work well, static typing be dammed. \n\nMy message here is simply that langauge choice does have an affect on an individual's productivity. You're right in indicating that it can have an effect on quality control as well. If I were writing software for an x ray machine, I'd prefer to write it in Haskell rather than in Perl. Again, language choice matters. \n &gt; Are you really one of those guys that doesn't acknowlegde that performance, in general, isn't that important anymore?\nIf you're doing some kind of hard core numerical processing, Perl has C based libraries for that. If you're just writing glue code, which I think is what most of us do nowadays, there's no need for the language to be lightning fast.\n\nSo Perl has been fast enough *for you* it sounds like. Sometimes interpreted implementations just don't cut it.\n\nPerformance is *always* an issue. Just because you don't care if a program takes 1 second or 10 doesn't mean there aren't plenty of scenarios where that does matter.\n\n&gt; Beyond that, Perl isn't that slow when compared to other commonly used languages.\n\nIt's extremely slow compared to any popular implementation of a programming language that compiles to native code (or the JVM). [Evidence](http://shootout.alioth.debian.org/u64q/which-programming-languages-are-fastest.php).\n\n&gt; The point stands. Sure, I'd rather code in Clojure than Perl. Sure, I'd love language level tools which keep my code from producing harmful side effects. \n\nNo, your point doesn't stand. Your code provides no guarantees precisely because your discipline provides no guarantees.\n\n&gt; That said, writing code in that style will produce fewer bugs than using an OO style.\n\nAn extremely dubious claim. I can *prove* static typing produces fewer bugs. You cannot prove your claim.\n\n&gt; And I don't see strict typing as valueless, but I don't have the unreasonable level of regard for it that some do. People manage to write correct, working programs without automatic type checking all the time. The fact that the Catalyst Web Framework isn't written in a strictly typed language doesn't mean that it is bug riddled. The fact that DBIx::Class isn't written in a strictly typed language doesn't mean that it inserts bad data into your database. Both tools work and work well, static typing be dammed.\n\nThis is a red herring. There are also plenty of tools that are correct and work well that are written in x86 Assembly. I think you're missing my point.\n\n&gt; My message here is simply that langauge choice does have an affect on an individual's productivity. You're right in indicating that it can have an effect on quality control as well. If I were writing software for an x ray machine, I'd prefer to write it in Haskell rather than in Perl. Again, language choice matters.\n\nOf course language choice matters... You were advocating use of a language with \"the most features\" willy nilly. (And if that were true, you'd be using Haskell.) I was pointing out that there is no panacea. Perl + \"programming in a functional style\" doesn't just make things automatically better, and it doesn't get rid of the price you're paying for using a dynamic programming language.\n\nThe point is to honestly and properly balance the trade offs. Dynamic languages have short development cycles, but they are slower and can become unwieldy as the project grows due to their typing schemes. &gt;Of course language choice matters... You were advocating use of a language with \"the most features\" willy nilly. (And if that were true, you'd be using Haskell.) I was pointing out that there is no panacea. Perl + \"programming in a functional style\" doesn't just make things automatically better, and it doesn't get rid of the price you're paying for using a dynamic programming language.\n\nWe'll have to agree to disagree on much of this. I don't see the point about static typing and correctness. I'm saying that the fact that lots of correct programs are written in Perl shows that static typing isn't some incredibly important thing. I get that it removes classes of bugs, but I think those bugs are generally very easy to avoid making anyway (by not doing silly things like using an add operator on a file for instance). I get that its useful, and all things being equal its something I would want. I just don't think its as useful as other features like closures for the majority of programming tasks.\n\nI said you should use the most powerful language that you can find work in. I did not say that you should use the most powerful language and that the language to use is Perl. Perl is as high up on the power continuum as I've personally been able to find work in. If I could choose any language to use, it would either be Haskell or Clojure. Realistically I'd like to switch over to Python, although that's more about avoiding the Perl stigma than it is about moving up on the hypothetical programming power continuum. I very much prefer Perl to the statically typed OOP languages though, and I stand by the claim that Perl is above them on the hypothetical programming power continuum. \n      Ballpark time complexity analysis. Set theory.  Did you ever actually learn set theory explicitly? I just took an interesting CS theory course, and they just expected it us to know it already or pick it up on the fly. Happily, it turns out that set theory (well, the bits needed for the class, anyhow) is fairly easy for me, so it went well. But I never have taken a formal class covering it.        That a good IDE is your best friend. Learning Java with eclipse made it seem like cheating compared to when I started out with python and notepad.    command f. I am not a graduate but that saves me so much time now that I guess that could be one. Also, before I started CompSci classes I was computer slow. I am now computer shmedium. Or for other OS's, control-f, if I am not mistaken in your meaning? You wouldn't believe how many people I work with who can't do a simple search. I mean, the IDE practically does the work for you... *sigh*",
    "url" : "http://www.reddit.com/r/compsci/comments/vwhwf/what_is_the_most_useful_thing_you_learned_from/"
  }, {
    "id" : 13,
    "title" : "THE PH.D. GRIND",
    "snippet" : "  [deleted] [deleted] Maths papers are listed alphabetically, or grouped together by address to make the title page narrower. Applied CS papers are listed \"in order\". You can get a pretty good feel for how theoretical a conference is just by looking at the proportion of papers that are listed alphabetically (remembering to factor in the chances of this happening anyway). Indeed, this method has been used in a seminal paper to evaluate how theoretical or practical a CS conference is based on the proportion of alphabetically to non-alphabetically listed authors: www.cs.princeton.edu/~appel/papers/science.pdf [deleted] [deleted] [deleted]     Everyone should read it. It is excellent and it is totally worth of the time that it requires.  As someone who is going through 3-d year limbo, this got me to open up my work after a week-long hiatus. Thank you very much for posting this. I guess there's hope for me yet :)            holy crap, that's really long. Can I get a TL;DR so I can decide whether I want to read it?\n\nedit: looks like I do. I didn't want to waste time reading something that was too far over my head to be useful, or similar.\n\nedit #2 (three days later): oh, fuck you downvoters, I was trying to be honest and clear about my reasons for being skeptical of it being useful; I had my question answered, it became clear that it would be useful, but you assholes still downvote me! this is fucking retarded. I can't even complain about it, either, because saying this is going to get me more downvotes (and going meta about it yet more). can't I ask questions? holy crap, that's really long. Can I get a TL;DR so I can decide whether I want to read it?\n\nedit: looks like I do. I didn't want to waste time reading something that was too far over my head to be useful, or similar.\n\nedit #2 (three days later): oh, fuck you downvoters, I was trying to be honest and clear about my reasons for being skeptical of it being useful; I had my question answered, it became clear that it would be useful, but you assholes still downvote me! this is fucking retarded. I can't even complain about it, either, because saying this is going to get me more downvotes (and going meta about it yet more). can't I ask questions? TL;DR: I went to grad school and wrote a memoir right after getting my PhD. holy crap, that's really long. Can I get a TL;DR so I can decide whether I want to read it?\n\nedit: looks like I do. I didn't want to waste time reading something that was too far over my head to be useful, or similar.\n\nedit #2 (three days later): oh, fuck you downvoters, I was trying to be honest and clear about my reasons for being skeptical of it being useful; I had my question answered, it became clear that it would be useful, but you assholes still downvote me! this is fucking retarded. I can't even complain about it, either, because saying this is going to get me more downvotes (and going meta about it yet more). can't I ask questions? holy crap, that's really long. Can I get a TL;DR so I can decide whether I want to read it?\n\nedit: looks like I do. I didn't want to waste time reading something that was too far over my head to be useful, or similar.\n\nedit #2 (three days later): oh, fuck you downvoters, I was trying to be honest and clear about my reasons for being skeptical of it being useful; I had my question answered, it became clear that it would be useful, but you assholes still downvote me! this is fucking retarded. I can't even complain about it, either, because saying this is going to get me more downvotes (and going meta about it yet more). can't I ask questions?  ",
    "url" : "http://pgbovine.net/PhD-memoir/pguo-PhD-grind.pdf"
  }, {
    "id" : 14,
    "title" : "Graph Problem",
    "snippet" : "Hello,  I have been working on an implementation of the self-organizing map that uses an irregular structure instead of a grid.  A problem I have run into is finding all neighboring vertices within n edges of a given vertex.  All edges connect reflexively and no edge can connect a vertex to itself.  Is this a problem that has been solved before?  I can't seem to find an algorithm that can do the job.  Thank you for any input!  Um, I may be missing some detail, but isn't this just a breadth-first search with an exit criterion of \"distance n away\"? Or are you looking for an optimal solution? \n Just an FYI, most graphs are generally not stored in grids unless they happen to be exactly grid-like. Most people use an adjacency list or an adjacency matrix. Depending on how you store your graph, one data structure may be more efficient.\n\nThis problem is a perfect case for a BFS. Adding onto unitarian's comment about data structures, since every edge in your graph is reflexive, I personally would use an adjacency list, since the matrix would be a mirror image of itself (i.e., decomposable into a series of simple lists of neighbors for a given node).\n\nOr more expressively put:\n\n    class/struct node {\n        public X Data;\n        public node* neighbors;\n    }\n [wikipedia/Parkinson's_Law_of_Triviality](http://en.wikipedia.org/wiki/Parkinson%27s_Law_of_Triviality) Adding onto unitarian's comment about data structures, since every edge in your graph is reflexive, I personally would use an adjacency list, since the matrix would be a mirror image of itself (i.e., decomposable into a series of simple lists of neighbors for a given node).\n\nOr more expressively put:\n\n    class/struct node {\n        public X Data;\n        public node* neighbors;\n    }\n  ",
    "url" : "http://www.reddit.com/r/compsci/comments/vuwyp/graph_problem/"
  }, {
    "id" : 15,
    "title" : "Area 51 « Gödel’s Lost Letter and P=NP",
    "snippet" : "  About that Simulation Argument. Computer code has been discovered in Superstring equations. http://www.youtube.com/watch?v=lYeN66CSQhg&amp;feature=player_detailpage#t=3701s 1:01:41\n\n**TLDW** http://www.youtube.com/watch?v=bp4NkItgf0E So, I don't think the speaker in your second link is a computer scientist. Error correction codes != computer code. Computer code usually implies machine instructions. Error correction codes are simply redundant bits in a stream of data. So no, that scientist did not find some \"universal assembly code\" in his equations (which he arrived at after making many, many assumptions). He is a physicist. Can you explain the error correcting code a bit more? Aren't  \"redundant bits in a stream of data\" the errors this correction code is supposed to fix? He is a physicist. Can you explain the error correcting code a bit more? Aren't  \"redundant bits in a stream of data\" the errors this correction code is supposed to fix?",
    "url" : "http://rjlipton.wordpress.com/2012/06/28/area-51/"
  }, {
    "id" : 16,
    "title" : "ResearchGate, I'm boycotting you for spamming me and my co-authors",
    "snippet" : "In the last couple of days, I got a few invitations to join ResearchGate from some of my co-authors. The wording of the invites---and indeed, the very idea of inviting colleagues to some such random \"ResearchBook\" site---seemed quite unlike the people who purportedly sent the emails (These are folk who do surely have better uses of their time.). So I cross-checked with one of them.\n\nSurprise, surprise! He never sent such an email. He did remember joining the website, but found it to be of no value and so did not pursue it further. He seemed quite uncomfortable at having \"invited\" me in this fashion, and told me that he promptly deleted his account at ResearchGate when he got my query about the invite.\n\nSo, ResearchGate: I have deleted my account with you, and I will actively try to spread the message that you are out to spam researchers.   If you go to their settings you'll see that they opt you in to all sorts of things. It's in bad taste and I hope they figure out it's undermining their chances of success.   Now what shall we call this research-related scandal? Now what shall we call this research-related scandal?       This is Reddit, not ResearchGate. If you have an issue with them, just tell them. Ever heard of a public service announcement? This is Reddit, not ResearchGate. If you have an issue with them, just tell them.  &gt; out to spam researchers.\n\nthat seems a bit of an overreaction, one email hardly qualifies as spam &gt; out to spam researchers.\n\nthat seems a bit of an overreaction, one email hardly qualifies as spam So are you saying it's not unsolicited or that it's not email? yeah, I'm pretty sure it's not actually email...\n\nIf you submit a publication to a journal, many of them will email all the authors; unsolicited, but not spam.\n\nThey certainly shouldn't be emailing each author once for each paper, so let's say that's bad programming, but (for example) a single email noting that you were added as a co-author on their site, while unsolicited, hardly constitutes their being \"out to spam researchers\" &gt; a single email noting that you were added as a co-author on their site, while unsolicited, hardly constitutes their being \"out to spam researchers\"\n\nPerhaps, but:\n\n* The email said : \"ABC, you have been invited to join ResearchGate by XYZ\".\n* The email headers were set up in such a way that the email seemed to have been sent by \"XYZ\".\n\nMy first feeling at seeing this invite from XYZ was one of surprise: my association with XYZ so far hasn't given me the impression that he is the kind of person who would do such a thing. My subsequent email exchange with him confirmed this, of course.\n\nSome senior person's first feeling at seeing this invite from *me* might be of irritation, and then perhaps anger that I am wasting his time. He may also not be net-savvy enough to realize that *I* didn't do this. And he probably won't bother checking with me---we all have more interesting things to do, really---but would likely keep this in mind, and make a mental note about me which would probably not be that positive.\n\nI don't know about others, but I am certainly not in a position to irritate people in this manner---and I don't think I would do it, even if I was. Hence: I get off this particular bus.\n",
    "url" : "http://www.reddit.com/r/compsci/comments/vqzel/researchgate_im_boycotting_you_for_spamming_me/"
  }, {
    "id" : 17,
    "title" : "The Speed Prior: A New Simplicity Measure Yielding Near-Optimal Computable Predictions",
    "snippet" : "   ",
    "url" : "http://www.idsia.ch/~juergen/speedprior/sld001.htm"
  }, {
    "id" : 18,
    "title" : "Computer Scientists Break Security Token Key in Record Time",
    "url" : "http://bits.blogs.nytimes.com/2012/06/25/computer-scientists-break-security-token-key-in-record-time/"
  }, {
    "id" : 19,
    "title" : "Is there a site that goes into the concept of algorithms and data structures and how they work? ",
    "snippet" : "Long story short, the professor I had for my \"Data Structures and Program Design\" class was the worst I ever had and now I am turning to the Internet to try to understand the concept of algorithms as well as stuff like stacks, linked lists, recursion, etc. over the summer so I can be better prepared for the re-take for the fall. \n\nAnyone have anything that can help me out?\n\nAlso, the code language the class uses is Java.  At the risk of sounding old-fashioned, you're better off sitting down with a good book, and going through it methodically.  If I had to pick one, I would definitely suggest:\n\n * [Introduction to Algorithms](http://www.amazon.com/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844/)\n\nHaving said that, you can do the course itself online thanks to MIT's OpenCourseware:\n\n * [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/](MIT OCW Introduction to Algorithms)\n\nThis is also available on iTunes U.\n\nAs for program design, Stanford and MIT have some great courses on iTunes U, but also try Coursera.     Hungarian and Romanian inspired dance videos to help visualize algorithms. Dead serious.\n\nhttp://www.youtube.com/user/AlgoRythmics/videos\n\nIt's definitely a weird video to watch, but I am a very visual person, and watching the dancing helps me get an idea on how to write the programs.  https://www.coursera.org/\nthe algorithm classes have already begun but its just been 2 weeks. Enroll for your desired classes.             There are literally tens (if not hundreds) of thousands of such sites. You can just google for \"algorithms and data structures\" and you'll get quite a few online course materials etc. Maybe he's asking if anyone has had success with any particular one. Quantity != Quality.  ",
    "url" : "http://www.reddit.com/r/compsci/comments/vnokd/is_there_a_site_that_goes_into_the_concept_of/"
  }, {
    "id" : 20,
    "title" : "Google constructs massive neural network and feeds it YouTube images. Network teaches itself to recognize cats.",
    "snippet" : "   Google builds skynet, instead of enslaving humanity it decides to watch cat videos. The future is weirder than I could have ever guessed... Google builds skynet, instead of enslaving humanity it decides to watch cat videos. The future is weirder than I could have ever guessed...  Damn, the [cat captcha](http://research.microsoft.com/en-us/um/redmond/projects/asirra/) is doomed! Damn, the [cat captcha](http://research.microsoft.com/en-us/um/redmond/projects/asirra/) is doomed!  I'd expect a more technical title posting to /r/compsci.\n\n\"Massive neural network\" = between 34-68 standard racks I'd expect a more technical title posting to /r/compsci.\n\n\"Massive neural network\" = between 34-68 standard racks [deleted]   As an repost from /AI subforum, I present you with this and ask for help. \"The Google brain assembled a dreamlike digital image of a cat by employing a hierarchy of memory locations to successively cull out general features after being exposed to millions of images\". Can anyone please explain to me, how this works? \"employing a a hierarchy of memory location\" in regard to a datasheet with 10 mil of 200px by 200px unlabbeled pictures? If I understand correctly, it had one layer that looked at the smallest-possible patterns (2px by 2 px), another for 4x4, considered as 2x2 grids of the 2x2 patterns it already knew, and so on up the chain.\n\nThese were grouped together based on which patterns were similar to each other, and eventually the top-level buckets were named by the researchers and were tested against some other test sets, including one that contained something like 50% cats / 50% noncats, where the neural net correctly sorted the images into cat and noncat with pretty good accuracy.   Color me impressed - I always thought giant NNs were a bad idea due to overfitting of the training data.   That's my problem with NN. It's not actually \"learning\" but rather just predicting with a high probability. Human brains do not work this way. I did not have to see a million cats before I could recognize what a cat looks like. In fact just seeing one cat enabled me to detect all other cats. I'm not worried about skynet for this exact reason. When a computer (regardless of the size/cores) can learn and extrapolate from a single data point, then we have to start getting worried. Actually human brains do work this way.  You did not have to see a million cats because you had seen millions of other things before, so you had learned the concepts of living things, animals, animal parts such as head, eyes, paws, tail etc.    The title is a bit bogus.  Sure, Neural Networks can \"learn\" things, but they do not \"teach themselves\".  Someone at Google taught this thing to recognize cat images. Quote from the article: \"Currently much commercial machine vision technology is done by having humans “supervise” the learning process by labeling specific features. In the Google research, the machine was given no help in identifying features.\"\n\nThe network apperently uses [unsupervised learning](http://en.wikipedia.org/wiki/Unsupervised_learning), so the title is actually right in that regard, no? The description in the article is very vague.  I did my honours project involved using neural networks to classify images into categories, and one of the key features of that process is that the neural network needs to have *feedback* on each result (positive or negative).  Without feedback it is impossible to learn anything.\n\n\"Labeling specific features\" and \"providing feedback\" are not the same thing, so I still think my point stands.\n\n The description in the article is very vague.  I did my honours project involved using neural networks to classify images into categories, and one of the key features of that process is that the neural network needs to have *feedback* on each result (positive or negative).  Without feedback it is impossible to learn anything.\n\n\"Labeling specific features\" and \"providing feedback\" are not the same thing, so I still think my point stands.\n\n Criticizing and trying to downplay other peoples work without even taking the time to actually understand it is just shameful. The network was not provided any sort of positive or negative feedback regarding catness. The only feedback is in the form of the network's ability to reconstruct input. The neural network they're using is called an autoencoder, and it is trained in an unsupervised manner without using labeled training data. You should read the [paper](http://arxiv.org/abs/1112.6209). I am not trying to down play their work at all.  Their work is amazing, and I know so because I have done research in their area myself.\n\nI would like to direct your attention to the following extracts from the paper.  You can find them right there in the summary.\n\n&gt; \"We train this network using model parallelism and asynchronous SGD\"\n\n&gt; \"it is possible to train a face detector without having to label images as containing a face or not\"\n\n&gt; \"we trained our network to obtain 15.8% accu- racy in recognizing 20,000 object categories from ImageNet\"\n\nThey trained the network.  The network did not train itself. Well, this is getting into semantics. :/\n\nThey did, of course, feed the network a set of (random!) images. After that, they let the network do it's thing, which requires no interaction on their part and inspected the result afterwards. In my book this is as good as a network can get to self-teaching. Also have a look at the [paper](http://icml.cc/2012/papers/73.pdf). The title is a bit bogus.  Sure, Neural Networks can \"learn\" things, but they do not \"teach themselves\".  Someone at Google taught this thing to recognize cat images. &gt; Currently much commercial machine vision technology is done by having humans “supervise” the learning process by labeling specific features. In the Google research, the machine was given no help in identifying features.\n\n\n&gt; “We never told it during the training, ‘This is a cat,’ ” said Dr. Dean, who originally helped Google design the software that lets it easily break programs into many tasks that can be computed simultaneously. “It basically invented the concept of a cat.",
    "url" : "https://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence-of-machine-learning.html"
  }, {
    "id" : 21,
    "title" : "An Empirical Analysis of Hardware Failures on a Million Consumer PCs",
    "snippet" : "   Great read. I was surprised most by the 1 in 190 initial failure rate of CPUs being higher than the 1 in 270 initial failure rate of hard drives. Did anyone else find that odd? Great read. I was surprised most by the 1 in 190 initial failure rate of CPUs being higher than the 1 in 270 initial failure rate of hard drives. Did anyone else find that odd?  TLDR?  On CPU Overclocking\n&gt; The table shows that CPUs from Vendor A are nearly 20x as likely to crash a machine during the 8 month observation period when they are overclocked, and CPUs from Vendor B are over 4x as likely\n\nIn conclusion, don't overclock AMD processors We have been arguing about this all day. No one knows for sure which of AMD and Intel are vendor A and B in the paper. Do you have a theory?  We have been arguing about this all day. No one knows for sure which of AMD and Intel are vendor A and B in the paper. Do you have a theory?  No, I was making a guess. I recall hearsay about AMD running closer to thresholds than Intel in order to keep up with intel processors, but that was a long time ago and I have no idea how true it is now.",
    "url" : "http://research.microsoft.com/pubs/144888/eurosys84-nightingale.pdf"
  }, {
    "id" : 22,
    "title" : "Algorithmic Problem Solving competition for Turing's anniversary -- quite cool!",
    "snippet" : "  I wish I heard about some of these competitions before they were well under way.  I've been having a hard time understanding the specific rules of a Turing machine, because as of now I don't see why the tape would be anything other than just the output of the program. What I mean is, are you allowed to used variables in your algorithm (thus not making the tape necessary for memory)? Are you even allowed to use numbers at all in the algorithm? The only memory is the tape, you can only use the basic operations: left move, right move, put something under the head and check what is under the head. Formally you combine this with states (like multiple \"if state0 and 'A' under the head then do left move and go to state1\"), but if you only use the tape (and its basic operations), conditions and loops in a programming language it's the same. You can even transform your program into states.\n\nBut for these problems there are specific rules for the game. Anyway you can only use the tape Okay I got a little better understanding, but I'm not quite there yet. Would any of these statements be allowed?\n\n\"if state0 then move **8** right\"\n\n\"if state1 then jump to this part in the algorithm then move right\"\n\nIn the first one we would have created a variable that counts downwards. In the second example we could have nested them 8 times to essentially read a normal bit representation of an integer and perform different operations depending on what it was. Okay I got a little better understanding, but I'm not quite there yet. Would any of these statements be allowed?\n\n\"if state0 then move **8** right\"\n\n\"if state1 then jump to this part in the algorithm then move right\"\n\nIn the first one we would have created a variable that counts downwards. In the second example we could have nested them 8 times to essentially read a normal bit representation of an integer and perform different operations depending on what it was.",
    "url" : "http://algorithmicproblemsolving.org/competitions/"
  }, {
    "id" : 23,
    "title" : "Smart way to map a tuple to a number?",
    "snippet" : "Hello, \n\nIf I have a non functional programming language similar to C and I want to map a int to another int like that \n\n    n -&gt; m {with n,m ∈ [0..N]}\n\nthe solution would be easy by declaring an array x with  N+1 elements and map x[n] = m. \n\nHowever what if I want to map a tuple like \n\n    (n,p,q) -&gt; m {with n,m,p,q ∈ [0..N]}\n\nI don't have a straight forward way to map this... \n\nof course I could do something like creating a struct like \n\n    struct TUPLE {\n        int n; \n        int p; \n        int q; \n    }\n\nand create a array holding values of TUPLE where the index of my array would be my m and than iterate over my array every time I look for a value. \n\nBut I don't know it's kinda bloated... Is there a more smarter way? \n\n  Map (n, p, q) to 2^n \\* 3^p \\* 5^q  . If you add more to the tuple, add another prime number.\n\nTo go backwards, just get the prime decomposition.\n\nNo additional space required.  However, it requires arbitrary-length integers, otherwise it's capped rather embarrassingly. This is what Gödel did, and it's pretty clever. Map (n, p, q) to 2^n \\* 3^p \\* 5^q  . If you add more to the tuple, add another prime number.\n\nTo go backwards, just get the prime decomposition.\n\nNo additional space required.  However, it requires arbitrary-length integers, otherwise it's capped rather embarrassingly. The problem being that this grows extremely fast:\n\n    def godel(n, p, q):\n        return 2 ** n * 3 ** p * 5 ** q\n\n    godel(1,1,1) = 30\n    godel(1,2,3) = 2250\n    godel(2,3,4) = 67500\n    godel(4,5,6) = 60750000\n\nNot exactly practical.  Well, if you want to be *really* thrifty with your data structures, **and** square roots are not too much to ask, you can use [Cantor's pairing function](http://jeux-et-mathematiques.davalan.org/divers/bij/bij_Nn2N-en.html) to get a bijection from your 3-tuple of ints to ints. Then you are brought back to the problem of creating a mapping between ints, which you have already solved.\n\n   (n, p, q) ⇄ z\n\n- z = f(f(n, p), q) where f(x,y)= ((x+y)² + 3x + y) / 2  \n- In the reverse direction, \n    - q= i(3 + i)/2 - z, \n    - p = j(3+j)/2 - k, \n    - n = k - j(j+1)/2, \n  \n    where i = (-1 + √(1 + 8z))/2, k = z - i(i + 1)/2, j = (-1 + √(1+8 k))/2\n\nNothing particularly functional about this, of course. Well, if you want to be *really* thrifty with your data structures, **and** square roots are not too much to ask, you can use [Cantor's pairing function](http://jeux-et-mathematiques.davalan.org/divers/bij/bij_Nn2N-en.html) to get a bijection from your 3-tuple of ints to ints. Then you are brought back to the problem of creating a mapping between ints, which you have already solved.\n\n   (n, p, q) ⇄ z\n\n- z = f(f(n, p), q) where f(x,y)= ((x+y)² + 3x + y) / 2  \n- In the reverse direction, \n    - q= i(3 + i)/2 - z, \n    - p = j(3+j)/2 - k, \n    - n = k - j(j+1)/2, \n  \n    where i = (-1 + √(1 + 8z))/2, k = z - i(i + 1)/2, j = (-1 + √(1+8 k))/2\n\nNothing particularly functional about this, of course. actually, I think I guess I use this method... Thanks! :) Note that it is possible that f(x,y) &gt; N even though x, y &lt; N and that this happens relatively fast. If N = MAXINT on a 32 bit computer, for instance, then f(x, 0) will overflow for x ≥ 92 681.  If the values are small enough, you can concatenate them. Otherwise, it sounds like you're fumbling your way towards a hash table. yeah I wouldlike to avoid a hasttable, because there is no standard implementation in C.  They're pretty easy to implement.\n\nEdit: Here you go. http://pastebin.com/Jvr8y8bb Oh! Thanks a lot, I will check it out later :)  Things like these are the reason I end up using C++ most of the time.\n\n    std::map&lt;std::tuple&lt;int, int, int&gt;, int&gt; your_map;\n\nAll done, off to coffee break… That approach does solve the problem, but std::map isn't a hashmap. If you want a hashmap, use:\n\n    std::unordered_map&lt;std::tuple&lt;int, int, int&gt;, int&gt; your_map;\n\n(new in C++11) They're pretty easy to implement.\n\nEdit: Here you go. http://pastebin.com/Jvr8y8bb  If you have a max value, then you can think of the numbers in the tuple as digits.\n\nFor example, let's say all of your numbers are in the range 0-9.  You can map your tuples to integers by taking the first value as the first digit in base 10, the second value as the second digit, and so on.  That makes the tuple (3, 2, 4) map to index 324. This is best if there is a maximum value. If there is not, a working solution is to interleave the binary digits of each number.\n\neg. (6,5,4) = (110,101,100) =&gt; 111 100 010 = 482\n\n[Some C++ code for this](http://ideone.com/w1zyY) If you have a max value, then you can think of the numbers in the tuple as digits.\n\nFor example, let's say all of your numbers are in the range 0-9.  You can map your tuples to integers by taking the first value as the first digit in base 10, the second value as the second digit, and so on.  That makes the tuple (3, 2, 4) map to index 324.   You could use a three-dimensional array, x[N+1][N+1][N+1]. Of course it will be huge if you have a large N.\nI would probably use some kind of hash table\nhttps://en.wikipedia.org/wiki/Hash_table .\nUnfortunatly, there is no default implementation of a hash table in the C standard library. Your post suggests that there is an obvious way to do this in a functional langauge. I am curious what that solution is? (I have almost no experience in functional languages.) You could use a three-dimensional array, x[N+1][N+1][N+1]. Of course it will be huge if you have a large N.\nI would probably use some kind of hash table\nhttps://en.wikipedia.org/wiki/Hash_table .\nUnfortunatly, there is no default implementation of a hash table in the C standard library. Your post suggests that there is an obvious way to do this in a functional langauge. I am curious what that solution is? (I have almost no experience in functional languages.)   During the past few years I've come to realize something: Memory is cheap. Unless you're processing gigabytes of information on your PC, you can pretty much not care.\n\nIf you do care, you can simply use a standard Binary Tree to make the seeking time logarithmic and not use too much memory. The hashtable will be a bit harder to use in C. yeah, you are like right. It's just that it might be easier to actually map my tuple like before than introducing hashtables...  using C++11:\n\n    #include &lt;unordered_map&gt;\n    #include &lt;tuple&gt;\n    #include &lt;utility&gt;\n    #include &lt;iostream&gt;\n    \n    typedef std::tuple&lt;int,int,int&gt; threeple;\n    typedef std::pair&lt;threeple,int&gt; keymap;\n    \n    int hashme(const threeple &amp;a) {\n        /** Hashing function goes here! **/\n    }\n\n    namespace std {\n        template &lt;&gt; struct hash&lt;threeple&gt; {\n            size_t operator()(const threeple &amp;a) const {\n                return hashme(a);\n            }\n        };\n    }\n    \n    int main() {\n        std::unordered_map&lt;threeple, int&gt; mappy;\n        threeple test1(2,3,5);\n        threeple test2(8,9,10);\n        mappy.insert(keymap(test1,hashme(test1)));\n        mappy.insert(keymap(test2,hashme(test2)));\n        std::cout &lt;&lt; mappy[test2] &lt;&lt; std::endl;\n    } ",
    "url" : "http://www.reddit.com/r/compsci/comments/vktql/smart_way_to_map_a_tuple_to_a_number/"
  }, {
    "id" : 24,
    "title" : "Enrolling in a Computer Science graduate program? This is what I wish I knew when I started.",
    "snippet" : "  Your advice is relevant to anyone studying any subject at any university  The never fall behind is easily the biggest thing for me anyway. I thought it would be easy to read that in my spare time if I skip this one lecture, then you don't understand the next lecture and you just fall further and further behind. [deleted]  I just finished myself and from that list the biggest things I agree with are. \n\n- The hardest stuff to learn is difficult material that does not interest you. Be very cautious when choosing your classes for this reason.\n\n- Always ask around about a professor before taking their class. A bad professor can ruin good subject matter.\n\nThese are so crucial (in all of college) that this can kill a semester or even a year.\n\nI want to add a few things that are possibly more CS related.\n\n- If the theoretical part doesn't interest you even a bit, you will have a bad time.\n\n- Focus on getting the programming basics, do ALL the work. Do ALL the extra work. Try helping others. The programming basics are so important to all of the classes you are going to take after that.\n\nI've TA'd programming intro and data structures a few times and my biggest focus is that they get don't just grasp the content a bit, but really get it. Good points.. I had a strong programming background and was very excited to learn theory, so maybe I took those things for granted. \n\nIf I may, an attempt to generalize your point a bit... 'Its important to have equal interest in practical application as well as foundational theory. And be prepared to apply extra effort to the one that is weaker.'\n\nI hope you don't mind if I steal that an amend the post :)   As a counterpoint, someone who went straight from undergraduate to a (theory) Ph.D. program, this advice is not at all relevant to my experience. \n\nIn my program, classes are barely relevant. The transitiion between taking clatsses and doing research however is huge and hard and scary and all those sorts of words. care to elaborate?  College senior considering a Theory Ph.D I just finished my Masters degree with thesis, so this is only from what I experienced.\n\nWhen you take classes, the professor determines/directs everything (material, coursework, etc). When you move to research, you become the one to do this. \n\n    YOU come up with the problem. \n    YOU research which techniques will be best. \n    YOU devise a possible solution. \n    YOU implement your solution.\n    And then YOU evaluate if it was a success or failure. \n\nSome people prefer this and really flourish. Others, not so much.\n\nTL;DR: The driving force of your experiences and learning shifts from your professors to you. care to elaborate?  College senior considering a Theory Ph.D      What stack are you pushing to on heroku - Cedar? Which framework?\n\nI ask because I'm about to set up an almost identical personal web page also using heroku. I might also use Github Pages, but I like the idea of being able to scale it up to do dynamic things if the need arises.  link appears to be dead give it another try.   Without further ado* And without further... Goodbye! It was 3am.. cut me some slack :) Oh, it's your blog?  Well your blog requires javascript to load the content.  And that is terrible. Oh, it's your blog?  Well your blog requires javascript to load the content.  And that is terrible. Yeah who the hell uses Javascript for anything these days? I write the posts via Tumblr, and the posts are retrieved via their API for display. Same deal with the Github and Twitter sidebars.  I like the github and twitter integration. Very slick. Yeah who the hell uses Javascript for anything these days? I write the posts via Tumblr, and the posts are retrieved via their API for display. Same deal with the Github and Twitter sidebars.  They have a fair point though. Of course using JS for loading stuff is nice, but it's even more nice if you provide the content statically as a fallback. It's called graceful degradation. A fair point (and put cordially, which I thank you for). If you notice this blog is very new. Perhaps I may just forward users directly to Tumblr if I detect that Javascript is disabled. Thanks for reading! A fair point (and put cordially, which I thank you for). If you notice this blog is very new. Perhaps I may just forward users directly to Tumblr if I detect that Javascript is disabled. Thanks for reading!",
    "url" : "http://young-flower-4705.herokuapp.com/post/25760728727"
  }, {
    "id" : 25,
    "title" : "When can a CS Major become valuable as an intern? I.e., What knowledge will I need to know before an employer will even consider my programming skills for a summer internship?",
    "snippet" : "I am getting pretty frustrated at the moment with my major. I'm just finishing up my second year (at community college..) and transferring to a UC to continue. \n\nCurrently the CS-related courses I have taken are: Intro to C++ (two quarters), Data Structures &amp; Algorithms in C++ (one quarter). C as a second language (one quarter), Intro to UNIX/Linux.\n\nI don't currently find any of these skills useful as no one is looking to pay me to create a program they can run in a console... \n\nI also noticed that most employers of interns are looking for either Python/Django experience or Java/Javscript experience... So what the hell am I doing with C++??\n\nFurthermore, what can I work on this Summer, and which courses should I sign up for (this coming school year) so that next Summer I will be able to actually flex my CS muscles and get a powerful internship?\n\nSome other ideas I have thought about was diving into Android App development. Where do I even start with this? I know it doesn't use C++ (I believe its Java based?), so would I need to start learning Java? What else? How long will learning this take? \n\nAlso any other idea on how I can make money with my major before I graduate?\n\n\n\nThanks for taking the time to answer my question!\n\n**EDIT: Wow! Thanks for all of the replies. I have read through every single one of them so far (~100 comments), and will keep this bookmarked for the future.**  I do a lot of work on intern hiring at my company, and the company in turn invests a lot in interns. We're too busy to treat it purely as a PR thing (which is to say, yeah we actually need you to do useful work), but we definitely wouldn't be doing it if it weren't for the fact that many of our best interns become full time employees after they graduate. \n\nOne thing I'd tell you to keep in mind regarding internships right off the bat is that there is a lot of variation out there in terms of the expectations and the care and thought that's gone into an internship position. Many companies treat it as nothing more then disposable cheap labor while others want to become an integral part of your education. Some might put you through the same rigorous process they use for full time hires while others might just look at your grades and call it a day. \n\nSo with that major caveat out of the way, I can give you some tips based on our own hiring process. We actually *do* try to put some care and thought in it and we review it on at least a yearly basis. \n\nFirst off, grades. We wish it didn't have to come to this but there's just too many applicants to sort through, so we need some really basic initial weeding-out mechanism we can farm out to HR, and that would be grades. You don't have to have fantastic grades but you shouldn't have too many bad ones. Relevant extracurricular experience *might* save you, but it's a crapshoot. You hear people saying grades don't really matter after you graduate and possibly this is true, but yeah sorry, before you graduate they do matter, at least a little.\n\nIf it doesn't look like you're struggling with grades, the very very next thing we look at is general programming ability. This is probably the most important thing you should work on. Do you want to be a programmer? Go program. Program all the time. Doesn't matter what or why, just keep working on stuff. Computer science curricula are shockingly light on actual programming assignments so this is something you *absolutely must* work on on your own if you want to get into programming. You can keep going into the academic post-graduate study track instead which actually works out really well for many people, but if programming is what you actually want to do, you just have to program. \n\nDon't sweat C++ (btw, you know who uses C++? *THE WHOLE GAMING INDUSTRY*, plenty of work there). It really *really* doesn't matter what technology you're playing with, if we see you're doing a lot of programming work we're gonna like you. Scheme and Haskell are effectively useless outside of academia but we still like seeing them on your project list. We also really love open source contributions, and those can come in many different flavors. We also really love experience with practical programming tools. How's your linux? Your regex + grep? Git?  All these things show us you're the kind of person that actually programs.\n\nAgain, this all comes down to general programing ability. I think just about any half-competent company is going to have a programming test during the interview (we certainly do). Watching you work through a simple assignment (even something like \"write a function to reverse a string\" or \"write a program that outputs the grade school multiplication table to N\") will let us know right away if you can actually program or not. Trust me, it's really easy to tell. We even find it super easy to spot if you simply happen to have memorized this particular problem ;) \n\nThat's about all we care about. Doesn't suck at school, can program. Pretty simple eh? Any specific technology we can pretty much teach you on the job. It's nice when somebody happens to be familiar with the tech stack we happen to use right off the bat, but that's just a nice little extra bonus sugar on top. Don't worry too much about languages - just get out there and start to program!\n\nedit: typos &gt;How's your linux? Your regex + grep? Git? All these things show us you're the kind of person that actually programs.\n\nI found this to be a particularly interesting question. My current linux teacher is utter crap (Community College course)... The guy essentially will just read off of the textbook and write commands on the board without any rhyme or reason. He is also very old and can't hear very well, and has bad memory -- he went over what will be on the final, twice. Basically, I have found it very hard to get excited or motivated about Linux or Unix, or even find it useful at all.\n\nI thought it was cool that I knew some commands when I opened the terminal emulator app on my Android, but that's about the extent of it's use. I know some ROM developers will put scripts into their ROM that you can execute through the TE, as well. But other than those few uses, what is knowledge of Unix useful for? I am asking due to ignorance, I truly don't know what people would use it for as from my perspective it is just a really primitive OS that other OSs are built off of.\n\nAdditionally, what would a company want me to know how to do with Linux? What kind of tasks should I absolutely have down?\n\nI have a hard time learning when I don't apply what I learn, so if I know a skill will be useful and have a way to apply it I can quickly learn it.  For one thing, thinking of Linux as a topic to study in a class room is kind of a red-flag. To know useful stuff about linux, your basically need to be using linux for actual work. This is pretty convenient as it happens, since linux has quite a few tools that are handy while developing things. Even if you mostly code in an IDE like Eclipse, there are often useful things you can do with standard linux/unix tools like grep, sed, vim, kurl, xargs etc. If the job isn't just general development and is say a sysadmin type job, they'll want to know you have all the hair-pulling experience of using linux as a primary OS, as well as a decent amount of shell-scripting and perl experience. I'm really not looking to be a sysadmin. Just want to be able to get internships/jobs in case they ask about my nix background. I'm really not looking to be a sysadmin. Just want to be able to get internships/jobs in case they ask about my nix background. &gt;How's your linux? Your regex + grep? Git? All these things show us you're the kind of person that actually programs.\n\nI found this to be a particularly interesting question. My current linux teacher is utter crap (Community College course)... The guy essentially will just read off of the textbook and write commands on the board without any rhyme or reason. He is also very old and can't hear very well, and has bad memory -- he went over what will be on the final, twice. Basically, I have found it very hard to get excited or motivated about Linux or Unix, or even find it useful at all.\n\nI thought it was cool that I knew some commands when I opened the terminal emulator app on my Android, but that's about the extent of it's use. I know some ROM developers will put scripts into their ROM that you can execute through the TE, as well. But other than those few uses, what is knowledge of Unix useful for? I am asking due to ignorance, I truly don't know what people would use it for as from my perspective it is just a really primitive OS that other OSs are built off of.\n\nAdditionally, what would a company want me to know how to do with Linux? What kind of tasks should I absolutely have down?\n\nI have a hard time learning when I don't apply what I learn, so if I know a skill will be useful and have a way to apply it I can quickly learn it.  Don't bother with a linux course, that sounds boring as hell. Install something like Ubuntu or Mint (I recommend the Debian Mint) on your computer and try using it for a while. That's how I learned. I started in order to earn nerd points and be able to make fun of Micro$oft a bit more, now it's all on my resume. Any tips on what to do with one of those distributions? Will I just find out once I install it? From what I've seen of Ubuntu its an OS just like Windows, where will my knowledge of command line commands come in? Any tips on what to do with one of those distributions? Will I just find out once I install it? From what I've seen of Ubuntu its an OS just like Windows, where will my knowledge of command line commands come in? Install Ubuntu, and have a terminal open at all times. Whenever you want to do something, like literally, *anything*, figure out how to do it with a command. Search google, use man pages, whatever. In a few weeks I guarantee you'll be very comfortable with the command line.\n\nAfter that I'd consider switching to a more \"advanced\" distro like Arch Linux or Gentoo (I use the former). That will teach you how the operating system itself works. You'll learn a whole lot about file systems, on how the kernel works, how the boot works, etc. Then start teaching yourself some of the more productive aspects of the linux shell, e.g. regular expressions, using grep, sed, cron; writing your own scripts in ruby/python/haskell/whathaveyou.\n\nFor someone who has the slightest foreknowledge of computer science, linux is really quite easy to pick up and learn. And other than games I can't really think of anything that a CS student would need that isn't available on linux.\n\nedit: grammar Any tips on what to do with one of those distributions? Will I just find out once I install it? From what I've seen of Ubuntu its an OS just like Windows, where will my knowledge of command line commands come in? Any tips on what to do with one of those distributions? Will I just find out once I install it? From what I've seen of Ubuntu its an OS just like Windows, where will my knowledge of command line commands come in? Any tips on what to do with one of those distributions? Will I just find out once I install it? From what I've seen of Ubuntu its an OS just like Windows, where will my knowledge of command line commands come in? &gt;How's your linux? Your regex + grep? Git? All these things show us you're the kind of person that actually programs.\n\nI found this to be a particularly interesting question. My current linux teacher is utter crap (Community College course)... The guy essentially will just read off of the textbook and write commands on the board without any rhyme or reason. He is also very old and can't hear very well, and has bad memory -- he went over what will be on the final, twice. Basically, I have found it very hard to get excited or motivated about Linux or Unix, or even find it useful at all.\n\nI thought it was cool that I knew some commands when I opened the terminal emulator app on my Android, but that's about the extent of it's use. I know some ROM developers will put scripts into their ROM that you can execute through the TE, as well. But other than those few uses, what is knowledge of Unix useful for? I am asking due to ignorance, I truly don't know what people would use it for as from my perspective it is just a really primitive OS that other OSs are built off of.\n\nAdditionally, what would a company want me to know how to do with Linux? What kind of tasks should I absolutely have down?\n\nI have a hard time learning when I don't apply what I learn, so if I know a skill will be useful and have a way to apply it I can quickly learn it.  I'll let someone else answer what unix is good for.  I feel like I'd write a very long answer to that. \n\n&gt; Additionally, what would a company want me to know how to do with Linux? What kind of tasks should I absolutely have down?\n\nI look at *nix as a rabbit hole filled with seemingly endless adventures and secret doors to explore.  Unless you are thinking about being a system administrator, you probably don't need to be able to recite a sed command for finding meta data in graphics files. I think you should be able to traverse a file system, make files &amp; folders, install shit, understand permissions, ssh, setup your dot files and generally not be intimidated by the command line.  It's not like you're going to have root access on any of our servers, but you'd be expected to hit the ground running if we set up a development environment for you.\n\nEDIT: Grammar. Well hopefully I am able to take a more advanced course in linux, because my current one teaches me really only the basics, and I wouldn't know what to do in a 'development environment' -- or even what one looks like. Don't take a course in linux.  Use it for your everyday OS, and when you don't like something, make it work the way you want.  That's how I learned a lot about linux.  It won't get you to sysadmin knowledge, but it will get you all the things in verginer's list, and quite a bit more. And don't forget to subscribe to /r/linux. It's nice over there and you pick up a lot of things just by reading. Well hopefully I am able to take a more advanced course in linux, because my current one teaches me really only the basics, and I wouldn't know what to do in a 'development environment' -- or even what one looks like. Well hopefully I am able to take a more advanced course in linux, because my current one teaches me really only the basics, and I wouldn't know what to do in a 'development environment' -- or even what one looks like. Well hopefully I am able to take a more advanced course in linux, because my current one teaches me really only the basics, and I wouldn't know what to do in a 'development environment' -- or even what one looks like. Well hopefully I am able to take a more advanced course in linux, because my current one teaches me really only the basics, and I wouldn't know what to do in a 'development environment' -- or even what one looks like. Well hopefully I am able to take a more advanced course in linux, because my current one teaches me really only the basics, and I wouldn't know what to do in a 'development environment' -- or even what one looks like. &gt;How's your linux? Your regex + grep? Git? All these things show us you're the kind of person that actually programs.\n\nI found this to be a particularly interesting question. My current linux teacher is utter crap (Community College course)... The guy essentially will just read off of the textbook and write commands on the board without any rhyme or reason. He is also very old and can't hear very well, and has bad memory -- he went over what will be on the final, twice. Basically, I have found it very hard to get excited or motivated about Linux or Unix, or even find it useful at all.\n\nI thought it was cool that I knew some commands when I opened the terminal emulator app on my Android, but that's about the extent of it's use. I know some ROM developers will put scripts into their ROM that you can execute through the TE, as well. But other than those few uses, what is knowledge of Unix useful for? I am asking due to ignorance, I truly don't know what people would use it for as from my perspective it is just a really primitive OS that other OSs are built off of.\n\nAdditionally, what would a company want me to know how to do with Linux? What kind of tasks should I absolutely have down?\n\nI have a hard time learning when I don't apply what I learn, so if I know a skill will be useful and have a way to apply it I can quickly learn it.  I would recommend you not start learning linux for the hell of it, but rather, start to program and then find out (usually very quickly) how linux can make your life easier, especially given your last sentence there.\n\nI think the first \"killer feature\" programmers run into in linux-land that makes them never look back at windows is probably package mangers (apt-get, rpm et al). Yes most popular things have .exe and .msi installers for everything, but as soon as you stray off the beaten path you'll run into no-fun-town. Try doing some image processing on windows and see how fun dependency installation is.\n\nBeyond that you just kind of run into useful things as you go. It does depend a lot on what type of work you do. Almost all my work is web or at least network centric. That means services and servers. That means SSH. Getting real familiar with POSIX process management. Netstat and tcpdump. Get to know the log files (/var/log/*) and config files (/etc/*). A lot of this sounds like sys admin stuff but if you're working on any kind of network service (yes this includes web development) you gotta know this stuff to get anything done.\n\nA few things that are almost certainly universally useful:\n1) Grep (or ack or whatever), and regex in general - you just gotta know how to find text. Programs are text so manipulating text is kind of a core skill...\n2) Any kind of version control (git, hg, svn, darcs, cvs, perforce, whatever). Absolutely nothing gets done in the real world without it. git and svn are the most popular ones atm, you should be competent with at least one of them. They all work fine on windows mind you, so possibly this isn't a unix specific skill, though git for example really isn't designed with windows in mind.\n &gt;How's your linux? Your regex + grep? Git? All these things show us you're the kind of person that actually programs.\n\nI found this to be a particularly interesting question. My current linux teacher is utter crap (Community College course)... The guy essentially will just read off of the textbook and write commands on the board without any rhyme or reason. He is also very old and can't hear very well, and has bad memory -- he went over what will be on the final, twice. Basically, I have found it very hard to get excited or motivated about Linux or Unix, or even find it useful at all.\n\nI thought it was cool that I knew some commands when I opened the terminal emulator app on my Android, but that's about the extent of it's use. I know some ROM developers will put scripts into their ROM that you can execute through the TE, as well. But other than those few uses, what is knowledge of Unix useful for? I am asking due to ignorance, I truly don't know what people would use it for as from my perspective it is just a really primitive OS that other OSs are built off of.\n\nAdditionally, what would a company want me to know how to do with Linux? What kind of tasks should I absolutely have down?\n\nI have a hard time learning when I don't apply what I learn, so if I know a skill will be useful and have a way to apply it I can quickly learn it.  &gt;How's your linux? Your regex + grep? Git? All these things show us you're the kind of person that actually programs.\n\nI found this to be a particularly interesting question. My current linux teacher is utter crap (Community College course)... The guy essentially will just read off of the textbook and write commands on the board without any rhyme or reason. He is also very old and can't hear very well, and has bad memory -- he went over what will be on the final, twice. Basically, I have found it very hard to get excited or motivated about Linux or Unix, or even find it useful at all.\n\nI thought it was cool that I knew some commands when I opened the terminal emulator app on my Android, but that's about the extent of it's use. I know some ROM developers will put scripts into their ROM that you can execute through the TE, as well. But other than those few uses, what is knowledge of Unix useful for? I am asking due to ignorance, I truly don't know what people would use it for as from my perspective it is just a really primitive OS that other OSs are built off of.\n\nAdditionally, what would a company want me to know how to do with Linux? What kind of tasks should I absolutely have down?\n\nI have a hard time learning when I don't apply what I learn, so if I know a skill will be useful and have a way to apply it I can quickly learn it.  I do a lot of work on intern hiring at my company, and the company in turn invests a lot in interns. We're too busy to treat it purely as a PR thing (which is to say, yeah we actually need you to do useful work), but we definitely wouldn't be doing it if it weren't for the fact that many of our best interns become full time employees after they graduate. \n\nOne thing I'd tell you to keep in mind regarding internships right off the bat is that there is a lot of variation out there in terms of the expectations and the care and thought that's gone into an internship position. Many companies treat it as nothing more then disposable cheap labor while others want to become an integral part of your education. Some might put you through the same rigorous process they use for full time hires while others might just look at your grades and call it a day. \n\nSo with that major caveat out of the way, I can give you some tips based on our own hiring process. We actually *do* try to put some care and thought in it and we review it on at least a yearly basis. \n\nFirst off, grades. We wish it didn't have to come to this but there's just too many applicants to sort through, so we need some really basic initial weeding-out mechanism we can farm out to HR, and that would be grades. You don't have to have fantastic grades but you shouldn't have too many bad ones. Relevant extracurricular experience *might* save you, but it's a crapshoot. You hear people saying grades don't really matter after you graduate and possibly this is true, but yeah sorry, before you graduate they do matter, at least a little.\n\nIf it doesn't look like you're struggling with grades, the very very next thing we look at is general programming ability. This is probably the most important thing you should work on. Do you want to be a programmer? Go program. Program all the time. Doesn't matter what or why, just keep working on stuff. Computer science curricula are shockingly light on actual programming assignments so this is something you *absolutely must* work on on your own if you want to get into programming. You can keep going into the academic post-graduate study track instead which actually works out really well for many people, but if programming is what you actually want to do, you just have to program. \n\nDon't sweat C++ (btw, you know who uses C++? *THE WHOLE GAMING INDUSTRY*, plenty of work there). It really *really* doesn't matter what technology you're playing with, if we see you're doing a lot of programming work we're gonna like you. Scheme and Haskell are effectively useless outside of academia but we still like seeing them on your project list. We also really love open source contributions, and those can come in many different flavors. We also really love experience with practical programming tools. How's your linux? Your regex + grep? Git?  All these things show us you're the kind of person that actually programs.\n\nAgain, this all comes down to general programing ability. I think just about any half-competent company is going to have a programming test during the interview (we certainly do). Watching you work through a simple assignment (even something like \"write a function to reverse a string\" or \"write a program that outputs the grade school multiplication table to N\") will let us know right away if you can actually program or not. Trust me, it's really easy to tell. We even find it super easy to spot if you simply happen to have memorized this particular problem ;) \n\nThat's about all we care about. Doesn't suck at school, can program. Pretty simple eh? Any specific technology we can pretty much teach you on the job. It's nice when somebody happens to be familiar with the tech stack we happen to use right off the bat, but that's just a nice little extra bonus sugar on top. Don't worry too much about languages - just get out there and start to program!\n\nedit: typos A bit late and slightly off topic, but does this mean that most of my programming time being spent in haskell/scheme and other languages that have little to no industry use just because I find them fun actually help my chances at an internship because it shows that I like programming?  One thing that's worth noting some companies (especially top-teir big name ones) aren't necessarily looking for **any** skills whatsoever.\n\nMany internship programs aren't about actually getting $$ worth of work out of an intern, rather they're investing in you, betting that you'll one day be a top worker. Either you work for them, and they gained a valuable employee, or you work for one of their competitors, but you're left saying \"Wow, company X is a great company\", it's great industry PR.\n\nThat being said, learning languages and working on projects is probably the best way to increase your value and get a job at this point. I just wanted to share a little insight I've gained over the past years. Well, that's a little bit of an exaggeration. Most companies will want you to at least know how to program. You would be surprised.  I interviewed for a number of internships, only one actually had me do any coding in the interview.  And it was a fizz-bang style problem. One thing that's worth noting some companies (especially top-teir big name ones) aren't necessarily looking for **any** skills whatsoever.\n\nMany internship programs aren't about actually getting $$ worth of work out of an intern, rather they're investing in you, betting that you'll one day be a top worker. Either you work for them, and they gained a valuable employee, or you work for one of their competitors, but you're left saying \"Wow, company X is a great company\", it's great industry PR.\n\nThat being said, learning languages and working on projects is probably the best way to increase your value and get a job at this point. I just wanted to share a little insight I've gained over the past years.  Most of the successful software developers I know learned every technology they use at their job on their own time.  University is for teaching you underlying concepts, like data structures and algorithm analysis.\n\nSee job openings for Python?  Get cracking on some personal project in Python.  It'll double as a code portfolio when you're done using it to teach yourself.\n\n(Note: Despite the username, I do have an undergrad degree in computer science)  The best advice I can give is to tell you to pick a project and then do it. Make a game, robot, home surveillance system, image editing program, or something like that. You'll learn a lot and have something to show prospective employers when you're done.\n\nIf you want to learn how to develop Android apps, I would recommend this book, http://commonsware.com/Android/, and stack overflow. I'm with Sklargbar. I was in your same position not too long ago and started developing Windows Phone apps because they give away all of the development tools and publishing rights to students for free. I was able to learn C# and XAML as well as get a much better idea of what really goes into a software project. Knowing C/C++, C# will have a relatively small learning curve. Plus, it will help you developing for Windows 8 which is an exciting intangible for me. Even though I already had a better WP7 device I was able to win one for a simple competition and am happy to pull out my phone and show off my apps so what I'm saying is if you want to procure a WP7 device there's a chance you could win one. Disclaimer: Microsoft announced WP7 devices won't run WP8 so I wouldn't recommend forking out money for a WP7 device.  While I love the WP UI, and think it is the best out right now, it is just not being used. Android, even with more market share, is still behind Apple in app development. I think I will stick with Android as my mobile app deving OS of choice. The best advice I can give is to tell you to pick a project and then do it. Make a game, robot, home surveillance system, image editing program, or something like that. You'll learn a lot and have something to show prospective employers when you're done.\n\nIf you want to learn how to develop Android apps, I would recommend this book, http://commonsware.com/Android/, and stack overflow.  ... I never thought C is not useful. You just blown my mind. I am paid $30 per hour writing C code as an intern. All my previous internships were all on C code base.\n\nOn the other hand I know why you are so anxious. I mean, the whole point of internships is to let you learn how to make money from your major. I never did this, but I recommend you that if you cannot find a posting asking for your skill set, go talk to the hiring manager of a company that you believe may want your skills. Well, could you give me an example of what you would code as an intern with C? \n\nI think the problem lies in that all I see is silly HW assignments that ask me to create a recipe-maker, or a slot-machine... \n\nI basically just don't know what I can actually offer a company even with my knowledge of C/C++. Mostly system software, device driver and device library. But my opinion after 4 internships is that you cannot learn anything directly useful in school, but you cannot learn anything directly useful without your knowledge learned in school. I had to code a vending machine simulator in C++ in my second year software enginnering class. The closest thing to real life applicaion was a fourth year project that programs a RTOS on an ARM board. Nothing is exactly like what I do as an intern.\n\nDon't worry too much for now. Keep learning what your prof thinks is important and do it well. Once you had an internship, which is for sure not easy to get, you will learn what you should focus on at school.\n\nI do not recommend QA, if you have to take one don't do it again. Well, could you give me an example of what you would code as an intern with C? \n\nI think the problem lies in that all I see is silly HW assignments that ask me to create a recipe-maker, or a slot-machine... \n\nI basically just don't know what I can actually offer a company even with my knowledge of C/C++. If the company writes their software in C/C++, then you as an intern with C/C++ experience are useful to them. What that software is could be anything, tons of stuff is written in C/C++. It's not like people think \"oh I need to write accounting software, I should find a Java developer, but if I need an inventory control system I should find a C++ developer\". C/C++ is more popular for games, OS, low-level network and DB architecture etc, mostly for predictability and performance reasons. It's also harder nowadays to find an experienced C/C++ developer than a Java one, so don't abandon it too easily. Java will probably get you an entry-level job faster, but C/C++ are very much worth knowing (and C and C++ are not really the same thing btw). The main difference between the two that I noticed is some slight syntax differences, and the lack of classes. \n\nIs there a reason to use C over C++?  C++ is not C with classes. That was true ages ago. C is simpler and easier to learn. It also maps to assembly more easily and it's easier to determine what the code will actually be doing. Many programmers use C because they say it's more portable, but when used correctly (correctly being the keyword) , C++ is just as portable. \n\nC++ and C are very different languages now. Knowing one does not mean you know the other. The idioms and methods for performing many tasks are completely different. Similarly, modern C++ (C++11) looks very different from C++ written in the past (unfortunately, however, there are still many people who still write C++ as C with classes). C has also recently gotten a new standard, I believe, with a number of new facilities (I don't use it that often). \n\nC is often used for embedded stuff, Linux development, and libraries that interop between different languages, among other things. It's pretty much the gold standard for that last item particularly. C++ is good for a similarly large swath of uses. It's good for larger projects (like games and other large applications), because of its built-in facilities for handling large code bases.  The main difference between the two that I noticed is some slight syntax differences, and the lack of classes. \n\nIs there a reason to use C over C++?  Well, could you give me an example of what you would code as an intern with C? \n\nI think the problem lies in that all I see is silly HW assignments that ask me to create a recipe-maker, or a slot-machine... \n\nI basically just don't know what I can actually offer a company even with my knowledge of C/C++. Well, could you give me an example of what you would code as an intern with C? \n\nI think the problem lies in that all I see is silly HW assignments that ask me to create a recipe-maker, or a slot-machine... \n\nI basically just don't know what I can actually offer a company even with my knowledge of C/C++.  &gt;I don't currently find any of these skills useful as no one is looking to pay me to create a program they can run in a console...\n\nPlenty of people do. What, do you think websites run with a GUI that you can click around in? I guess the web page can be considered a GUI, but on the server side it all runs on the command line.\n\n&gt;I also noticed that most employers of interns are looking for either Python/Django experience or Java/Javscript experience... So what the hell am I doing with C++??\n\nLearning basic programming skills and basic CS concepts (like data structures and algorithms). You need to know these things now, because in future classes, you will be expected to pick up languages on your own and know when to use these data structures.\n\nWhat UC are you transferring to? [deleted] Look, are you aiming for web/app development, or lower level stuff? Python/Javascript is a completely different target industry than C++, which is also slightly different from Java.\n\nBrush up on the background of the programming industry... While it's always good to learn programming fundamentals, you seem to be taking courses just to take courses, as opposed to appreciating their value and understanding how they can be applied. While I agree with your sentiment, I don't believe there is much in the way of differentiation in college courses. Any one university usually standardizes around a few languages and what you're aiming for after college has very little relevance to what they pick.\n\nIn fact, I imagine most people are lucky if they go to a school where they learn anything other than Java. Look, are you aiming for web/app development, or lower level stuff? Python/Javascript is a completely different target industry than C++, which is also slightly different from Java.\n\nBrush up on the background of the programming industry... While it's always good to learn programming fundamentals, you seem to be taking courses just to take courses, as opposed to appreciating their value and understanding how they can be applied. Actually, I'm taking courses based on a pre-determined plan that I was required to follow in order to transfer from my CC into a UC.\n\nOnce I get into the UC system (this fall), I will have a lot more freedom to focus on certain aspects of CS.\n\nI will be quite frank and say that the main reason I'm getting a CS degree is so that I have a skill that I could use in the business world. I actually see myself as an entrepreneur and hope to start my own businesses in due time, but I know that in order to do so, I need to pick up some useful skills. Since I love computers and my Android phone -- and I really did not like chem, bio, or other sciences, I chose CS.\n\nHaving said that, I think that the most exciting CS application I would like to pursue is developing apps, because as an entrepreneur I come up with dozens of ideas a month, and eventually when I decide to go with one I would be able to start working on it myself without need for other partners or funding (at least temporarily). \n\nSo I think that the mobile app development field is what I would like to focus on. \n\nHowever, I think its important to point out my ignorance of the world of CS, I may know more than the average layperson about it, but I still understand that I know very little -- so there could be a field of CS that I am unaware of, or have false preconceived notions about, that might actually be a great fit.         I would say, pick up python (taking a [Udacity](http://www.udacity.com/) class is a good start), and then hack around a bit. Get some practice thinking of projects, finding appropriate tools(there is a python library for everything), and developing a finished product. Start a dev blog and learn to use git - keep a nice log of your work and make your source publicly available. I actually signed up for the discrete math class they have starting really soon. I didn't see a python class on there. Maybe they have it at a different time? I actually signed up for the discrete math class they have starting really soon. I didn't see a python class on there. Maybe they have it at a different time? I actually signed up for the discrete math class they have starting really soon. I didn't see a python class on there. Maybe they have it at a different time?  You can use C and C++ to write Android applications with the NDK.\n\nKnowing C and OO, you can learn in less than one week Objective-C http://developer.apple.com/library/mac/#documentation/Cocoa/Conceptual/ObjectiveC/Introduction/introObjectiveC.html  and then learn about iOS or MacOSX and write iOS or MacOSX applications. http://www.youtube.com/watch?v=xQzLHgls63E\nAt the end of the holidays, you may have an app selling on the AppStore and get more revenue than selling your time to an \"employer\".\n I ideally want to program apps. I think that is the most rewarding way to use my skills, because I can program apps that could actually be useful to me in my day-to-day life -- and make me boatloads of cash :)\n\nI'm an Android-head so I don't really see myself programming for iOS, especially since I don't have an iPhone to dev on... Though, I guess I could use my parents' iPad as a guinea pig. I ideally want to program apps. I think that is the most rewarding way to use my skills, because I can program apps that could actually be useful to me in my day-to-day life -- and make me boatloads of cash :)\n\nI'm an Android-head so I don't really see myself programming for iOS, especially since I don't have an iPhone to dev on... Though, I guess I could use my parents' iPad as a guinea pig. I ideally want to program apps. I think that is the most rewarding way to use my skills, because I can program apps that could actually be useful to me in my day-to-day life -- and make me boatloads of cash :)\n\nI'm an Android-head so I don't really see myself programming for iOS, especially since I don't have an iPhone to dev on... Though, I guess I could use my parents' iPad as a guinea pig. Just for the record, you're probably never going to make boatloads of cash from an Android application.   I didn't know where to start with any GUI applications when I started my first internship (Sophomore year)... internship tripled my knowledge in software dev. How did you get an internship during your sophomore year? Was that after only one year of courses? What courses did you take? What was your internship for?  Computer engineering student (3rd year undergrad starts in the fall) here, my current co-op pays me pretty well to create \"console\" programs/scripts. But I do know java/php/python/bash scripts well enough to be able to make what I need to, as well. What courses have you taken that became useful for your co-op (is that an internship?)    Learn Python and Java on your own! Java especially is VERY easy to learn if you already know C++. I had to do it when I switched from civil engineering to CS, since they only taught C++ to engineering majors. You should definitely take a class in databases (normalization, SQL and all that jazz), as it will help you immensely. You could also take a class on HTML+Javascript, although it's so easy a class will be probably overkill. I took one that was actually intended for Graphic Design majors, it was probably the easiest A I have ever gotten. You should also learn some systems engineering such as design patterns, unit testing, OOP, etc.\n\nPython is also very easy to learn if you already know how to program. Just quickly read a tutorial, you don't need to read it all since you already know the basic concepts (looping, conditions, boolean expressions) which are common to all imperative programming languages.\n\nInstall Linux on your own computer, launch an Amazon EC2 instance and play around with it. Install Apache on it and make it serve some static pages. Learning a bit about DNS and IP in the process won't hurt either. Also, install git and learn how to use it. It will make teamwork (or even solo work) much easier and nicer. &gt; Install Linux on your own computer, launch an Amazon EC2 instance and play around with it. Install Apache on it and make it serve some static pages. Learning a bit about DNS and IP in the process won't hurt either. Also, install git and learn how to use it. It will make teamwork (or even solo work) much easier and nicer.\n\nAre there some guides online that you could recommend that would help me through this process? Have you installed Linux before?  No, I have not. No, I have not. Download Ubuntu and install it! If your computer is running Windows 7 or Vista already you can shrink its partition without much trouble using the Windows' built-in Disk Manager. Once you have done that you can burn a CD using the ISO image provided by Ubuntu and boot that. I'd recommend getting to know Linux on your own machine before starting to learn how to use the Amazon EC2.      I would learn some web development stuff. It seems like there are more web dev jobs out there than anything else.\n\nEDIT: Also, try to get your hands on the book Cracking the Coding Interview. It's really awesome, and gives all kinds of general advice. I'm working my way through it now, very slowly :).  Thanks for the book idea, I'll check it out!         &gt; I don't currently find any of these skills useful as no one is looking to pay me to create a program they can run in a console...\n\nI have exactly that sort of job, and make six figures.  As an intern...? As an intern...?  &gt; no one is looking to pay me to create a program they can run in a console\n\nWhere I work, almost everything we code and use are console programs. It's just too productive and easy not to do it this way.\n\n&gt; I also noticed that most employers of interns are looking for either Python/Django experience or Java/Javscript experience... So what the hell am I doing with C++??\n\nYou must be looking at a very specific set of potential employers. C++ is very widely used in industry.   Trust me, you dont want to write the BS business apps that are written in java, c#, etc. You *want* to program in C. If you want to break out of the console teach yourself the win32 api and/or MFC. Learn how to write socket code, learn how to use threads correctly, learn how to write code in *nix.\n\nEmbedded software engineers live in linux and C programming. Buy a raspberry pi and get familiar with logging into a machine over a serial port and using uboot and compiling linux kernels and loading filesystems onto a flash card or nand memory.\n\nJavascript is good if you want to become a web monkey, java is good if you want to write tax / banking software for a living. Trust me, you dont want to write the BS business apps that are written in java, c#, etc. You *want* to program in C. If you want to break out of the console teach yourself the win32 api and/or MFC. Learn how to write socket code, learn how to use threads correctly, learn how to write code in *nix.\n\nEmbedded software engineers live in linux and C programming. Buy a raspberry pi and get familiar with logging into a machine over a serial port and using uboot and compiling linux kernels and loading filesystems onto a flash card or nand memory.\n\nJavascript is good if you want to become a web monkey, java is good if you want to write tax / banking software for a living.    &gt; Python/Django experience or Java/Javscript experience\n\nWut Not sure what your question is about. But I have personal connections at a highly-funded web startup, but they are looking for interns who know python, so I was pretty bummed that I'm spending all this time on languages that companies aren't looking to hire me for. Not sure what your question is about. But I have personal connections at a highly-funded web startup, but they are looking for interns who know python, so I was pretty bummed that I'm spending all this time on languages that companies aren't looking to hire me for. I was commenting that Java has absolutely nothing to do with Javascript. I mean they literally have nothing whatsoever in common. They do have some common keywords and syntax. Not sure what your question is about. But I have personal connections at a highly-funded web startup, but they are looking for interns who know python, so I was pretty bummed that I'm spending all this time on languages that companies aren't looking to hire me for. ",
    "url" : "http://www.reddit.com/r/compsci/comments/vic2m/when_can_a_cs_major_become_valuable_as_an_intern/"
  }, {
    "id" : 26,
    "title" : "GPU Resource Allocation",
    "snippet" : "How is GPU time and memory allocated?  Is it similar to how CPU time and RAM is allocated?\n\nEDIT: Spelling  ",
    "url" : "http://www.reddit.com/r/compsci/comments/vi7d6/gpu_resource_allocation/"
  }, {
    "id" : 27,
    "title" : "Turing Centenary Conference livestreamed! 20 Turing Award winners give lectures.",
    "snippet" : "  Is there any way to see the lectures that were already made? Or is it just live streaming?",
    "url" : "http://www.turing100.manchester.ac.uk/index.php/conference/conference/livestream"
  }, {
    "id" : 28,
    "title" : "Turing Machine made out of Lego",
    "snippet" : " ",
    "url" : "http://www.dailymotion.com/video/xrmfie_the-turing-machine-comes-true_tech#from=embediframe"
  }, {
    "id" : 29,
    "title" : "What's the difference between Human-Computer Interaction and UX design?",
    "snippet" : "From what I understand, they are very similar topics.    THERE IS NO SUCH THING AS UX DESIGN! period.\nUX means user experience. You can, however, design a User Interface which is part of User eXperience.\n\n/message-of-hate I am pretty sure UX design is a thing. Google UX design and you'll see a bunch of books come up. THERE IS NO SUCH THING AS UX DESIGN! period.\nUX means user experience. You can, however, design a User Interface which is part of User eXperience.\n\n/message-of-hate",
    "url" : "http://www.reddit.com/r/compsci/comments/vhfjq/whats_the_difference_between_humancomputer/"
  }, {
    "id" : 30,
    "title" : "The CAP Theorem Twelve Years Later: How the \"Rules\" Have Changed",
    "url" : "http://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed"
  }, {
    "id" : 31,
    "title" : "Happy 100th Birthday, Alan Turing!",
    "snippet" : "  If you go to http://www.google.com.au, there's a great Google doodle for Turing's 100th. It took me a little while to figure out what the game was, but once  you do, it's quite fun. Only if they had more levels. I would play this as an app on Android/iOS, for sure. Someone should make this happen! I don't have a iOS developer account, since it's $99 a year. Don't know iOS app dev either. \n\nI'm going to make a Turing machine, in javascript probably, this weekend. I'll post it to /r/compsci if I end up finishing it. If you want, I'll also send you the link. That would be fantastic, thank you! I'm not very familiar with Javascript but I've been looking at it a little more and think it would be worth learning. If you go to http://www.google.com.au, there's a great Google doodle for Turing's 100th.   Looks like you're a day early...   &gt;When his body was discovered an apple lay half-eaten beside his bed, and although the apple was not tested for cyanide,[78] it is speculated that this was the means by which a fatal dose was consumed.\n\nHoly shit! Is this where Apple's logo comes from? It's widely believed, but apparently not the case.\n\nI found [this nice summary](http://www.turing.org.uk/turing/scrapbook/wondrous.html) (at the bottom of that page):\n\n&gt; It is often stated as a matter of established fact that the Apple Macintosh logo was drawn from the story of Alan Turing's death. But according to Apple Inc., whose name was chosen in 1976, the logo was designed in 1977 to allude to Newton but also to 'the symbol of lust and knowledge, the colors of the rainbow in the wrong order.. hope and anarchy.' See [this full discussion](http://www.greggore.com/dln021203.htm), and [the designer's account](http://creativebits.org/interview/interview_rob_janoff_designer_apple_logo). It should be remembered that in 1977 Alan Turing's story was very little known. Even if it had been known, suicide would hardly have been a happy allusion for the young Apple company to evoke. \n  Isn't he dead? Isn't he dead? No. Actually, he is. He committed suicide in 1954 after being arrested &amp; subjected to horrible \"experiments\" for being gay.   Worth checking out ACM's celebration of the occasion: \n\nhttp://turing100.acm.org/ This is excellent. I've been watching since last night. I suspect that never again will this many Turing Award winners get together for such discussion. Really a treat. Thanks. You should make a new thread to share this with others. I'm surprised reddit and hackernews aren't all over this. These are the fucking Turing Award winners here, man. Glad you enjoyed it! I have to admit I haven't watched it yet (heard great things), but hope to soon.  Since you did watch, why don't you submit the link? :)      ",
    "url" : "http://en.wikipedia.org/wiki/Alan_Turing"
  }, {
    "id" : 32,
    "title" : "Filtering: a method for solving graph problems in MapReduce",
    "url" : "http://research.google.com/pubs/pub37240.html"
  }, {
    "id" : 33,
    "title" : "Professional Engineer title as a Computer Scientist",
    "snippet" : "I'm trying to register to take the Fundamentals of Engineering exam this Fall (need to register very soon), and I'm wondering something.. Should I even be doing this as a Computer Science major? Does the FE exam and the PE title only pertain to \"XYZ Engineers\"?  I think something like this might make sense for someone who develops safety-critical software, but to my knowledge it doesn't exist.      ",
    "url" : "http://www.reddit.com/r/compsci/comments/vg1g5/professional_engineer_title_as_a_computer/"
  }, {
    "id" : 34,
    "title" : "r/CompSci logo",
    "snippet" : "Shouldn't the reddit alien be saying \"P=NP?\", since this is still an unresolved problem?     That's what it is saying.",
    "url" : "http://www.reddit.com/r/compsci/comments/vg2iw/rcompsci_logo/"
  }, {
    "id" : 35,
    "title" : "Algorithm for superimposition of 3d points",
    "snippet" : "Hey compsci,\n\nI need to superimpose two groups of points on top of each other; i.e. find rotation and translation matrices to minimize the RMSD (root mean square deviation) between their coordinates.\n\nI currently use [Kabsch algorithm](http://en.wikipedia.org/wiki/Kabsch_algorithm), which is not very useful for many of the cases I need to deal with. Kabsch requires equal number of points in both data sets, plus, it needs to know which point is going to be aligned with which one beforehand. For my case, the number of points will be different, and I don't care which point corresponds to which in the final alignment, as long as the RMSD is minimized.\n\nI know some algorithms that deal with different number of points, however they all are protein-based, that is, they try to align the backbones together (some continuous segment is aligned with another continuous segment etc), which is not useful for floating points without connections. (OK, to be clear, some points are connected; but there are points without any connections which I don't want to ignore during superimposition.)\n\nOnly algorithm that I found is [DIP-OVL](http://www.bioinformatics.org/strap/superposition.html), found in STRAP software module (open source). I tried the code, but the behaviour seems erratic; sometimes it finds good alignments, sometimes it can't align a set of few points with itself after a simple X translation.\n \nAnyone know of an algorithm that deals with such limitations? I'll have at most ~10^2 to ~10^3 points if the performance is an issue.\n\nThanks!   You could look into ICP (http://en.wikipedia.org/wiki/Iterative_closest_point).\n\nI should know this, but my memory is a bit hazy. You should be able to first get the COG and the scaling out of the way (as they are invariant to the rotation), then focus on the rotation. \n\nStiching the boundaries of overlapping 3D-scans needs this all the time (registration), maybe you can google something in that direction. I remember all sorts of nasty stuff with quaternions, but it's been too long. Thanks! That looks like it can work. Also, new keywords for research always welcome.   I have a friend doing something with the intersection of two proteins that sounds like this. Possibly similar?",
    "url" : "http://www.reddit.com/r/compsci/comments/vg1mp/algorithm_for_superimposition_of_3d_points/"
  }, {
    "id" : 36,
    "title" : "What theoretical CS videos should everybody watch?",
    "snippet" : "  [Donald Duck in Mathemagic Land](http://www.youtube.com/watch?v=YRD4gb0p5RM). [Donald Duck in Mathemagic Land](http://www.youtube.com/watch?v=YRD4gb0p5RM).   Gerald Sussman discussing how we're going down the wrong path when it comes to computing things, and that nature is much more suited to dynamic environments.  \n\nhttp://www.infoq.com/presentations/We-Really-Dont-Know-How-To-Compute\n\nOn the more practical side, every compsci student should watch the SICP lectures.     [Alan Kay: Doing With Images Makes Symbols](http://video.google.com/videoplay?docid=-533537336174204822)\n\n[Alan Kay](http://en.wikipedia.org/wiki/Alan_Kay) talking about where the GUI came from, among other things. Also includes bits from [The Mother of All Demos](http://en.wikipedia.org/wiki/The_Mother_of_All_Demos) and [Sketchpad](http://en.wikipedia.org/wiki/Sketchpad).\n\nI think this video is really inspiring \"I once asked Ivan Sutherland how could you possibly have done the first interactive graphics program, the first non-procedural programming language, the first object orientated software system, all in one year. He said 'well I didn't know it was hard'\" I wouldn't call these \"theoretical computer science\" though - just computing. Sure it's not formal languages and complexity theory but Douglas Engelbart, Alan Kay, and Ivan Sutherland all received Turing Awards (the \"highest distinction in Computer science\") for the work that's talked about in this video.    http://www.youtube.com/watch?v=ywWBy6J5gz8\n\nQuick-sort with Hungarian (Küküllőmenti legényes) folk dance http://www.youtube.com/watch?v=ywWBy6J5gz8\n\nQuick-sort with Hungarian (Küküllőmenti legényes) folk dance I would've been upset if this wasn't here.",
    "url" : "http://cstheory.stackexchange.com/q/1198/1761"
  }, {
    "id" : 37,
    "title" : "OpenFlow: A Radical New Idea in Networking",
    "snippet" : "  I personally find this very scary. I could see how ISPs would implement this because of its advantages in performance and at the same time it could be used to have more control over the internet.  I find their arguments to be very weak.  Their core argument is that it is hard to change router algorithms without changing hardware, so we should switch to this totally new centralized system where one computer know the entire network.\n\nWhy not promote open routers based on upgradeable hardware if that's what they really want.\n\nAlso, making a centralized network with the ability to inspect and rewrite packets and calling it \"OpenFlow\" is a just a little bit ridiculous.\n\n",
    "url" : "http://queue.acm.org/detail.cfm?id=2305856"
  }, {
    "id" : 38,
    "title" : "Graph theory algorithms could improve hurricane forecasts",
    "url" : "http://ascr-discovery.science.doe.gov/universities/hurricane1.shtml"
  }, {
    "id" : 39,
    "title" : "Duolingo opens to the public after 125,000 users translated 75 million sentences during beta",
    "snippet" : "  Wow, this product is brilliant.  One of those ideas that instantly makes me think \"Why didn't I think of that?\"\n\nCrowdsourcing / wiki style content is obviously not new, but pairing it with learning a new language, something that many people desire to do, is really just an inspired idea.  Kudos to the founders.  Subscribe to /r/duolingo for an active community!   what about this is relevant to computer science? I mean, it's pretty cool, but I don't think this is the appropriate subreddit. [Luis Von Ahn](http://en.wikipedia.org/wiki/Luis_von_Ahn) is a computer scientist doing work in human computation.  See an exceptional talk [here](http://www.youtube.com/watch?v=dtFroEJN1nI) where he talks about doing some hard AI tasks, like OCR, image recognition and language parsing by piggy backing off of humans playing games.  He's the inventor (I think) of re-captcha and the site called [GWAP](http://www.gwap.com/gwap/) (Games With A Purpose).  GWAP frames hard AI tasks as games, essentially creating a data set for supervised learning tasks by asking people to identify features, but of course, in the framework of a fun game.\n\nI haven't looked into Duolingo much, but since he's using real-world web pages and I believe the same verification procedure (comparing answers to the database of answers received) I imagine this is much in the same vein.\n\nUntil we have hard AI, or, at least, as a way to generate data to train hard AI, this is (imo) a really fruitful way to go.\n\nEDIT: Lusi -&gt; Luis what about this is relevant to computer science? I mean, it's pretty cool, but I don't think this is the appropriate subreddit. Do you really want to narrow the scope of computer science to keep out fascinating work like this?\n\nHuman computation should certainly be considered part of computer science. And it is if you look at published papers on it:\n\nhttp://scholar.google.com/scholar?q=author%3Avon+ahn Then post a link to the paper, instead of a press release on a crappy tech news site. what about this is relevant to computer science? I mean, it's pretty cool, but I don't think this is the appropriate subreddit. TIL Web applications and HCI are not computer science relevant.\n\nAlso why only 4 languages. I wanted to learn Norwegian..  From playing around with it a bit, there are parts of the software that are clearly hand tailored to each language. From a quick look there is only French,German and Spanish currently available. I guess I'll brush up my German a bit, just for the novelty factor. TIL Web applications and HCI are not computer science relevant.\n\nAlso why only 4 languages. I wanted to learn Norwegian..  TIL Web applications and HCI are not computer science relevant.\n\nAlso why only 4 languages. I wanted to learn Norwegian..  what about this is relevant to computer science? I mean, it's pretty cool, but I don't think this is the appropriate subreddit. ",
    "url" : "http://www.webpronews.com/duolingo-goes-public-after-125000-users-translated-75-million-sentences-during-beta-2012-06"
  }, {
    "id" : 40,
    "title" : "The Rich Legacy of Alan Turing",
    "url" : "http://www.wired.com/wiredscience/2012/06/alan-turing-legacy/"
  }, {
    "id" : 41,
    "title" : "We're thinking of trying to organise a CS related field trip (1-3 days) for second year students. Is there anywhere in Europe that would be suitable?",
    "snippet" : "  Does the UK count as \"in Europe\"? If so [Bletchley Park](http://www.bletchleypark.org.uk/content/museum1.rhtm) would be the perfect choice. Alan Turing worked there during the war, using some of the first computers to break Nazi codes during the war. \n\nTheir work there was invaluable, and the museum is fantastic.  Does the UK count as \"in Europe\"? If so [Bletchley Park](http://www.bletchleypark.org.uk/content/museum1.rhtm) would be the perfect choice. Alan Turing worked there during the war, using some of the first computers to break Nazi codes during the war. \n\nTheir work there was invaluable, and the museum is fantastic.  Does the UK count as \"in Europe\"? If so [Bletchley Park](http://www.bletchleypark.org.uk/content/museum1.rhtm) would be the perfect choice. Alan Turing worked there during the war, using some of the first computers to break Nazi codes during the war. \n\nTheir work there was invaluable, and the museum is fantastic.   your entire trip would be in the UK, since that is really the birthplace of CS   Being from the US and having absolutely no good ideas to give you... CERN!\n\nhttp://en.wikipedia.org/wiki/CERN#Computer_science I've been to CERN (and was computer scientist but it was a physics trip) I've been to CERN (and was computer scientist but it was a physics trip) interesting.. my school also did a physics trip to CERN.. Interesting... what country are you from?",
    "url" : "http://www.reddit.com/r/compsci/comments/vat55/were_thinking_of_trying_to_organise_a_cs_related/"
  }, {
    "id" : 42,
    "title" : "C++ guide for experienced Programmer",
    "snippet" : "I can program in a few languages well (erlang, java, scheme, perl...) but I need to learn C++ for the summer because I am taking a course that has that as its requirement in the fall. Does anyone know a guide where I can get started? Thanks   I don't know of any free, online guides, but here are some good resources:\n\n[http://www.cplusplus.com/](http://www.cplusplus.com/) Basically the documentation for the entire language.  \n[C++ FAQ Lite](http://www.parashift.com/c++-faq-lite/). Tons of useful information about random different features of the language.\n\nYou'll probably want to learn these topics in roughly this order:  \n\n* The basics of the C language. This is the core language that C++ was built upon. You don't have to learn C in detail, but you should be familiar with writing functions, using pointers, arrays, and other basics.  \n\n* OOP in C++. How to create a class, how to use inheritance, what the virtual keyword does, etc. I'm assuming you already know the basics of OOP, but C++ is different than the languages you listed for a couple reasons. It allows the creation of objects on the stack, and it allows both early and late binding (by using the virtual keyword).  \n\n* The C++ standard library &amp; STL. Especially the libraries for I/O and data structures. At this point you'll have to learn a little bit about templates but for now you can treat them as you would Java generics (even though they work completely differently).  \n\n* const. This is a really tricky keyword in C++, but it is very important. If you don't understand it, you're going to have a heck of a time understanding declarations like:  \n\n    const char * f(const char * const s) const;\n I don't know of any free, online guides, but here are some good resources:\n\n[http://www.cplusplus.com/](http://www.cplusplus.com/) Basically the documentation for the entire language.  \n[C++ FAQ Lite](http://www.parashift.com/c++-faq-lite/). Tons of useful information about random different features of the language.\n\nYou'll probably want to learn these topics in roughly this order:  \n\n* The basics of the C language. This is the core language that C++ was built upon. You don't have to learn C in detail, but you should be familiar with writing functions, using pointers, arrays, and other basics.  \n\n* OOP in C++. How to create a class, how to use inheritance, what the virtual keyword does, etc. I'm assuming you already know the basics of OOP, but C++ is different than the languages you listed for a couple reasons. It allows the creation of objects on the stack, and it allows both early and late binding (by using the virtual keyword).  \n\n* The C++ standard library &amp; STL. Especially the libraries for I/O and data structures. At this point you'll have to learn a little bit about templates but for now you can treat them as you would Java generics (even though they work completely differently).  \n\n* const. This is a really tricky keyword in C++, but it is very important. If you don't understand it, you're going to have a heck of a time understanding declarations like:  \n\n    const char * f(const char * const s) const;\n wtf is that last abomination?\n&gt;const char * f(const char * const s) const;\n\n\"const char * const s\" would be a constant pointer to a constant character. This pointer is named s. You are then using this to construct a char * pointer named f? What does that even mean?\n\nAlso you seem like a decent person to ask. If I have a vector of vectors, is there anyway to allocate my space dynamically but still have array2d[i][j] work? The problem is if I make my 2nd level vectors using the new keyword and use these to fill the top level vector my array2d[i] elements are pointers so that to get to my actual bottom level members I have to use (*array2d[i])[j]. I was guessing you could use references for this but after a small amount of research I'm not sure if you can get references to dynamic memory. -\n\nEDIT: ooh perhaps it could be done using:\n\n* vector&lt;T&gt; &amp;row = *(new vector&lt;T&gt;)\n\ntoo lazy to try it now, and I don't want to delete the question.  &gt; \"const char * const s\" would be a constant pointer to a constant character. This pointer is named s. You are then using this to construct a char * pointer named f? What does that even mean?\n\nSo it's actually a signature for a method named f() that takes one argument named c. The type of c you described correctly, it's a character pointer. The 'const' after the '*' means that what it's pointing at can't be modified, and the const before the '*' means the pointer itself can't be modified. The function f() returns a \"const char *\" which is just a const pointer to a character (although the char it's pointing at is not const in this case). The \"const\" after the \")\" means that this method won't modify the object instance it's being called by; it's useful if you have a \"const\" reference to an object, you can still call methods declared \"const.\"\n\n[const correctness](http://www.parashift.com/c++-faq-lite/const-correctness.html) is probably the most useful feature of C++ that most programmers don't understand. They almost never touch the topic in schools just because it doesn't really fit any broad computer science topic, it's strictly an implementation detail.\n\nI'll get back to you on the second question, I don't have time to play around with the code right now.\n\nK just thought about it. I'm pretty sure what you want is just a vector&lt;vector&lt;T&gt; &gt;. Vectors always allocate their space dynamically, even if they're on the stack (ie the vector itself is on the stack but the elements are on the heap).\n\nI just ran this and it works\n\n    vector&lt;vector&lt;int&gt; &gt; v;\n    v.push_back(vector&lt;int&gt;()); //The \"vector&lt;int&gt;\" being created ends up on the heap.\n    v[0].push_back(10);\n    cout &lt;&lt; v[0][0] &lt;&lt; endl; //prints 10\n\n The const thing makes sense. I was misreading it as the declaration and calling an object constructor for char... somehow. It isn't quite my fault though because it's famously difficult to tell apart C++ function declarations and object constructions. \n\n&gt;K just thought about it. I'm pretty sure what you want is just a vector&lt;vector&lt;T&gt; &gt;. Vectors always allocate their space dynamically, even if they're on the stack (ie the vector itself is on the stack but the elements are on the heap).\n\nYeah, a vector of vectors on the stack isn't too terrible. However if I you've got a 2d array with a lot of rows wouldn't you end up consuming a lot of space on the stack? You'd have a vector on the stack for every row of your 2d array. The way I was suggesting would work it seems if the second level vectors were wanted on the heap. Vectors are stored on the heap internally. If they weren't, they wouldn't be dynamically resizable. oooooooooooooh. Now it makes sense. So after you did. After \n\n* vector&lt; vector&lt;int&gt; &gt; v;\n\n* vector&lt;int&gt; vectorRow\n\n* v.push_back(vectorRow) \n\n\nyou would have a copy of vectorRow on the stack and in the heap. And if this was in a function that then finished your vectorRow would stick around on the heap while the one on the stack would go out of scope.  Think of std::vector being implemented something like this:\n\n    template &lt;typename T&gt;\n    class vector {\n        T* elts;\n        size_t n_elts;\n        void push_back(const T&amp; elt) {\n             n_elts++;\n             elts = realloc(elts, n_elts * sizeof(T));\n             elts[n_elts - 1] = elt;\n       }\n    }\n\nAnd so on. So, the vector values aren't actually stored on the stack at all. I'm not entirely sure what you mean about \"in a function that finished\". Perhaps you could rephrase it? arg I made a silly error and flipped heap and stack in my last message at one point. Fixed it. \n\nWhat I was saying is relatively straightforward; to put a vector in a vector you first have to have the inner vector that you will outer_vector.push_back(). This  inner vector is declared on the stack. Once you push it back the outer vector makes a copy of it on the heap. \n\nThe original copy that you made to put in will be destroyed when it goes out of scope.  &gt; \"const char * const s\" would be a constant pointer to a constant character. This pointer is named s. You are then using this to construct a char * pointer named f? What does that even mean?\n\nSo it's actually a signature for a method named f() that takes one argument named c. The type of c you described correctly, it's a character pointer. The 'const' after the '*' means that what it's pointing at can't be modified, and the const before the '*' means the pointer itself can't be modified. The function f() returns a \"const char *\" which is just a const pointer to a character (although the char it's pointing at is not const in this case). The \"const\" after the \")\" means that this method won't modify the object instance it's being called by; it's useful if you have a \"const\" reference to an object, you can still call methods declared \"const.\"\n\n[const correctness](http://www.parashift.com/c++-faq-lite/const-correctness.html) is probably the most useful feature of C++ that most programmers don't understand. They almost never touch the topic in schools just because it doesn't really fit any broad computer science topic, it's strictly an implementation detail.\n\nI'll get back to you on the second question, I don't have time to play around with the code right now.\n\nK just thought about it. I'm pretty sure what you want is just a vector&lt;vector&lt;T&gt; &gt;. Vectors always allocate their space dynamically, even if they're on the stack (ie the vector itself is on the stack but the elements are on the heap).\n\nI just ran this and it works\n\n    vector&lt;vector&lt;int&gt; &gt; v;\n    v.push_back(vector&lt;int&gt;()); //The \"vector&lt;int&gt;\" being created ends up on the heap.\n    v[0].push_back(10);\n    cout &lt;&lt; v[0][0] &lt;&lt; endl; //prints 10\n\n &gt; \"const char * const s\" would be a constant pointer to a constant character. This pointer is named s. You are then using this to construct a char * pointer named f? What does that even mean?\n\nSo it's actually a signature for a method named f() that takes one argument named c. The type of c you described correctly, it's a character pointer. The 'const' after the '*' means that what it's pointing at can't be modified, and the const before the '*' means the pointer itself can't be modified. The function f() returns a \"const char *\" which is just a const pointer to a character (although the char it's pointing at is not const in this case). The \"const\" after the \")\" means that this method won't modify the object instance it's being called by; it's useful if you have a \"const\" reference to an object, you can still call methods declared \"const.\"\n\n[const correctness](http://www.parashift.com/c++-faq-lite/const-correctness.html) is probably the most useful feature of C++ that most programmers don't understand. They almost never touch the topic in schools just because it doesn't really fit any broad computer science topic, it's strictly an implementation detail.\n\nI'll get back to you on the second question, I don't have time to play around with the code right now.\n\nK just thought about it. I'm pretty sure what you want is just a vector&lt;vector&lt;T&gt; &gt;. Vectors always allocate their space dynamically, even if they're on the stack (ie the vector itself is on the stack but the elements are on the heap).\n\nI just ran this and it works\n\n    vector&lt;vector&lt;int&gt; &gt; v;\n    v.push_back(vector&lt;int&gt;()); //The \"vector&lt;int&gt;\" being created ends up on the heap.\n    v[0].push_back(10);\n    cout &lt;&lt; v[0][0] &lt;&lt; endl; //prints 10\n\n The outer `const` on function parameters is fairly pointless though. So you get a copy of something and then promise not to mutate your copy. Big deal! :V True, that outter const would make a lot more sense if the parameter were:\n\n    (const char *&amp; c)\n\ninstead You can't have `const` on that side of a reference! haha true, that wouldn't make any sense. Edited wtf is that last abomination?\n&gt;const char * f(const char * const s) const;\n\n\"const char * const s\" would be a constant pointer to a constant character. This pointer is named s. You are then using this to construct a char * pointer named f? What does that even mean?\n\nAlso you seem like a decent person to ask. If I have a vector of vectors, is there anyway to allocate my space dynamically but still have array2d[i][j] work? The problem is if I make my 2nd level vectors using the new keyword and use these to fill the top level vector my array2d[i] elements are pointers so that to get to my actual bottom level members I have to use (*array2d[i])[j]. I was guessing you could use references for this but after a small amount of research I'm not sure if you can get references to dynamic memory. -\n\nEDIT: ooh perhaps it could be done using:\n\n* vector&lt;T&gt; &amp;row = *(new vector&lt;T&gt;)\n\ntoo lazy to try it now, and I don't want to delete the question.      vector&lt;T&gt; &amp;row = *(new vector&lt;T&gt;)\n\nThis heap-alloctes a new vector, then makes a temporary copy of it when it gets dereferenced, and tried to hook the `row` reference to that copy. Even if it compiled which I don't think it does, it would not work because the temporary copy would be immediately discarded.\n\nFor a 2d array I'd probably just make a single vector sized `i*j` and do all the indexing in a wrapper function or type. If you want differently sized \"inner\" vectors I'd use `boost::ptr_vector&lt;std::vector&lt;T&gt;&gt;`. &gt;Even if it compiled which I don't think it does, it would not work because the temporary copy would be immediately discarded. \n\nNaw, there's the beauty, it wouldn't because you haven't used delete anywhere. It would stick around and unless you did something like\n\n    delete &amp;row;\n\nlater on (which would call delete on the address of the reference).\n\nThe part that makes it hideous however is it would make it difficult to avoid memory leaking.  wtf is that last abomination?\n&gt;const char * f(const char * const s) const;\n\n\"const char * const s\" would be a constant pointer to a constant character. This pointer is named s. You are then using this to construct a char * pointer named f? What does that even mean?\n\nAlso you seem like a decent person to ask. If I have a vector of vectors, is there anyway to allocate my space dynamically but still have array2d[i][j] work? The problem is if I make my 2nd level vectors using the new keyword and use these to fill the top level vector my array2d[i] elements are pointers so that to get to my actual bottom level members I have to use (*array2d[i])[j]. I was guessing you could use references for this but after a small amount of research I'm not sure if you can get references to dynamic memory. -\n\nEDIT: ooh perhaps it could be done using:\n\n* vector&lt;T&gt; &amp;row = *(new vector&lt;T&gt;)\n\ntoo lazy to try it now, and I don't want to delete the question.   I haven't read it all the way through, but I really enjoy Bjarne Stroustrup's (the creator of C++) book: http://www.amazon.ca/The-Programming-Language-Special-Edition/dp/0201700735.\n\nNot only will it teach you the language, but it gives insight into the ways C++ was intended to be used and a feel for some of the design decisions that need to be made when designing a language and/or building a compiler. However, with some of the modern editions to C++, some feel the book is becoming a bit outdated. I still find it useful.\n\nAnother great resource is the [C++ Faq Lite](http://www.parashift.com/c++-faq-lite/). What do you recommend for the opposite direction?  I'm at a C++ school, but the OO class is in Java.  Are there any recommendations for teaching oneself Java?  I think it's easier to go C++-&gt;Java versus the other way around...   If you know Java, C++ is fairly similar.  There's a book called \"C++ for Java Programmers\" by Mark Weiss that will get you up and going pretty quickly.",
    "url" : "http://www.reddit.com/r/compsci/comments/va926/c_guide_for_experienced_programmer/"
  }, {
    "id" : 43,
    "title" : "Research in private sector vs universities",
    "snippet" : "I'm curious about the relationship between the kind of research private sector software companies do and the research university CS departments do. What are the differences between them? Does the private sector do more research than in universities, or vice versa?  the general thinking is that unis do research in things 10 years down the line and companies do it 1-5 years down the line. or unis do basic research while copmanies do applied. \n\nthe reality is actually very different though. first of all, there are only a handful of companies that even have something that you could call an R&amp;D department without laughing. and depending on your specialty, there could be huge overlap or no overlap at all. so this really depends on your area. \n\nfor example, if you are are in theory or algo or math, then unis basically do all the research, very few companies do much in those areas in terms of publishable stuff. for something like AI or ML, unis will continue to do random publishable research while companies in that area will focus on applied research. if you are in systems and networking, there is a lot of overlap with what guys like MSR or google or amazon. that means you'll see papers from these companies at all the top system/networking conferences along with universities. for graphics, you'll see a lot from industry too, like pixar or disney or msr. eda is the same, you'll see a lot of papers from cadence, mentor graphics, synopsis along with the VLSI research labs from universities. i would say that the research here is most overlapping than any other field. overall, you might see uni research as being a bit more far-sighted, while often times industry research is focused on something that is more in the present, but this isn't always true, and a lot of times you'll see research that could be from either. at the end of the day, everyone just wants to be published. \n\nin aggregate, unis do more research than the R&amp;D departments of the large companies. R&amp;D is a huge cost for companies and thus they want to minimize it. \n\n\n\n\n\n\n\n\n\n\n\n   The University vs Private distinction here is a silly one.\n\nThe Universities are doing research as a proxy for private companies. &gt; The Universities are doing research as a proxy for private companies.\n\nRarely. Most university research is funded by government. Uhh...please cite this.  This would be news to me and everybody I know. NSF.gov ........... Which accounts for what percentage of overall research? I dunno, and I really don't care. I'm sure it's &gt;&gt;&gt;50%",
    "url" : "http://www.reddit.com/r/compsci/comments/va91f/research_in_private_sector_vs_universities/"
  }, {
    "id" : 44,
    "title" : "California Nuke Simulator Is World’s Most Powerful Computer",
    "snippet" : "  This raises a question: what happens to the older super computers that the new ones replace? I am going to assume that they are not still running because the marginal benefit of running the older system is outweighed by investing that money into the new system. I spent some time in Lawrence Livermore. They keep the old machines running; there is high demand for CPU time by all the scientists in the lab. They get decommissioned once they are several generations behind. Look at the top500 list; national labs typically have more than 1 ranking computer.  This raises a question: what happens to the older super computers that the new ones replace? I am going to assume that they are not still running because the marginal benefit of running the older system is outweighed by investing that money into the new system. Eventually they will be replaced, but they last quite a while. I have accounts on two computers in the top 40 which at one point were much higher on the list. However last I heard one of them will be shut down within a year or two and the resources used to matain it will go toward a new system. So you are exactly right, eventually they get replaced but not right away, you get many years out of them.\n\nI believe everything listed on the top 500 list are computers that are still being used and there are computers on the list built in 2008. I am sure there are much older systems that are still in use just some have dropped off the top 500 list. Man, I only have an account on one in the top 25. I have an account on another machine that would be around 300 or so if they ever bothered to submit the benchmarks. Can you (two) do a compsci subreddit AMA? Personally, I'd love to know what it's like to use one of those supercomputers, both the procedure and the types of problems you tackle and how you go about solving them.  16.3 petaflops per second\n\n&amp;#3232;\\_&amp;#3232; 16.3 petaflops per second\n\n&amp;#3232;\\_&amp;#3232; [Wikipedia](http://en.wikipedia.org/wiki/FLOPS) suggests this is OK:\n\n&gt;Alternatively, the singular FLOP (or flop) is used as an abbreviation for \"FLoating-point OPeration\", and a flop count is a count of these operations (e.g., required by a given algorithm or computer program). In this context, \"flops\" is simply the plural rather than a rate.\n\nIt's not the most common usage, but at least its not 'Watts per second'... 16.3 petaflops per second\n\n&amp;#3232;\\_&amp;#3232;   Seriously, why is there so much to simulate with nukes?        Joshua?",
    "url" : "http://www.wired.com/wiredenterprise/2012/06/top500-llnl/"
  }, {
    "id" : 45,
    "title" : "Help with (probably simple) lambda calculus beta-normalisation.",
    "snippet" : "I've been going over Lambda Calculus pretty much all day today and have gotten to beta-conversion, but there are a couple of bumps that I can't quite understand here.\n\nJust so that you know, I'm working from Hindley &amp; Seldin's 'Lambda-Calculus and Combinators an Introduction'.\n\nFirst of all a bit of background on some knowledge that I've picked up that I think is relevant in finding the beta-normal form of the \\\\-term (so that you can see if there's a flaw at the fundamental level):\n\nAll variables are \\\\-terms.\nTerms are considered left-associatively e.g. the variables a b c d are considered (((a b)c)d).\nA \\\\-term \\xy.x is synonymous with (\\x.(\\y.x)) and consists of two \\-terms, namely \\y.x and \\x.(\\y.x).\n\nTo the problem:\n\nReduce (\\xyz.xz(yz))((\\xy.yx)u)((\\xy.yx)v)w to its beta-normal form.\n\nSo to begin with, I will reduce the highest-level beta-redexes, namely those of the second and third \\\\-terms, which beta-reduces the entire term to:\n(\\xyz.xz(yz))(\\y.yu)(\\y.yv)w\n\nThis is where I'm not sure of the proper method.  What I've been doing is sticking with the left-associativity and thus working with:\n\n( ((\\xyz.xz(yz))(\\y.yu)) (\\y.yv)) w\n\nwhich reduces to\n\n((\\yz.(\\y.yu)z(yz)) (\\y.yv)) w\n\nBut from here, how do I proceed?  And what about those brackets around (yz)?  Do I treat that as one term when contracting?\nI've tried several ideas, but none of them have come up with the correct answer.\n\nTL;DR Could you please reduce (\\xyz.xz(yz))((\\xy.yx)u)((\\xy.yx)v)w to its beta-normal form for me and maybe explain step-by-step how the result was achieved?\n\nedit: \\\\-terms rather than \\-terms  First reduce the inner terms (\\xy.yx)u -&gt; \\y.yu, (\\xy.yx)v -&gt; \\y.yv\n\n(\\xyz.xz(yz)) (\\y.yu) (\\y.yv) w\n\nalpha convert the dependant variables first\n\n(\\xyz.xz(yz)) (\\y'.y'u) (\\y''.y''v) w\n\nplug first argument to x\n\n(\\yz.(\\y'.y'u)z(yz)) (\\y''.y''v) w\n\nplug second argument to y, 3rd to z\n\n(\\z.(\\y'.y'u)z((\\y''.y''v)z)) w\n\n(\\y'.y'u)w((\\y''.y''v)w))\n\nnow reduce what you can\n\n(wu)((\\y''.y''v)w))\n\n(wu)(wv) = w u (w v)\n\nThe order of reduction is called reduction strategy, some find the normal form, some might not, some will loop if loop is possible. You can read about that on wiki or in that book (is it the blue one?)\n\nThe mistake you did was not renaming bound variables. (\\xy.xy) y a -&gt; (\\y.yy) a -&gt; (aa) which is wrong, should be (ya). But if we rename (\\xy'.xy') y a -&gt; (\\y'.yy') a -&gt; (ya) OK\n\nBut it's still possible to resolve (only becaue you are lucky!)\n\n((\\yz.(\\y.yu)z(yz)) (\\y.yv)) w\n\n(\\z.(\\y.yu)z((\\y.yv)z)) w\n\n(\\z.(\\y.yu)z(zv)) w\n\n(\\z.zu(zv)) w\n\n(wu(wv)) = w u (w v)\n\nAnd to answer your question about (ab), yes, that is one term when doing subsitutiton. (\\xy.yx) (ab) c -&gt; c(ab)\n\n Ah of course, the leftmost term takes three arguments and there were indeed three arguments in the entire term for it to use.\n\nI see how it makes sense in that fashion, but this was an exercise within the book and up to this point there's been nothing on any reduction strategy, and as far as I can understand, nothing to really help me come up with this strategy up to this point.  Strange.  Suppose I'll read on and see if it brings it up.\n\nI know which book you're talking about but no, I'm using the red one from this link http://www.amazon.com/Lambda-Calculus-Combinators-Introduction-Roger-Hindley/dp/0521898854\n\nedit: I forgot myself; thank you for helping me out! Consider this example: (\\xy.yy) ((\\x.xx)(\\x.xx)) a\n\nIf we use strategy \"reduce arguments first\" we get infinite reduction since (\\x.xx)(\\x.xx) has no normal form. If we use \"outermost reduction first\", we get \\xy.yy (...) a -&gt; aa, done.\n\nReal life applications of lambda calc (lisp, haskell etc.) usually use some strategy that will lead to normal form, if one exist. There are strategies that, for example, will always loop infinitely if it is possible for the term to loop infinitely. Bah, I see!  I'm not so confident on choosing when to alpha-convert, so I went through the problem again, converting any duplicate bound variables and came out with the correct answer.\n\nThanks again; I'm going to go over alpha-conversion in a little bit more detail now. ",
    "url" : "http://www.reddit.com/r/compsci/comments/va2kl/help_with_probably_simple_lambda_calculus/"
  }, {
    "id" : 46,
    "title" : "Computing Texas Hold 'em -\nA computer scientist goes all in for poker.\n",
    "snippet" : "  This is neither a good article about Computer Science, nor a good article about Texas Hold'em.  Kind of tangential, but:\n\n&gt; Not the strongest hand, but he has $13,000 already invested in this pot.\n\nThat's not the right attitude to take in poker. What matters is the pot before you, not what you contributed to it. Yes, \"I've spent 13000 on this, so I might as well throw away the rest of my money\" is a terrible strategy. Well, that $13,000 is gone and has blended with everyone else's.\n\nWhat does it matter if you contributed $13,000 to a $40,000 pot or $1? In either case, you have two options: win or lose $40,000. The fact that you put it in makes no difference anymore.  That's what I'm trying to say.  The $13k is sunk cost. No, it's not. The $13K (that's only nominal value, remember) hasn't cost him *yet* and may still turn into profit for him. His expected value is greater if he calls the all in than if he folds and tries to recoup his losses with only 3 big blinds in his stack.\n\n Yes it is.  Once you put your money in the pot, it's not yours, it's part of the pot.  It doesn't really matter who put the money in; at that point there's the contents of the pot that you can win, and the money you have.  It's always a bad strategy (other than bluffing) to lose money just because you've already lost other money. It's a tournament, not a cash game. His best chance of making any money is to go all in. It's a tournament, not a cash game. His best chance of making any money is to go all in. No, it's not. The $13K (that's only nominal value, remember) hasn't cost him *yet* and may still turn into profit for him. His expected value is greater if he calls the all in than if he folds and tries to recoup his losses with only 3 big blinds in his stack.\n\n Ok, A lot of people downvote you, but no one tried to explain it. So I'll take a crack at it:\n\nIf you magically appear at a poker game, before the river and you see that there are 40 000$ in the pot. What would you do? You would of course look at your hand, see how much you needed to pay to call, and calculate the expected winnings.\n\nIf I then gave you the info that 13 000 $ of the money in the pot were your own, why should it change what you do? Either you expect to win money (in which case you should always call) or you expect to lose money (in which case you should fold). The money you have already spent don't influence your expected winnings as they are for all purposes lost unless you win all the money in the pot (and if you win you gain the money, no matter who put them there) Ok, A lot of people downvote you, but no one tried to explain it. So I'll take a crack at it:\n\nIf you magically appear at a poker game, before the river and you see that there are 40 000$ in the pot. What would you do? You would of course look at your hand, see how much you needed to pay to call, and calculate the expected winnings.\n\nIf I then gave you the info that 13 000 $ of the money in the pot were your own, why should it change what you do? Either you expect to win money (in which case you should always call) or you expect to lose money (in which case you should fold). The money you have already spent don't influence your expected winnings as they are for all purposes lost unless you win all the money in the pot (and if you win you gain the money, no matter who put them there) Kind of tangential, but:\n\n&gt; Not the strongest hand, but he has $13,000 already invested in this pot.\n\nThat's not the right attitude to take in poker. What matters is the pot before you, not what you contributed to it. Kind of tangential, but:\n\n&gt; Not the strongest hand, but he has $13,000 already invested in this pot.\n\nThat's not the right attitude to take in poker. What matters is the pot before you, not what you contributed to it. &gt; That's not the right attitude to take in poker. What matters is the pot before you, not what you contributed to it.\n\nthis is flawed logic when it comes to poker.  Ignoring what cards he has in his hand due to the betting patterns, anyhow...\n\nIf he has $13k in the pot and $8k left, it's safe to assume there's a bare minimum of $26k in the pot (assuming it's 2 players only, and they were both the blinds ... more than likely there's more like $40k in the pot)...\n\nIf there are still cards left to come, he's getting a chance to win $26k for just $8k -- that's 3.25:1 odds...\n\n1 / 4.25 = 0.23, or 23%.  If he has more than a 23% chance of winning the pot, given the following factors, he should *definitely* put in his $8k:\n\n1) Any potential cards to come that could improve his hand\n\n2) The chances he's ahead of his opponent already\n\nEven if he would lose this pot 3/4ths of the time -- if he's getting the right price on his money, he'd make money in the long term if he played this hand the exact same way dozens or hundreds of times...\n\nNow, when you consider the fact that the pot far more than likely has more like $40k in it - these odds change significantly...  at $40k, he's getting 5:1 on his money to risk $8k more... dropping that 23% to 16%...\n\nSee where this is going?  This concept is called \"pot odds\" in poker, and is crucial to understand if you want to be a skilled player....   At first look poker looks very easy game for computer. There is not so much stuff to compute, it looks like you could easily build program that plays good poker. Then  you realize that the game has multiple levels. \n\n1. Calculate  probability distributions, then  expected values and use them to determine right amounts of  bets and correct play \n2. Once your opponent knows this strategy, you must add bluffing (you can also calculate the optimal amount of bluff) \n3. Once your opponent knows that this is how you play, you must adjust your strategy slightly\n4. ...\n5.  Poker is game where  you model your opponent's imperfect strategies who are also modeling your strategies (there can be bluff at every level). There can be multiple players in the one hand, so you must model opponent 1 modeling opponent 2. This reflective modeling goes several levels deep and it can evolve during the game and across multiple games (it's reflective online learning problem). \n6.  Ultimately you realize that you must be able to model the minds of your opponents to get the perfect strategy for each game. At first look poker looks very easy game for computer. There is not so much stuff to compute, it looks like you could easily build program that plays good poker. Then  you realize that the game has multiple levels. \n\n1. Calculate  probability distributions, then  expected values and use them to determine right amounts of  bets and correct play \n2. Once your opponent knows this strategy, you must add bluffing (you can also calculate the optimal amount of bluff) \n3. Once your opponent knows that this is how you play, you must adjust your strategy slightly\n4. ...\n5.  Poker is game where  you model your opponent's imperfect strategies who are also modeling your strategies (there can be bluff at every level). There can be multiple players in the one hand, so you must model opponent 1 modeling opponent 2. This reflective modeling goes several levels deep and it can evolve during the game and across multiple games (it's reflective online learning problem). \n6.  Ultimately you realize that you must be able to model the minds of your opponents to get the perfect strategy for each game.  is A-4 really an automatic fold with just 4 people left? 9 people.  The top 5 are paid, and 4 needed to be eliminated to get to this point.  Since most people are playing to \"hold on\" till the money, this is a great time to steal blinds.  In late position (folded to the player) A4s would an instant all-in.  In other situations, easy fold. Ah, yea 9 people fuck that. Misread it. Thanks. I was thinking I might be playing a bit to aggressive and wanted to see the math behind it at least. Yeah it sounded like he was small-blind.  Seemed like a weak play to raise only to call the all-in.  \n  \n ",
    "url" : "http://magazine.jhu.edu/summer-2012/computing-texas-hold-em"
  }, {
    "id" : 47,
    "title" : "Scientific computing and plasma physics blog",
    "url" : "http://www.particleincell.com/blog/"
  }, {
    "id" : 48,
    "title" : "Summer personal study",
    "snippet" : "Hey r/compsci\n2nd year cs student at university. Looking for suggestions for stuff I could learn/study over the summer. Have learned basic syntax for java and c++, and done basic programming problems in both. Next semester I'll be taking data structures, intro to comp systems, and discrete structures. Any ideas for things to work on/study/get ahead in over the summer?   You could do something fun like [Project Euler](http://projecteuler.net/). I always find time put in to their questions to be time well spent, though I do have a little bit of a mathematical bias.\n\nOr maybe pickup [Learn You a Haskell For Great Good](http://www.amazon.com/Learn-You-Haskell-Great-Good/dp/1593272839) and get some exposure to a functional language and a different way of thinking about programming. \n\nIf you told us your interests and goals, we could probably give you better advice! WHY do people always recommend Project Euler? It's great for developing general problem solving skills and offering interesting math problems, but it is an utterly misguided way to develop programming skills. WHY do people always recommend Project Euler? It's great for developing general problem solving skills and offering interesting math problems, but it is an utterly misguided way to develop programming skills. You could do something fun like [Project Euler](http://projecteuler.net/). I always find time put in to their questions to be time well spent, though I do have a little bit of a mathematical bias.\n\nOr maybe pickup [Learn You a Haskell For Great Good](http://www.amazon.com/Learn-You-Haskell-Great-Good/dp/1593272839) and get some exposure to a functional language and a different way of thinking about programming. \n\nIf you told us your interests and goals, we could probably give you better advice! Here's the [online version](http://learnyouahaskell.com/chapters). It's free! You could do something fun like [Project Euler](http://projecteuler.net/). I always find time put in to their questions to be time well spent, though I do have a little bit of a mathematical bias.\n\nOr maybe pickup [Learn You a Haskell For Great Good](http://www.amazon.com/Learn-You-Haskell-Great-Good/dp/1593272839) and get some exposure to a functional language and a different way of thinking about programming. \n\nIf you told us your interests and goals, we could probably give you better advice!  For discrete structures (I'm assuming it's just discrete mathematics by another name) get a handle on how to prove things; I've met far too many computer science majors at my university who don't have a good grasp on induction! You can learn induction (and how to prove things more generally) from a text on Number Theory (unfortunately my only number theory text is Rosen which is only alright) or just picking up a discrete math book over the summer and just read the book and do some of the exercises. \"How to Prove It\" by Velleman is a great text on logic and induction and will definitely be good leg up on your fellow classmates in discrete. I was a discrete math TA at my University which also used the Rosen book which was... Eh  [Udacity](http://udacity.com) has all you need        I would check out some of the CS courses at \n\n[coursera](http://www.coursera.org)\n\nThey might be a bit advanced for a second year student, but you'll be getting into the material soon enough.  Register for 1 or 2 courses that you think are interesting.  Analysis of Algorithms is going on right now.\n   Lemme hijack this thread for a moment, since I have a relevant question.\n\nI asked this over at r/learnprogramming but haven't gotten any good answers.  I've been learning to program for the past year and a half now, having had a very solid math (and mathematical modeling) background.  I have $50 that I'd like to spend on programming/CS books.  Does anyone here have any recommendations on something I can get for fairly cheap that will go deeper into computer *science*?  Something that would be appropriate for a CS 2 or CS 3 class, rather than CS 1 (I'm making these numbers up)?  Thanks, r/compsci!   Are internships an option?  It might help you understand where the rubber meets the road. Not really. I'll be co-oping next spring, but nothing until then    Some thing that will probably put you well ahead of other students:\n\n* Read Knuth,\n* Learn an assembly language and program some non-trivial things in it (like the algorithms from Knuth),\n* Read SICP, learn Scheme/Lisp. Knuth might be over his head.  Some thing that will probably put you well ahead of other students:\n\n* Read Knuth,\n* Learn an assembly language and program some non-trivial things in it (like the algorithms from Knuth),\n* Read SICP, learn Scheme/Lisp. ",
    "url" : "http://www.reddit.com/r/compsci/comments/v75uf/summer_personal_study/"
  }, {
    "id" : 49,
    "title" : "Formal Languages: Finding the intersection of two regular expressions",
    "snippet" : "In formal languages if I have two regular expressions to define some languages, they are defined as R1 and R2.  If I want to find the intersection of the two I know one method would be:\n\n* Convert R1 and R2 into Finite Automata\n* Create the intersection FA of these two\n* Convert this new FA into a regular expression\n\nIs there an easier way to do this as if you have a complicated regular expressions initially then this process takes a long time and converting back to a regex can be quite complicated.     &gt; ... when constructing a regular expression defining the intersection of a fixed and an arbitrary number of regular expressions, an exponential and double exponential size increase, respectively, can not be avoided.\n  \nFrom abstract of http://tocl.acm.org/accepted/401neven.pdf .  \n  \n(For clarity, the exponent is in terms of the size of the expression, not just the number of intersecting expressions.)",
    "url" : "http://www.reddit.com/r/compsci/comments/v7ads/formal_languages_finding_the_intersection_of_two/"
  } ],
  "processing-time-source" : 72,
  "processing-result.title" : "compsci12_reddit.xml",
  "XmlDocumentSource.xml" : {
    "absolutePath" : "/Users/cole/code/cs467/r-topics/carrot2-cli-3.6.2/../data/compsci12_reddit.xml"
  }
}
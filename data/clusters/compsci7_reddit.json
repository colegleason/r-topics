{
  "processing-time-total" : 2093,
  "clusters" : [ {
    "id" : 0,
    "size" : 8,
    "score" : 71.21328314506698,
    "phrases" : [ "Method" ],
    "documents" : [ 8, 16, 27, 29, 37, 39, 43, 48 ],
    "attributes" : {
      "score" : 71.21328314506698
    }
  }, {
    "id" : 1,
    "size" : 7,
    "score" : 64.04490705621826,
    "phrases" : [ "Graph" ],
    "documents" : [ 0, 6, 13, 22, 40, 42, 46 ],
    "attributes" : {
      "score" : 64.04490705621826
    }
  }, {
    "id" : 2,
    "size" : 7,
    "score" : 72.41066321279715,
    "phrases" : [ "Software Engineer" ],
    "documents" : [ 4, 30, 33, 34, 37, 43, 46 ],
    "attributes" : {
      "score" : 72.41066321279715
    }
  }, {
    "id" : 3,
    "size" : 7,
    "score" : 80.02828762570637,
    "phrases" : [ "Write a Function" ],
    "documents" : [ 1, 4, 7, 8, 35, 37, 46 ],
    "attributes" : {
      "score" : 80.02828762570637
    }
  }, {
    "id" : 4,
    "size" : 6,
    "score" : 66.95356965436497,
    "phrases" : [ "Real World" ],
    "documents" : [ 7, 27, 34, 35, 37, 47 ],
    "attributes" : {
      "score" : 66.95356965436497
    }
  }, {
    "id" : 5,
    "size" : 5,
    "score" : 57.022660337369246,
    "phrases" : [ "Implemented Efficiently" ],
    "documents" : [ 8, 9, 23, 37, 38 ],
    "attributes" : {
      "score" : 57.022660337369246
    }
  }, {
    "id" : 6,
    "size" : 5,
    "score" : 104.04464366368553,
    "phrases" : [ "Programming Language Theory" ],
    "documents" : [ 7, 8, 23, 37, 46 ],
    "attributes" : {
      "score" : 104.04464366368553
    }
  }, {
    "id" : 7,
    "size" : 4,
    "score" : 54.3139136723177,
    "phrases" : [ "CS Majors" ],
    "documents" : [ 13, 27, 29, 44 ],
    "attributes" : {
      "score" : 54.3139136723177
    }
  }, {
    "id" : 8,
    "size" : 4,
    "score" : 83.42823555027547,
    "phrases" : [ "Return Number" ],
    "documents" : [ 1, 3, 8, 31 ],
    "attributes" : {
      "score" : 83.42823555027547
    }
  }, {
    "id" : 9,
    "size" : 4,
    "score" : 45.146995619626225,
    "phrases" : [ "Video" ],
    "documents" : [ 1, 5, 29, 37 ],
    "attributes" : {
      "score" : 45.146995619626225
    }
  }, {
    "id" : 10,
    "size" : 3,
    "score" : 101.02272749857039,
    "phrases" : [ "Addressable Memory" ],
    "documents" : [ 8, 20, 38 ],
    "attributes" : {
      "score" : 101.02272749857039
    }
  }, {
    "id" : 11,
    "size" : 3,
    "score" : 48.235973034808225,
    "phrases" : [ "Encryption" ],
    "documents" : [ 28, 35, 49 ],
    "attributes" : {
      "score" : 48.235973034808225
    }
  }, {
    "id" : 12,
    "size" : 3,
    "score" : 106.8395168345992,
    "phrases" : [ "Module can Interact with other Modules" ],
    "documents" : [ 9, 37, 46 ],
    "attributes" : {
      "score" : 106.8395168345992
    }
  }, {
    "id" : 13,
    "size" : 3,
    "score" : 65.89220271947548,
    "phrases" : [ "X-post" ],
    "documents" : [ 6, 28, 41 ],
    "attributes" : {
      "score" : 65.89220271947548
    }
  }, {
    "id" : 14,
    "size" : 2,
    "score" : 53.59918407089697,
    "phrases" : [ "AVL Trees" ],
    "documents" : [ 15, 17 ],
    "attributes" : {
      "score" : 53.59918407089697
    }
  }, {
    "id" : 15,
    "size" : 2,
    "score" : 88.70256508465768,
    "phrases" : [ "Balanced Trees" ],
    "documents" : [ 7, 17 ],
    "attributes" : {
      "score" : 88.70256508465768
    }
  }, {
    "id" : 16,
    "size" : 2,
    "score" : 51.23915314619666,
    "phrases" : [ "Lecture Slides" ],
    "documents" : [ 26, 29 ],
    "attributes" : {
      "score" : 51.23915314619666
    }
  }, {
    "id" : 17,
    "size" : 2,
    "score" : 47.895780132265465,
    "phrases" : [ "Literal" ],
    "documents" : [ 37, 48 ],
    "attributes" : {
      "score" : 47.895780132265465
    }
  }, {
    "id" : 18,
    "size" : 2,
    "score" : 65.83032759264731,
    "phrases" : [ "Scientists Study" ],
    "documents" : [ 12, 46 ],
    "attributes" : {
      "score" : 65.83032759264731
    }
  }, {
    "id" : 19,
    "size" : 12,
    "score" : 0.0,
    "phrases" : [ "Other Topics" ],
    "documents" : [ 2, 10, 11, 14, 18, 19, 21, 24, 25, 32, 36, 45 ],
    "attributes" : {
      "other-topics" : true,
      "score" : 0.0
    }
  } ],
  "processing-time-algorithm" : 1999,
  "documents" : [ {
    "id" : 0,
    "title" : "Succinct Data Structures - specifically graphs",
    "snippet" : "Can any one direct me to some material on succinct representation of data structures. I am specifically looking for succinct representation of graphs. After some searching, I found that Guy Jacobson's PhD thesis was on succinct data structures but I couldn't find any paper on it.    ",
    "url" : "http://www.reddit.com/r/compsci/comments/119u2h/succinct_data_structures_specifically_graphs/"
  }, {
    "id" : 1,
    "title" : "Good resource to learn scheme?",
    "snippet" : "We're learning Scheme (along with PLAI and Racket) in my programming languages class, and I (and most of the rest of my class) am having far too much trouble wrapping my head around it. We moved onto JavaCC and everything made sense again, but our prof realized no one had a clue how to do Scheme so he's trying to (unsuccessfully) teach us it again. I'm trying to find a good resource on my own, but quite frankly it seems like people who do understand it have trouble explaining it :(  No, no, no.  I knew people would recommend SICP.  It's a great book, but it's not for learning scheme--it just happens to use scheme in order to illustrate larger points about computer science, the design of language, etc.  It's definitely not for beginners.\n\nFor scheme itself, stick to:\n\n[The Scheme Programmig Language, 4e](http://www.scheme.com/tspl4/)\nor\n[How to Design Programs](http://htdp.org/)\n\nNow, those are about the language itself, but if you're having trouble conceptualizing, I would recommend shelling out to buy:\n\n[The Little Schemer](http://www.amazon.com/The-Little-Schemer-4th-Edition/dp/0262560992/ref=sr_1_1?ie=UTF8&amp;qid=1349795339&amp;sr=8-1&amp;keywords=the+little+schemer)\n I came here to mention The Little Schemer. This is how I learned scheme, and it was written by my professor Daniel Freedman. He's a great guy and a genius, and if you get a copy of this cheap paper book, you'll see how simple it is. It's not a \"book\" book, it's 100% examples which makes it so nice for learning quickly.  Agreed. Really cool way to teach scheme. And dirt cheap.  I came here to mention The Little Schemer. This is how I learned scheme, and it was written by my professor Daniel Freedman. He's a great guy and a genius, and if you get a copy of this cheap paper book, you'll see how simple it is. It's not a \"book\" book, it's 100% examples which makes it so nice for learning quickly.  I came here to mention The Little Schemer. This is how I learned scheme, and it was written by my professor Daniel Freedman. He's a great guy and a genius, and if you get a copy of this cheap paper book, you'll see how simple it is. It's not a \"book\" book, it's 100% examples which makes it so nice for learning quickly.  Oh, he seems like a really nice guy. A colleague and I did a feedback video of it and he sent us a free copy each. No, no, no.  I knew people would recommend SICP.  It's a great book, but it's not for learning scheme--it just happens to use scheme in order to illustrate larger points about computer science, the design of language, etc.  It's definitely not for beginners.\n\nFor scheme itself, stick to:\n\n[The Scheme Programmig Language, 4e](http://www.scheme.com/tspl4/)\nor\n[How to Design Programs](http://htdp.org/)\n\nNow, those are about the language itself, but if you're having trouble conceptualizing, I would recommend shelling out to buy:\n\n[The Little Schemer](http://www.amazon.com/The-Little-Schemer-4th-Edition/dp/0262560992/ref=sr_1_1?ie=UTF8&amp;qid=1349795339&amp;sr=8-1&amp;keywords=the+little+schemer)\n Also, the [2nd edition](http://www.ccs.neu.edu/home/matthias/HtDP2e/) of HtDP is better, though some sections haven't been written yet. I wouldn't say HtDP is actually about \"how to learn Scheme\" as much as \"how to learn programming [in any language]\", but it's still worth reading. No, no, no.  I knew people would recommend SICP.  It's a great book, but it's not for learning scheme--it just happens to use scheme in order to illustrate larger points about computer science, the design of language, etc.  It's definitely not for beginners.\n\nFor scheme itself, stick to:\n\n[The Scheme Programmig Language, 4e](http://www.scheme.com/tspl4/)\nor\n[How to Design Programs](http://htdp.org/)\n\nNow, those are about the language itself, but if you're having trouble conceptualizing, I would recommend shelling out to buy:\n\n[The Little Schemer](http://www.amazon.com/The-Little-Schemer-4th-Edition/dp/0262560992/ref=sr_1_1?ie=UTF8&amp;qid=1349795339&amp;sr=8-1&amp;keywords=the+little+schemer)\n After \"The Little Schemer\", there's also the [The Seasoned Schemer](http://www.amazon.com/The-Seasoned-Schemer-Daniel-Friedman/dp/026256100X/) and [The Reasoned Schemer](http://www.amazon.com/The-Reasoned-Schemer-Daniel-Friedman/dp/0262562146/). No, no, no.  I knew people would recommend SICP.  It's a great book, but it's not for learning scheme--it just happens to use scheme in order to illustrate larger points about computer science, the design of language, etc.  It's definitely not for beginners.\n\nFor scheme itself, stick to:\n\n[The Scheme Programmig Language, 4e](http://www.scheme.com/tspl4/)\nor\n[How to Design Programs](http://htdp.org/)\n\nNow, those are about the language itself, but if you're having trouble conceptualizing, I would recommend shelling out to buy:\n\n[The Little Schemer](http://www.amazon.com/The-Little-Schemer-4th-Edition/dp/0262560992/ref=sr_1_1?ie=UTF8&amp;qid=1349795339&amp;sr=8-1&amp;keywords=the+little+schemer)\n No, no, no.  I knew people would recommend SICP.  It's a great book, but it's not for learning scheme--it just happens to use scheme in order to illustrate larger points about computer science, the design of language, etc.  It's definitely not for beginners.\n\nFor scheme itself, stick to:\n\n[The Scheme Programmig Language, 4e](http://www.scheme.com/tspl4/)\nor\n[How to Design Programs](http://htdp.org/)\n\nNow, those are about the language itself, but if you're having trouble conceptualizing, I would recommend shelling out to buy:\n\n[The Little Schemer](http://www.amazon.com/The-Little-Schemer-4th-Edition/dp/0262560992/ref=sr_1_1?ie=UTF8&amp;qid=1349795339&amp;sr=8-1&amp;keywords=the+little+schemer)\n No, no, no.  I knew people would recommend SICP.  It's a great book, but it's not for learning scheme--it just happens to use scheme in order to illustrate larger points about computer science, the design of language, etc.  It's definitely not for beginners.\n\nFor scheme itself, stick to:\n\n[The Scheme Programmig Language, 4e](http://www.scheme.com/tspl4/)\nor\n[How to Design Programs](http://htdp.org/)\n\nNow, those are about the language itself, but if you're having trouble conceptualizing, I would recommend shelling out to buy:\n\n[The Little Schemer](http://www.amazon.com/The-Little-Schemer-4th-Edition/dp/0262560992/ref=sr_1_1?ie=UTF8&amp;qid=1349795339&amp;sr=8-1&amp;keywords=the+little+schemer)\n No, no, no.  I knew people would recommend SICP.  It's a great book, but it's not for learning scheme--it just happens to use scheme in order to illustrate larger points about computer science, the design of language, etc.  It's definitely not for beginners.\n\nFor scheme itself, stick to:\n\n[The Scheme Programmig Language, 4e](http://www.scheme.com/tspl4/)\nor\n[How to Design Programs](http://htdp.org/)\n\nNow, those are about the language itself, but if you're having trouble conceptualizing, I would recommend shelling out to buy:\n\n[The Little Schemer](http://www.amazon.com/The-Little-Schemer-4th-Edition/dp/0262560992/ref=sr_1_1?ie=UTF8&amp;qid=1349795339&amp;sr=8-1&amp;keywords=the+little+schemer)\n   [Simply Scheme](http://www.eecs.berkeley.edu/~bh/ss-toc2.html)\nBrian Harvey(UC Berkeley) wrote the text and also has some online lectures.  Sort of a \"SICP LIte\", really.  The Little Schemer  Teach Yourself Racket.\n\nhttps://cs.uwaterloo.ca/~plragde/tyr/index.html\n\nNo idea if it's appropriate, but one of my former professors recently started working on this. Teach Yourself Racket.\n\nhttps://cs.uwaterloo.ca/~plragde/tyr/index.html\n\nNo idea if it's appropriate, but one of my former professors recently started working on this. Teach Yourself Racket.\n\nhttps://cs.uwaterloo.ca/~plragde/tyr/index.html\n\nNo idea if it's appropriate, but one of my former professors recently started working on this.    What in particular are you having trouble wrapping your head around? The syntax? Recursion? Something else?\n The syntax mainly. It's actually not as hard as it might initially look. Let's take a look at some of the basics.\n\nYou have the following syntax for all the function calls:\n  \n    (function-name param1 param2)\n\nthis is really no different than what you have in Java with\n\n    function(int param1, int param2);\n\nnow let's look at what happens when you define a function\n\n    (define (foo bar)\n      (print bar))\n\nTo create a new function you call define and pass it some arguments. It takes 1 to n arguments, where the first argument is a list, in our case (foo bar). This list describes the name of the function we're defining and the rest of the items in the list are its arguments. \n\nThe rest of the items represent the body of the function. In our case we passed in a call to print function (print bar). You could pass in as many items as you want and nest them as deep as you want:\n\n    (define (baz a b)\n      (print a)\n      (print b)\n      (* 2 \n         (+ a b)))\n\nabove we define function baz which takes parameters a and b. Then we pass it in the body, which consists of 2 print statements and and a call to sum a and b and multiply the result by 2.\n\nAll the expressions are evaluated from the inside out, so above (+ a b) is evaluated, then its result is passed to (* 2 result) and the result of that is returned by baz. Notice that the last statement is returned implicitly.\n\nBecause Scheme is a functional language, you can do everything you do with variables with functions as well. This means that you can assign a function to a variable, pass it as a parameter, or return a function from another function.\n\nFunctions which take other functions as parameters are called higher order functions. On example of such a function is map:\n\n    (map (lambda (x) (* 2 x)) '(1 2 3 4 5))\n\nhere we pass in 2 parameters to map, the first one is a function representing the logic we want applied to a collection, and the second is a list of numbers. The map function will visit each item in the list and apply the lambda to it, which doubles the item in this case.\n\nAnother example is foldl\n\n    (foldl + 0 '(1 2 3 4 5))\n\nYou can of course combine these functions together:\n\n    (filter even? \n            (map (lambda (x) (* 3 x)) \n                 '(1 2 3 4 5)))\n\nHere we multiply each item by 3, then filter even items from the resulting list.\n\nBecause of higher order functions, you should practically never have to write loops or do explicit recursion. Any time you need to iterate a collection, you should use a function such as map, filter, foldl, etc. Practically any data transformation can be achieved by a combination of these.\n\nFunctions which return other functions can act as constructors. For example if you want to initialize some variables before using a function you can do it as follows:\n\n     (define (init-client url)\n       (lambda (request)\n         (string-append url \"-&gt;\" request)))\n\n     (let ((client (init-client \"http://foo.org\")))\n       (print (client \"foo\"))\n       (print (client \"bar\")))\n\nhere we create a function which accepts a url as a parameter and returns a function which accepts a request. However, since the inner function was defined within the scope where url is visible it will have access to it implicitly.\n\nThis type of function is called a closure, because it closes over the parameters, in our case url and makes them available to anything defined with in it. This is equivalent to what happens when you call a constructor on a class to create an instance of an object.\n\nThe big difference between this kind of code and what you might be used to from Java is that you don't change variables in place. In an imperative language you assign a label to a memory location, and then update the contents of that location by referencing the label.\n\nFor example, you might write something like:\n\n    x = 0;\n    x = x + 1;\n    x = x + 2;\n\nWhere you continuously update the value of x. In Scheme, you would chain statements together to do this:\n\n    (+ 2 (+ 1 0))\n\nEach outer function takes the result of the inner function as its input, and does something with it, and pass it up the chain.\n\nObviously, this kind of nesting can get messy, so you use let statements the same way would declare variables in an imperative language, eg:\n\n    (define (baz a)\n      (let* ((x (+ 1 a))\n             (y (+ 2 x)))\n        y))\n\nhere we assign the result of (+ 1 a) to x, then assign the result of (+ 2 x) to y and then return y.\n\nThat's really the basics of Scheme in a nutshell. The way to approach problems in Scheme is to think of them in terms of data transformations. You start out with a piece of data, and you want to get a different piece of data at the end. \n\nSo, think of the steps you need to take to get from where you are to where you need to be. Write a function for each of these steps, and then chain them together and pass the data through. It's kind of like having an assembly line, and data flowing through it.\n\nHope this clears things up a bit. It's a very simple language once you wrap your head around it. Once you get past the fact that it looks different you'll see that it makes a lot of things very easy and that a lot of stuff you already know maps to it rather nicely as well. \n\n\n Holy shit. I owe you an e-beer, or something. Have an up vote at the very least It's actually not as hard as it might initially look. Let's take a look at some of the basics.\n\nYou have the following syntax for all the function calls:\n  \n    (function-name param1 param2)\n\nthis is really no different than what you have in Java with\n\n    function(int param1, int param2);\n\nnow let's look at what happens when you define a function\n\n    (define (foo bar)\n      (print bar))\n\nTo create a new function you call define and pass it some arguments. It takes 1 to n arguments, where the first argument is a list, in our case (foo bar). This list describes the name of the function we're defining and the rest of the items in the list are its arguments. \n\nThe rest of the items represent the body of the function. In our case we passed in a call to print function (print bar). You could pass in as many items as you want and nest them as deep as you want:\n\n    (define (baz a b)\n      (print a)\n      (print b)\n      (* 2 \n         (+ a b)))\n\nabove we define function baz which takes parameters a and b. Then we pass it in the body, which consists of 2 print statements and and a call to sum a and b and multiply the result by 2.\n\nAll the expressions are evaluated from the inside out, so above (+ a b) is evaluated, then its result is passed to (* 2 result) and the result of that is returned by baz. Notice that the last statement is returned implicitly.\n\nBecause Scheme is a functional language, you can do everything you do with variables with functions as well. This means that you can assign a function to a variable, pass it as a parameter, or return a function from another function.\n\nFunctions which take other functions as parameters are called higher order functions. On example of such a function is map:\n\n    (map (lambda (x) (* 2 x)) '(1 2 3 4 5))\n\nhere we pass in 2 parameters to map, the first one is a function representing the logic we want applied to a collection, and the second is a list of numbers. The map function will visit each item in the list and apply the lambda to it, which doubles the item in this case.\n\nAnother example is foldl\n\n    (foldl + 0 '(1 2 3 4 5))\n\nYou can of course combine these functions together:\n\n    (filter even? \n            (map (lambda (x) (* 3 x)) \n                 '(1 2 3 4 5)))\n\nHere we multiply each item by 3, then filter even items from the resulting list.\n\nBecause of higher order functions, you should practically never have to write loops or do explicit recursion. Any time you need to iterate a collection, you should use a function such as map, filter, foldl, etc. Practically any data transformation can be achieved by a combination of these.\n\nFunctions which return other functions can act as constructors. For example if you want to initialize some variables before using a function you can do it as follows:\n\n     (define (init-client url)\n       (lambda (request)\n         (string-append url \"-&gt;\" request)))\n\n     (let ((client (init-client \"http://foo.org\")))\n       (print (client \"foo\"))\n       (print (client \"bar\")))\n\nhere we create a function which accepts a url as a parameter and returns a function which accepts a request. However, since the inner function was defined within the scope where url is visible it will have access to it implicitly.\n\nThis type of function is called a closure, because it closes over the parameters, in our case url and makes them available to anything defined with in it. This is equivalent to what happens when you call a constructor on a class to create an instance of an object.\n\nThe big difference between this kind of code and what you might be used to from Java is that you don't change variables in place. In an imperative language you assign a label to a memory location, and then update the contents of that location by referencing the label.\n\nFor example, you might write something like:\n\n    x = 0;\n    x = x + 1;\n    x = x + 2;\n\nWhere you continuously update the value of x. In Scheme, you would chain statements together to do this:\n\n    (+ 2 (+ 1 0))\n\nEach outer function takes the result of the inner function as its input, and does something with it, and pass it up the chain.\n\nObviously, this kind of nesting can get messy, so you use let statements the same way would declare variables in an imperative language, eg:\n\n    (define (baz a)\n      (let* ((x (+ 1 a))\n             (y (+ 2 x)))\n        y))\n\nhere we assign the result of (+ 1 a) to x, then assign the result of (+ 2 x) to y and then return y.\n\nThat's really the basics of Scheme in a nutshell. The way to approach problems in Scheme is to think of them in terms of data transformations. You start out with a piece of data, and you want to get a different piece of data at the end. \n\nSo, think of the steps you need to take to get from where you are to where you need to be. Write a function for each of these steps, and then chain them together and pass the data through. It's kind of like having an assembly line, and data flowing through it.\n\nHope this clears things up a bit. It's a very simple language once you wrap your head around it. Once you get past the fact that it looks different you'll see that it makes a lot of things very easy and that a lot of stuff you already know maps to it rather nicely as well. \n\n\n This is why I always a bit sad when I realize that I could work in scheme/CL... sigh... This is why I always a bit sad when I realize that I could work in scheme/CL... sigh... I get to use some Clojure at work and I have to say it's pretty fantastic in my opinion. recruiting ? It's actually not as hard as it might initially look. Let's take a look at some of the basics.\n\nYou have the following syntax for all the function calls:\n  \n    (function-name param1 param2)\n\nthis is really no different than what you have in Java with\n\n    function(int param1, int param2);\n\nnow let's look at what happens when you define a function\n\n    (define (foo bar)\n      (print bar))\n\nTo create a new function you call define and pass it some arguments. It takes 1 to n arguments, where the first argument is a list, in our case (foo bar). This list describes the name of the function we're defining and the rest of the items in the list are its arguments. \n\nThe rest of the items represent the body of the function. In our case we passed in a call to print function (print bar). You could pass in as many items as you want and nest them as deep as you want:\n\n    (define (baz a b)\n      (print a)\n      (print b)\n      (* 2 \n         (+ a b)))\n\nabove we define function baz which takes parameters a and b. Then we pass it in the body, which consists of 2 print statements and and a call to sum a and b and multiply the result by 2.\n\nAll the expressions are evaluated from the inside out, so above (+ a b) is evaluated, then its result is passed to (* 2 result) and the result of that is returned by baz. Notice that the last statement is returned implicitly.\n\nBecause Scheme is a functional language, you can do everything you do with variables with functions as well. This means that you can assign a function to a variable, pass it as a parameter, or return a function from another function.\n\nFunctions which take other functions as parameters are called higher order functions. On example of such a function is map:\n\n    (map (lambda (x) (* 2 x)) '(1 2 3 4 5))\n\nhere we pass in 2 parameters to map, the first one is a function representing the logic we want applied to a collection, and the second is a list of numbers. The map function will visit each item in the list and apply the lambda to it, which doubles the item in this case.\n\nAnother example is foldl\n\n    (foldl + 0 '(1 2 3 4 5))\n\nYou can of course combine these functions together:\n\n    (filter even? \n            (map (lambda (x) (* 3 x)) \n                 '(1 2 3 4 5)))\n\nHere we multiply each item by 3, then filter even items from the resulting list.\n\nBecause of higher order functions, you should practically never have to write loops or do explicit recursion. Any time you need to iterate a collection, you should use a function such as map, filter, foldl, etc. Practically any data transformation can be achieved by a combination of these.\n\nFunctions which return other functions can act as constructors. For example if you want to initialize some variables before using a function you can do it as follows:\n\n     (define (init-client url)\n       (lambda (request)\n         (string-append url \"-&gt;\" request)))\n\n     (let ((client (init-client \"http://foo.org\")))\n       (print (client \"foo\"))\n       (print (client \"bar\")))\n\nhere we create a function which accepts a url as a parameter and returns a function which accepts a request. However, since the inner function was defined within the scope where url is visible it will have access to it implicitly.\n\nThis type of function is called a closure, because it closes over the parameters, in our case url and makes them available to anything defined with in it. This is equivalent to what happens when you call a constructor on a class to create an instance of an object.\n\nThe big difference between this kind of code and what you might be used to from Java is that you don't change variables in place. In an imperative language you assign a label to a memory location, and then update the contents of that location by referencing the label.\n\nFor example, you might write something like:\n\n    x = 0;\n    x = x + 1;\n    x = x + 2;\n\nWhere you continuously update the value of x. In Scheme, you would chain statements together to do this:\n\n    (+ 2 (+ 1 0))\n\nEach outer function takes the result of the inner function as its input, and does something with it, and pass it up the chain.\n\nObviously, this kind of nesting can get messy, so you use let statements the same way would declare variables in an imperative language, eg:\n\n    (define (baz a)\n      (let* ((x (+ 1 a))\n             (y (+ 2 x)))\n        y))\n\nhere we assign the result of (+ 1 a) to x, then assign the result of (+ 2 x) to y and then return y.\n\nThat's really the basics of Scheme in a nutshell. The way to approach problems in Scheme is to think of them in terms of data transformations. You start out with a piece of data, and you want to get a different piece of data at the end. \n\nSo, think of the steps you need to take to get from where you are to where you need to be. Write a function for each of these steps, and then chain them together and pass the data through. It's kind of like having an assembly line, and data flowing through it.\n\nHope this clears things up a bit. It's a very simple language once you wrap your head around it. Once you get past the fact that it looks different you'll see that it makes a lot of things very easy and that a lot of stuff you already know maps to it rather nicely as well. \n\n\n The syntax mainly.  The famous MIT textbook SICP is very good, and free: http://mitpress.mit.edu/sicp/full-text/book/book.html\n The famous MIT textbook SICP is very good, and free: http://mitpress.mit.edu/sicp/full-text/book/book.html\n  [Structure and Interpretation of Computer Programs](http://mitpress.mit.edu/sicp/full-text/book/book.html) teaches Scheme.  The books is free online, with [companion lectures](http://groups.csail.mit.edu/mac/classes/6.001/abelson-sussman-lectures/) by the authors.  I'd start with the first lecture, since it gives a really good \"Scheme in a nutshell\" overview of the language, then start reading the book.\n\nAlso, I've been doing the SICP exercises and examples in DrRacket, so you'll be able to learn Racket as well. SICP is a pretty generalized view on computer programs as a whole. It just happens to use Scheme for examples -- I'd say this is a little much for someone who just wants to learn Scheme.\n\nGreat resource though, for a time when learning the language itself isn't top priority.      Just a guess. Do you go to UT?        What does JavaCC have to do with Scheme?\n\nIs this the \"programming language\" class? indeed. We're doing SQL/SPARQL/jython stuff right now I was in school in 10 years ago, you will notice that a lot of people take the same courses, all over the US and the courses haven't changed in 10-15 years.\n\nThe only \"new\" class I took was a Java/OOP course.  That was \"new age\" stuff back then.\n\nAnd I am surprised you are using JavaCC instead of ANTLR. ",
    "url" : "http://www.reddit.com/r/compsci/comments/1173ql/good_resource_to_learn_scheme/"
  }, {
    "id" : 2,
    "title" : "A Boolean Logic Question",
    "snippet" : "is there a simple expression to find if exactly 2 or 3 out of 4 inputs are high?\n\nEx. ( In: A,B,C,D Out: Sum(m3, m5, m6, m7, m9, m10, m11, m12, m13, m14))\n\ni think i did that right... min-terms right? /me is only a freshmen CE....   Keep in mind that you can also reverse an output. One thing you can do, if it makes the problem simpler, is craft the expression that looks for either **all four high** or **exactly one high** (a quadruple AND and a quadruple XOR do these quite nicely) and then reverse the value with a prepended NOT &gt;all four high or exactly one high \n\nOr none high.\n\nEDIT: It was my understanding that a quadruple XOR would also yield true for 3 states being true. Am I wrong or misunderstanding? Keep in mind that you can also reverse an output. One thing you can do, if it makes the problem simpler, is craft the expression that looks for either **all four high** or **exactly one high** (a quadruple AND and a quadruple XOR do these quite nicely) and then reverse the value with a prepended NOT ... it sounds so simple when you explain it like that...   In C ^ is an XOR so to do 3/4 high:\n\n!A ^ !B ^ !C ^ !D\n\n\nYou can also cast each to 0/1 and add them to see how many are high.",
    "url" : "http://www.reddit.com/r/compsci/comments/117sla/a_boolean_logic_question/"
  }, {
    "id" : 3,
    "title" : "Understanding the Principles of Algorithm Design | Nettuts+",
    "snippet" : "  &gt;For instance, let’s say you wanted to create an algorithm for adding 1 to any negative number, and subtracting 1 from any positive number, and doing nothing to 0. You might do something like this (in JavaScript-esque pseudo code):\n\n    function addOrSubtractOne(number){\n        if (number &lt; 0) {            //&lt;- Condition 1\n            return number + 1\n        } else if (number &lt; 0) {     //&lt;- Condition 2, same as condition 1\n            return number - 1\n        } else if (number == 0) {\n            return 0;\n        }\n    }\n\nWell, that was a good start to the tutorial. I think it should be like this:\n\n\nfunction addOrSubtractOne(number){\n\n    if (number &lt; 0) {\n\n        return number + 1\n\n    } else if (number &gt; 0) {\n\n        return number - 1\n\n    } else if (number == 0) {\n\n        return 0;\n\n    }\n\n}\n\nSorry for the formatting I don't know how it works in Reddit. I'd ask for trouble and probably do this. ;)\n\n    return number + (number&gt;0 ? -1 : number&lt;0 ? +1 : 0) I'd ask for trouble and probably do this. ;)\n\n    return number + (number&gt;0 ? -1 : number&lt;0 ? +1 : 0)     return (number &gt; 0)*(-1) + number + (+1)*(0 &gt; number);\nBecause why not try put symmetry in things while multiplying by 0? I think it should be like this:\n\n\nfunction addOrSubtractOne(number){\n\n    if (number &lt; 0) {\n\n        return number + 1\n\n    } else if (number &gt; 0) {\n\n        return number - 1\n\n    } else if (number == 0) {\n\n        return 0;\n\n    }\n\n}\n\nSorry for the formatting I don't know how it works in Reddit. &gt;For instance, let’s say you wanted to create an algorithm for adding 1 to any negative number, and subtracting 1 from any positive number, and doing nothing to 0. You might do something like this (in JavaScript-esque pseudo code):\n\n    function addOrSubtractOne(number){\n        if (number &lt; 0) {            //&lt;- Condition 1\n            return number + 1\n        } else if (number &lt; 0) {     //&lt;- Condition 2, same as condition 1\n            return number - 1\n        } else if (number == 0) {\n            return 0;\n        }\n    }\n\nWell, that was a good start to the tutorial. &gt;For instance, let’s say you wanted to create an algorithm for adding 1 to any negative number, and subtracting 1 from any positive number, and doing nothing to 0. You might do something like this (in JavaScript-esque pseudo code):\n\n    function addOrSubtractOne(number){\n        if (number &lt; 0) {            //&lt;- Condition 1\n            return number + 1\n        } else if (number &lt; 0) {     //&lt;- Condition 2, same as condition 1\n            return number - 1\n        } else if (number == 0) {\n            return 0;\n        }\n    }\n\nWell, that was a good start to the tutorial. Are you saying it's good, or not? I found this article of interest, so I posted it here, hoping to see what others thought. I am not expert on algorithms, but trying to improve my understanding. Can you elaborate?\n\nIt looked like it was written at the right level for someone in my position, but maybe it is not the best starting point for learning more about algorithms? Read the comments in that code snippet, you have `number &lt; 0` twice as the if condition.  &gt; Take an hour or two every week and read The Art of Computer Programming\n\nIf comprehension matters, you will only get through a few pages a week. There are far gentler and more focused algorithm design books, such as [Algorithm Design by Kleinberg and Tardos](http://www.amazon.com/Algorithm-Design-Jon-Kleinberg/dp/0321295358), [Algorithm Design by Goodrich and Tamassia](http://www.amazon.com/Algorithm-Design-Foundations-Analysis-Internet/dp/0471383651), and [Data Structures &amp; Algorithms: The Basic Toolbox](http://www.mpi-inf.mpg.de/~mehlhorn/Toolbox.html).\n\nWhy start with material that's intended for an advanced audience to begin with? Cormen? &gt; Take an hour or two every week and read The Art of Computer Programming\n\nIf comprehension matters, you will only get through a few pages a week. There are far gentler and more focused algorithm design books, such as [Algorithm Design by Kleinberg and Tardos](http://www.amazon.com/Algorithm-Design-Jon-Kleinberg/dp/0321295358), [Algorithm Design by Goodrich and Tamassia](http://www.amazon.com/Algorithm-Design-Foundations-Analysis-Internet/dp/0471383651), and [Data Structures &amp; Algorithms: The Basic Toolbox](http://www.mpi-inf.mpg.de/~mehlhorn/Toolbox.html).\n\nWhy start with material that's intended for an advanced audience to begin with?      function naiveSearch(needle, haystack){\n        for (var i = 0; i &lt; haystack.length; i++){ // Theta(N) runtime\n            if (haystack[i] == needle) { return needle; }\n        }\n        return false;\n    }\n\n&gt; Fortunately, we can do better than this for search.\n\n    sortedHaystack = recursiveSort(haystack); // Omega(N log N)\n    function bSearch(needle, sortedHaystack, firstIteration){ // Theta(log N)\n        if (firstIteration){\n            if (needle &gt; sortedHaystack.last || needle &lt; sortedHaystack.first){\n                return false;\n            }\n        }\n        if (haystack.length == 2){\n            if (needle == haystack[0]) {\n                return haystack[0];\n                } else {\n                return haystack[1];\n                }\n        }\n        if (needle &lt; haystack[haystack.length/2]){\n            bSearch(needle, haystack[0..haystack.length/2 -1], false);\n        } else {\n            bSearch(needle, haystack[haystack.length/2..haystack.length], false);\n        }\n    }\n\n&amp;#3232;\\_&amp;#3232; There's a time and place for the correct algorithms, and quicksort/mergesort and binary search are two very powerful algorithms. However if the article intends to talk about when this time and where this place is, it better get things right. Yea, so many things are wrong with this...\n\nIf you are sorting and then doing binary search, this takes longer than naiveSearch. Also, bSearch is incorrect. Both of the base cases (searching an array of size 1 or 2) are broken, which means everything is broken. ",
    "url" : "http://net.tutsplus.com/tutorials/tools-and-tips/understanding-the-principles-of-algorithm-design/"
  }, {
    "id" : 4,
    "title" : "Need help finding a good place to learn about grammar and parsers.",
    "snippet" : "As a personal project, I started to write an automated makefile generator that would parse C files and identify their full dependency list based on the #include macro as well as identify if the program contained int main().\n\nI was able to get the functionality I wanted by implementing an FSM with the rules I wanted, but I'm pretty sure it's not going to be able to handle edge cases.\n\nI know that the best way to go about this would probably be to create some sort of parser that utilizes grammar rules, but as a freshman in Comp Sci, I have no idea where to even begin finding a resource. It seems every source I can find on the topic is written with more discrete math than I understand.\n\nIs there a simple, down-to-earth way to learn about what I'm trying to do? And if so, where? Or am I going to have to take a couple of discrete math courses first.   Check out the GOLD Parser generator.  There's C grammar files for it.\n\nParsing doesn't require any math skills.  They are made of a tokenizer and either a LALR or Recursive Descent parser.  The tokenizer uses regular expressions to grab tokens from the input stream.  Each token has an ID associated with it.  The grammar is written using token IDs.  Basically, at runtime, the parser will fire an event when it recognizes a grammar construct, such as an if/then statement.  Each element of the statement can be retrieved programmatically, as the original lexer tokens will be associated with the grammar IDs.\n\nAlthough it's pretty dry to read, the Dragon Book is a good source.  Or just Google it.  There's tons of resources online.  To create a lexer (the first step), you will convert regular expressions first to an NFA, then to a DFA.  Optionally, you can optimize the DFA.  Creating an NFA from a regex is pretty simple, just Google it.\n\nI would suggest messing with recursive descent parsing first.  LALR is pretty hard to wrap your head around if you're knew to the whole thing.\n\n\nGood luck! Oh no. Not gold. Gold does not create a parser for you. It spits out a string representation and draws derivations, nothing you can use. For LALR go JavaCUP. What?  Yes it does.  It doesn't create a parser in the same way as Flex/Bison does.  Those create source code that you compile into your project.  GOLD creates the DFA for the tokenizer and the parse table for the LALR engine, which one of the selected runtime engines loads at runtime.  GOLD is nice in that it has a visual debugger.  It's easy to see how your grammar is working in real-time.  Source:  I'm a contributor to the GOLD project.  I also surpassed it and wrote my own similar parser generator which is much more powerful. Gold creates Gold specific grammar files and tables, you cannot access it and it is very \"magician\"-y. I would hardly recommend it to anyone looking to use a parser for anything more than just educational purposes. Though OP can use it to illustrate and get a name on what language class his grammar is absolutely part of, he will switch to another generator (like JavaCUP) for the actual parser-making. Cut out the middle man and go straight for the actual generator, is my advice.\n\n\n\n\n---\nSource: I made a compiler that took C-lite and spat out assembly last semester, and I wrote to the guy who started the GOLD project about why the fuck I got no source output using GOLD - confirming me that it doesn't and is thus not usable for compiler / parser building. This was very unclear on the GOLD website.\n---\n\n\nPS: The engines GOLD use are incredibly outdated, sucky documented and an overall pain to even understand. I don't think you fully understand what GOLD does.  The \"source\" for GOLD is the dozens of run-time engines (there's one for pretty much every language).  Every one of those engines can read the GOLD output tables and parse.  That is what a parser is, and GOLD is perfectly suited for building a compiler/parser.  Further, there are GOLD grammars to parse many different languages, so you can parse real C and not just C-lite.  All you do is include the source for the appropriate run-time, call init(\"filename.dat\") (or whatever the function/filename is), then call parse() or something.  It's very easy to do.  If you want to understand how the LALR stuff works, the GOLD gui is better than most tools I've seen.\n\nIf you're think that Flex/Bison are somehow better because you get source ... you're WAY better off with a utility that lets you step through your parsing than debugging a grammar by stepping through source code.  Have you ever looked at Flex/Lex output source code?  It's not human readable in any way, shape, or form.\n\nHaving said this, I don't use GOLD myself, as I asked him to create some special \"commands\" in the syntax for thinks like \"take longest match\", \"take shortest match\", \"skip to a new state...\" as well as defining lexer and parser \"states\" which lets you easily parse any grammar.  He didn't want to add that stuff so I wrote my own that does all that stuff.  However, GOLD is a good solution for most people.\n\n\nDoes JavaCUP output java-based parsers, or for any language?  Because there aren't a lot of parser generators that can spit out more than one or two languages. GOLD simulates a parster, it does not create an actual parser you can use, to create your own programming language - for example, as was what I did last semester (C-lite is my codename for a language I made - I think you got my point confused).\n\n---\nJflex is a scanner generator and sure, while the source code is very... Fluffy - lol, the macros and token specification you feed it, is insanely simple. JavaCUP generates a parser and works well with JFlex - it's as easy as 1, 2, 3. You write the parser in Java, but you can give it a language specification of anything, so I am not sure what you mean by your question - feed it C tokens / grammar and you got yourself a C parser written in Java.\n---\n\n\nMy argument is, GOLD is good for illustrating a grammar, and it sucks at everything else. I have no more to add to this, than I haven't already stated. &gt; GOLD simulates a parster, it does not create an actual parser you can use, to create your own programming language - for example, as was what I did last semester (C-lite is my codename for a language I made - I think you got my point confused).\n\nNo no no.  It creates parse tables which are loaded by one of the many parse engines to create a full-fledged parser.  I think you misunderstood what GOLD does.  I promise you it creates a full-fledged LALR driven parser.  I used it to implement a programming language with a virtual machine and everything.  I wrote the compiler using GOLD as the parser.\n\n\nWhat I mean by the Java question is that I'm limited to creating compilers in Java using JavaCUP.  If I use Flex/Bison, I'm limited to C/C++.  If I use GOLD, I can use Java,C/C++,.Net,PHP, etc.  As well as any language that I am able to write a GOLD runtime for. Good luck using the java engine to make a parser with gold. If you can, make it an open source project to optimize the documentation so it is useful and as easy to learn as the competition. You're probably right that the runtime isn't well documented.  I haven't looked at the java one.  The one I did (I'll tell you which one over PM if you want to know because it will give away my real name), I did because one didn't exist for that language (this was 10 years ago), so I wrote it from the documentation for the parser table files.  The file format *is* very well documented.  When I wrote mine, I included a simple example program that has a grammar file as well as the compiled grammar tables.  It creates an instance of the parser, loads the parse tables, and prints out what it parses.  It also included a sample file to parse.  So using that one, you'd be able to get a parser working in &lt;5 minutes.  I can't speak for the other engines, it sounds like the documentation sucks lol.  I'm not trying to push GOLD as I said it came up short for my needs, but it is a good thing for a noob to get their feet wet.  I'm also not saying that JavaCUP sucks because I've never used it, and you said it's good.  I'm just saying GOLD does a little more than you got the impression that it does. Check out the GOLD Parser generator.  There's C grammar files for it.\n\nParsing doesn't require any math skills.  They are made of a tokenizer and either a LALR or Recursive Descent parser.  The tokenizer uses regular expressions to grab tokens from the input stream.  Each token has an ID associated with it.  The grammar is written using token IDs.  Basically, at runtime, the parser will fire an event when it recognizes a grammar construct, such as an if/then statement.  Each element of the statement can be retrieved programmatically, as the original lexer tokens will be associated with the grammar IDs.\n\nAlthough it's pretty dry to read, the Dragon Book is a good source.  Or just Google it.  There's tons of resources online.  To create a lexer (the first step), you will convert regular expressions first to an NFA, then to a DFA.  Optionally, you can optimize the DFA.  Creating an NFA from a regex is pretty simple, just Google it.\n\nI would suggest messing with recursive descent parsing first.  LALR is pretty hard to wrap your head around if you're knew to the whole thing.\n\n\nGood luck! Thanks! I'll look into those!        Are you aware of Cmake? Also Clang? Clang already has a tool for generating header dependancies, and cmake is great for generating makefiles. Im not sure if these tools cover everything you desire (or if you just want to do it anyway for personal gain), but I wouldnt want you to put a lot of effort into this and then find something that already does it. I know I'm reinventing the wheel, but it's really just an excuse to go put into practice what I've just finished learning. It's a simple idea since it's really just a single feature, but gives me the opportunity to practice working with all sorts of algorithms and data structures in a practical application.",
    "url" : "http://www.reddit.com/r/compsci/comments/115592/need_help_finding_a_good_place_to_learn_about/"
  }, {
    "id" : 5,
    "title" : "Recursive Drawing with Video Demonstration",
    "snippet" : "  Why would you link to the completely unintelligible draw.html page when the website has a perfectly good explanation video and instruction set?\n\nhttp://recursivedrawing.com/\n\nEdit: Sorry for being a dick.  It really is a cool site, thank's for sharing :)   What do you mean by \"recursive\"?     ",
    "url" : "http://recursivedrawing.com/draw.html"
  }, {
    "id" : 6,
    "title" : "Tulip 4.0, a graph visualization software sees its 4.0 release (x-post from /r/programmming)",
    "url" : "http://tulip.labri.fr/TulipDrupal/?q=node/1891"
  }, {
    "id" : 7,
    "title" : "Deutsch: The laws of physics imply that artificial intelligence must be possible. What's holding us up?",
    "snippet" : "  There's a huge leap between \"artificial intelligence must be possible,\" and \"understanding in detail how it works must be possible.\"  By all appearances, our brains are an enormous conglomerate of hacks and spaghetti wiring. There may be no simpler design, and ultimately AI may *need* to be generated by some sort of evolutionary process (artificial or not), and even if produced by us in a lab may remain mysterious to us (i.e., there's no explanation simple enough for even our brightest scientists to make any sense of). The laws of physics say that inter-stellar slower than light travel must be possible. Why aren't we on Alpha centauri yet?  There's a huge leap between \"artificial intelligence must be possible,\" and \"understanding in detail how it works must be possible.\"  By all appearances, our brains are an enormous conglomerate of hacks and spaghetti wiring. There may be no simpler design, and ultimately AI may *need* to be generated by some sort of evolutionary process (artificial or not), and even if produced by us in a lab may remain mysterious to us (i.e., there's no explanation simple enough for even our brightest scientists to make any sense of). &gt; By all appearances, our brains are an enormous conglomerate of hacks and spaghetti wiring. \n\nNot only is the brain's mode of computation different from anything that we etched into silicon (though both seem to be universal) - it also has the ability to manipulate its own hardware. Self modifying code could be the answer, but it comes with overhead, which implies you'd have to increase the complexity of the computation substrate beyond the levels of an equivalent powerful brain. And if the program makes a mistake it runs into the risk of generating an endless loop or even worse artifacts. Self modifying code is not unprecedented, if not well understood. It was for this attribute that Lisp was (is?) so popular. I've never entirely understood this association with Lisp (unless it's a conflation with macros). Isn't any interpreter that can modify it's own instruction set (i.e., any interpreter written in any language ever) at least capable of self-modifying code?   Technically, yes. I don't know of a language that is better at it than Lisp, though. Most languages attempt to implement some form of self-modification as an afterthought, whereas Lisp has it as a core principle. Lisp has far less overhead and much more flexibility in this regard. I still don't get it.\n\nHow is Lisp (or specifically CL, Scheme, or Clojure) specially suited to this?\n\nHow does Python or Javascript involve more overhead or less flexibility. You can't redefine Java binary operators, and you'd be have a very difficult time writing a Java interpreter in Java. Python is slightly better in this regard. Though it's true that both languages have libraries for ASTs, reflection, etc., it's much more verbose than it is in Lisp, and much slower. If you want more evidence, you'll have to to Google some benchmarks and examples of self-evaluating interpreters. Redefining operators in Java is more equivalent to using lisp macros.\nI've used Scheme and Clojure for years, but I've never even encountered any examples of self-modifying-code in either of them. \nThat, and while the name \"self-modifying-code\" sounds pretty hairy, it's equivalent to interpreting some data, changing it, and then reinterpreting it (which also sounds pretty hairy) but certainly any language can do that. Have you encountered many Java or Python self-interpreters? I can only think of a few. Yet in Lisp, a relative newbie can code one in a few hundred LOC. It's true that all languages should be capable of parsing their own syntax (excepting esoteric ones) and modifying program structure from there, but the difference lies in that Lisp is already written in its own primary data structure.\n\nI won't preach to you about it, since I've had much less exposure to Lisp, but I find it hard to believe that I could write a Java compiler (with decent optimization) in Java more easily than in Lisp. &gt; By all appearances, our brains are an enormous conglomerate of hacks and spaghetti wiring. \n\nNot only is the brain's mode of computation different from anything that we etched into silicon (though both seem to be universal) - it also has the ability to manipulate its own hardware. Self modifying code could be the answer, but it comes with overhead, which implies you'd have to increase the complexity of the computation substrate beyond the levels of an equivalent powerful brain. And if the program makes a mistake it runs into the risk of generating an endless loop or even worse artifacts.  His criticism is good, but when he begins to speak about his own ideas...\n\n&gt; Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\n\nThat may be, but here's the thing: humans do not pop out of vaginas with the ability to create theories by conjecture and criticism. In fact they don't learn to use this ability properly, fully consciously, for like five to ten years. And they don't acquire it in a sudden flash of enlightenment, but gradually, and not always even completely. So this ability can't be all that \"qualitatively different\", either you have it or not, as he makes it to be.\n\nOn one hand this means that it makes sense to learn to make a fruit-fly-level intelligence first, before an ape-like, and toddler-like, and maybe then it would become clear how to move from that to human-level intelligence. It happens in nature that way, so it probably could be reproduced.\n\nOn the other hand his optimism regarding the possibility of bypassing all the intermediate steps and propelling ourselves straight to AGI with a purely philosophical breakthrough seems quite unfounded.\n\n&gt; Clearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees.\n\nGuys, you guys, I have a better idea, let's examine the differences between the DNA of toddlers and grownups, the answer _must_ be there, guys? &gt; On one hand this means that it makes sense to learn to make a fruit-fly-level intelligence first, before an ape-like, and toddler-like, and maybe then it would become clear how to move from that to human-level intelligence.\n\nI think this  is a mistake. The nature of the intelligence in all of these species probably isn't all that different. All animals have genetically predisposed behaviours, and learned behaviours from interacting with our environment during which we ingrain predictions about how our environment will respond to various stimuli. Intelligence is in the latter category.\n\nThe critical point is figuring out the algorithm responsible for processing feedback with memory to produce these predictions. In a sense, this really is a purely logical/philosophical problem. &gt; The critical point is figuring out the algorithm responsible for processing feedback with memory to produce these predictions. In a sense, this really is a purely logical/philosophical problem.\n\nThat's exactly what rubs me wrong. The implication that there's _the_ algorithm that does it, and an algorithm in the traditional sense, something that you can express precisely rather than grow.\n\nLike, it seems to me that Deutsch (and you) are trapped in the same seventies' mindset that produced [SHRDLU](http://en.wikipedia.org/wiki/SHRDLU), except the author of that thought that he can bruteforce his way to a general AI, while Deutsch believes that that approach misses a crucial philosophical ingredient, but if we had that, we could make that approach work.\n\nI don't believe that at all. Interesting algorithms in the Nature (and, increasingly, in the programming proper) are fundamentally different, they have layers, man. You (possibly) think of an algorithm as an amplifier and extension of the programmer's mind, kind of like an excavating machine is a tool in the operator's hands. Like, if you wanted to paint a tree in Photoshop, instead of painting it all by hand you can write a function for painting a leaf, a function for painting a piece of trunk, a function for painting a branch calling the above two and itself recursively, but in the end every pixel could be traced to the programmer's decisions.\n\nConsider the heart of Google's search algorithm, the [Page rank](http://en.wikipedia.org/wiki/Page_rank). There are _two_ algorithms there, really. The one created by Google programmers implements the transfer of page rank between pages. The one that was emphatically __not__ implemented by the Google programmers is the actual configuration of webpages, and it's _that_ which gives pages so and so pagerank. You can't come to a programmer who've implemented the framework necessary for the actual algorithm to work and expect him to tell why this website got such and such pagerank. No more than you can come to a brick-maker and ask where's the bathroom in a house built from his bricks.\n\nSo yeah, it seems to me that there's no such thing as an \"intelligence-enabling algorithm\", in the traditional sense of the word \"algorithm\". The thing is an emergent phenomenon that is grown, by interactions, not created from DNA blueprints or anything like that. &gt; The implication that there's the algorithm that does it, and an algorithm in the traditional sense, something that you can express precisely rather than grow.\n\nA neural, genetic or hill climbing solution is simply an algorithm you don't understand.\n\nAs for the rest of your post, you seem to misunderstand the definition of an algorithm. Your example of Google's \"two algorithms\". Firstly, the configuration of pages are simply data, not an algorithm. Secondly, algorithms can be viewed at many levels of abstraction. You have algorithms that deal with primitive entities like integers, then you have algorithms that deal with systems of other algorithms, like Paxos. Taken all together, a system composed of multiple algorithms communicating via Paxos is as a whole, just another algorithm.\n\nFinally, regarding emergent phenomena, I already explained that the algorithm will entail processing memory in the presence of constant feedback. You may very well have to \"train\" such an algorithm in order to build it's memory, but training does not necessarily entail actual interaction with the world. All you need is a script of real world interaction that performs the training. This too is yet another algorithm. &gt; The critical point is figuring out the algorithm responsible for processing feedback with memory to produce these predictions. In a sense, this really is a purely logical/philosophical problem.\n\nThat's exactly what rubs me wrong. The implication that there's _the_ algorithm that does it, and an algorithm in the traditional sense, something that you can express precisely rather than grow.\n\nLike, it seems to me that Deutsch (and you) are trapped in the same seventies' mindset that produced [SHRDLU](http://en.wikipedia.org/wiki/SHRDLU), except the author of that thought that he can bruteforce his way to a general AI, while Deutsch believes that that approach misses a crucial philosophical ingredient, but if we had that, we could make that approach work.\n\nI don't believe that at all. Interesting algorithms in the Nature (and, increasingly, in the programming proper) are fundamentally different, they have layers, man. You (possibly) think of an algorithm as an amplifier and extension of the programmer's mind, kind of like an excavating machine is a tool in the operator's hands. Like, if you wanted to paint a tree in Photoshop, instead of painting it all by hand you can write a function for painting a leaf, a function for painting a piece of trunk, a function for painting a branch calling the above two and itself recursively, but in the end every pixel could be traced to the programmer's decisions.\n\nConsider the heart of Google's search algorithm, the [Page rank](http://en.wikipedia.org/wiki/Page_rank). There are _two_ algorithms there, really. The one created by Google programmers implements the transfer of page rank between pages. The one that was emphatically __not__ implemented by the Google programmers is the actual configuration of webpages, and it's _that_ which gives pages so and so pagerank. You can't come to a programmer who've implemented the framework necessary for the actual algorithm to work and expect him to tell why this website got such and such pagerank. No more than you can come to a brick-maker and ask where's the bathroom in a house built from his bricks.\n\nSo yeah, it seems to me that there's no such thing as an \"intelligence-enabling algorithm\", in the traditional sense of the word \"algorithm\". The thing is an emergent phenomenon that is grown, by interactions, not created from DNA blueprints or anything like that. His criticism is good, but when he begins to speak about his own ideas...\n\n&gt; Thinking of an AGI as a machine for translating experiences, rewards and punishments into ideas (or worse, just into behaviours) is like trying to cure infectious diseases by balancing bodily humours: futile because it is rooted in an archaic and wildly mistaken world view.\n\nThat may be, but here's the thing: humans do not pop out of vaginas with the ability to create theories by conjecture and criticism. In fact they don't learn to use this ability properly, fully consciously, for like five to ten years. And they don't acquire it in a sudden flash of enlightenment, but gradually, and not always even completely. So this ability can't be all that \"qualitatively different\", either you have it or not, as he makes it to be.\n\nOn one hand this means that it makes sense to learn to make a fruit-fly-level intelligence first, before an ape-like, and toddler-like, and maybe then it would become clear how to move from that to human-level intelligence. It happens in nature that way, so it probably could be reproduced.\n\nOn the other hand his optimism regarding the possibility of bypassing all the intermediate steps and propelling ourselves straight to AGI with a purely philosophical breakthrough seems quite unfounded.\n\n&gt; Clearing this logjam will not, by itself, provide the answer. Yet the answer, conceived in those terms, cannot be all that difficult. For yet another consequence of understanding that the target ability is qualitatively different is that, since humans have it and apes do not, the information for how to achieve it must be encoded in the relatively tiny number of differences between the DNA of humans and that of chimpanzees.\n\nGuys, you guys, I have a better idea, let's examine the differences between the DNA of toddlers and grownups, the answer _must_ be there, guys? &gt; humans do not pop out of vaginas with the ability to create theories by conjecture and criticism.\n\nThis is off-base. Sure, newborns don't understand much regarding what they're experiencing or doing, but they *do* try to interpret things. That interpretation (theory) requires guesswork (conjecture) and the ability to recognize and/or correct mistakes (criticism).  Any progress made in developing satisfactory interpretations about the world is due to this process, whether we understand what we are doing or not.\n And toddlers do that in a way that's qualitatively different from what apes do?  I did not like this article. For one, I think the disrespect toward the accomplishments made in AI is untrue and unwarranted. I also don't see any basis for his claims that AGI will require a fundamental new insight/shift in thinking.\n\nAn ant has about 250,000 neurons, meaning that just to write down an adjacency matrix describing its connections would require something like 250 GB. Figuring out how to actually simulate it would require orders of magnitude more computing power than that.\n\nThat's an *ant*. I doubt Deutsch would consider a simulation of an ant to be true AGI, and we're probably barely capable of that right now. So I don't see why he dismisses the argument that our hardware isn't yet up to the challenge.\n\nTo me it sounds like Deutsch, as smart of a guy as he is, seems to vastly underestimate the incredible hardware that is our brains.\n\nEdit: After thinking about it, I'd phrase my objection like this. Deutsch seems to think there is something amazingly special that separates human thinking from, say, that of mice. But we are still very far even from building computers as smart and adaptive to new situations as mice are (as far as I know). So this special human factor can't be the bottleneck; not yet. http://m.spectrum.ieee.org/computing/hardware/ibm-unveils-a-new-brain-simulator\n\nCat brain in 2009 That's super cool.\n\n&gt;  Dawn is one of the most powerful and power-efficient supercomputers in the world, but it takes 500 seconds for it to simulate 5 seconds of brain activity, and it consumes 1.4 MW.\n\nOuch. I wonder also what they found with it? What kinds of tasks could it solve? I did not like this article. For one, I think the disrespect toward the accomplishments made in AI is untrue and unwarranted. I also don't see any basis for his claims that AGI will require a fundamental new insight/shift in thinking.\n\nAn ant has about 250,000 neurons, meaning that just to write down an adjacency matrix describing its connections would require something like 250 GB. Figuring out how to actually simulate it would require orders of magnitude more computing power than that.\n\nThat's an *ant*. I doubt Deutsch would consider a simulation of an ant to be true AGI, and we're probably barely capable of that right now. So I don't see why he dismisses the argument that our hardware isn't yet up to the challenge.\n\nTo me it sounds like Deutsch, as smart of a guy as he is, seems to vastly underestimate the incredible hardware that is our brains.\n\nEdit: After thinking about it, I'd phrase my objection like this. Deutsch seems to think there is something amazingly special that separates human thinking from, say, that of mice. But we are still very far even from building computers as smart and adaptive to new situations as mice are (as far as I know). So this special human factor can't be the bottleneck; not yet. Your ant example might be completely wrong. If an ant's brain has duplicate structures then it would be possible to use both less space and possibly less computation. a very complicated neural  circuit might be doing something very simple, and it might be repeated all over the place.  \n\nThe first stage or so of frogs' vision systems are well characterized and have a lot of neurons, yet we can get the same input output relationship in a way that is computationally much cheaper than a naive simulation of the neurons. I did not like this article. For one, I think the disrespect toward the accomplishments made in AI is untrue and unwarranted. I also don't see any basis for his claims that AGI will require a fundamental new insight/shift in thinking.\n\nAn ant has about 250,000 neurons, meaning that just to write down an adjacency matrix describing its connections would require something like 250 GB. Figuring out how to actually simulate it would require orders of magnitude more computing power than that.\n\nThat's an *ant*. I doubt Deutsch would consider a simulation of an ant to be true AGI, and we're probably barely capable of that right now. So I don't see why he dismisses the argument that our hardware isn't yet up to the challenge.\n\nTo me it sounds like Deutsch, as smart of a guy as he is, seems to vastly underestimate the incredible hardware that is our brains.\n\nEdit: After thinking about it, I'd phrase my objection like this. Deutsch seems to think there is something amazingly special that separates human thinking from, say, that of mice. But we are still very far even from building computers as smart and adaptive to new situations as mice are (as far as I know). So this special human factor can't be the bottleneck; not yet. I did not like this article. For one, I think the disrespect toward the accomplishments made in AI is untrue and unwarranted. I also don't see any basis for his claims that AGI will require a fundamental new insight/shift in thinking.\n\nAn ant has about 250,000 neurons, meaning that just to write down an adjacency matrix describing its connections would require something like 250 GB. Figuring out how to actually simulate it would require orders of magnitude more computing power than that.\n\nThat's an *ant*. I doubt Deutsch would consider a simulation of an ant to be true AGI, and we're probably barely capable of that right now. So I don't see why he dismisses the argument that our hardware isn't yet up to the challenge.\n\nTo me it sounds like Deutsch, as smart of a guy as he is, seems to vastly underestimate the incredible hardware that is our brains.\n\nEdit: After thinking about it, I'd phrase my objection like this. Deutsch seems to think there is something amazingly special that separates human thinking from, say, that of mice. But we are still very far even from building computers as smart and adaptive to new situations as mice are (as far as I know). So this special human factor can't be the bottleneck; not yet.  I don't quite understand why he focuses on computers not being able to create explanations. Because they can. There are proof algorithms that produce the intermediary steps that lead to a final result. I believe that counts as creating a new explanation.\n\nNow we certainly don't have anything as sophisticated as taking arbitrary human language input and proving that but doesn't that merely reflect the complexity of the human language rather then showing how computers can't create explanations.    &gt; That AGIs are people has been implicit in the very concept from the outset. If there were a program that lacked even a single cognitive ability that is characteristic of people then by definition it would not qualify as an AGI.\n\nNonsense. An agent that lacked, say, face-recognition, but was able to carry on a good conversation via a terminal and was able to solve lots of problems in diverse domains and write a poem about cherry trees  would be an AGI. But isn't face-recognition just a special application of pattern recognition? Isn't pattern-recognition itself the general solution to nearly everything else we do? Without pattern recognition, an agent can't carry on a meaningful conversation, and it can't solve problems in any domain unless you hold its hand through every step. That's a good point. I'm not sure I agree that lack of face recognition would imply lack of a general pattern recognition ability. For a trivial case, what if the AGI has never been plugged into a camera? Saying that it's not an AGI would be a bit like saying that a blind person isn't intelligent.\n\nAnyway, I only picked facial recognition because it's one of the cognitive abilities that is strongly tied to our social nature. Those are good candidates for abilities which a non-human intelligence might not need or want. Here's another example: humans are better at detecting rule violations when they are framed as \"cheating\" in social situations, rather than violations of abstract rules. We have special circuitry for detecting social cheating. ([Wason Selection Test.](http://www.psychologytoday.com/blog/fulfillment-any-age/201202/train-your-brain-simple-exercise)) An agent that lacked that special circuitry could still be an AGI.\n\nMore broadly, I think that using *human* cognitive abilities in the definition of artificial general intelligence is a bit parochial. We ought to be prepared for very foreign-seeming AGIs which don't fully share our assumptions, our abilities, our limitations, or our goals. What I meant was that facial recognition can be generalized as pattern recognition; an AGI that can't *learn* to recognize faces has issues with pattern recognition. It can be an AGI if it can't recognize faces. It can't be an AGI without the ability to eventually recognize faces if that ability is useful or necessary to it.\n\nIn the trivial case, the AGI would either be able to learn how to use the visual input by generalizing whatever pattern recognition it does use (like a human) or it could be taught. That is the definition of an AGI; an artificial intelligence that has human-like cognitive abilities.\n\nI agree with you: an AGI that evolves (per se) on its own, with its own desires and requirements, will likely be very different from us. But there are some core abilities that we must accept that an AGI must have, if for no reason other than knowing whether we have created one or not.\n\n1. It must be able to create new information.\n2. It must be able to categorize and store information for later use.\n3. It must be able to generalize known information to accept unknown information.\n\nI'm not a professional; I don't even have a degree, and I'm nowhere near an expert on anything. That list is my layman's point of view. Feel free to correct me, add to the list, or remove items (but please explain, so I can learn). Those three things look, to me, to be the hallmarks of an intelligence. The enable everything we do or can learn to do. An AGI that can't learn is simply programmed; and AGI that can't remember is defective.      we don't understand how our brains work yet... So what? You don't need that to create AI... So what? You don't need that to create AI... you do if you want to create an ai that will passing the turing test... You need to understand how the human brain works in order to create an AI which does the same thing that the human brain does.\n\nYou do not need to understand how the human brain works in order to create an AI which does not do the same thing the human brain does. then this all depends on what your definition of intelligence is...",
    "url" : "http://www.aeonmagazine.com/being-human/david-deutsch-artificial-intelligence/"
  }, {
    "id" : 8,
    "title" : "The $5000 Compression Challenge",
    "snippet" : "  Although Patrick clearly won the bet as worded - Mike screwed up by allowing multiple files with no explicit caveats - he did not in fact \"compress\" the data.\n\nTo properly run this challenge, you'd want to require submissions to be a single self-extracting file. If you can get compression by splitting things into pieces, great, but you've got to store those pieces efficiently yourself. &gt; To properly run this challenge, you'd want to require submissions to be a single self-extracting file.\n\nThere are tons of other rules you'd need to 'properly' run the challenge. Don't forget about limits on the file name itself.\n\nThe hilarious thing about this is that the guy was so arrogant about his challenge that he didn't see the holes in his rules.  Although Patrick clearly won the bet as worded - Mike screwed up by allowing multiple files with no explicit caveats - he did not in fact \"compress\" the data.\n\nTo properly run this challenge, you'd want to require submissions to be a single self-extracting file. If you can get compression by splitting things into pieces, great, but you've got to store those pieces efficiently yourself. Also, to limit what OS calls the program is allowed to make, to avoid situations where it uses information from the environment to reproduce some of the data (e.g. download it from the internet). Not to mention, a limit of the OS it runs on.  I could make a custom OS that encodes the file you want really efficiently. Although Patrick clearly won the bet as worded - Mike screwed up by allowing multiple files with no explicit caveats - he did not in fact \"compress\" the data.\n\nTo properly run this challenge, you'd want to require submissions to be a single self-extracting file. If you can get compression by splitting things into pieces, great, but you've got to store those pieces efficiently yourself. Mike did not allow mulitple files. He specifically said that the contestant had to send him two files. 1. A decompressor and 2. a compressed file.  \nI'm clearly missing something.  \nEdit: I see later he responded to the contestant the he could send him mulitple files, but that was clearly not the bet in the first place.   I love the solution he came up with. Hilarious. \n\nFor those that haven't read through the whole thing: \n\nPatrick took the 3mb file and broke it up into 218 pieces, each of them ending with a 5. He then removed the 5 from the end of all of those files, and his decompressor was a very simple script that put all the 5s back at the end of the files and combined them all together.   If he was required to include the filesystem overhead in the total byte calculation, then making use of the filename to store part of the data should have been allowed within the rules.\n\nAlso, I feel that he was never going to get paid no matter what. Had he completed it in some other way (say his \"gunzip $1\" script) then he would have lost due to the fact that the size of gzip needed to be included in the calculation also. Rules were going to be added to ensure he never won the challenge. If he was required to include the filesystem overhead in the total byte calculation, then making use of the filename to store part of the data should have been allowed within the rules.\n\nAlso, I feel that he was never going to get paid no matter what. Had he completed it in some other way (say his \"gunzip $1\" script) then he would have lost due to the fact that the size of gzip needed to be included in the calculation also. Rules were going to be added to ensure he never won the challenge. If he was required to include the filesystem overhead in the total byte calculation, then making use of the filename to store part of the data should have been allowed within the rules.\n\nAlso, I feel that he was never going to get paid no matter what. Had he completed it in some other way (say his \"gunzip $1\" script) then he would have lost due to the fact that the size of gzip needed to be included in the calculation also. Rules were going to be added to ensure he never won the challenge. This is why you need an escrow service to hold the $5000 and a neutral arbitrator to evaluate the submission based on the agreed upon rules when starting the competition.   Is this challenge still up? Yes, and it will stay up, because it's an impossible challenge. The whole point is to illustrate to people that you can't compress random data. Sure but this pretty well shows that just because compressing random data is impossible doesn't mean the challenge is. If he's unwilling to pay up if someone figures out a clever (but perfectly valid within the rules) loophole that he didn't think of, he really needs to take the challenge down. Otherwise he's not illustrating jack. [deleted] Please don't make antisemitic (or otherwise blatantly prejudiced) in this subreddit. It adds nothing to the discussion. Yes, and it will stay up, because it's an impossible challenge. The whole point is to illustrate to people that you can't compress random data. Also, the author of the challenge will evade any sort of useful trick you try to pull.\n\nFor instance:\n\n* I send author $100, he sends me a seed file\n\n* I write a decompression tool in &lt;language X, to be released soon. Maybe something like perl 6&gt;, leveraging a builtin compression/decompression whose implementation is stored in a library file in /usr/lib or whatever.\n\n* &lt;I cheat and as a contributer to perl 6, I upload his seed file as part of a dictionary coder in the built-in subroutine&gt; (this step has an astronomically small chance of occurring without my intervention, but is not impossible)\n\n* After perl 6 is released, I send my perl 6 executable file:\n\n        #!/usr/bin/env perl6\n        decompress_file_in_place($ARGV[1]) #perl 5 syntax, btw\n\n\nAnd lo and behold, the dictionary has compressed his down arbitrarily small. \n\nWould you consider that a win, a loss, or a cheat?\n\nWhat if I wasn't the one who put his file into the dictionary?\n\nIf it's a cheat, how could I possibly use any program that uses other programs/files without possibly having liability for this exploit?\n\nWhat if I had done that exploit to bash itself?  How could I write any shell scripts at all?\n\nWhat about operating system calls?  Do I get in trouble if the execution of my code happens to call into the arbitrarily large x86 opcodes of my OS?  Do those bytes count against me? Also, the author of the challenge will evade any sort of useful trick you try to pull.\n\nFor instance:\n\n* I send author $100, he sends me a seed file\n\n* I write a decompression tool in &lt;language X, to be released soon. Maybe something like perl 6&gt;, leveraging a builtin compression/decompression whose implementation is stored in a library file in /usr/lib or whatever.\n\n* &lt;I cheat and as a contributer to perl 6, I upload his seed file as part of a dictionary coder in the built-in subroutine&gt; (this step has an astronomically small chance of occurring without my intervention, but is not impossible)\n\n* After perl 6 is released, I send my perl 6 executable file:\n\n        #!/usr/bin/env perl6\n        decompress_file_in_place($ARGV[1]) #perl 5 syntax, btw\n\n\nAnd lo and behold, the dictionary has compressed his down arbitrarily small. \n\nWould you consider that a win, a loss, or a cheat?\n\nWhat if I wasn't the one who put his file into the dictionary?\n\nIf it's a cheat, how could I possibly use any program that uses other programs/files without possibly having liability for this exploit?\n\nWhat if I had done that exploit to bash itself?  How could I write any shell scripts at all?\n\nWhat about operating system calls?  Do I get in trouble if the execution of my code happens to call into the arbitrarily large x86 opcodes of my OS?  Do those bytes count against me? Yes, and it will stay up, because it's an impossible challenge. The whole point is to illustrate to people that you can't compress random data. Well as the number of people who participate in the challenge goes to infinity, the probability of getting a file that contains all 1's reaches 1. So...let's just keep trying for a few billion years. Yes, and it will stay up, because it's an impossible challenge. The whole point is to illustrate to people that you can't compress random data. That is true. What is also true is that he will receive a lot of money from reddit now :)\n\nCan you please provide a link? since he tried to weasel out of his own mistake, he's permanently lost bettings rights in my book.\n\nI highly doubt Reddit is stupid enough to throw their money away on someone who is just going to steal it. [deleted]  Realistically, this is not an impossible challenge.  From an information theoretic perspective, clearly it is impossible to write a compression algorithm that compresses every possible file by at least one bit.  But that's not what the challenge is asking for.  The challenge is asking for a program that compresses a *given* file by at least one bit.  Now the challenge boils down to whether Mike Goldman can generate an algorithmically random string of a given length.  He can do this with high probability, but it's not feasible to generate a string that's guaranteed to be algorithmically random. Afraid not - else, your process (your 'algorithm') of making an executable + compressed.file of a combined size less than the original file _is_ a compression algorithm. \n\nOr are you saying that it is merely possible to build a executable that can decompress a single smaller file than the original? That's easy - thats why the answer needs to include the sizes of both files. I think the point is that given some (possibly random) data *d*, it is entirely possible that one can find some program *M* and input *c* such that *M(c) = d* and the size of *M* plus the size of *c* is less than the size of *d*.\n\nHowever, it is **not** possible that one could create a program *N* which takes an arbitrary *d* and creates a corresponding *M* and *c*.\n\n(In the same sense that I might be able to write a program which can tell if a particular program every halts, but I cannot write a program which can answer this for all possible input programs.) From GP:\n\n&gt; Realistically, this is not an impossible challenge.\n\nIt is certainly possible that it could be solved for a sufficiently poor body of data, but I was attempting to point out that 'realistically', someone given one chance with an arbitrarily sized file, it wasn't going to happen. \n\nCertainly, some of the time, random data _might_ be regular enough that one could compress it, but this case is far from probable. I think the point is that given some (possibly random) data *d*, it is entirely possible that one can find some program *M* and input *c* such that *M(c) = d* and the size of *M* plus the size of *c* is less than the size of *d*.\n\nHowever, it is **not** possible that one could create a program *N* which takes an arbitrary *d* and creates a corresponding *M* and *c*.\n\n(In the same sense that I might be able to write a program which can tell if a particular program every halts, but I cannot write a program which can answer this for all possible input programs.) The problem is that d is arbitrary: its chosen by Mike.  Information theory says that for most d of size s(d), there is no M and c such that M(c)=d and s(M)+s(c)&lt;s(d).  No such M and c exist. I agree with you up to the point that you said \"No such M and c exist\".\n\nNo such M and c exist for *most* choices of d.  Mike can't determine whether M and c exist for a *given* choice of d.  He cannot generate a d such that M and c are known not to exist. Its just as difficult for him to verify that as it is for Jeremy to find m and c for that d No, they are solving opposite problems.  Mike wins if he can verify that for all M and c with s(M) + s(c) &lt; s(d), M(c) != d.  Jeremy wins if he can prove that there exists an M and c such that s(M) + s(c) &lt; s(d) and M(c) = d.  Jeremy's problem is semi-decidable, but Mike's is not.\n\nA (slightly) more realistic argument is that Mike has bounded computational resources (since he has to produce a string within some time limit), whereas Jeremy does not.\n\nI'm not saying that it's likely that Jeremy will win.  It's highly probable that Mike will generate an algorithmically random sequence, even if he can't verify that it's true.  I'm just saying that it's *possible* for Jeremy to win. ... in the same sense that it's also possible for free-floating atoms to randomly arrange themselves in the shape of a written-down P != NP proof on Jeremy's desk. Afraid not - else, your process (your 'algorithm') of making an executable + compressed.file of a combined size less than the original file _is_ a compression algorithm. \n\nOr are you saying that it is merely possible to build a executable that can decompress a single smaller file than the original? That's easy - thats why the answer needs to include the sizes of both files. Realistically, this is not an impossible challenge.  From an information theoretic perspective, clearly it is impossible to write a compression algorithm that compresses every possible file by at least one bit.  But that's not what the challenge is asking for.  The challenge is asking for a program that compresses a *given* file by at least one bit.  Now the challenge boils down to whether Mike Goldman can generate an algorithmically random string of a given length.  He can do this with high probability, but it's not feasible to generate a string that's guaranteed to be algorithmically random.    What if you write a random number generator, and by brute force identified a seed that would eventually produce that sequence within the memory and cpu limits of a modern system, then just included the rng as the decompressor and the seed plus the starting and ending address of of that sequence as the compressed file? What if you write a random number generator, and by brute force identified a seed that would eventually produce that sequence within the memory and cpu limits of a modern system, then just included the rng as the decompressor and the seed plus the starting and ending address of of that sequence as the compressed file? You would need a RNG with an internal state with same or bigger size as the file. Seed would be obviously same size. I am still not seeing this. Maybe you can help me out. Your generator does not have to produce an arbitrary file. It just has to produce that one file. It seems like even in good random data there will always be some structure that you can reproduce using a smaller program. Or does the decompresser actually have to work on other files.  If you have RNG with, say, 64bit internal state, then it can (ideally) produce every 64bit sequence, every other 65bit sequence, 1/4 of 66bit sequences and so on. You can't reliably say that the particular huge sequence will ever appear. \nThis problem is similar to a card shuffling problem. (52!&gt;2^64) I think I might of hijacked the wrong thread for my question. I can see the issue with your random number generator not being the right one and finding the right one being difficult. However I am thinking of just some program that generates numbers. Assuming you asked for a big enough data file and that data file was produced randomly there are going to be some long strings of repeated ones or zeros. So why not just use a program that will insert one of those long strings with a loop. And send back that program and the original file but without that string. The only reason I can see for this not working is if by decompression program they mean something that actually has to accurately compress and decompress in every case not just the case of the specific file. But that was kind of unclear from the problem description.  Or am I totally missing something. How often will 1 byte full of zeroes appear? Every 256th byte. How big index do you need for indexing in the file, to say where to begin encoding? 1 byte. How often will 32bit of zeroes appear? Every 2^32 th 32bit block. How big index do you need to index in the file? 32bit.  Can you see the problem?  You would need a RNG with an internal state with same or bigger size as the file. Seed would be obviously same size. You would need a RNG with an internal state with same or bigger size as the file. Seed would be obviously same size. You are right if perfection is your goal, but if our PRNG simply was a \"very lucky guesser\" it could suffice in meeting the requirement.  Is there some deep Shannon-entropy theory about the limits of compressing white noise or is this an open question? You would need a RNG with an internal state with same or bigger size as the file. Seed would be obviously same size. I don't think this challenge cares about how much temporary storage (like RAM) is used in the compressing or decompressing. Just how much data would need to be sent over the wire to share it.\n\nEdit: Duh, I'm wrong.\n\nEdit Edit: Seriously, don't upvote me, I misunderstood Hi_Kate's point. It shouldn't matter if you use extra RAM but that isn't the point of the internal state comment. How can you reliably tell the RNG what state to create without providing it with that much state in the first place? &gt; Seed would be obviously same size. What if you write a random number generator, and by brute force identified a seed that would eventually produce that sequence within the memory and cpu limits of a modern system, then just included the rng as the decompressor and the seed plus the starting and ending address of of that sequence as the compressed file? How many bytes in a seed value?\n\nHow many bytes in a file/sequence?\n\nHow many combinations of bits are there for each?\n\nThere's got to be *some* sequence of data that couldn't be produced this way, unless we allow the size of a seed value to be very large. What if you write a random number generator, and by brute force identified a seed that would eventually produce that sequence within the memory and cpu limits of a modern system, then just included the rng as the decompressor and the seed plus the starting and ending address of of that sequence as the compressed file? What if you write a random number generator, and by brute force identified a seed that would eventually produce that sequence within the memory and cpu limits of a modern system, then just included the rng as the decompressor and the seed plus the starting and ending address of of that sequence as the compressed file? What if you write a random number generator, and by brute force identified a seed that would eventually produce that sequence within the memory and cpu limits of a modern system, then just included the rng as the decompressor and the seed plus the starting and ending address of of that sequence as the compressed file? This would work, if the file size was absolutely tiny.\n\nHowever you need at least 200kb just for the random generator, probably more (that's just a rough guess based on [this](http://timelessname.com/elfbin/)).\n\nSo you would need to be generating a number 201kb in size. Considering a long is 64bits, 205,824 bits is humongous! The number of possible numbers is vastly huge. There are computationally secure systems which have a smaller search space.\n\nThat's presuming you could fit the decompresser into 200kb. How do you get 200kb?  The starting point in that article is 6kb, and assuming you're not doing something absolutely retarded with your PRNG+seed there's no reason it shouldn't fit in 20-30kb with a little effort.  Or do you want to statically link libc? Damn, I read it as kb not bytes.\n\nSilly me. I take it you never code in C, nor have to consider space constraints on binaries.  Lucky guy :-). I'm up very late working on a problem that crosses the inner depths of both my compiler and compiler-compiler; so my sense of reality is pretty confused right now. This would work, if the file size was absolutely tiny.\n\nHowever you need at least 200kb just for the random generator, probably more (that's just a rough guess based on [this](http://timelessname.com/elfbin/)).\n\nSo you would need to be generating a number 201kb in size. Considering a long is 64bits, 205,824 bits is humongous! The number of possible numbers is vastly huge. There are computationally secure systems which have a smaller search space.\n\nThat's presuming you could fit the decompresser into 200kb. What if you write a random number generator, and by brute force identified a seed that would eventually produce that sequence within the memory and cpu limits of a modern system, then just included the rng as the decompressor and the seed plus the starting and ending address of of that sequence as the compressed file?  Excellent read. I actually believe he met the rules of the challenge and deserves to be rewarded. I didn't believe he really compressed the data up until the very end. Afterall, EOF takes up more space than '5'. However, he really is correct - \"It's not my fault that a file system uses up more space storing the same amount of data in two files rather than a single file.\"\n\nTaken to an extreme, if the data was delivered as a series of printed 1s and 0s on physical pieces of paper, and then the compressed data was returned the same way, plus the size of the program, he would have completed the challenge. I guess the size of the program becomes kind of abstract here, but the point is the same - in this case, information hiding definitely appears in bounds according to the stated rules in the email exchange. [deleted] [deleted] EOF is not a byte. It's a marker used by stream-reading functions to say that there are no more bytes to read. [deleted] [deleted] Excellent read. I actually believe he met the rules of the challenge and deserves to be rewarded. I didn't believe he really compressed the data up until the very end. Afterall, EOF takes up more space than '5'. However, he really is correct - \"It's not my fault that a file system uses up more space storing the same amount of data in two files rather than a single file.\"\n\nTaken to an extreme, if the data was delivered as a series of printed 1s and 0s on physical pieces of paper, and then the compressed data was returned the same way, plus the size of the program, he would have completed the challenge. I guess the size of the program becomes kind of abstract here, but the point is the same - in this case, information hiding definitely appears in bounds according to the stated rules in the email exchange. Excellent read. I actually believe he met the rules of the challenge and deserves to be rewarded. I didn't believe he really compressed the data up until the very end. Afterall, EOF takes up more space than '5'. However, he really is correct - \"It's not my fault that a file system uses up more space storing the same amount of data in two files rather than a single file.\"\n\nTaken to an extreme, if the data was delivered as a series of printed 1s and 0s on physical pieces of paper, and then the compressed data was returned the same way, plus the size of the program, he would have completed the challenge. I guess the size of the program becomes kind of abstract here, but the point is the same - in this case, information hiding definitely appears in bounds according to the stated rules in the email exchange. &gt; However, he really is correct - \"It's not my fault that a file system uses up more space storing the same amount of data in two files rather than a single file.\"\n\nActually, it is his fault. It is his fault that he stored the data in 200ish separate files. The filesystem _has_ to use at least as much data (on average) to store these files separately as the random data he is supposed to compress. So, he still didn't compress anything.\n\n I remain unconvinced. From a comp sci compression stand point, sure, I agree completely, the extra files take up more total space. Even from a theoretical stand point, I agree. The new file acts as an information hider and couldn't be used in pure theory.\n\nBut I still think he won the contest, according to the specific rules of byte sizes. &gt; However, he really is correct - \"It's not my fault that a file system uses up more space storing the same amount of data in two files rather than a single file.\"\n\nActually, it is his fault. It is his fault that he stored the data in 200ish separate files. The filesystem _has_ to use at least as much data (on average) to store these files separately as the random data he is supposed to compress. So, he still didn't compress anything.\n\n You also need an operating system too, does *that* get included in the overhead costs? The challenge as worded was met (albeit under altered but agreed-upon terms). Mike even said that he would have accepted the submission if it had a 400 byte decompressor, a 599 byte compressed file and a target of less than 1000 bytes, *despite* the fact that the file-system overhead costs would push it past 1000 bytes.    I'm not sure why people find this clever or interesting, the method is clearly against the spirit of the challenge. It's almost impossible to have completely water-tight rules, that doesn't make evading those rules worthwhile. I'm not sure why people find this clever or interesting, the method is clearly against the spirit of the challenge. It's almost impossible to have completely water-tight rules, that doesn't make evading those rules worthwhile.      [deleted]",
    "url" : "http://www.patrickcraig.co.uk/other/compression.htm"
  }, {
    "id" : 9,
    "title" : "What is the most novel, interesting, and original aspect of a programming language you have seen in the last 10 years?",
    "snippet" : "Like many people here, I have a big interest in programming languages. I keep my eye open for new languages coming out, and am interested in what they are doing which is different.\n\nHowever I have noticed many languages tend to re-invent the same ideas.\n\nSo I am interested in what languages, or new research idea in languages, over the last decade (or slightly further) do people think really helped to bring something that was new, original, productive, and/or novel?  Another one, not sure how novel or recent an innovation but anyway: mixfix operators, as seen in Agda. Basically lets you define arbitrary syntax:\n\n    if_then_else_ : {A : Set} -&gt; Bool -&gt; A -&gt; A -&gt; A\n    if true  then x else y = x\n    if false then x else y = y\n\nPrefer the Python syntax? Why not!\n\n    _if_else_ : {A : Set} -&gt; A -&gt; Bool -&gt; A -&gt; A\n    x if true  else y = x\n    x if false else y = y\n\nIt's the ultimate DSL tool. I imagine it would be batshit insane if used liberally in any larger program, but fun to play with! It is interesting, but how is the priority of operators handled? I know that in many languages this is not exactly straightforward even if you use only the few operators provided by the language itself, I imagine that code using this would be very difficult to manage. Another one, not sure how novel or recent an innovation but anyway: mixfix operators, as seen in Agda. Basically lets you define arbitrary syntax:\n\n    if_then_else_ : {A : Set} -&gt; Bool -&gt; A -&gt; A -&gt; A\n    if true  then x else y = x\n    if false then x else y = y\n\nPrefer the Python syntax? Why not!\n\n    _if_else_ : {A : Set} -&gt; A -&gt; Bool -&gt; A -&gt; A\n    x if true  else y = x\n    x if false else y = y\n\nIt's the ultimate DSL tool. I imagine it would be batshit insane if used liberally in any larger program, but fun to play with!  I think that [persistent data structures](http://en.wikipedia.org/wiki/Persistent_data_structure) are one of the most interesting ideas to gain attention recently. When working with mutable data structures, you either have the option of passing by reference or by value. The first is cheap, but makes code more difficult to reason about, as the data can be shared between many sections. The second is safe but also expensive. \n\nPersistent data structures provide a third option. Any time a change is made a revision of the existing data is created. This approach ensures that all changes are inherently localized, and you only pay the price proportional to the change. \n\nIn my opinion this is akin to having garbage collection. Instead of having to ensure that data is not changed out of context by hand the language does it for you. From user perspective you simply create new data any time you make a change and don't worry about it.    [Factor](http://factorcode.org)\n\nA truly great concatenative language. The concatenative paradigm is very interesting to learn. I recommend it.  One interesting language I've seen recently is [Church](http://projects.csail.mit.edu/church/wiki/Church), which is a probabilistic programming language (that is, it is designed to let you express probability distributions in a systematic and uniform way). I believe there are also several other probabilistic languages, but I haven't looked at them at all.\n\nA very interesting idea--rather than a language--is program synthesis. The basic concept (generating programs based on some high-level specification) has been around for a very long time; however, fairly recent (at least I think they're fairly recent) advances in SAT and SMT solver performance have made writing synthesizers based on these solvers far more practical, which significantly lowers the barrier to implementing new synthesizers for domain-specific languages.    Interaction Points in Agda: While developing you can leave holes in your program and the typechecker won't bother you too much about it. You can enter the holes, query what's in scope, which goal you have to achieve and try to get closer. It's probably more of an implementation thing but it does interact nicely with the language and it's the one feature I miss most in other environments. Good news! GHC 7.8 is getting TypeHoles. Don't think they're *interactive* yet, though, and I haven't looked at Interaction Points yet. What you're describing sounds like debugging in Python though:\n\n    import pdb; pdb.set_trace()\n\nIs it anything like that?  [Lua Tables](http://www.lua.org/pil/2.5.html).\n\nTables are 'just' associative arrays, but they are implemented efficiently [1].  Tables are the only data structure provided by the language itself, and tables are all you need to implement other data structures elegantly, as well as to do object oriented programming, modules, and more.\n\nThis emphasis on tables has numerous benefits to the design and compactness of the language.  For starters, there is only one set of syntax for creation and accessing table elements. [2] There are also shortcuts built into the language that make common usage scenarios work nicely.  For example, if you have table elements with string keys, you can access those values with mytable.keyname instead of using mytable[\"keyname\"].\n\nI liken Lua to a Scheme with a more Pascal-ish syntax, no macros [3], and built around a much more useful fundamental data structure instead of a linked list.\n\n[1] [The Implementation of Lua 5.0 - PDF](http://www.lua.org/doc/jucs05.pdf)\n\n[2] As a counter-example: Python programmers have to remember to use square brackets for lists, braces for dictionaries, and parens for tuples. [Lua Tables](http://www.lua.org/pil/2.5.html).\n\nTables are 'just' associative arrays, but they are implemented efficiently [1].  Tables are the only data structure provided by the language itself, and tables are all you need to implement other data structures elegantly, as well as to do object oriented programming, modules, and more.\n\nThis emphasis on tables has numerous benefits to the design and compactness of the language.  For starters, there is only one set of syntax for creation and accessing table elements. [2] There are also shortcuts built into the language that make common usage scenarios work nicely.  For example, if you have table elements with string keys, you can access those values with mytable.keyname instead of using mytable[\"keyname\"].\n\nI liken Lua to a Scheme with a more Pascal-ish syntax, no macros [3], and built around a much more useful fundamental data structure instead of a linked list.\n\n[1] [The Implementation of Lua 5.0 - PDF](http://www.lua.org/doc/jucs05.pdf)\n\n[2] As a counter-example: Python programmers have to remember to use square brackets for lists, braces for dictionaries, and parens for tuples.      Closures. Maybe they're older as a concept than 10 years, but I'm pretty sure I hadn't ever heard of them more than a decade ago. Closures were conceived [about 40 years ago](http://en.wikipedia.org/wiki/History_of_the_Scheme_programming_language) with lexical scoping in Scheme. Maybe. But closures were only added to Java like last year or so. (I don't know, I don't really do Java so I don't follow developments there too  closely.) So to the world at large, it's a relatively new concept. Closures. Maybe they're older as a concept than 10 years, but I'm pretty sure I hadn't ever heard of them more than a decade ago.  ",
    "url" : "http://www.reddit.com/r/compsci/comments/110a2b/what_is_the_most_novel_interesting_and_original/"
  }, {
    "id" : 10,
    "title" : "Programming in other (actual) languages.",
    "snippet" : "Can someone who speaks another language tell me what programming in another actual language (i.e German, Spanish, French) is like? I've always been curious, then I remembered there are people I can actually ask. I speak a bit of German, if that helps.    I'm Scottish and currently programming in Austria, the entire company programs in American English and it's pretty much the same as programming with native English speakers. Sometimes you do get things that are named oddly because they don't know the word for it, but not so badly that you can't understand it. That's interesting. I wondered if other people use other keywords or such. Apperently not.  English. For school projects, private projects, etc... I sometimes used Hungarian variable names (or especially when teaching someone else), but production code is always US English. I imagine that's a bother. Not at all, in programming you learn English or you die. On the first programming lecture of freshman year the prof asked who couldn't speak English. Some people raised their hands, the prof then said \"You have three weeks to learn it\". The documentation is always in English, the references are in English, the forums are in English, it just makes sense to code in English. That's a bitch. Sorry bro.  As others noted, English is the global language of computing, but in the past there have programming languages in other languages, and fortunately [Wikipedia has a list](http://en.wikipedia.org/wiki/Non-English-based_programming_languages)\n\nSome of these are esoteric or visual language but others are based on real-world languages like Arabic, Polish or Russian, to name a few... As others noted, English is the global language of computing, but in the past there have programming languages in other languages, and fortunately [Wikipedia has a list](http://en.wikipedia.org/wiki/Non-English-based_programming_languages)\n\nSome of these are esoteric or visual language but others are based on real-world languages like Arabic, Polish or Russian, to name a few...  The only language you should be programming in is C. C and pen and paper!",
    "url" : "http://www.reddit.com/r/compsci/comments/110zv9/programming_in_other_actual_languages/"
  }, {
    "id" : 11,
    "title" : "NAND to Tetris: A completely open course that takes the learner from the basics of logic to operating system design while creating a general purpose computer from the ground up",
    "snippet" : "  &gt;commercial use prohibited\n\n&gt;\"open\"\n\nThe term is officially worthless I think you are confusing the terms \"open\" with \"public domain.\" Things that are \"open\" allow visibility into internals, so that they can be examined (and often changed). This goes very well with the dictionary definition of \"open.\"\n\nA classical example of an open thing that cannot be used arbitrarily is GPL-licensed software. You can use it, but cannot redistributed altered binaries without source. Almost all copyrighted software, regardless of openness, cannot be redistributed without some kind of attribution in the source code, lest you be accused of copyright violations.\n\nIn short, this project is completely allowed to call itself open while restricting how you can use it. Because it is open, and they are unrelated concepts.  This is where the idea for that 16-bit Minecraft arithmetic logic unit came from! The Hack computer!  ",
    "url" : "http://www.nand2tetris.org/"
  }, {
    "id" : 12,
    "title" : "TMI: Genome sequence data overwhelms scientists; Time for some HPC tools ",
    "snippet" : "  &gt; Researchers will start by identifying a large set of building blocks frequently used in genomic studies. They’ll develop the parallel algorithms and high performance implementations needed to do the necessary data analysis. And they’ll wrap all of those technologies in software libraries researchers can access for help. On top of all that, they’ll design a domain specific language that automatically generates computing codes for researchers.\n\nEvery large scale genome analysis team is already doing this, and has been doing it since at least 2004. The story here is that the NSF has decided to fund it directly. All of the systems I worked with were developed purely out of necessity to complete research. The funding for the software teams is part of the overhead on genome-specific research grants.   If you think you can do this type of work, we're hiring. We do everything from HPC to \"Big Data\" pipelines, to SIMD/GPU/FPGA programming.  Everything is on the table. If you think you can do this type of work, we're hiring. We do everything from HPC to \"Big Data\" pipelines, to SIMD/GPU/FPGA programming.  Everything is on the table. ",
    "url" : "http://www.news.iastate.edu/news/2012/10/03/bigdata"
  }, {
    "id" : 13,
    "title" : "[Algorithms] Assigning subjects to studies with restrictions on both: how would you solve this?",
    "snippet" : "My girlfriend's a PhD student who, as an assistant researcher, is tasked with assigning subjects (people answering surveys) to studies. Each study requires a certain number of subjects *(it's not useful to go over that number of subjects requested)* who meet certain criteria (a particular race/ethnicity, background, stuff like that). *Each study is estimated to take ~~at least half an hour~~ between half an hour and one hour.* The subjects themselves are restricted in that each subject must and may only participate in something like 3 hours or 4 different studies, whichever \"comes first\" for that subject. *The order in which a subject completes assigned surveys doesn't matter.* There are about a dozen studies and ~~a few hundred~~ about 1,100 student subjects.\n\nSo... I'm just a senior-level CS student, but I've heard of something called the \"assignment problem,\" which seems to be a generalization of the \"hospitals-residents problem...\" (http://www.nrmp.org/res_match/about_res/algorithms.html) And I think algorithms that provide solutions to those problems might work on this problem, but I have no idea how to design such a thing.\n\nedit: clarifications in italics  What's the balance between subjects and studies? That is, are you overflowing with people and very few studies to assign them to, or are subjects scarce compared to the sum of the minimum numbers for each study, or is it roughly balanced where you have close to the amount of \"people resources\" for the study places available?  (What the balance is like can affect the sort of algorithm that would work best.) \n\nThe information that it's ~12 studies and a few hundred student subjects isn't quite sufficient here as you haven't said how time-consuming these sorts of studies are (studies can vary pretty widely), nor how many places the studies have to fill (or have capacity for), on average.  \n\nAlso, what is the goal here? Are you trying to just find *any* old assignment of people to studies that doesn't go against the criteria?\nAre you trying to get as many people for the studies as you can? (i.e. making maximal use of the human resources available) Or are you just trying to find sufficient people for the particular studies and it doesn't matter whether there are humans left over?\n\nYou might find that a matching algorithm helps to solve it, but only if the shape of the problem is the same. If it isn't, you might need a different line of approach, like a backtracking algorithm.  These kinds of problems (assign X's to Y's) can typically be solved using a network flow/circulation. The simplest example of such an assignment problem is bipartite matching, where you would put studies in one of the partite sets and subjects in the other. You express compatibility between studies &amp; subjects via the presence/absence of edges. A matching in the graph (which can be found by a simple network flow) represents a feasible assignment of studies to subjects.\n\nIn your case, to express conditions like \"participating in exactly 3 hours or exactly 4 studies\" you may need to turn things into a network circulation problem, and add some more \"gadgets\" to the graph.\n\nI suggest you read the section on network flows from the Kleinberg &amp; Tardos textbook (the \"flows &amp; cuts\" chapters of [Jeff Erickson's lecture notes](http://compgeom.cs.uiuc.edu/~jeffe/teaching/algorithms/) are likely to also be a good reference) .. K&amp;T do a very good job of describing these things. I think some of their examples (like the airline scheduling section) would be quite helpful for you. Unfortunately, because this is just a reddit comment and some aspects of your question are a little unclear, I can't give any more specific pointers. I hope this has helped though.    [deleted] No, doesn't matter. I'll add that to the OP as well. Thanks! [deleted] I'm not set on a language, but I'm more comfortable with C++ than with anything else. It's about 1,100 (not a few hundred as I'd originally thought). [deleted] No problems with studies overlapping.\n\nThe way I understand it, it's typically harder to get all the researchers as many subjects as they want. So they want to maximize how much each student participates in studies.\n\nedit: It's even more complicated than that. Pre-tenure faculty and PhD dissertation students get priority on subject assignments over tenured faculty and other students. [deleted] They want to maximize participation by the subjects so that all the studies have the most subjects overall, so basically that is how they are getting the most data and the most relevant data possible. ~~Studies have lower limits on number of subjects but not really upper limits.~~ [deleted]  Sounds like someone is trying very hard to guarantee that none of their conclusions have any scientific validity whatsoever.\n\nYou do not achieve random sampling by intentionally skewing your sample set. I think they're not trying to achieve random sampling. ~~The studies involve only black and African-American people, so responses from white people would be of no use.~~ Correction: some of the studies target particular races/ethnicities, others target particular majors, others are more general.\n\nShe's in social psychology. I understand her work about as well as she understands mine. If you are going to claim that your results apply to \"African-Americans\" then you have to make certain that your sampling is random over that particular population - meaning you have to select randomly from different socioeconomic backgrounds, different lifestyles, etc.  Social psychology is very lax about this, and it makes most of their conclusions completely invalid.  If they were to change their conclusions to be phrased correctly, and express the limitations on their sampling, they could avoid this invalidation.  For instance, if you are selecting from college students you would have to write your conclusion like \"Among middle class African-American college students aged X-Y who grew up in modern industrialized North American culture, Z holds.\"  Unfortunately, they want to make a big splash, so they throw science out the window and say \"Z holds among African Americans\" applying their conclusion to tons of groups they never even glanced at in their studies.  There is no reason to expect the life experiences, behaviors, or even BIOLOGY of people with radically different socioeconomic experiences, or different lifestyles, or growing up in markedly different cultures, would be similar. If you are going to claim that your results apply to \"African-Americans\" then you have to make certain that your sampling is random over that particular population - meaning you have to select randomly from different socioeconomic backgrounds, different lifestyles, etc.  Social psychology is very lax about this, and it makes most of their conclusions completely invalid.  If they were to change their conclusions to be phrased correctly, and express the limitations on their sampling, they could avoid this invalidation.  For instance, if you are selecting from college students you would have to write your conclusion like \"Among middle class African-American college students aged X-Y who grew up in modern industrialized North American culture, Z holds.\"  Unfortunately, they want to make a big splash, so they throw science out the window and say \"Z holds among African Americans\" applying their conclusion to tons of groups they never even glanced at in their studies.  There is no reason to expect the life experiences, behaviors, or even BIOLOGY of people with radically different socioeconomic experiences, or different lifestyles, or growing up in markedly different cultures, would be similar.",
    "url" : "http://www.reddit.com/r/compsci/comments/10xe0i/algorithms_assigning_subjects_to_studies_with/"
  }, {
    "id" : 14,
    "title" : "Microsoft Research releases Z3 source code",
    "snippet" : "  &gt; Z3 is a high-performance theorem prover being developed at Microsoft Research. Z3 supports linear real and integer arithmetic, fixed-size bit-vectors, extensional arrays, uninterpreted functions, and quantifiers. Z3 is integrated with a number of program analysis, testing, and verification tools from Microsoft Research. These include: VCC, Spec#, Boogie, Pex, Yogi, Vigilante, SLAM, F7, F*, SAGE, VS3, FORMULA, and HAVOC.\n\nNeeded that  Not sure if serious.\n\nOut of curiosity, would anybody who needs this care to explain what for?  Not sure if serious.\n\nOut of curiosity, would anybody who needs this care to explain what for?  Not sure if serious.\n\nOut of curiosity, would anybody who needs this care to explain what for?  Not sure if serious.\n\nOut of curiosity, would anybody who needs this care to explain what for?  Proving theorems, presumably. I meant consumer or industrial applications, not just pure science.  How does this differ from Coq? Coq is a [proof assistant](http://en.wikipedia.org/wiki/Proof_assistant) while Z3 is a [SMT](http://en.wikipedia.org/wiki/Satisfiability_Modulo_Theories) solver. How does this differ from Coq? You're not allowed to use it if your research is funded by industry partners (\"non-commercial use only\").   ",
    "url" : "https://research.microsoft.com/en-us/um/people/leonardo/blog/2012/10/02/open-z3.html"
  }, {
    "id" : 15,
    "title" : "Interactive Visualization of How AVL Trees Work",
    "snippet" : "  ",
    "url" : "http://qmatica.com/DataStructures/Trees/AVL/AVLTree.html"
  }, {
    "id" : 16,
    "title" : "Differential Synchronization: the algorithm behind Google Docs' collaborative editing",
    "snippet" : "    Where'd you get the idea that this is the strategy used in Docs? As far as I know, it isn't. It must use some method, right?  And this method is simply a sophisticated version of the most obvious method, instant-automatic version control.",
    "url" : "http://neil.fraser.name/writing/sync/"
  }, {
    "id" : 17,
    "title" : "Relatable applications of balanced trees",
    "snippet" : "I'm prepping a lecture that is an introduction/motivation for balanced tree structures/algorithms, and would like to tap the hivemind.\n\nAs a part of this lecture I would like to talk about some different applications, that are understandable by 1st/2nd years, where the different balancing algorithms/trees are \"best\" (or, at least, typically used). The idea is to help motivate *why* we're bothering to cover a variety of trees instead of just the one or two \"best\" ones. This is where I hope you can come in.\n\nWhat applications, that could be quickly explained to a 1st/2nd year student, do you know of where the following balanced trees are a better choice than the others, and why?\n\n* Perfectly balanced binary tree\n* AVL tree\n* 2-3 tree\n* Red-black tree\n* B-tree\n\nI know of a few (ex: B-tree for filesystems due to large nodes working well with high latency retrieval), but don't claim to know everything. Thanks a million for your help.  B-trees are great where you need stupid fast look-up times, and will want to get a large range of values with little overhead.  The downside is that the overhead can hurt when holding small amounts of data.  Each node is also larger, and larger branching factors can cause the key storage on the node to go off the end of a cache line.  That being said, the massive branching factor means that the O(log(n)) access time has a massive base on the log.  Common applications are DB indexes and filesystems.\n\nAVL trees are rigidly balanced, meaning that they can give a better guarantee of the O(log(n)) seek time.  The downside is that insertion will occasionally take a while while the tree re-balances.  These are best used when you'll mostly be reading, like in dictionaries and lookup structures.\n\nRed-Black trees generally have decent guarantees on both insertion and retrieval, so neither will ever take an exceptionally long time.  IIRC, one of the Linux schedulers uses it for this reason, as does the C++ set class.  I see these used a lot as general purpose trees, partly due to predictable performance, partly due to ease of implementation.\n\nI don't think I've ever seen a 2-3 tree (or any variation) in the wild.\n\nWhat do you mean by \"Perfectly balanced binary tree\"?  There are several ways of implementing binary trees that give guarantees about the balance.",
    "url" : "http://www.reddit.com/r/compsci/comments/10x1hi/relatable_applications_of_balanced_trees/"
  }, {
    "id" : 18,
    "title" : "PhD (in CS): expectations and reality",
    "snippet" : "  I wish the article had some reflections about the length of time in the PhD programme. The author had a master's degree already and spent six extra years dedicated to his doctorate. Is this normal or a bit excessive? Why so many years? Were there funding problems in the later years? I assume an average person can complete a doctorate (already having a master's) in three to four years; I see the author sometimes had TA duties but does this account for the extra two to three years? Would love to hear more! I agree, although it may have something to do with this statement the author made:\n  \n  &gt;In this phase you start to become an expert in your field: you have claimed a little spot in a bigger field that you extend with your research. Your adviser will try to keep you in this phase as long as you are able to produce more good papers (given that your adviser has enough funding and that you do not run into any hard time limits given by your university).\n  \n   \n  Side note: does anybody have any blogs/books about experiences getting a PhD in CS (or any field but preferably CS) while managing the duty of having family/expenses? &gt; Side note: does anybody have any blogs/books about experiences getting a PhD in CS (or any field but preferably CS) while managing the duty of having family/expenses?\n\nI'm currently in that position (entered full-time CS PhD with one kid, had my second kid last year).  I don't have any pithy advice other than it challenges your time management skills even moreso (of which I'm horrible at) as well as your expectations.  You can try to push your work as hard as you did when you didn't have such obligations but you are likely to (a) fall behind in either your work or family commitments or (b) kill yourself trying to keep up.\n\nFor better or worse I've resigned myself to taking it slower and steadier which is completely counter to how I've worked in the past --- I'm still trying to figure that bit out.  I've also had the good fortune of surrounding myself with a research group that understands family life, so I've had great support along the way.  Finding good support structures like that helps a great deal. Thanks for your response. My main concern is that doing the PhD would put a strain on my relationship as we would go back to having no money and stressing about that again (my SO is a teacher). Have you encountered anything like that?  Definitely.  I came from industry as well, so my wife and I were used to having a comfortable income.  We went from not having to worry too much about budgets (we're both pretty frugal people) to barely breaking even each month with mortgage payments and child expenses.  We've had quite a few heated arguments about over-spending and such as a result.\n\nThe upside is that my wife is incredibly supportive of my endeavor even though she hates where we live now (which is also very far away from the rest of our family).  We decided up front (and I think this is key: including your SO in the \"do it, or not\" decision process) that the PhD is necessary for my career goals so this would be a burden we'd have to bear. I wish the article had some reflections about the length of time in the PhD programme. The author had a master's degree already and spent six extra years dedicated to his doctorate. Is this normal or a bit excessive? Why so many years? Were there funding problems in the later years? I assume an average person can complete a doctorate (already having a master's) in three to four years; I see the author sometimes had TA duties but does this account for the extra two to three years? Would love to hear more!",
    "url" : "http://secnerd.blogspot.com/2012/09/phd-expectations-and-reality.html"
  }, {
    "id" : 19,
    "title" : "Any good audio lectures?",
    "snippet" : "Hey guys I'm doing prep for interviews and was wondering if there were any good audio lectures that I could listen to while driving or going out on a run. I'm mainly interested in algorithms and data structures, but anything would be nice!\n\nThanks!  Most of the Berkeley CS iTunesU courses have audio only components.  I like listening to Steve Skiena's [algorithm lectures](http://www.cs.sunysb.edu/~algorith/video-lectures/), because his accent and delivery are more interesting than the MIT lectures. Most of the Berkeley CS iTunesU courses have audio only components.  I like listening to Steve Skiena's [algorithm lectures](http://www.cs.sunysb.edu/~algorith/video-lectures/), because his accent and delivery are more interesting than the MIT lectures.",
    "url" : "http://www.reddit.com/r/compsci/comments/10tqqk/any_good_audio_lectures/"
  }, {
    "id" : 20,
    "title" : "Memory help...",
    "snippet" : "Hi reddit!\n\nThis week we were talking about the different types of storage and RAM and stuff. We were talking about how if you had addresses that were 32bits, then you would have ( 2^32 ) addresses (4,294,967,296 addresses).\n\n\nHowever, back in high school I was told on two separate equations that memory is allocated to various programs/processes as is needed. \n\nFor instance (this is a ridiculous example): \n\nSay you have 4GB of RAM (that's not the ridiculous part)\n\n* Process 1 :: Requires 247MB of RAM\n* Process 2 :: Requires 3GB of RAM (that's the ridiculous part)\n* Process 3 :: Requires 256MB of RAM\netc...\n\nSo the system allocates 200MB for Process 1, 1GB for Process 2 etc...\n\nAnd you end up with 521MB of RAM free.\n\nHowever, now I'm confused because of this new found formula that I mentioned above: \n\n2^n = # of addresses\n\nSo I'm confused because first I was told that memory is assigned proportionally to the processes that require it, but now I'm told there is a definitive way to accurately number/address how many memory locations you have. \n\nOr as an analogy:\n\n*Originally* I was told that RAM is like a big warehouse, and someone (a process) comes in and says, \"I need a section of this warehouse thats 5ft x 10ft\" and the owner of the warehouse says, \"Okay here you go!\". And that's the end of it.\n\nNow I'm being told that RAM is more like a hotel, where you *know* there are 4 floors, each with 100 rooms on them (ranging from 000-499 / 000000000 - 111110011), and a process comes in and says, \"I need a room (wink wink)\", and the system says, \"Okay here you go!\". And that's the end of that.\n\nSo what made sense about the first analogy is that if a process only requires 4 bits of memory, the system would only give it 4 bits, which makes things super efficient. \n\nBut now if the process (again) only requires 4 bits, then how big are the rooms? If they are only one size then what happens to the leftover space?\n\nCan anyone help me? Tell me what I've got wrong? Link me to an article? Cheesecake?\n\nThanks!  You should read [What every programmer should know about memory](http://www.akkadia.org/drepper/cpumemory.pdf), starting with section 4. It goes into a bunch more depth that you probably need, but will answer your questions more accurately than I am able to do in a single comment.\n\nThe important point is that there is a layer between physical memory and the memory your process sees. Understanding how this \"virtual memory\" layer works is key to understanding how modern operating systems use memory. You should read [What every programmer should know about memory](http://www.akkadia.org/drepper/cpumemory.pdf), starting with section 4. It goes into a bunch more depth that you probably need, but will answer your questions more accurately than I am able to do in a single comment.\n\nThe important point is that there is a layer between physical memory and the memory your process sees. Understanding how this \"virtual memory\" layer works is key to understanding how modern operating systems use memory.  You are getting confused by the difference between virtual memory and physical memory. Before virtual memory, all programs shared an address space which was the physical address space. This was not ideal for a number of reasons, one being that processes could muck about with the memory of other processes. Virtual memory provides an abstraction to that in which each process \"thinks\" it has access to the entire address space and its the only program running.\n\nThe formula 2^n = #addresses refers to the number of addressable regions of memory where n is the width of addresses (32bit or 64bit for all modern desktops).\n\nplease reply with questions. I know this is not going to answer all of what you're getting hung up on, but its not clear to me what else you are asking. \n\nEDIT: I should add that vram is mapped to physical memory in the OS with the help of hardware, so there is no notion of an absolute physical memory address to the running (user-mode) processes.  That's a really good explanation, I think I kinda understand now.\n\n&gt;each process \"thinks\" it has access to the entire address space and its the only program running.\n\nSo the hardware you're talking about says to the process, \"Sure, somprocess.exe, you've got this entire space to run in\"\n\nHow big is the space? Is it whatever remains from what is not being used by other processes?\n\n\n&gt;The formula 2n = #addresses refers to the number of addressable regions of memory where n is the width of addresses (32bit or 64bit for all modern desktops).\n\nSo, how can you say I have 2^64 number of addressable regions when a process requires an \"unknown\" amount of memory? / How can you come up with a finite answer for the number of regions when you don't know how big the regions are?\n\n&gt;I should add that vram is mapped to physical memory in the OS with the help of hardware, so there is no notion of an absolute physical memory address to the running (user-mode) processes. \n\nSo, as you said above, each process thinks it has all this space to use, but the piece of hardware you mentioned REALLY knows that someprocess.exe is using such-and-such amount of  *physical* memory at such-and-such address? That's a really good explanation, I think I kinda understand now.\n\n&gt;each process \"thinks\" it has access to the entire address space and its the only program running.\n\nSo the hardware you're talking about says to the process, \"Sure, somprocess.exe, you've got this entire space to run in\"\n\nHow big is the space? Is it whatever remains from what is not being used by other processes?\n\n\n&gt;The formula 2n = #addresses refers to the number of addressable regions of memory where n is the width of addresses (32bit or 64bit for all modern desktops).\n\nSo, how can you say I have 2^64 number of addressable regions when a process requires an \"unknown\" amount of memory? / How can you come up with a finite answer for the number of regions when you don't know how big the regions are?\n\n&gt;I should add that vram is mapped to physical memory in the OS with the help of hardware, so there is no notion of an absolute physical memory address to the running (user-mode) processes. \n\nSo, as you said above, each process thinks it has all this space to use, but the piece of hardware you mentioned REALLY knows that someprocess.exe is using such-and-such amount of  *physical* memory at such-and-such address? &gt;How big is the space? Is it whatever remains from what is not being used by other processes?\n\nNo, each process is given 2^n bits of addressable memory where n is set by the type of processor (16, 32, or 64 typically). This is how virtual memory works, each process gets a full space of 4GB (for 32 bit systems) in which to work. This doesn't mean the OS allocated 4GB of physical memory (even if there is more than that available). When a program actually wants to use memory it needs to ask the OS to allocate some (in c this is malloc()) and then the process can use that memory. Accessing un-allocated memory causes a system failure (segmentation fault).\n\n&gt;So, as you said above, each process thinks it has all this space to use, but the piece of hardware you mentioned REALLY knows that someprocess.exe is using such-and-such amount of physical memory at such-and-such address?\n\nBasically, the OS (ie. the kernel) has full reign over physical memory. Processes can NEVER touch physical memory. Every time memory is accessed by a processes the OS takes over and preforms address translation to turn the virtual address into some physical address. The OS is free to allocate more virtual memory than exists in physical memory and it does this by \"paging\" unneeded memory off of the RAM and onto a hard drive disk. When the the \"paged\" memory is needed again, it is brought back on to the RAM and some other bit of memory is paged onto the hard disk. &gt;How big is the space? Is it whatever remains from what is not being used by other processes?\n\nNo, each process is given 2^n bits of addressable memory where n is set by the type of processor (16, 32, or 64 typically). This is how virtual memory works, each process gets a full space of 4GB (for 32 bit systems) in which to work. This doesn't mean the OS allocated 4GB of physical memory (even if there is more than that available). When a program actually wants to use memory it needs to ask the OS to allocate some (in c this is malloc()) and then the process can use that memory. Accessing un-allocated memory causes a system failure (segmentation fault).\n\n&gt;So, as you said above, each process thinks it has all this space to use, but the piece of hardware you mentioned REALLY knows that someprocess.exe is using such-and-such amount of physical memory at such-and-such address?\n\nBasically, the OS (ie. the kernel) has full reign over physical memory. Processes can NEVER touch physical memory. Every time memory is accessed by a processes the OS takes over and preforms address translation to turn the virtual address into some physical address. The OS is free to allocate more virtual memory than exists in physical memory and it does this by \"paging\" unneeded memory off of the RAM and onto a hard drive disk. When the the \"paged\" memory is needed again, it is brought back on to the RAM and some other bit of memory is paged onto the hard disk. That's a really good explanation, I think I kinda understand now.\n\n&gt;each process \"thinks\" it has access to the entire address space and its the only program running.\n\nSo the hardware you're talking about says to the process, \"Sure, somprocess.exe, you've got this entire space to run in\"\n\nHow big is the space? Is it whatever remains from what is not being used by other processes?\n\n\n&gt;The formula 2n = #addresses refers to the number of addressable regions of memory where n is the width of addresses (32bit or 64bit for all modern desktops).\n\nSo, how can you say I have 2^64 number of addressable regions when a process requires an \"unknown\" amount of memory? / How can you come up with a finite answer for the number of regions when you don't know how big the regions are?\n\n&gt;I should add that vram is mapped to physical memory in the OS with the help of hardware, so there is no notion of an absolute physical memory address to the running (user-mode) processes. \n\nSo, as you said above, each process thinks it has all this space to use, but the piece of hardware you mentioned REALLY knows that someprocess.exe is using such-and-such amount of  *physical* memory at such-and-such address? You are getting confused by the difference between virtual memory and physical memory. Before virtual memory, all programs shared an address space which was the physical address space. This was not ideal for a number of reasons, one being that processes could muck about with the memory of other processes. Virtual memory provides an abstraction to that in which each process \"thinks\" it has access to the entire address space and its the only program running.\n\nThe formula 2^n = #addresses refers to the number of addressable regions of memory where n is the width of addresses (32bit or 64bit for all modern desktops).\n\nplease reply with questions. I know this is not going to answer all of what you're getting hung up on, but its not clear to me what else you are asking. \n\nEDIT: I should add that vram is mapped to physical memory in the OS with the help of hardware, so there is no notion of an absolute physical memory address to the running (user-mode) processes.  [deleted] [deleted]  &gt; We were talking about how if you had addresses that were 32bits, then you would have ( 232 ) addresses (4,294,967,296 addresses).\n\nAddresses, yes. Not all of those addresses are necessarily usable. Your machine may only have 1GB. Or even if you have 4GB, some addresses are considered special because they are reserved for the system, or correspond to certain hardware devices. (For instance, reading/writing a certain \"memory\" address may actually causes I/O to happen.)\n\n3GB of RAM for a single process is not at all ridiculous for many professional applications.\n\nThe main thing that would solve your conundrum is understanding *virtual memory*. There are physical addresses that correspond to actual cells of circuitry on a DRAM chip in your computer. And then there are logical addresses that a program works with, that get converted automatically to physical addresses by the system without the program knowing or caring. Each process has its own memory mappings from logical to physical addresses, so two different processes may both refer to the same address and get completely different unshared pieces of actual memory.\n\nThe management of these mappings from logical to physical addresses is done by the operating system. This is why a process needs to request more memory, and how it can be allocated on a per-need basis. If there were no such thing as virtual memory, every process would just read and write whatever part of physical memory it wanted. There would be no requesting, just a flat chunk of 4GB (or less). That makes a lot of sense. How does one determine the size of memory chunk at a physical address? Is it the amount of RAM / 2^32?\n\nThanks for the response!  On your 32-bit machine, all programs get 32-bit pointers, they don't get \"4 bits of memory\". If they try to access memory that isn't theirs, they crash. It isn't allocated by the bit, it's like a range of numbers. Meaning the address is a 32 bit address, like\n\n10001011010110010110011010110101\n\nSo how big are the chunks of memory?\n\nThanks for the comment! Meaning the address is a 32 bit address, like\n\n10001011010110010110011010110101\n\nSo how big are the chunks of memory?\n\nThanks for the comment! Meaning the address is a 32 bit address, like\n\n10001011010110010110011010110101\n\nSo how big are the chunks of memory?\n\nThanks for the comment!      [deleted] Um...",
    "url" : "http://www.reddit.com/r/compsci/comments/10s6qf/memory_help/"
  }, {
    "id" : 21,
    "title" : "Neural network designs?",
    "snippet" : "I have had introductory experience with using neural networks for regression and classification problems but I have several questions about intelligently creating a network architecture.\n\nI suppose the most general question would be: What are the most common network architectures and what/why are they used for specific tasks.\n\nI'm not quite sure how to elaborate my question further but I suppose I could start by describing some of what I know.\n\n2 layer networks, (input/output only), are essentially the same as linear regression, and for this reason aren't really used.\n\n3 layer networks, (1 hidden layer), are the most common and can be used to model phenomenon with highly non-linear characteristics. These are the most popular because it has been mathematically shown that any phenomenon that can be modeled by a differentiable function can be fit with this type of network, (given enough data, and unbounded neurons in the hidden layer). \n\n4 layer, 2 hidden layer, networks are used either because they could be more computationally efficient than a single hidden layer with tons of neurons or they are used to model phenomenon that have discontinuities (not sure how that is true). \n\nBasically I'm looking for elaborated information like the above. For example I've seen pictures of networks with 'split' layers, e.g. not all of the neurons in layer j are connected to every other neuron in layer j+1 or such. I would like to know what other kinds of networks there are and if there is a reason that they are good at what they are being used for.\n\n I realize this isn't necessarily a totally answerable question, but any kind of help or link to relevant resources would be great!\n\nThank you !  Slightly off-topic, but if it's of any interest to anybody then 'Neural Networks for Machine Learning' course actually starts today on coursera.org :) thank you for this! I love you! Glad I could help :)  Are you in it?  I signed up today.  A lot of the time you have to rely on trial-and-error to get all the variables optimized for your purpose.\n\nAlso, there's more to it than just number of layers and neurons. There are also things like bias, learning rate, learning algorithms, etc. Ah yes, I had similar questions about bias but I forgot to put it in my post. Why exactly is bias used? It seems like it's purpose is to shift the activation functions by some amount, but I'm not sure why that helps. In practice, is this just another trial and error variable to optimize?  Ah yes, I had similar questions about bias but I forgot to put it in my post. Why exactly is bias used? It seems like it's purpose is to shift the activation functions by some amount, but I'm not sure why that helps. In practice, is this just another trial and error variable to optimize?   I'm currently a student, and have done (some) introductory neural net stuff and am planning to do more. When asking professors about neural net architecture, they have just said basically what crwcomposer said, \"just try stuff out and see what works\"...\n\nI'm interested to hear what anyone with more experience has to say as well. That was the most disappointing lecture in my AI class. He draw a three boxes: \"input\" -&gt; \"magic\" -&gt; \"output\". \"Alright, so you have all your inputs coming in over here, then youdosomemagicrighthere and then you have your output neurons over here, and you're done!\" Like finding out Santa Clause wasn't real. Yeah, I remember hearing in lecture about how researchers somehow \"gave a neural net dyslexia\" by messing with connections, but because it was a neural net they had no idea why it happened.  I believe [this](http://www.cs.toronto.edu/~hinton/absps/sciam93.pdf) is the work you're referring to, and they do offer some compelling mathematical explanations for some of the symptoms of dyslexia. It's a very interesting read, albeit a bit dated. That was the most disappointing lecture in my AI class. He draw a three boxes: \"input\" -&gt; \"magic\" -&gt; \"output\". \"Alright, so you have all your inputs coming in over here, then youdosomemagicrighthere and then you have your output neurons over here, and you're done!\" Like finding out Santa Clause wasn't real. I'm currently a student, and have done (some) introductory neural net stuff and am planning to do more. When asking professors about neural net architecture, they have just said basically what crwcomposer said, \"just try stuff out and see what works\"...\n\nI'm interested to hear what anyone with more experience has to say as well.    I'm an undergrad students that's been itching to learn more neural networks; I won't get to take the AI course offered at my University.  Can anybody recommend me a good book? Maybe a good machine learning book? Currently I'm reading \" On Intelligence\"  (can't remember the author right now)  which talked briefly about neural networks. ",
    "url" : "http://www.reddit.com/r/compsci/comments/10qlh2/neural_network_designs/"
  }, {
    "id" : 22,
    "title" : "Fast Algorithms for the Maximum Clique Problem on Massive Sparse Graphs",
    "snippet" : "  ",
    "url" : "http://arxiv.org/abs/1209.5818"
  }, {
    "id" : 23,
    "title" : "What other models are there for parallel computation?",
    "snippet" : "I've been reading about parallel algorithms and have so far encountered:\n \n- [PRAM](http://en.wikipedia.org/wiki/Parallel_Random_Access_Machine): Simultaneous computation with access to shared memory. \n\n- [Bulk Synchronous](http://en.wikipedia.org/wiki/Bulk_synchronous_parallel): Simultaneous computation on processors accessing only local memory, interwoven with global synchronization/communications steps. \n\n- [Message Passing](http://en.wikipedia.org/wiki/Message_passing): Independent computational elements communicating without shared mutable memory or global synchronizations. \n\nWhat other ways are there to express/encode parallel computation? \n\nedit: Just to clarify, I'm not looking for complexity classes (like NC) or hardware models (like SIMD). Also, by \"parallel\" I mean roughly \"usefully spread work across thousands of processors\". \n  'Message Passing' usually means 'actors', but a kinda similar scheme is CSP. The practical difference in implementations for the developer is that with actors you usually hold on to the processes, as actual values. With CSP however you instead hold on to the channels that connect them, and don't hold the process.\n\nCSP also only communicates when both sides are reading and writing simultaneously; with many actor implementations one side can usually write, or read, without the other side being involved. A mailbox often sits in between, allowing writing to always be non-blocking, and reading happen in the future.\n\nI did a little programming with Occam-Pi at University, which is a buffed up version of Occam, which in turn is built around CSP. I really like the CSP model, and it feels like it would be a lot more efficient then most actor implementations; namely because there is no overhead from handling a mailbox (yet you can build one if you wish to).\n\nI don't know if this is specific to CSP, or an extension, but Occam-Pi also supports selecting multiple channels when reading. This is so they are tested in sequence (i.e. try to read from channel A, then B, then C, and so on), or it picks them psudo-randomly (so one channel can't starve out the others, although in practice, it often can).\n\nOccam also supports these neat PAR and SEQ blocks, where you state if the code in the block should be run in parallel or sequence. In theory PAR blocks can be run in parallel, but in practice it's more to infer your design. You see Occam can also detect dead locks, not only at runtime, but also at compile time (which is one of the cool things about Occam). So if you create two processes in sequence which both interact straight away, the first will be deadlocked the moment it begins, because the other doesn't yet exist. PAR gives your intention that conceptually, they start in parallel, and so avoids certain deadlocks (even if they really do start in sequence).\n\nThere is also far more to CSP than this; I am not an expert on the subject, and certainly haven't looked at the formal side of it such as the Pi calculus. However if you are interested in more, you could try contacting the [concurrency research team at the University of Kent](http://blogs.kent.ac.uk/crg-group/), who do a fair amount of CSP and Occam related projects. They are nice guys, and the model is awesome. However the Occam-Pi language they developed, when I used it, needed a lot more maturity. There is also a lot of CSP info around online.  I'm not sure if this falls under some of your models, but there's also [Software transactional memory](https://en.wikipedia.org/wiki/Software_transactional_memory)   Since I learned of it earlier this year I've come to consider Oz's Data Flow concurrency model to be underrated or perhaps just not widely enough known:\n\nhttp://en.wikipedia.org/wiki/Oz_(programming_language)#Dataflow_variables_and_declarative_concurrency\n\nI guess maybe part of the problem is that as far as I know Oz is the only working implementation of this (hopefully I'm wrong and people will post other examples) and Oz is in practice, about 50x slower than e.g. java/c etc for most things... preventing it from being used outside of Academia.  As far as I know these are just limitations of the current implementation and there's no reason why this couldn't be corrected or this same model couldn't be implemented in another language without any slowdown. \n\nETA: I consider Oz to be one of the most interesting fringe programming languages and if it were at least as fast as my current goto, python, I would be tempted to (after getting at least competent at it first) try using it as my goto language for a while to see how it went, and I would be expecting good results. I found Oz to be an annoying language to debug, it was used as a language in programming language course I took.  Can you elaborate on what some of the problems were? Since I learned of it earlier this year I've come to consider Oz's Data Flow concurrency model to be underrated or perhaps just not widely enough known:\n\nhttp://en.wikipedia.org/wiki/Oz_(programming_language)#Dataflow_variables_and_declarative_concurrency\n\nI guess maybe part of the problem is that as far as I know Oz is the only working implementation of this (hopefully I'm wrong and people will post other examples) and Oz is in practice, about 50x slower than e.g. java/c etc for most things... preventing it from being used outside of Academia.  As far as I know these are just limitations of the current implementation and there's no reason why this couldn't be corrected or this same model couldn't be implemented in another language without any slowdown. \n\nETA: I consider Oz to be one of the most interesting fringe programming languages and if it were at least as fast as my current goto, python, I would be tempted to (after getting at least competent at it first) try using it as my goto language for a while to see how it went, and I would be expecting good results.    Intel has a really nice library called Thread Building Blocks, it is mostly for multi-processor systems with shared memory, not clusters.\nI am personally researching Wait-Free Algorithm design, which is a form of progress guarantee that guarantees in a finite amount of time a process will complete, regardless of what other processes are doing. In order to meet this progress guarantee, no locks or mutual exclusion can be used.\n\nAlso LIBCDS (libcds.sourceforge.net) CBBS (amino-cbbs.sourceforge.net) are good concurrent data structure libraries.\n\nAlso google has its own language \"GO\" http://golang.org/project/\n\nAre your more interested in clusters or shared memory systems?",
    "url" : "http://www.reddit.com/r/compsci/comments/10q19h/what_other_models_are_there_for_parallel/"
  }, {
    "id" : 24,
    "title" : "Are Shannon entropy and Boltzmann entropy mutually convertible?",
    "snippet" : "  They're related but not exactly the same thing. The two concepts are trying to measure fundamental properties of systems but they make generally different assumptions.\n\nThe Boltzmann entropy is a measure of a physical system's energy distribution. Naturally it is in units of Energy per Temperature.\n\nShannon's Entropy concerns the physical properties of signals (in the sense that information is physical). Specifically, it concerns how we can relate signals to eachother. Assuming very reasonable conditions, the signals 11111111111111111 and 1111 carry the same information so one may be transformed into the other and preserve a certain class of physical characteristics. \n\nSignals are actually a wider class of physical systems than are considered in Boltzmann entropy. Here is the connection then:\n\nIf we consider the a signal as a physical system that conveys information through energy distribution, the dynamics of the system will be describable by both Boltzmann entropy and Shannon entropy.\n\nEdit: Hahaha. I just noticed in your link that Peter Shor's answer is at the bottom of your link with no replies.  &gt; The Boltzmann entropy is a measure of a physical system's energy distribution.\n\nMore pedantically, the Boltzmann entropy is a measure of an observer's state of knowledge of a physical system's energy distribution. I could be wrong, but wouldn't that be implied by it being a physical \"measure\" of something? They're related but not exactly the same thing. The two concepts are trying to measure fundamental properties of systems but they make generally different assumptions.\n\nThe Boltzmann entropy is a measure of a physical system's energy distribution. Naturally it is in units of Energy per Temperature.\n\nShannon's Entropy concerns the physical properties of signals (in the sense that information is physical). Specifically, it concerns how we can relate signals to eachother. Assuming very reasonable conditions, the signals 11111111111111111 and 1111 carry the same information so one may be transformed into the other and preserve a certain class of physical characteristics. \n\nSignals are actually a wider class of physical systems than are considered in Boltzmann entropy. Here is the connection then:\n\nIf we consider the a signal as a physical system that conveys information through energy distribution, the dynamics of the system will be describable by both Boltzmann entropy and Shannon entropy.\n\nEdit: Hahaha. I just noticed in your link that Peter Shor's answer is at the bottom of your link with no replies. ",
    "url" : "http://cstheory.stackexchange.com/questions/12763/are-shannon-entropy-and-boltzmann-entropy-mutually-convertible"
  }, {
    "id" : 25,
    "title" : "Profiling Field Initialisation in Java",
    "snippet" : "  There are several odd things about the paper. The Java platform allows writing to final fields in principle, using reflection. (The semantics are a bit iffy, but this is used to implement serialization, for instance. This should be visible in the ) Even without reflection, it is quite straightforward to construct cyclic data structures based on final fields because you can pass around references to partially constructed objects.\n\nAnyway, apart from these minor nits, the conclusion\n\n&gt; The extremely high number of stationary fields that we found suggests that [...] VM authors should optimise for immutability.\n\nsimply doesn't follow from the data because their statistics are mapped back from run-time behavior to individual class fields. For all we know, some of the benchmarks could contain a staggeringly high number of repeated writes to a very small number of fields.\n\nThe authors also missed prior work in this area, in the form of`java.lang.invoke.SwitchPoint`, which can be used to implement quasi-constants which the VM will optimize as if they were constant, but can still be changed, resulting in deoptimization. &gt; The semantics are a bit iffy\n\nRight, this is a bad idea in general --- see e.g. [this question](http://stackoverflow.com/questions/3301635/change-private-static-final-field-using-java-reflection) and [this question](http://stackoverflow.com/questions/9391401/java-reflection-change-private-static-final-field-didnt-do-anything) on stackoverflow.\n\n&gt; Even without reflection, it is quite straightforward to construct cyclic  \n&gt; data structures based on final fields because you can pass around   \n&gt; references to partially constructed objects.\n\nNo, this does not solve the problem presented.  You must construct one object before the other.  The only way around that is if one of the constructors itself constructs the other object --- but this is not a general solution to the problem.\n\n&gt; simply doesn't follow from the data because their statistics are  \n&gt; mapped back from run-time behavior to individual class fields.\n\nRight, its indicative --- i.e. testing more inputs would increase confidence in the results.  This limitation is explicitly identified in the paper.\n\n&gt;  missed prior work in this area, in the form of java.lang.invoke.SwitchPoint\n\nI'm not really sure why this is relevant?? &gt; No, this does not solve the problem presented. You must construct one object before the other. The only way around that is if one of the constructors itself constructs the other object --- but this is not a general solution to the problem.\n\nSure, but the authors claimed it's not possible *at all*.\n\n&gt; Right, its indicative --- i.e. testing more inputs would increase confidence in the results.\n\nI doubt that, because they measure the wrong thing (or tally the measurements in the wrong way).\n\n&gt; I'm not really sure why this is relevant??\n\n`SwitchPoint`s can be used to make values immutable (and subject to compiler optimizations such as constant propagation) even if you can't prove that they won't change at a later point in the execution of the program.",
    "url" : "http://whiley.org/2012/09/30/profiling-field-initialisation-in-java/"
  }, {
    "id" : 26,
    "title" : "Strategy for PacMan AI",
    "snippet" : "For fun I will be programming a PacMan player AI. I was wondering if there is an optimal strategy to use, or if anyone could point me towards some resources for this. \n\nThanks   Take a look at Berkeley's AI course (https://berkeley.edx.org/). The course covers a gambit of AI topics and all the projects involve making Pacman AI. It has live lectures, lecture slides, gradable homework/projects for the public. Plus, it's free and Klein (the lecturer) is super awesome. &gt; The course covers a gambit of AI topics\n\n'Gamut', unless it somehow risks something in order to achieve a better position in the end. Maybe he meant it he was going to (metaphorically) throw charged playing cards at it and talk with a Cajun accent. &gt; The course covers a gambit of AI topics\n\n'Gamut', unless it somehow risks something in order to achieve a better position in the end. embarrassing! Though I really didn't know there were two distinct spellings, so thanks for saving me for the future! \"Two distinct spellings\"?!\n\nThey are different words. Yep. My confusion was that the two distinct words were spelled the same. \"Gambit\" and \"gamut\" are not spelled the same. Yep. And before today, I did not know that.  They don't sound alike, either.  The 'b' in \"gambit\" is not silent. :/ not my day. Can't spell or hear. Take a look at Berkeley's AI course (https://berkeley.edx.org/). The course covers a gambit of AI topics and all the projects involve making Pacman AI. It has live lectures, lecture slides, gradable homework/projects for the public. Plus, it's free and Klein (the lecturer) is super awesome.  Avoid the ghosts. Avoid the ghosts. while player.location in ghosts.locations:\n    player.location = get_random_location()   Been there, done this. At each intersection, check available directions. Two ghosts favor vertical, two favor horizontal. Take the direction that gets closer to PacMan. If a tie (as faras which gets closer), favored  axis wins. To make it easier (and mimic original PacMan behavior IIRC), have them only change direction when can no longer go in current direction.   For blue ghosts (evading), take direction that moves it further away instead of closer. Edit: for distance comparison, compute cell position to target (abs value) in each direction. For example, if in (leading position) in cell 5 but ghost is in 8, distance is 3 cells away for that axis.  Edit2: because you are working with 2 axis, your code can work for a generic axis, then run it for each axis. The original game has different strategies for each ghost which is why it works so well.  That said I think hes looking for player strategies not ghost ones. Been there, done this. At each intersection, check available directions. Two ghosts favor vertical, two favor horizontal. Take the direction that gets closer to PacMan. If a tie (as faras which gets closer), favored  axis wins. To make it easier (and mimic original PacMan behavior IIRC), have them only change direction when can no longer go in current direction.   For blue ghosts (evading), take direction that moves it further away instead of closer. Edit: for distance comparison, compute cell position to target (abs value) in each direction. For example, if in (leading position) in cell 5 but ghost is in 8, distance is 3 cells away for that axis.  Edit2: because you are working with 2 axis, your code can work for a generic axis, then run it for each axis.        ",
    "url" : "http://www.reddit.com/r/compsci/comments/10o69r/strategy_for_pacman_ai/"
  }, {
    "id" : 27,
    "title" : "\"I wish there were general literacy in computer science,\" Jaron Lanier says, \"Which is different from learning to program.\"",
    "snippet" : "  One of my profs was especially determined to pound the math behind computer science into our thinking.  There was a *lot* of muttering from students because of this, since they were sure that C.S. was actually about producing lines of debugged code, and all he really needed to do was give the assignments and let them do it.  \n\n(This is a total tangent, but this is one of the reasons that I think students are the worst people to consult on the topic of education or prof's performance.  They don't know what they're talking about, and they refuse to separate personal issues from objective reasons.) Regarding your tangent, should a professor's performance be graded on how effectively they're transferring their knowledge onto students? It doesn't matter what the professor knows if he can't properly explain it to his students, so who better to ask about his performance? (I'm legitimately curious to know if you have an alternative, not just asking leading questions)\n\nI suppose there's an argument for the students not understanding the methods, but by the end of the semester/quarter/year, it should be clear if the methods were valid or not. As a tangent to the tangent...  Can most students students, most of the time, be objective about the knowledge transfered by a professor?  Are their self-reported judgements the best gauge of the knowledge transfered, or of the professor's role in knowledge transfer?  I think there's a bias for students who did well in a class to rate highly, and a bias for students who did poorly in a class to give low ratings that has less to do with the professor's ability or performance and more to do with the student's mood at the time of evaluation.   If the students who did poorly rated low and the students who did well rated high, the system's working properly in my opinion. If professor performance is a rating of his well he conveys knowledge, the students who did well *should* feel he's a better teacher, because he worked for their learning pattern. We have a huge stigma against ratings being less than perfect, but it's incredibly difficult to match every learning style. \n\nWhen rating teachers, I've always tried to be conscious of who's responsible for how I did. Did I spend 4 hours a night studying because I didn't learn anything in class? Did I do poorly because, despite the instructor's best efforts, I didn't put any effort into the class? Obviously not everyone will do so, but there will always be a signal-to-noise ratio. Regarding your tangent, should a professor's performance be graded on how effectively they're transferring their knowledge onto students? It doesn't matter what the professor knows if he can't properly explain it to his students, so who better to ask about his performance? (I'm legitimately curious to know if you have an alternative, not just asking leading questions)\n\nI suppose there's an argument for the students not understanding the methods, but by the end of the semester/quarter/year, it should be clear if the methods were valid or not. &gt; how effectively  they're transferring their knowledge onto students?\n\nSure, I'm just saying that students aren't useful sources of that metric.  If we want to evaluate instructors, it should be by people qualified to do the judging.  Student's are invariably self-centered in their voting, both good and bad.  Since it's the bad reviews we're interested in, I find that they relate more to  low marks, tough subjects, or the instructors unwillingness to give assignments for extra credit.  I'm not saying they are or aren't bad instructors, I'm saying I wouldn't take a student's word for it, there's too much noise in the signal.  \n\nAlso, we're talking about university level.  A student has to be prepared to do the heavy lifting themselves, the working world (or research world) that comes after won't have any instructors at all. They will have to deal with the coworkers that they get, so to speak.  The prof I got for electronic design had an astounding accent, still the material remained my problem.  (sir quee uts?  what?) \n\nAs for a solution, it would require a lot more financial interest in education and in north america we've been going the other way since the mid-80s.  I know several profs  (science profs, in fact) whose tenure isn't in lab-research but research in their classroom, ostensibly trying to find better ways to teach.  These days, unfortunately, that winds up meaning \"ways to reach more students with the same resources\" because funding keeps going down every year.  If we can get ahead of that, I would suggest my solution would be the formation of an organisation that does nothing but evaluates teaching, either on a classroom or school-wide level.  Not to issue permits or penalties, but to offer help for improvement where needed (and to take advantage new approaches when they find them.)\n\nBasic, basic things are still in flux, even though mankind has been teaching for thousands of years: what is the objective of teaching in general and of the university level?  What is a useful evaluation?  Is it testing?  If so, what marks mean 'good enough', and at what level should we tell a student they're wasting their time?  (Frankly, imo, not everyone is cut out for university.  I went and finished, but I wouldn't say that it has been useful professionally or that I'm cut out for academic pursuits.)  \n\nI moved around a lot as a kid  (I went to 6 different schools by grade 6, 8 by the time I graduated) and I can tell you that was 8 completely different programs, approachs, atmospheres and attitudes.  Sometimes schools change programs/approaches from one year to the next for year after year.  And I've never heard of any evaluation having been done after the changes to decide what was gained or lost in doing so.  Chaos.  I was talking to a friend last night who told me that at his old high school they graduated 200 students this year, and 40 were 'Ontario Scholars'.  That number was 4 or 5 in 200 when he graduated.  Did they get magically better or, more likely, are students getting marked easier to make it easier to get into university?  The requirements *at* university haven't really changed, in fact there is more new knowledge to learn all the time, so that means it's likely that students are finding it even harder when they get there, given the relatively easy time they've had up until then.  That frustration just as often as not gets expressed as 'my prof sucks'.  \n\n I would argue that it's very easy to determine if a student has valid complaints, though that does still mean that somebody has to go through and read every evaluation. There's a big difference between \"the tests were too hard\" and \"the questions on tests were full of small 'gotcha's, and made unnecessarily difficult because of it.\" There are other factors (students leave \"other comments\" or explanation fields blank, for example), but it's a fairly simple matter to establish legitimate complaints. I would argue that properly balancing the results and their consequences is much more difficult. (Students who do poorly are more likely to complain than students who do well are likely to praise, etc).  At the risk of losing all credibility, I'll admit my wife is a science prof.  She goes through all of the complaint slips every semester and talks with me about the negatives  (because they upset her).  Between that and my own university experience  (a total of about 8 years at two universities &amp; one college.  One period when I was 19-20, then a 3 year diploma program and another 3 yrs in another degree when I was roughly 30) I really just feel that the overwhelming majority of complaints are just externalising faults.\n\nSome of the complaints she has received, for example, conflict: \"the prof reviewed too much at the beginning of each class and left too little time for the new material each day\", \"the reviews were too short\".  I've sat alongside students like those, blaming everyone and everything except themselves.\n\nThe problem with reading every evaluation is the setup: Department chairs are themselves profs who know the score.  How many can/will they read before they start going through the motions, for that one pearl amongst all the other dross that might be relevant?  This is not what I would call 'low hanging fruit\". &gt; how effectively  they're transferring their knowledge onto students?\n\nSure, I'm just saying that students aren't useful sources of that metric.  If we want to evaluate instructors, it should be by people qualified to do the judging.  Student's are invariably self-centered in their voting, both good and bad.  Since it's the bad reviews we're interested in, I find that they relate more to  low marks, tough subjects, or the instructors unwillingness to give assignments for extra credit.  I'm not saying they are or aren't bad instructors, I'm saying I wouldn't take a student's word for it, there's too much noise in the signal.  \n\nAlso, we're talking about university level.  A student has to be prepared to do the heavy lifting themselves, the working world (or research world) that comes after won't have any instructors at all. They will have to deal with the coworkers that they get, so to speak.  The prof I got for electronic design had an astounding accent, still the material remained my problem.  (sir quee uts?  what?) \n\nAs for a solution, it would require a lot more financial interest in education and in north america we've been going the other way since the mid-80s.  I know several profs  (science profs, in fact) whose tenure isn't in lab-research but research in their classroom, ostensibly trying to find better ways to teach.  These days, unfortunately, that winds up meaning \"ways to reach more students with the same resources\" because funding keeps going down every year.  If we can get ahead of that, I would suggest my solution would be the formation of an organisation that does nothing but evaluates teaching, either on a classroom or school-wide level.  Not to issue permits or penalties, but to offer help for improvement where needed (and to take advantage new approaches when they find them.)\n\nBasic, basic things are still in flux, even though mankind has been teaching for thousands of years: what is the objective of teaching in general and of the university level?  What is a useful evaluation?  Is it testing?  If so, what marks mean 'good enough', and at what level should we tell a student they're wasting their time?  (Frankly, imo, not everyone is cut out for university.  I went and finished, but I wouldn't say that it has been useful professionally or that I'm cut out for academic pursuits.)  \n\nI moved around a lot as a kid  (I went to 6 different schools by grade 6, 8 by the time I graduated) and I can tell you that was 8 completely different programs, approachs, atmospheres and attitudes.  Sometimes schools change programs/approaches from one year to the next for year after year.  And I've never heard of any evaluation having been done after the changes to decide what was gained or lost in doing so.  Chaos.  I was talking to a friend last night who told me that at his old high school they graduated 200 students this year, and 40 were 'Ontario Scholars'.  That number was 4 or 5 in 200 when he graduated.  Did they get magically better or, more likely, are students getting marked easier to make it easier to get into university?  The requirements *at* university haven't really changed, in fact there is more new knowledge to learn all the time, so that means it's likely that students are finding it even harder when they get there, given the relatively easy time they've had up until then.  That frustration just as often as not gets expressed as 'my prof sucks'.  \n\n Regarding your tangent, should a professor's performance be graded on how effectively they're transferring their knowledge onto students? It doesn't matter what the professor knows if he can't properly explain it to his students, so who better to ask about his performance? (I'm legitimately curious to know if you have an alternative, not just asking leading questions)\n\nI suppose there's an argument for the students not understanding the methods, but by the end of the semester/quarter/year, it should be clear if the methods were valid or not. One of my profs was especially determined to pound the math behind computer science into our thinking.  There was a *lot* of muttering from students because of this, since they were sure that C.S. was actually about producing lines of debugged code, and all he really needed to do was give the assignments and let them do it.  \n\n(This is a total tangent, but this is one of the reasons that I think students are the worst people to consult on the topic of education or prof's performance.  They don't know what they're talking about, and they refuse to separate personal issues from objective reasons.) &gt;(This is a total tangent, but this is one of the reasons that I think students are the worst people to consult on the topic of education or prof's performance. They don't know what they're talking about, and they refuse to separate personal issues from objective reasons.)\n\nThey're parroting what industry wants and what will get them a job. If there were less economic pressure on them I'm sure they'd be asking for different things. I know I did when I realized this wasn't just about getting a job; I'd be getting a job as a programmer *anyway* so why not also learn something more advanced than just spitting out lines of code? One of my profs was especially determined to pound the math behind computer science into our thinking.  There was a *lot* of muttering from students because of this, since they were sure that C.S. was actually about producing lines of debugged code, and all he really needed to do was give the assignments and let them do it.  \n\n(This is a total tangent, but this is one of the reasons that I think students are the worst people to consult on the topic of education or prof's performance.  They don't know what they're talking about, and they refuse to separate personal issues from objective reasons.) It depends on what you are trying to judge when it comes to a professors performance.\n\nI've had professors which are badly organized, rarely respond to mails (or just take weeks/months to do so), talk far too quietly, just read slides (so no real engagement), and so on. Another issue is that some professors will put their own personal ideology, above real practical benefits, when it comes to course material (i.e. expects students to use Windows, because that's what the CS machines all run, but the only gives practical help for students running Mac OS, because that is what he uses).\n\nBeing an expert in a field, doesn't mean you can teach. One of my profs was especially determined to pound the math behind computer science into our thinking.  There was a *lot* of muttering from students because of this, since they were sure that C.S. was actually about producing lines of debugged code, and all he really needed to do was give the assignments and let them do it.  \n\n(This is a total tangent, but this is one of the reasons that I think students are the worst people to consult on the topic of education or prof's performance.  They don't know what they're talking about, and they refuse to separate personal issues from objective reasons.)   I know it's a small and insignificant detail, but\n\n&gt; h30565.www3.hp.com\n\n\\*sigh\\* What's wrong with that hostname? It's hideous, and it implies a messed up server architecture.\n\nA quick google search yields: http://daringfireball.net/linked/2010/03/09/hp-license-plate-domains Classy looking website you got there my good man It's not mine.  iirc that's the guy who wrote markdown. It's hideous, and it implies a messed up server architecture.\n\nA quick google search yields: http://daringfireball.net/linked/2010/03/09/hp-license-plate-domains  \"One problem is economic, he says: If we pretend that the people don't exist, then people (like artists and musicians) don't get paid. Eventually we'll have self-driving cars and automated manufacturing processing... which means even less employment for humans.\" \n\nWhy is that bad? Why do we have to come up with these worthless time-wasting jobs that could be automated using technology? Keeping innovation down to keep jobs up is not good for the economy. \"One problem is economic, he says: If we pretend that the people don't exist, then people (like artists and musicians) don't get paid. Eventually we'll have self-driving cars and automated manufacturing processing... which means even less employment for humans.\" \n\nWhy is that bad? Why do we have to come up with these worthless time-wasting jobs that could be automated using technology? Keeping innovation down to keep jobs up is not good for the economy.  Oh, another \"every skill is useful except programming\" post. Is it that time of the week already? Oh, another \"every skill is useful except programming\" post. Is it that time of the week already? Oh, another \"every skill is useful except programming\" post. Is it that time of the week already? ",
    "url" : "http://h30565.www3.hp.com/t5/HPIO-Video/Jaron-Lanier-Let-s-Unmask-the-Great-and-Powerful-Oz-of/ba-p/7664"
  }, {
    "id" : 28,
    "title" : "An implementation of the 250 year old encryption algorithm by Leibniz that was recently rediscovered by Dr. Rescher at the University of Pittsburgh [x-post /r/crypto]",
    "url" : "http://www.reddit.com/r/crypto/comments/10mpz9/an_implementation_of_the_250_year_old_encryption/"
  }, {
    "id" : 29,
    "title" : "Pretty sure reddit just saved my career in computer science...",
    "snippet" : "I'm a Sophomore Computer Science major and I'm currently taking Discrete Math.  My professor is a visiting instructor from Japan who not only has an incredibly difficult accent to understand, but also speaks in recursive riddles.  The worst part is when you ask a question, most times my prof. doesn't even understand what you're trying to ask.\n\nTo further complicate matters, the textbook is also incredibly vague.\n\nSo I did what anyone would do: I searched reddit for Discrete Math (lol).\n\nOne of you fine redditors posted a link to University of Illinois Discrete Math class web page, complete with full and comprehensive lecture notes.  All of a sudden Discrete Math sounds a lot less like gibberish.\n\nTL;DR - Thank you reddit, you have quite conceivably just saved my career in computer science.\n\nedit:\nIf you were curious, the link: [http://courses.engr.illinois.edu/cs173/fa2011/](http://courses.engr.illinois.edu/cs173/fa2011/)\n  You should email the author of those lecture notes to say thanks. I've met her -- she's nice ;) Took your advice and did send her that email, quoting it below (mostly a repeat of what I already said, but still).\n\nedit: bolded the portion that was more personal message for the instructor.\n\n&gt;Professor Fleck,\n\n&gt;Obviously you don't know me.  Ostensibly by now you're figured out I'm __.\n\n&gt;I'm a Sophomore Computer Science student at Florida Atlantic University and currently taking Discrete Math.\n\n&gt;Unfortunately for me, my Discrete Math teacher is a visiting instructor from Japan whom I have a lot of trouble understanding and ironically (or not?), she doesn't understand most of what I ask either.\n\n&gt;To further complicate matters, our assigned textbook is unusually vague and teaches from a higher level of understanding than anyone would expect from a Discrete Math class with no pre-requisites (FAU doesn't require any at all for Discrete Math).\n\n&gt;To make a long story short: after just about failing my first Exam and continuing to struggle to understand between the text and my instructor I was in peril, ready to drop the class (and honestly even re-evaluating my path in Computer Science).  I took to the internet, specifically reddit.\n\n&gt;Within 20 minutes I stumbled across the link to your class, http://courses.engr.illinois.edu/cs173/fa2011/ .\n\n&gt; **I would like to take this opportunity to give you an immense THANK YOU!  If not for your lecture notes (as they go in the exact order as our class, but with much more detail and better explanation). I would likely be dropping this class and still be feeling as if Discrete Math is some horribly abstract subject that I will never be able to understand.  All of a sudden, it all sounds a lot less like gibberish.**\n\n&gt; **So, again, as I can not say it enough: THANK YOU!  You have quite conceivably just saved my career in Computer Science.**\n\n&gt;Best Regards,\n\n&gt; __\n\n&gt; **P.S.  Good teachers are horribly underappreciated, and hard to come by.**\n Took your advice and did send her that email, quoting it below (mostly a repeat of what I already said, but still).\n\nedit: bolded the portion that was more personal message for the instructor.\n\n&gt;Professor Fleck,\n\n&gt;Obviously you don't know me.  Ostensibly by now you're figured out I'm __.\n\n&gt;I'm a Sophomore Computer Science student at Florida Atlantic University and currently taking Discrete Math.\n\n&gt;Unfortunately for me, my Discrete Math teacher is a visiting instructor from Japan whom I have a lot of trouble understanding and ironically (or not?), she doesn't understand most of what I ask either.\n\n&gt;To further complicate matters, our assigned textbook is unusually vague and teaches from a higher level of understanding than anyone would expect from a Discrete Math class with no pre-requisites (FAU doesn't require any at all for Discrete Math).\n\n&gt;To make a long story short: after just about failing my first Exam and continuing to struggle to understand between the text and my instructor I was in peril, ready to drop the class (and honestly even re-evaluating my path in Computer Science).  I took to the internet, specifically reddit.\n\n&gt;Within 20 minutes I stumbled across the link to your class, http://courses.engr.illinois.edu/cs173/fa2011/ .\n\n&gt; **I would like to take this opportunity to give you an immense THANK YOU!  If not for your lecture notes (as they go in the exact order as our class, but with much more detail and better explanation). I would likely be dropping this class and still be feeling as if Discrete Math is some horribly abstract subject that I will never be able to understand.  All of a sudden, it all sounds a lot less like gibberish.**\n\n&gt; **So, again, as I can not say it enough: THANK YOU!  You have quite conceivably just saved my career in Computer Science.**\n\n&gt;Best Regards,\n\n&gt; __\n\n&gt; **P.S.  Good teachers are horribly underappreciated, and hard to come by.**\n [Ostensibly : Apparently or purportedly, but perhaps not actually: \"portrayed as a blue-collar type, ostensibly a carpenter\".](https://www.google.com/search?sugexp=chrome,mod=3&amp;sourceid=chrome&amp;ie=UTF-8&amp;q=Ostensibly#hl=en&amp;q=ostensibly&amp;tbs=dfn:1&amp;tbo=u&amp;sa=X&amp;ei=3dZkUIyFHLOD0QH8q4CYDg&amp;ved=0CCAQkQ4&amp;bav=on.2,or.r_gc.r_pw.r_cp.r_qf.&amp;fp=e144f1b25e45ee57&amp;biw=1207&amp;bih=788)\n\n OP presumably meant to say \"presumably.\" Purportedly meant to say reportedly. Allegedly meant to say supposedly. [Ostensibly : Apparently or purportedly, but perhaps not actually: \"portrayed as a blue-collar type, ostensibly a carpenter\".](https://www.google.com/search?sugexp=chrome,mod=3&amp;sourceid=chrome&amp;ie=UTF-8&amp;q=Ostensibly#hl=en&amp;q=ostensibly&amp;tbs=dfn:1&amp;tbo=u&amp;sa=X&amp;ei=3dZkUIyFHLOD0QH8q4CYDg&amp;ved=0CCAQkQ4&amp;bav=on.2,or.r_gc.r_pw.r_cp.r_qf.&amp;fp=e144f1b25e45ee57&amp;biw=1207&amp;bih=788)\n\n Took your advice and did send her that email, quoting it below (mostly a repeat of what I already said, but still).\n\nedit: bolded the portion that was more personal message for the instructor.\n\n&gt;Professor Fleck,\n\n&gt;Obviously you don't know me.  Ostensibly by now you're figured out I'm __.\n\n&gt;I'm a Sophomore Computer Science student at Florida Atlantic University and currently taking Discrete Math.\n\n&gt;Unfortunately for me, my Discrete Math teacher is a visiting instructor from Japan whom I have a lot of trouble understanding and ironically (or not?), she doesn't understand most of what I ask either.\n\n&gt;To further complicate matters, our assigned textbook is unusually vague and teaches from a higher level of understanding than anyone would expect from a Discrete Math class with no pre-requisites (FAU doesn't require any at all for Discrete Math).\n\n&gt;To make a long story short: after just about failing my first Exam and continuing to struggle to understand between the text and my instructor I was in peril, ready to drop the class (and honestly even re-evaluating my path in Computer Science).  I took to the internet, specifically reddit.\n\n&gt;Within 20 minutes I stumbled across the link to your class, http://courses.engr.illinois.edu/cs173/fa2011/ .\n\n&gt; **I would like to take this opportunity to give you an immense THANK YOU!  If not for your lecture notes (as they go in the exact order as our class, but with much more detail and better explanation). I would likely be dropping this class and still be feeling as if Discrete Math is some horribly abstract subject that I will never be able to understand.  All of a sudden, it all sounds a lot less like gibberish.**\n\n&gt; **So, again, as I can not say it enough: THANK YOU!  You have quite conceivably just saved my career in Computer Science.**\n\n&gt;Best Regards,\n\n&gt; __\n\n&gt; **P.S.  Good teachers are horribly underappreciated, and hard to come by.**\n I graduated from FAU (CS), which teacher is it ? I don't do this in any way to bash her, because she definitely knows her stuff.  I just have trouble understanding her, which is incredibly rare for me (I'm pretty smart).  Don't get me wrong, some people are doing pretty well, but class average for Exam #1 was 52%.  \n\nWith that disclaimer out of the way, it is Keiko Ito. Not sure if she's taught much at FAU before, like I said she is listed as a \"Visiting Instructor\". You failed to mention that she is apparently an owl. No wonder you can't understand her http://www.math.fau.edu/faculty/ito.html &gt; No wonder you can't understand her\n\nUnderstand whoo? You failed to mention that she is apparently an owl. No wonder you can't understand her http://www.math.fau.edu/faculty/ito.html You failed to mention that she is apparently an owl. No wonder you can't understand her http://www.math.fau.edu/faculty/ito.html I laughed entirely too hard at this, and now my coworkers think I'm crazy. You failed to mention that she is apparently an owl. No wonder you can't understand her http://www.math.fau.edu/faculty/ito.html You failed to mention that she is apparently an owl. No wonder you can't understand her http://www.math.fau.edu/faculty/ito.html I don't do this in any way to bash her, because she definitely knows her stuff.  I just have trouble understanding her, which is incredibly rare for me (I'm pretty smart).  Don't get me wrong, some people are doing pretty well, but class average for Exam #1 was 52%.  \n\nWith that disclaimer out of the way, it is Keiko Ito. Not sure if she's taught much at FAU before, like I said she is listed as a \"Visiting Instructor\". Yea, never heard of her, however I did have my fair share of instructors there I didn't understand.   If you're like me and can learn on your own, take advantage of all the online resources, and learn on your own, you should be fine.  Discrete isn't all that hard once you understand the concepts.\n\nGood luck man. &gt;Discrete isn't all that hard once you understand the concepts.\n\nWell yeah. Pretty much everything is easy once you understand it. Took your advice and did send her that email, quoting it below (mostly a repeat of what I already said, but still).\n\nedit: bolded the portion that was more personal message for the instructor.\n\n&gt;Professor Fleck,\n\n&gt;Obviously you don't know me.  Ostensibly by now you're figured out I'm __.\n\n&gt;I'm a Sophomore Computer Science student at Florida Atlantic University and currently taking Discrete Math.\n\n&gt;Unfortunately for me, my Discrete Math teacher is a visiting instructor from Japan whom I have a lot of trouble understanding and ironically (or not?), she doesn't understand most of what I ask either.\n\n&gt;To further complicate matters, our assigned textbook is unusually vague and teaches from a higher level of understanding than anyone would expect from a Discrete Math class with no pre-requisites (FAU doesn't require any at all for Discrete Math).\n\n&gt;To make a long story short: after just about failing my first Exam and continuing to struggle to understand between the text and my instructor I was in peril, ready to drop the class (and honestly even re-evaluating my path in Computer Science).  I took to the internet, specifically reddit.\n\n&gt;Within 20 minutes I stumbled across the link to your class, http://courses.engr.illinois.edu/cs173/fa2011/ .\n\n&gt; **I would like to take this opportunity to give you an immense THANK YOU!  If not for your lecture notes (as they go in the exact order as our class, but with much more detail and better explanation). I would likely be dropping this class and still be feeling as if Discrete Math is some horribly abstract subject that I will never be able to understand.  All of a sudden, it all sounds a lot less like gibberish.**\n\n&gt; **So, again, as I can not say it enough: THANK YOU!  You have quite conceivably just saved my career in Computer Science.**\n\n&gt;Best Regards,\n\n&gt; __\n\n&gt; **P.S.  Good teachers are horribly underappreciated, and hard to come by.**\n Took your advice and did send her that email, quoting it below (mostly a repeat of what I already said, but still).\n\nedit: bolded the portion that was more personal message for the instructor.\n\n&gt;Professor Fleck,\n\n&gt;Obviously you don't know me.  Ostensibly by now you're figured out I'm __.\n\n&gt;I'm a Sophomore Computer Science student at Florida Atlantic University and currently taking Discrete Math.\n\n&gt;Unfortunately for me, my Discrete Math teacher is a visiting instructor from Japan whom I have a lot of trouble understanding and ironically (or not?), she doesn't understand most of what I ask either.\n\n&gt;To further complicate matters, our assigned textbook is unusually vague and teaches from a higher level of understanding than anyone would expect from a Discrete Math class with no pre-requisites (FAU doesn't require any at all for Discrete Math).\n\n&gt;To make a long story short: after just about failing my first Exam and continuing to struggle to understand between the text and my instructor I was in peril, ready to drop the class (and honestly even re-evaluating my path in Computer Science).  I took to the internet, specifically reddit.\n\n&gt;Within 20 minutes I stumbled across the link to your class, http://courses.engr.illinois.edu/cs173/fa2011/ .\n\n&gt; **I would like to take this opportunity to give you an immense THANK YOU!  If not for your lecture notes (as they go in the exact order as our class, but with much more detail and better explanation). I would likely be dropping this class and still be feeling as if Discrete Math is some horribly abstract subject that I will never be able to understand.  All of a sudden, it all sounds a lot less like gibberish.**\n\n&gt; **So, again, as I can not say it enough: THANK YOU!  You have quite conceivably just saved my career in Computer Science.**\n\n&gt;Best Regards,\n\n&gt; __\n\n&gt; **P.S.  Good teachers are horribly underappreciated, and hard to come by.**\n Took your advice and did send her that email, quoting it below (mostly a repeat of what I already said, but still).\n\nedit: bolded the portion that was more personal message for the instructor.\n\n&gt;Professor Fleck,\n\n&gt;Obviously you don't know me.  Ostensibly by now you're figured out I'm __.\n\n&gt;I'm a Sophomore Computer Science student at Florida Atlantic University and currently taking Discrete Math.\n\n&gt;Unfortunately for me, my Discrete Math teacher is a visiting instructor from Japan whom I have a lot of trouble understanding and ironically (or not?), she doesn't understand most of what I ask either.\n\n&gt;To further complicate matters, our assigned textbook is unusually vague and teaches from a higher level of understanding than anyone would expect from a Discrete Math class with no pre-requisites (FAU doesn't require any at all for Discrete Math).\n\n&gt;To make a long story short: after just about failing my first Exam and continuing to struggle to understand between the text and my instructor I was in peril, ready to drop the class (and honestly even re-evaluating my path in Computer Science).  I took to the internet, specifically reddit.\n\n&gt;Within 20 minutes I stumbled across the link to your class, http://courses.engr.illinois.edu/cs173/fa2011/ .\n\n&gt; **I would like to take this opportunity to give you an immense THANK YOU!  If not for your lecture notes (as they go in the exact order as our class, but with much more detail and better explanation). I would likely be dropping this class and still be feeling as if Discrete Math is some horribly abstract subject that I will never be able to understand.  All of a sudden, it all sounds a lot less like gibberish.**\n\n&gt; **So, again, as I can not say it enough: THANK YOU!  You have quite conceivably just saved my career in Computer Science.**\n\n&gt;Best Regards,\n\n&gt; __\n\n&gt; **P.S.  Good teachers are horribly underappreciated, and hard to come by.**\n i hope your name and college name is a random one, the internet is not a safe place. Oh yes, because we on /r/compsci have a desire to needlessly harass someone. \n\nNo. \n\nThe rule is never give out more information than you would to a stranger in the street. I'd give my name and college. \n\nThe only possible detriment is linking a name/identity to his reddit handle.  The rule is never give more information than they need to know. Learned in operating systems. I suppose it can vary by person, but yours is decent. I personally don't mind sharing name and state of residence. Because any mutual friend of a friend could easily attain that information, and I trust them equally as little as the Internet.  well, those guys can easily browse through your facebook albums too; depending on your settings. Oh yes, because we on /r/compsci have a desire to needlessly harass someone. \n\nNo. \n\nThe rule is never give out more information than you would to a stranger in the street. I'd give my name and college. \n\nThe only possible detriment is linking a name/identity to his reddit handle.  Oh yes, because we on /r/compsci have a desire to needlessly harass someone. \n\nNo. \n\nThe rule is never give out more information than you would to a stranger in the street. I'd give my name and college. \n\nThe only possible detriment is linking a name/identity to his reddit handle.  i hope your name and college name is a random one, the internet is not a safe place. i hope your name and college name is a random one, the internet is not a safe place. Took your advice and did send her that email, quoting it below (mostly a repeat of what I already said, but still).\n\nedit: bolded the portion that was more personal message for the instructor.\n\n&gt;Professor Fleck,\n\n&gt;Obviously you don't know me.  Ostensibly by now you're figured out I'm __.\n\n&gt;I'm a Sophomore Computer Science student at Florida Atlantic University and currently taking Discrete Math.\n\n&gt;Unfortunately for me, my Discrete Math teacher is a visiting instructor from Japan whom I have a lot of trouble understanding and ironically (or not?), she doesn't understand most of what I ask either.\n\n&gt;To further complicate matters, our assigned textbook is unusually vague and teaches from a higher level of understanding than anyone would expect from a Discrete Math class with no pre-requisites (FAU doesn't require any at all for Discrete Math).\n\n&gt;To make a long story short: after just about failing my first Exam and continuing to struggle to understand between the text and my instructor I was in peril, ready to drop the class (and honestly even re-evaluating my path in Computer Science).  I took to the internet, specifically reddit.\n\n&gt;Within 20 minutes I stumbled across the link to your class, http://courses.engr.illinois.edu/cs173/fa2011/ .\n\n&gt; **I would like to take this opportunity to give you an immense THANK YOU!  If not for your lecture notes (as they go in the exact order as our class, but with much more detail and better explanation). I would likely be dropping this class and still be feeling as if Discrete Math is some horribly abstract subject that I will never be able to understand.  All of a sudden, it all sounds a lot less like gibberish.**\n\n&gt; **So, again, as I can not say it enough: THANK YOU!  You have quite conceivably just saved my career in Computer Science.**\n\n&gt;Best Regards,\n\n&gt; __\n\n&gt; **P.S.  Good teachers are horribly underappreciated, and hard to come by.**\n You should email the author of those lecture notes to say thanks. I've met her -- she's nice ;) I had Margaret Fleck for that class. Yes she is very nice. I feel bad about never going to the lectures :( I had Margaret Fleck for that class. Yes she is very nice. I feel bad about never going to the lectures :( You should email the author of those lecture notes to say thanks. I've met her -- she's nice ;) You should email the author of those lecture notes to say thanks. I've met her -- she's nice ;) Are you that author?  You should email the author of those lecture notes to say thanks. I've met her -- she's nice ;) As someone who has taken her Discrete Math class, I agree :).\n\nProfessor Fleck's lecture notes are great. Despite the fact that I never went to class, I managed to get an A- in the class due to the lecture notes\n You should email the author of those lecture notes to say thanks. I've met her -- she's nice ;) The winky face tells me she's more than just nice...... Either that, or she's rosulek. You guys sure read into punctuation...\n\n...\n\n;)  UIUC is an amazing school. We have great computer science teachers and I'm glad they were able to help you out.  UIUC is an amazing school. We have great computer science teachers and I'm glad they were able to help you out.  Really the whole UofI system is fairly strong in CS.....except UIS. and except UIC...\n\n/trolol  Ah, good ol' CS 173.  \n\nIt always makes me feel warm and fuzzy whenever I see someone using a CS resource from my alma mater, so I'll promote another.  Whenever you start taking Algorithms (or in order to prepare), you should check out [Jeff Erickson's lecture notes and homework problems](http://compgeom.cs.uiuc.edu/~jeffe/teaching/algorithms/).  He's basically compiled an entire textbook worth of material and gives it away.\n\nConsidering that you're still in Discrete Math at the moment, it's probably still a bit too advanced to dive into, but you should take a peek if you're feeling adventurous. Most CS majors at U of I take that course 2nd semester Junior year or 1st semester Senior year. The *73 series still gives me nightmares. [Why, yes.](http://scp-wiki.wikidot.com/scp-173) What in the actual fuck? [Why, yes.](http://scp-wiki.wikidot.com/scp-173) What... what is that? [Here is some explanation.](https://www.youtube.com/watch?v=p5w4nkwKtQU) The *73 series still gives me nightmares. The *73 series still gives me nightmares. The *73 series still gives me nightmares. Ah, good ol' CS 173.  \n\nIt always makes me feel warm and fuzzy whenever I see someone using a CS resource from my alma mater, so I'll promote another.  Whenever you start taking Algorithms (or in order to prepare), you should check out [Jeff Erickson's lecture notes and homework problems](http://compgeom.cs.uiuc.edu/~jeffe/teaching/algorithms/).  He's basically compiled an entire textbook worth of material and gives it away.\n\nConsidering that you're still in Discrete Math at the moment, it's probably still a bit too advanced to dive into, but you should take a peek if you're feeling adventurous. Most CS majors at U of I take that course 2nd semester Junior year or 1st semester Senior year. is this 473 material?\nand indeed, good ol' 173. taking 373 atm, hopefully it doesnt blow up in my face.. Yep, that's 473. If you get the chance to take it with Jeff, definitely do it. I've heard Sariel is really good too, so you probably can't go wrong. \n\nAnd don't worry about 373 too much. Yeah, it's definitely challenging and the problem sets take forever, but if you've made this far, you've got what it takes.  Ah, good ol' CS 173.  \n\nIt always makes me feel warm and fuzzy whenever I see someone using a CS resource from my alma mater, so I'll promote another.  Whenever you start taking Algorithms (or in order to prepare), you should check out [Jeff Erickson's lecture notes and homework problems](http://compgeom.cs.uiuc.edu/~jeffe/teaching/algorithms/).  He's basically compiled an entire textbook worth of material and gives it away.\n\nConsidering that you're still in Discrete Math at the moment, it's probably still a bit too advanced to dive into, but you should take a peek if you're feeling adventurous. Most CS majors at U of I take that course 2nd semester Junior year or 1st semester Senior year. Ah, good ol' CS 173.  \n\nIt always makes me feel warm and fuzzy whenever I see someone using a CS resource from my alma mater, so I'll promote another.  Whenever you start taking Algorithms (or in order to prepare), you should check out [Jeff Erickson's lecture notes and homework problems](http://compgeom.cs.uiuc.edu/~jeffe/teaching/algorithms/).  He's basically compiled an entire textbook worth of material and gives it away.\n\nConsidering that you're still in Discrete Math at the moment, it's probably still a bit too advanced to dive into, but you should take a peek if you're feeling adventurous. Most CS majors at U of I take that course 2nd semester Junior year or 1st semester Senior year. Gonna be takin' that (173) within the year! :D Gonna be takin' that (173) within the year! :D    Discrete math is the worst. Probably because it's usually taught by terrible professors (or ones who don't speak English). \n\nI wish I'd found this last semester :( It would have saved my GPA. The first time I took Discrete, my professor spoke with a thick Russian accent. I could barely understand a word he said, and when he could, I'd already missed the context of what he was saying so I could never really follow the course. I squeaked through with a D. Took it again the following year and proved the difficulty of language barrier: I cleared it with an A-. So your professor the second time around spoke English well? Discrete math is the worst. Probably because it's usually taught by terrible professors (or ones who don't speak English). \n\nI wish I'd found this last semester :( It would have saved my GPA. My horrible discrete math class was one of the important reasons leading to why I didn't go through more college. It wasn't unintelligible but the teaching method was mainly just 'assign hours and hours of worksheets for homework every class'. I despise busywork, and as someone who was working about 60 hour weeks while taking a full course load to support myself and pay for the classes, I simply couldn't go through with it.  I'm actually a music performance major with a minor/almost major in computer science. Thus, I don't have time to learn everything on my own when the teacher is too incompetent to teach. I cant remember any point in college where I actually learned something in class, its all outside of the classroom. Discrete math is the worst. Probably because it's usually taught by terrible professors (or ones who don't speak English). \n\nI wish I'd found this last semester :( It would have saved my GPA.     Which textbook are you using?  Link?  I could refresh my discrete math skills. [http://courses.engr.illinois.edu/cs173/fa2011/](http://courses.engr.illinois.edu/cs173/fa2011/)\n\nPretty awesome.  Best part for me was that it seems to be taught from a much more basic/fundamental understanding than the textbook we have.\n\n While I was waiting for your response I looked around reddit and came across this link http://patrickjmt.com/#discrete-math.  I haven't tried it myself yet so I can't comment on the quality of it though.  I am sure you will find a lot of useful material here too: https://www.coursera.org/courses Have you actually taken a class at Coursera? How does it work? Is it like an online class? Do you have TAs to help you with problems and questions? Is the certificate recognized by anyone?  &gt;Have you actually taken a class at Coursera?\n\nI took 2 classes to try it out. Algorithms and Natural Language Processing\n\n&gt;How does it work? \n\nIt's pretty neat. They have video lectures and also notes, slides, etc to go with it that they release every week or may be twice a week. They have regular assignments and there are exams at the end. I thought the course was taught well.\n\n&gt; Is it like an online class?\n\nIt is an online class.\n\n&gt; Do you have TAs to help you with problems and questions?\n\nProfessors who teach the course get TAs from the previous classes (including the ones at their universities). So, there are quite a few TAs.\n\n&gt; Is the certificate recognized by anyone?\n\nI am not sure about that. I didn't even take the course towards a certificate. I just wanted to try it out (Algorithms) and also learn something new (Natural Language Processing). \n\nThey had very very few courses and they are growing fast. There are professors from very good universities teaching the courses. Even if the certificate is not recognized at this point, it may change in the future. Moreover, I posted the link because you were excited about finding good course material and I thought this link would be worth knowing about too. Thanks for all that information. Is it as much work as a regular course? Were you taking these classes in addition to your regular workload? Do you need textbooks or are the course materials enough?  &gt;Have you actually taken a class at Coursera?\n\nI took 2 classes to try it out. Algorithms and Natural Language Processing\n\n&gt;How does it work? \n\nIt's pretty neat. They have video lectures and also notes, slides, etc to go with it that they release every week or may be twice a week. They have regular assignments and there are exams at the end. I thought the course was taught well.\n\n&gt; Is it like an online class?\n\nIt is an online class.\n\n&gt; Do you have TAs to help you with problems and questions?\n\nProfessors who teach the course get TAs from the previous classes (including the ones at their universities). So, there are quite a few TAs.\n\n&gt; Is the certificate recognized by anyone?\n\nI am not sure about that. I didn't even take the course towards a certificate. I just wanted to try it out (Algorithms) and also learn something new (Natural Language Processing). \n\nThey had very very few courses and they are growing fast. There are professors from very good universities teaching the courses. Even if the certificate is not recognized at this point, it may change in the future. Moreover, I posted the link because you were excited about finding good course material and I thought this link would be worth knowing about too. Have you actually taken a class at Coursera? How does it work? Is it like an online class? Do you have TAs to help you with problems and questions? Is the certificate recognized by anyone?    There's also [Gilbert Strang's Linear Algebra OpenCourseWare @ MIT](http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/), accompanied by [lectures on YouTube](http://www.youtube.com/watch?v=ZK3O402wf1c).\n        Man, I love you. I'm a sophomore Comp Sci major as well. My discrete math class as been kind of challenging. My professor is NOT a good teacher. He understands the concepts, but he has virtually no idea how to share this knowledge with others. He often uses the phrase, \"but this should be obvious to you\" when I am clueless as can be, as is the rest of the class(the look on the two girls faces when going over challenge problems, OH LORD HA!) BECAUSE HE NEVER TAUGHT US IT. I'm motivated as shit right now, I'm just digesting these lectures. They are so well thought out and not boring as shit. The math refreshers are so helpful as examples throughout my book assume you understand and remember math terminology. \n\nFuck I love comp sci, I went to a meeting today about \"security consulting penetration tests\". They had VMs set up and did a live exploit to gain shell access on Windows XP. I've spent my evening compiling gentoo onto a VM to refresh my memory on bash, and adapt to a linux enviroment. \n\nI think you just saved my discrete math grade as well.\n\n\nTL;DR THANK YOU FERNANINO! any chance you are in OSU?   &gt; but also speaks in recursive riddles.\n\nTo understand recursion, one must first understand recursion.        My teacher likes to ramble about history and random ee stuff.  Currently I am trying to figure out the best easy to spend that class time.  \n\nShould I work on doilies, Afghan squares (crochet), or embroidery?  Any suggestions welcome.      ",
    "url" : "http://www.reddit.com/r/compsci/comments/10kme1/pretty_sure_reddit_just_saved_my_career_in/"
  }, {
    "id" : 30,
    "title" : "Science and Engineering Beyond Moore's Law",
    "url" : "http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&amp;arnumber=6186749"
  }, {
    "id" : 31,
    "title" : "The constant 0x5f3759df in inverse square root estimation",
    "snippet" : "   I think it might seem less like magic if they would stop giving the constant in hexadecimal notation. Would it really? I think the point of the hexadecimal notation is that the meaning of the value of the number is not readily apparent.\n\n    float FastInvSqrt(float x) {\n      float xhalf = 0.5f * x;\n      int i = *(int*)&amp;x;         // evil floating point bit level hacking\n      i = 1597463007 - (i &gt;&gt; 1);  // what the fuck?\n      x = *(float*)&amp;i;\n      x = x*(1.5f-(xhalf*x*x));\n      return x;\n    }\n\nNot exactly less magical-seeming. I think the point is that hex notation leads one to believe that the value is part of the \"evil bit hacking\", when really it is just a numerical constant that gives the newton-raphson a kick start. I'd argue that it *is* part of the \"evil bit level hacking\", since it's an operation that makes sense on the numbers in their *integer* form but not quite in their *float* form (since it can't even be represented as a basic float operation). I don't know what might qualify for bit level hacking if this doesn't. I think it might seem less like magic if they would stop giving the constant in hexadecimal notation. I agree, and the article itself sort of backs this up:\n\n&gt; So the constant is not a bit pattern as you might think from the fact that it’s written in hex, it’s the result of a normal calculation rounded to an integer.   This is a good explanation, but this trick has been posted here and on /r/programming a bazzilion times. It's not at all new, and not used so much any more. Perhaps there should be a stick of common cool stuff, but this should not be posted so regularly. This is a good explanation, but this trick has been posted here and on /r/programming a bazzilion times. It's not at all new, and not used so much any more. Perhaps there should be a stick of common cool stuff, but this should not be posted so regularly.  ",
    "url" : "http://blog.quenta.org/2012/09/0x5f3759df.html"
  }, {
    "id" : 32,
    "title" : "The U-Skate World, a newly-discovered continuous cellular automata that supports gliders.",
    "url" : "https://www.youtube.com/watch?v=F5oKgVZ6bTk"
  }, {
    "id" : 33,
    "title" : "The Research Software Engineer.",
    "snippet" : "  I have always wanted to be involved in writing software for the purposes of research... and was very disappointed when I found out that there is essentially no market for this.  Scientists that do see the benefits of computer systems to their research generally cobble together terrible abominations of Excel and other closed source tools having tantric sex and then expect the rest of the scientific community to just take their word for it that their software heresy didn't introduce any kind of bias or flaw into their data.  And the scientific community just believes them.  They think it would be way too hard to present the source code for the software they use along with their data, so they just don't.  Sure, this exact same issue has come up dozens of times in history, and relaxing the standards of science to skip over some 'hard' aspect has only resulted in loads of death and suffering, so let's just invite that right on in to modern science.  Let's just cross our fingers and hope we don't fuel a new 'eugenics' based on ignoring limitations we could have accounted for all along.  Maybe history will be kind to us?\n\n &gt; Scientists that do see the benefits of computer systems to their research generally cobble together terrible abominations of Excel and other closed source tools having tantric sex and then expect the rest of the scientific community to just take their word for it that their software heresy didn't introduce any kind of bias or flaw into their data\n\nPerhaps your lack of success in pursuing your goal to become a research developer is in your attitude. The purpose of a research developer is not to build a magnificent palace that allows you to flex the good software practice muscles that you've been focusing on during your training. \n\nResearch advances the knowledge of its field, and the software is there to expedite the process, not to show off itself as a pillar of good software design. You won't win any researcher over by practically saying \"What you use doesn't conform to the good software practices that I learned at school. Hire me so that you can continue to do what you have been doing, but now your software will at least adhere, under the hood, to good practices in my own field.\"\n\nYour job is to make the researcher's job easier, and good software practice makes your job easier. &gt; Scientists that do see the benefits of computer systems to their research generally cobble together terrible abominations of Excel and other closed source tools having tantric sex and then expect the rest of the scientific community to just take their word for it that their software heresy didn't introduce any kind of bias or flaw into their data\n\nPerhaps your lack of success in pursuing your goal to become a research developer is in your attitude. The purpose of a research developer is not to build a magnificent palace that allows you to flex the good software practice muscles that you've been focusing on during your training. \n\nResearch advances the knowledge of its field, and the software is there to expedite the process, not to show off itself as a pillar of good software design. You won't win any researcher over by practically saying \"What you use doesn't conform to the good software practices that I learned at school. Hire me so that you can continue to do what you have been doing, but now your software will at least adhere, under the hood, to good practices in my own field.\"\n\nYour job is to make the researcher's job easier, and good software practice makes your job easier. &gt; Scientists that do see the benefits of computer systems to their research generally cobble together terrible abominations of Excel and other closed source tools having tantric sex and then expect the rest of the scientific community to just take their word for it that their software heresy didn't introduce any kind of bias or flaw into their data\n\nPerhaps your lack of success in pursuing your goal to become a research developer is in your attitude. The purpose of a research developer is not to build a magnificent palace that allows you to flex the good software practice muscles that you've been focusing on during your training. \n\nResearch advances the knowledge of its field, and the software is there to expedite the process, not to show off itself as a pillar of good software design. You won't win any researcher over by practically saying \"What you use doesn't conform to the good software practices that I learned at school. Hire me so that you can continue to do what you have been doing, but now your software will at least adhere, under the hood, to good practices in my own field.\"\n\nYour job is to make the researcher's job easier, and good software practice makes your job easier. Yes, I understand, but my attitude really doesn't affect the situation.  Researchers do not look to hire people to do software at all, whether rigorous or not.\n\nThat's fine, it just makes their science invalid.  It's no different than if they used duct tape and dirty soda bottles for lab equipment and used procedures they cobbled together out of guesses led by their gut.  It's bad science, and it makes their findings at BEST correct but supported by entirely wrong arguments, at worst simply false. ",
    "url" : "http://dirkgorissen.com/2012/09/13/the-research-software-engineer/"
  }, {
    "id" : 34,
    "title" : "Are Loop Invariant's ever actually used in the real world?",
    "snippet" : "I'm in a Data Structures course at a community college and the instructor told us \"In my entire career I've never seen anyone perform a loop invariant test... but we're going to learn them anyway\"... These seem like the most useless things in the world..    No-one really does proofs by hand, but people do create machine checked proofs (see Isabelle/HOL, Coq, etc) which use loop invariants and some verification strategies use loop invariants to help you find potential bugs.\n\nI would say it is mostly relegated to academia and hardware verification, though some people in the software industry do make use of it.\n\nHaving said that, the idea of a loop invariant should not be a foreign concept to people if they want to be able to reason about their own code, even in an informal manner. Most of what you say I agree with, except the first sentence: Its a gross overstatement to say that noone does proofs by hand.  Hah, I guess I should have chosen my words more carefuly responding to a comment that no-one uses loop invariants.\n\nStill, who actually does proofs of code by hand outside of teaching? I'm a PhD student in computer science and I do regularly. If you want to write a paper on a new data structure or algorithm you wont really be taken serious unless you prove that it does what you claim.  Hah, I guess I should have chosen my words more carefuly responding to a comment that no-one uses loop invariants.\n\nStill, who actually does proofs of code by hand outside of teaching? Hah, I guess I should have chosen my words more carefuly responding to a comment that no-one uses loop invariants.\n\nStill, who actually does proofs of code by hand outside of teaching? No-one really does proofs by hand, but people do create machine checked proofs (see Isabelle/HOL, Coq, etc) which use loop invariants and some verification strategies use loop invariants to help you find potential bugs.\n\nI would say it is mostly relegated to academia and hardware verification, though some people in the software industry do make use of it.\n\nHaving said that, the idea of a loop invariant should not be a foreign concept to people if they want to be able to reason about their own code, even in an informal manner.  What is a loop invariant test? What's being tested, and by whom?\n\nLoop invariants in correctness proofs of imperatively stated algorithms are true at the end of every iteration, allowing you to use induction to prove something which is true after all iterations. Automatically inferring loop invariants is used in any sufficiently smart imperative compiler. Invariants in general are hugely important in program transformation.  I don't prove stuff by hand, but I find assertions invaluable when writing complex code.  A well-commented assertion is as worthwhile as a proof, and your reasoning is being tested every time you run the code. Assuming of course you write good tests. Unlike assertions in tests, assertions in code are stimulated every time the code _runs_. So even if you write bad tests (or if the code is effectively too complex to unit test it), you may be able to catch problematic cases before they corrupt your data.  It depends on how you define 'the real world'. \n\nIs academia included in the real world? Is algorithm engineering and theoretical computer science part of the real world? We have used loop invariants in every algorithms course and some other courses as well at my university. Sometimes they are implicit .. the book does not say 'we use a loop-invariant to show that ... ' but it is still part of the correctness proof of most algorithms. \n\nThey are the CS equivalent of proofs by induction. Do you ever see proofs of induction in the real world? Maybe not so often in industry, but it is still a fundamental part of understanding what it means for something to be correct! &gt; They are the *imperative programming* equivalent of proofs by induction\n\nFTFY.\n\n       ",
    "url" : "http://www.reddit.com/r/compsci/comments/10jg61/are_loop_invariants_ever_actually_used_in_the/"
  }, {
    "id" : 35,
    "title" : "Alice and Bob in Cipherspace: encryption that allows you to compute with data you cannot read.",
    "snippet" : "  How soon can we see fully homomorphic encryption being used in the real world, like in cloud computing, for example? Not very. The techniques presented are likely multiple orders of magnitude of overhead. What's interesting is that it's been shown to be *possible*, which is always a prerequisite for making something *practical*.\n\nJust guessing wildly, but I think you might first see this in multi-party protocols or such. Just trying out an idea:\n\nPerhaps I'm hanging out with some cryptographers, and we're trying to decide where to go for dinner -- Pizza or Tacos. Oh, and none of them trust each other, but as I'm not a cryptographer like them, they trust that I'm too ignorant to even conceive of cheating (or perhaps my name is Trent). Also, being cryptographers, they don't trust letting anyone else know their dining preferences -- after all, perhaps someone might use that to tamper with some important election somehow. Also, we left all our key pairs at home, so I have no way to authenticate anyone. So we agree... homomorphic encryption with a little program to tally the votes and secret ids. Everyone gets to pick a secret id, and will throw a fit if they don't see their id in the output, and if the count is too high we know someone stuffed the ballot.\n\nSo I write up this program, and include an integrity field that shouldn't be modified by any valid program step. I send that program to Alice, and she puts her vote in and runs the computation forward, then sends it to Bob, who does the same, then sends to Eve, Mallory, then back to me where I add my vote then reveal the count.\n\nAs for motives, Eve is anxious to break up Alice and Bob's relationship by proving they've got incompatible tastes in food. Mallory want to get tacos and is willing to cheat to get them. \n\nI go and decrypt the result program, to see its current state, which will be two counters, the list of the secret id values. Eve couldn't make heads or tails of the program -- it looked statistically random when she saw it, so learned nothing about either Alice or Bob's votes. Mallory might have found a copy of the earlier program state, but then Alice, Bob or Eve might complain that their secret ids are missing! Or Mallory could vote twice, but then everyone would demand a recount as the counts summed greater than the expected 5. \n\nSure enough, I decrypt the program and find 3 votes for pizza, 2 for tacos, and everyone nods that their secret id appeared. Mmm, pizza. Not very. The techniques presented are likely multiple orders of magnitude of overhead. What's interesting is that it's been shown to be *possible*, which is always a prerequisite for making something *practical*.\n\nJust guessing wildly, but I think you might first see this in multi-party protocols or such. Just trying out an idea:\n\nPerhaps I'm hanging out with some cryptographers, and we're trying to decide where to go for dinner -- Pizza or Tacos. Oh, and none of them trust each other, but as I'm not a cryptographer like them, they trust that I'm too ignorant to even conceive of cheating (or perhaps my name is Trent). Also, being cryptographers, they don't trust letting anyone else know their dining preferences -- after all, perhaps someone might use that to tamper with some important election somehow. Also, we left all our key pairs at home, so I have no way to authenticate anyone. So we agree... homomorphic encryption with a little program to tally the votes and secret ids. Everyone gets to pick a secret id, and will throw a fit if they don't see their id in the output, and if the count is too high we know someone stuffed the ballot.\n\nSo I write up this program, and include an integrity field that shouldn't be modified by any valid program step. I send that program to Alice, and she puts her vote in and runs the computation forward, then sends it to Bob, who does the same, then sends to Eve, Mallory, then back to me where I add my vote then reveal the count.\n\nAs for motives, Eve is anxious to break up Alice and Bob's relationship by proving they've got incompatible tastes in food. Mallory want to get tacos and is willing to cheat to get them. \n\nI go and decrypt the result program, to see its current state, which will be two counters, the list of the secret id values. Eve couldn't make heads or tails of the program -- it looked statistically random when she saw it, so learned nothing about either Alice or Bob's votes. Mallory might have found a copy of the earlier program state, but then Alice, Bob or Eve might complain that their secret ids are missing! Or Mallory could vote twice, but then everyone would demand a recount as the counts summed greater than the expected 5. \n\nSure enough, I decrypt the program and find 3 votes for pizza, 2 for tacos, and everyone nods that their secret id appeared. Mmm, pizza. Now you've made me hungry for pizza and its 10 in the morning. Important documents that are constantly being updated and from multiple sources, this is something I see being able use it even in its current form with all the overhead, somebody should just put it out there, just to get the ball rolling.  This may be a stupid question, but, how can FHE be secure? e.g, if I can do two operations: subtract and multiply, I can find the original number.\n\nEventually I'll either hit 1, in which case E(1)\\*E(n) == E(n) or 0, and E(0)*E(n) == E(0)...\n\nCan FHE have multiple representations for the same value, or are we relying on \"It'd take too long to try all numbers anyway\"? You (the adversary) will not have E(1). He can/will obtain E(m), and then can calculate E(m * k) for any k ; but he can't calculate E(k), let alone E(m) * E(k).  &gt; You (the adversary) will not have E(1).\n\nWhy not? Because simply put, the function E(1) actually is E(p,1) - where p is a secret private key. So Bob can encrypt 'm' and then send it to Alice and Eve, who can then add/subtract to it - but can't get the message back. Correct me if I'm wrong, but, isn't the point of FHE that I have some operations such that D(s, E(p, x) \\*' E(p, y)) == x*y and so on?\n\nOf these, I need 2, namely *' and -', and a constant E(p, 1). If so, unless E(p, 0) can have more than one representation (which I'm not sure how it can be handled), I know E(p,x), and I do:\n\n  1. n &lt;- E(p,x) -' E(p, 1); k &lt;- k+1\n  2. if n *' E(p,x) == n, END // that is, E(p, 0) *' E(p, n) == E(p, 0)\n  3. goto 1\n\nThen, n == E(p,  0) (properties of product) and k == x. &gt; D(s, E(p, x) *' E(p, y)) == x*y \n\nNot exactly. In FHE, you have \\*' and +' such that given E(x) and E(y), you can obtain E(x\\*y) = E(x)\\*'E(y)\n\nFor example, RSA is homomorphic for multiplication. In RSA, you have a public key P, and a private key S. To encrypt a message 'm1' you do E(m) = m^P. To decrypt you do D(c) = c^S.  Similarly E(n) = n^P. To obtain E(m\\*n) you simply do E(m\\*n)=E(m)\\*E(n). Given E(m) and E(n), although you can't obtain m or n, you can obtain E(m\\*n). \n\n\n&gt; E(p, 0) can have more than one representation (which I'm not sure how it can be handled)\n\nIt is handled by having a different value of E(p,0) for every p (for a perfectly secure scheme). This is the same idea a 'semantic security' which is mentioned in  a comment further down. So your algorithm will give different results for different values of 'p', and each of these are perfectly valid for some 'p'.  &gt; It is handled by having a different value of E(p,0) for every p \n\nCan there be several different p that can be decrypted with the same s? &gt; Can there be several different p that can be decrypted with the same s?\n\ni.e. D(s,E(p1,m)) = D(s,E(p2,m)) where m is any message ? Short answer - no. Also, if you try to decrypt E(p1,m) by 's' you will get 'm1', and if you try to decrypt E(p2,m) by 's' you will get 'm2'. But note that you have no idea whether 'm1' is the correct decryption or whether 'm2' is the correct decryption, because you have no idea what 's' actually is. Ok, then... I don't understand your previous point.\n\n&gt; In FHE, you have \\*' and +' such that given E(x) and E(y), you can obtain E(x\\*y) = E(x)*'E(y)\n\nI take it you elided the keys for simplicity. I don't see exactly how it's different from what I said above (other than providing me with a stronger property).\n\nIf I have this:\n\n1. E(n) \\*' E(m) = E(n*m)\n2. E(n) -' E(m) = E(n-m)\n3. E(1)\n4. E(x)\n\n(meaning each number has a unique (even if secret) representation)\n\nI can find x with the algorithm I detailed above.\n\nHowever, if there's a weaker property:\n\n1. D(E(n) \\*' E(m)) = n*m\n2. D(E(n) -' E(m)) = n-m\n\n(meaning that E(n) \\*' E(m) does not have a unique representation)\n\nOR if the representation of E(m) *' E(0) is not necessarily equal to the representation of E(m) -' E(1) -' E(1)-' E(1) ... then my algorithm is clearly not guaranteed to terminate. This may be a stupid question, but, how can FHE be secure? e.g, if I can do two operations: subtract and multiply, I can find the original number.\n\nEventually I'll either hit 1, in which case E(1)\\*E(n) == E(n) or 0, and E(0)*E(n) == E(0)...\n\nCan FHE have multiple representations for the same value, or are we relying on \"It'd take too long to try all numbers anyway\"? This may be a stupid question, but, how can FHE be secure? e.g, if I can do two operations: subtract and multiply, I can find the original number.\n\nEventually I'll either hit 1, in which case E(1)\\*E(n) == E(n) or 0, and E(0)*E(n) == E(0)...\n\nCan FHE have multiple representations for the same value, or are we relying on \"It'd take too long to try all numbers anyway\"?",
    "url" : "http://www.americanscientist.org/issues/pub/2012/5/alice-and-bob-in-cipherspace"
  }, {
    "id" : 36,
    "title" : "New study says language not as hierarchical as previously thought. What do you say, NLP people?",
    "snippet" : "   ",
    "url" : "http://rspb.royalsocietypublishing.org/content/early/2012/09/05/rspb.2012.1741.short?rss=1"
  }, {
    "id" : 37,
    "title" : "Definition of a 'module': help a philosopher of mind!",
    "snippet" : "Greetings. I'm doing my PhD in the philosophy of mind, more specifically in an area of cognitive science interested in modularity (more commonly known as the Massive Modularity Thesis). In cognitive science, a 'module' is most famously characterised by informational encapsulation (this originally comes from Jerry Fodor's book 'The Modularity of Mind'); that is, a module only has access to information within itself, it cannot access information contained in another module (or anywhere else, for that matter). For various reasons which I won't bore you with, this characterization, although highly influential, is really quite awful. I've read the odd paper or two which mention that computer scientists tend to think of modules very differently, but I haven't had much luck in finding any sources or papers that go into any detail about how modules are defined by computer scientists (closest I got was a computer science text book which used it almost synonymously with 'process', yet mentioned they were technically different without really saying in what way). Does anyone know of any academic papers, articles, or books where a computer scientist discusses/defines what is meant by a 'module'? Any help would be much appreciated.\n\nP.S. If anyone has any questions about why philosophers/psychologists/etc. would be interested in the idea of modules, I can go into that a bit if anyone is interested.  \"Good\" modular programming is essentially packaging a complex program (procedure) in to many smaller interconneced programs each of which is much simpler (even trivially simple).\n\nIn this frame of reference, a module can interact with other modules, but has limited input (maybe a couple pieces of data, hard to say), and most importantly, always does the same thing on the same input (this allows you to practice \"abstraction\" which is essentially what lets programmers understand huge complex programs). \n\nBasically, you want to strike a balance between having to pass around tons of data between tons of tiny modules (which is bad), and having single huge complex modules that are hard to understand (also bad). This creates \"natural\" places to fragment a program into many smaller programs (modules).  So, is a module simply an ultimately arbitrary sectioning off of particular tasks? 'This stuff here I'll call module A, that over there can be B'. \nOr do certain criteria have to be met in order for something to be a 'module'? I don't think there is a formal definition of a module. It is simply a piece of encapsulated functionality with low [coupling](http://en.wikipedia.org/wiki/Coupling_\\(computer_programming\\)) and high [cohesion](http://en.wikipedia.org/wiki/Cohesion_\\(computer_science\\)). Right. In practice, one of the most useful aspects of a module is its use for [Separation of Concerns](http://en.wikipedia.org/wiki/Separation_of_concerns).\n\nEssentially, a module establishes a contract with whatever context it is in to provide data or functionality. The mechanics by which it fulfills that contract are of no concern to the context.  \n\nSo for instance, if you have a module, FOO, with a function or method called \"give_me_four.\"  This \"give_me_four\" method will always return the value 4.  If we looked inside the module, it may implement this by returning 1+1+1+1 or by returning 1+3, or 5-1, or just returning plain 4.  However the logic of the module works, it doesn't matter, it is of no concern to whatever context is consuming the module.    (doing my PhD in computational cognitive neuroscience)\n\nThis is exactly how Fodor (1970s-1980s) uses modules. But he argues for the functional modularity of mind--that the mind can be explained by breaking up and reducing the functions of the mind into modules. Unfortunately, he uses the intuition that how the human mind functions is similar to how we program computers to argue that how those modules are implemented in the brain is irrelevant. In programming, you construct the functions to be modular because it's easier to code with. The brain is not programmed by humans. There is no constraint for its \"code\" to be easily understood and interacted with. The brain does not create libraries to be used when it seems necessary. It would be convenient for scientists if it were, but it's not. \n\nThe brain is a massively parallel, small world network of neurons, where each neuron is forward/backward connected to between 1000 and 10000 other neurons, and the functioning of each neuron is dependent on the functioning of those 1000-10000 neurons on the millisecond time scale. imagine writing modular code on that. It still seems plausible, looking at only what you wrote, that a higher level model of brain activity could still function somewhat \"modularly\", even if the underlying neurons are hopelessly tangled (I hesitate to say \"mind\" because I'm not sure how far \"up\" we have to go in terms of abstraction to call it that).\n\nNote that I believe \"somewhat modular\" is a meaningful phrase within the context of programming, that modularity works in shades of gray rather than a binary description. Yes, you're right to say that some functions of the network can still be near modular. But the assumption of modularity puts very serious constraints on anatomy, metabolism, and development. In Fodor's time, this information (by which his theories should be constrained) was much further off. The Turing machine is only universal with arbitrarily long tape (there are no constraints on how efficient it must be). It's not the case that any program we could imagine can be implemented by our brains without modification.\n\nMy point is that it just doesn't make sense to ignore implementation when scientifically modeling what our brains can do. For efficiency of reason, it is useful at higher levels of abstraction to consider things as modular, but it would be false to  say that they are. Fodor tried to appeal to \"early vision\" as an example of something seemingly modular, but he made those claims without knowing the anatomy of LGN/visual cortex. In a recent paper, I tried to even mention this in a review, and my advisor literally highlighted it with the annotation \"Fodor is wrong.\" \n\nAgain, it doesn't rule out some domain where neural processing could be considered modular, but the assumption that it exists so you can plug your ears when someone brings in physiological constraints is flawed. Also, you are making the problem potentially harder by trying to visualize what those modules might do, or be able to do. It won't be the case that those modules simply need to fit the requirements document.\n\nIt's almost like neurons themselves are modular, and the real story is how to build a brain that functions as if things are modular, even if their power comes from the fact that they aren't.\n\n\n\nAs an aside, check out the McGurk effect on YouTube. It shows how we, without our awareness, integrate audio and visual information to form our perception. The clip will play the exact same sound, but just change the video. Your experience will be perceiving different sounds dependent on what video is playing.  All we care about is our perception of the sound or word ( modular) but what we perceive is dependent on the integration of the audio visual information.\n\n\n\n I don't have any real response, but I do appreciate your detailed explanation, and an upvote didn't seem sufficient to express that.\n\nIf you're up for a tangental discussion, though - have you read Godel Escher Bach?  If so, could you comment on what has been discovered in the last few decades that has confirmed or disproved Hofstadter's  expositions about how the mind is formed from the brain? I haven't read Gödel, Escher, Bach, but many in my graduate group have and cite it as motivational. He is still a good resource for an introduction to Cognition. Since I haven't read him myself, I don't feel comfortable saying if he is correct or not. From what I gather, his account is insufficient, but on the right track. I apologize if this is unsatisfying. I don't think there is a formal definition of a module. It is simply a piece of encapsulated functionality with low [coupling](http://en.wikipedia.org/wiki/Coupling_\\(computer_programming\\)) and high [cohesion](http://en.wikipedia.org/wiki/Cohesion_\\(computer_science\\)). So, is a module simply an ultimately arbitrary sectioning off of particular tasks? 'This stuff here I'll call module A, that over there can be B'. \nOr do certain criteria have to be met in order for something to be a 'module'? Basically, but not entirely arbitrarily, no. Modules are divided by function. Let's pretend I have a website that is comprised of several modules. One such module might be search. Some new software comes out that's faster than my current search, so I swap out my existing module with a new faster one. wordpress, drupal, and most other content-management-systems work this way, except their modules are usually called \"plug-ins.\" Your browser probably has support for this kind of thing as well.\n\nThe idea is being able to plug pieces in and out of the application without breaking the whole thing. So, is the entire program/website comprised of modules? So for instance, Firefox has add ons (which we'll call modules for now), so I have the module 'Ad Block Plus' and whatnot. Can Firefox itself be understood as several modules, or are some components (presumably the very-stripped-down, bare-bones browser bit) better understood as something else (like, I don't know, a 'program', 'daemon', or whatever). \nHere's why I ask. In cog. sci. there's a problem with explaining how, if the brain is comprised of lots of modules, you get 'global processes'. For example, your visual system is thought (by many) to contain many modules, each doing their own bit to contribute to the overall visual image you experience (so, one module detects edges, one module adds depth, etc.). But when you think about, say, a chair, your thoughts are made up of all sorts of information coming from very different systems (visual system, abstract thoughts about what it takes for something to be a chair, tactile sensations from sitting on a chair, etc.); as modules are thought to be necessarily local ('they do their own specialised thing and that's that'), it isn't clear how you get information and thinking across many different modules. The question I'm asking here is, in computer science, do you have modules that bring other modules together? An analogy might be a factory; you have the grunt work assembly line stuff (the edge detecting modules, and whatnot; the ones that are relatively simple in function) and then you have the floor managers, the CEOs, the board directors, and so on, who have to oversee information from many different departments; are they still modules in just the same way as the more simple ones?\n(I hope that's clear?)  So, is the entire program/website comprised of modules? So for instance, Firefox has add ons (which we'll call modules for now), so I have the module 'Ad Block Plus' and whatnot. Can Firefox itself be understood as several modules, or are some components (presumably the very-stripped-down, bare-bones browser bit) better understood as something else (like, I don't know, a 'program', 'daemon', or whatever). \nHere's why I ask. In cog. sci. there's a problem with explaining how, if the brain is comprised of lots of modules, you get 'global processes'. For example, your visual system is thought (by many) to contain many modules, each doing their own bit to contribute to the overall visual image you experience (so, one module detects edges, one module adds depth, etc.). But when you think about, say, a chair, your thoughts are made up of all sorts of information coming from very different systems (visual system, abstract thoughts about what it takes for something to be a chair, tactile sensations from sitting on a chair, etc.); as modules are thought to be necessarily local ('they do their own specialised thing and that's that'), it isn't clear how you get information and thinking across many different modules. The question I'm asking here is, in computer science, do you have modules that bring other modules together? An analogy might be a factory; you have the grunt work assembly line stuff (the edge detecting modules, and whatnot; the ones that are relatively simple in function) and then you have the floor managers, the CEOs, the board directors, and so on, who have to oversee information from many different departments; are they still modules in just the same way as the more simple ones?\n(I hope that's clear?)  So, is the entire program/website comprised of modules? So for instance, Firefox has add ons (which we'll call modules for now), so I have the module 'Ad Block Plus' and whatnot. Can Firefox itself be understood as several modules, or are some components (presumably the very-stripped-down, bare-bones browser bit) better understood as something else (like, I don't know, a 'program', 'daemon', or whatever). \nHere's why I ask. In cog. sci. there's a problem with explaining how, if the brain is comprised of lots of modules, you get 'global processes'. For example, your visual system is thought (by many) to contain many modules, each doing their own bit to contribute to the overall visual image you experience (so, one module detects edges, one module adds depth, etc.). But when you think about, say, a chair, your thoughts are made up of all sorts of information coming from very different systems (visual system, abstract thoughts about what it takes for something to be a chair, tactile sensations from sitting on a chair, etc.); as modules are thought to be necessarily local ('they do their own specialised thing and that's that'), it isn't clear how you get information and thinking across many different modules. The question I'm asking here is, in computer science, do you have modules that bring other modules together? An analogy might be a factory; you have the grunt work assembly line stuff (the edge detecting modules, and whatnot; the ones that are relatively simple in function) and then you have the floor managers, the CEOs, the board directors, and so on, who have to oversee information from many different departments; are they still modules in just the same way as the more simple ones?\n(I hope that's clear?)  The concept of modularity in programming is a development paradigm designed to make it easy to upgrade software. Since \"modules\" in your philosophical jargon are not entities designed to be swapped out or exchange in any way (unless you think there's some implication between neuro-modularity and neuro-plasticity) I don't think the computer science notion of modularity is going to be useful to you. \n\nAs far as your analogy goes, the best analog is convoluted neural netwroks, where the output of several \"lower level\" networks  are used as the inputs to a \"higher level\" network. The lower level networks are thought of as feature extracting machines that find high level features (e.g. edges, depth) in the raw data (e.g. pixels) that get passed to the higher level network (e.g. image classification). The lower level networks are not really modular: they evolved together and removing one would probably have an unexpected result on the global output. \n\n(If this sounds like the visual pathway, it's because CNN research has been heavily influenced by it.)\n\nThe exception to this is if you train the lower level networks to identify specific features. Let's say you specifically train a neural network to identify edges. You could swap that out with another edge detection algorithm, like support vector machines or whatever, and theoretically nothing should break although the output might change.\n\ntl;dr: I don't think the notion of \"modularity\" in programming is going to be useful to you, since we're talking about flexible development paradigms and you're talking about emergent phenomena. I think this is similar to the difference between what \"intention\" means to Dennett vs. Freud. Same word, very different context. I these different versions of \"modularity\" are incompatible homonyms. So, is a module simply an ultimately arbitrary sectioning off of particular tasks? 'This stuff here I'll call module A, that over there can be B'. \nOr do certain criteria have to be met in order for something to be a 'module'? So, is a module simply an ultimately arbitrary sectioning off of particular tasks? 'This stuff here I'll call module A, that over there can be B'. \nOr do certain criteria have to be met in order for something to be a 'module'? So, is a module simply an ultimately arbitrary sectioning off of particular tasks? 'This stuff here I'll call module A, that over there can be B'. \nOr do certain criteria have to be met in order for something to be a 'module'? So, is a module simply an ultimately arbitrary sectioning off of particular tasks? 'This stuff here I'll call module A, that over there can be B'. \nOr do certain criteria have to be met in order for something to be a 'module'? \"Good\" modular programming is essentially packaging a complex program (procedure) in to many smaller interconneced programs each of which is much simpler (even trivially simple).\n\nIn this frame of reference, a module can interact with other modules, but has limited input (maybe a couple pieces of data, hard to say), and most importantly, always does the same thing on the same input (this allows you to practice \"abstraction\" which is essentially what lets programmers understand huge complex programs). \n\nBasically, you want to strike a balance between having to pass around tons of data between tons of tiny modules (which is bad), and having single huge complex modules that are hard to understand (also bad). This creates \"natural\" places to fragment a program into many smaller programs (modules).  \"Good\" modular programming is essentially packaging a complex program (procedure) in to many smaller interconneced programs each of which is much simpler (even trivially simple).\n\nIn this frame of reference, a module can interact with other modules, but has limited input (maybe a couple pieces of data, hard to say), and most importantly, always does the same thing on the same input (this allows you to practice \"abstraction\" which is essentially what lets programmers understand huge complex programs). \n\nBasically, you want to strike a balance between having to pass around tons of data between tons of tiny modules (which is bad), and having single huge complex modules that are hard to understand (also bad). This creates \"natural\" places to fragment a program into many smaller programs (modules).   The classic treatment of modularization in software engineering is by [David Parnas](http://en.wikipedia.org/wiki/David_Parnas), beginning with his 1972 paper, [\"On the Criteria To Be Used in Decomposing Systems into Modules\"](http://www.cs.umd.edu/class/spring2003/cmsc838p/Design/criteria.pdf).\n\nIn short, his theory is that, properly understood, modules are a means of hiding information and design decisions so they can be updated without impacting the rest of the system.   The other discussions about what is a module are very interesting, but just for the sake of providing a different viewpoint, the [Ocaml language](http://caml.inria.fr) has an explicit notion of module. In the [manual](http://caml.inria.fr/pub/docs/manual-ocaml-4.00/manual004.html), we have the following definition: \n\n&gt; A primary motivation for modules is to package together related definitions (such as the definitions of a data type and associated operations over that type) and enforce a consistent naming scheme for these definitions. This avoids running out of names or accidentally confusing names. Such a package is called a structure and is introduced by the struct…end construct, which contains an arbitrary sequence of definitions. The structure is usually given a name with the module binding.\n\nAt another point: http://caml.inria.fr/pub/docs/u3-ocaml/ocaml-modules.html\n\n&gt; The benefits of modules are numerous. They make large programs compilable by allowing to split them into pieces that can be separately compiled. They make large programs understandable by adding structure to them. More precisely, modules encourage, and sometimes force, the specification of the links (interfaces) between program components, hence they also make large programs maintainable and reusable. Additionally, by enforcing abstraction, modules usually make programs safer.\n\nXavier Leroy is one of the architects of the module system of OCaml, and he published the paper \"A modular module system\" (http://caml.inria.fr/pub/papers/xleroy-modular_modules-jfp.pdf). It might be a good academic reference (although some parts might be quite technical).     I think it's a very interesting topic.\n\nMany people are thinking of \"module\" in the programming language sense. In that sense, the Parnas article mentioned below is seminal and a must-cite.\n\nBut that is only a small part of the idea. Any type of system can have modules: for example a bicycle has a wheel module. The parts of a wheel are all more closely connected to each other than they are to elements of the bicycle outside that module. Here \"connected\" is meant in the sense of being physically connected, or having functional dependencies, or being well-adapted to each other. I think that this basic idea is one of the commonalities among the many different senses in which people talk about modules: many or strong internal connections, few or weak external connections.\n\nMany people also talk about *reuse* of modules -- the same wheel design can be re-used for the front and back wheel. But that seems not quite essential. After all, a unicycle's wheel still seems like a module.\n\nIn my own area (evolutionary computation), I can recommend taking a look at these papers:\n\n1. Spontaneous evolution of modularity and network motifs, Nadav Kashtan and Uri Alon\n\n2. A Simple Modularity Measure for Search Spaces based on Information Theory, Peter Dauscher, Daniel Polani, Richard A. Watson\n\nThe latter refers to Herbert Simon's \"Sciences of the Artificial\", which talks about \"decomposability\". I suppose that is not precisely the same as \"modularity\" -- but it's pretty close.\n\nPM me if you don't have access to those papers.\n\nAlso I have a colleague whose PhD topic is in the use of modularity in evolutionary computing systems. I'll point him here. Got hold of all of the papers mentioned, thank you.\nBy evolutionary computation, do you mean you are working on modelling evolutionary processes? Or do you mean that you are working on computational systems that 'adapt' in some sense? Got hold of all of the papers mentioned, thank you.\nBy evolutionary computation, do you mean you are working on modelling evolutionary processes? Or do you mean that you are working on computational systems that 'adapt' in some sense?      &gt;  philosophy of mind\n\na lover of mindful wisdom or a lover of wisdom of the mind?  Confusing nomenclature.\n\n&gt; I haven't had much luck in finding any sources or papers that go into any detail about how modules are defined by computer scientists\n\nA module is an overloaded term.  Don't focus on it too much or it will confuse people.  Try to find and use a synonym.\n\nOne great place to look is at [Domain Driven Design](http://domaindrivendesign.org/).  In there, modules are called Bounded Contexts there are many rules in how they are written and interact with other bounded contexts. &gt;  philosophy of mind\n\na lover of mindful wisdom or a lover of wisdom of the mind?  Confusing nomenclature.\n\n&gt; I haven't had much luck in finding any sources or papers that go into any detail about how modules are defined by computer scientists\n\nA module is an overloaded term.  Don't focus on it too much or it will confuse people.  Try to find and use a synonym.\n\nOne great place to look is at [Domain Driven Design](http://domaindrivendesign.org/).  In there, modules are called Bounded Contexts there are many rules in how they are written and interact with other bounded contexts.",
    "url" : "http://www.reddit.com/r/compsci/comments/10gk3v/definition_of_a_module_help_a_philosopher_of_mind/"
  }, {
    "id" : 38,
    "title" : "Why is the smallest amount of addressable memory generally a byte?",
    "snippet" : "I have been wondering this as it seems it'd be more efficient for it to be smaller because something like a boolean doesn't seem like it'd need all 8 bits. I haven't been able to find a thorough answer via google.  Wikipedia has some info on this:\n\n* http://en.wikipedia.org/wiki/Byte\n\nThere is a history of processors with 4-bit and 6-bit addressable units but they were superseded.\n\n* [4-bit](http://en.wikipedia.org/wiki/4-bit)\n* [6-bit](http://en.wikipedia.org/wiki/Sixbit)\n\nIf you think about the addressable space of 4-bit (16 values) and 6-bit (64 values), they aren't as useful for business computing because they cannot represent a full set of English alphanumeric characters. For that, the following is needed:\n\n* 26 uppercase letters\n* 26 lowercase letters\n* 10 digits\n* 30 punctuation marks\n\nThe total needed is 92 unique values, which explains why the set was expanded to [7-bit in 1963 with the ASCII character set.](http://en.wikipedia.org/wiki/ASCII)  7-bit allows for up to 128 values which is the smallest bit size needed for representing alphanumerics and a few non-printable characters.\n\nThe reason we don't use 7-bit as the smallest addressable unit is because that size is difficult to work when building logic gates in hardware. You can't evenly split 7 bits in half or in quarters. 2^8 is a friendlier number because 8 can be factored in a greater number of ways: [1, 2, 4, 8]. It makes the nibble size a nice round value of 4 and the half nibble size of 2. Compare to the factors of 6 [1, 2, 3, 6] and 7 [1, 7].\n\nChoosing 2^8 is the smart choice. Using a power of 8 is *very* handy for making logic gates simpler to implement and it meets the requirement for representing 92 alphanumeric characters. You mean 2^3 not 2^8 btw You mean 2^3 not 2^8 btw I'm confused. What made you think it should be 2^3 ? a \"2^3 bit\" as in \"an 8-bit\" space. There are 2^8 possible combinations with 8 bits.   You can thank IBM and their system 360 for that, I think - they came up with the first eight bits/one byte control code format: EBCDIC - http://en.m.wikipedia.org/wiki/Extended_Binary_Coded_Decimal_Interchange_Code  Since it was so widespread, microprocessor designers designed for optimization of byte-level instructions, skip forty years or so, here we are holding in with the same minimum addressable unit. Has anyone ever looked into going to a standard of say 4 bits? I guess I'm just curious to know if anyone has seen if there'd be any practical benefits, I'm guessing no but it'd be interesting to see a paper on it regardless.  Off topic, but 4 bits is a nibble.  My favorite word in compsci.\n\nedit: thanks Reshen s/bytes/bits/ Off topic, but 4 bits is a nibble.  My favorite word in compsci.\n\nedit: thanks Reshen &gt;Off topic, but 4 bit is a nibble. My favorite word in compsci.`\n\nFTFY\n\nCompsci humor is always the best though. Off topic, but 4 bits is a nibble.  My favorite word in compsci.\n\nedit: thanks Reshen Off topic, but 4 bits is a nibble.  My favorite word in compsci.\n\nedit: thanks Reshen Off topic, but 4 bits is a nibble.  My favorite word in compsci.\n\nedit: thanks Reshen Has anyone ever looked into going to a standard of say 4 bits? I guess I'm just curious to know if anyone has seen if there'd be any practical benefits, I'm guessing no but it'd be interesting to see a paper on it regardless.  Has anyone ever looked into going to a standard of say 4 bits? I guess I'm just curious to know if anyone has seen if there'd be any practical benefits, I'm guessing no but it'd be interesting to see a paper on it regardless.    some platforms (arm?) require accesses to be aligned to word size. So arguably smallest addressable unit is 32 bits in those cases. I've worked with ARM, MIPS, and x86. None of the require aligned access for a regular load/store. Unaligned accesses are often more complicated and slower though.   I can't answer this necessarily perfectly, but I can try.\n\nIt could be anything &gt;= 1 bit, but there are very few things that are actually held in &lt;8 bits, while there are tons of things that are held in exactly one byte. (when I say things, I mean data structures, obviously)\n\nTechnically, we could also have the addressable memory be multiple bytes, but that would cost more (if we wanted to, hypothetically, fill our entire memory with just ASCII characters) and would be fairly inefficient for what is typically the smallest size of data structure that must be referred to explicitly.\n\nI think a lot of it also has to do with backwards compatibility, which is gigantic in terms of hardware is basically the only reason Intel's conventions are widely accepted.  Ok so theoretically the hardware is capable of handling it?       [deleted]",
    "url" : "http://www.reddit.com/r/compsci/comments/10ho71/why_is_the_smallest_amount_of_addressable_memory/"
  }, {
    "id" : 39,
    "title" : "Looking for a simple explanation of the Particle mesh ewald method.",
    "snippet" : "So far I've found the wikipedia article (http://en.wikipedia.org/wiki/Ewald_summation#Particle_mesh_Ewald_.28PME.29_method ) quite useful, but lots of the papers that describe the method are mostly obscured by complicated error bounds and proofs. Does anybody know of a good resource for learning to implement this method?",
    "url" : "http://www.reddit.com/r/compsci/comments/10hwl9/looking_for_a_simple_explanation_of_the_particle/"
  }, {
    "id" : 40,
    "title" : "Need your input: automatic refactoring of numerical functions for locality ",
    "snippet" : "I'm doing a project for my university and i'd like to hear if you have some inputs how to achieve the goal.\n\n**Input**: a numerical function written in c (with some limitations), like an operation on a vector, a filter on an image or similar\n\n**Output**: modified input function (several versions, different transformation should be possible)\n\n**What it should do to the function** (an example, stride transformation):\nThe function looks like f(const int* input, int* output, int size), i don't know what's inside (Assumptions: no function calls, no aliasing and some more). It computes something on the input and stores it to the output.\n\nNow someone wants to use this function on a column of a matrix instead a row (vector). One solution is to transpose the column first, apply the function and transform it back but this includes that the data goes through the memory system 3 times instead of once.\n\nMy tool should now analyse the function code and modify it. Specifically, add a new parameter *int stride* and modify the source that it doesnt access the vector one element by one, it should access it *stride* by *stride* (jump always *stride* elements). As a result, the data goes only once through the memory system.\n\nThat is just one transformation, there are others like pre-scale the data, post-scale (scale it before it stores it in the output) and operate only on a part of an image (don't copy the sub-image to a new memory part, the function should take some new parameters so it knows how much to jump for the next line). There are possible more transformations.\n\nI've searched for papers if someone did something similar, but most is merging different loops into one (also reducing the amount of data going through memory). \n\nSo far i just have the c parser reader (creating an ast), a dependency graph which local variables depend on which parameters (like int *row = input+i * width, row depends on the input vector) and an analysis which of the parameters of the function are only read or written in the function (to determine input and output).\n\nDo you have any idea how this could be solved? I don't need a solution, i need hints/input what to try and maybe some topics to read. I have 6 month for this project, plenty of time to create some fancy analyses.\n\nThanks for your input\n  First of all, I would probably normalize and desugar (things like converting for-loops to while-loops) and minimize (requiring some pretty hardcore analysis). Perhaps you can use an open source compiler stack for parsing and transformation, like LLVM or GCC.\n\nAs for the actual analysis and modification, I believe we're in an area here, where you cannot make any general solution, but I assume that is okay, since you only mention one specific example.\n\nYou have to come up with a way to identify exactly where it does the pointer arithmetic on int* input, and replace the it with the equivalent multiplied with stride. Once that is working, the rest should be straight forward.\n\nThat said, be careful if the code is required to be high performance. It might negatively impacting performance with such spaced out memory access patterns. Don't forget to benchmark it against a transposed version of the code, which is much simpler. Thanks for the input on normalize, something i didn't thought of. I'll think that over. \n\nI'm using libclang to get AST of the function. I'll have to check if working on LLVM-Code would be easier.\n\nThe perfromance benefit should come from less data have to be passed through the memory sub-system (once instead of 3 times). There's a trade-off between cache benefits for the transposed version (better cache behaviour) and the benefits of transferring less data. But of course i need to benchmark it. ",
    "url" : "http://www.reddit.com/r/compsci/comments/10hzfj/need_your_input_automatic_refactoring_of/"
  }, {
    "id" : 41,
    "title" : "Can someone familiar with Hidden Markov Models answer a question about one of the Baum-Welch equations? [x-post]",
    "url" : "http://www.reddit.com/r/statistics/comments/10fqhc/can_someone_familiar_with_hidden_markov_models/"
  }, {
    "id" : 42,
    "title" : "Working on an interesting graph algorithm? Try out my Pregel clone and run it in the cloud with no fuss!",
    "snippet" : "  Hey, this is really cool. Thanks for sharing! Did you do this for school for just for fun? I'm going to continue checking this out and maybe I could lend a hand here and there. I'm really looking to get involved with open sourced development. This is my Masters project. Development is ongoing, but I'm really hoping to get feedback from algorithm people, on how to make the api more clean or deployment easier.  Nice, I'll have to try it out a little. It can't currently be deployed outside of an AWS environment, can it? Otherwise, I guess it couldn't hurt to try AWS on the free tier. This is my Masters project. Development is ongoing, but I'm really hoping to get feedback from algorithm people, on how to make the api more clean or deployment easier.   This looks neat, I'll try it. I'm building a graph library on top of Hadoop (MapReduce) to do some interesting stuff like large scale distance estimation, computing graph centrality and graph clustering. My framework runs on MapReduce, but the algorithms I develop can easily be ported to work with a Pregel clone. MapReduce is a pretty poor tool for processing graphs, as you have to copy the graph on supersteps. How large are your graphs? I've done experiments with a graph having 40 million nodes in a cluster with 70 computers and I was satisfied with the outcome. You have a point, you have to pass the graph from mappers to reducers at each step, but the real challenge comes from sorting and shuffling the messages that are sent from a node to its neighbors. There has been an extensive amount of research on caching data locally in MapReduce frameworks, twister is one of them. I know you don't have to pass the graph in Pregel, but you need to keep the graph metadata in memory, which is another limitation. The kind of graph algorithms I build, however, can be ported to any such framework because I follow the basic principle of graph processing in large scale: Transmit a message (any data) from a vertex to its neighbors in parallel, aggregate all messages for a particular vertex, and finally batch process all messages.  Keeping graph metadata in memory is not such a huge limitation - amazon rents 60GB memory machines on spot for about $0.25 per hour. \n\nAs the graph grows, you're going to want that many cores anyway, so memory is less of an issue. I agree that it's a limitation if you're running on a local cluster, but running your job with x cores for y hours costs the same as running your job with 2x cores for 0.5y hours on EC2. Are you targeting many core, single machine architectures or a distributed environment with commodity hardware? I'm more focused on the latter.  How would your pregel clone compare with Apache Giraph? Well, mine doesn't run within Hadoop, which means deployment is stupidly easy. You can go from not having an amazon account to running your first job on AWS in 5 minutes or less. I got started on this project after watching other grad students (who were more algorithms guys than systems or coding guys) have to write their stuff in MPI and then wait for time on a supercomputer somewhere. \n\nMy project is heavily focused on ease of use. I have no idea how they compare performancewise, but I'd be surprised if theirs was more than an order of magnitude better or worse than mine.   No one wants to read an API. What would you like to read?  Either a nontrivial tutorial or a manual such as that offered by Python or PHP would be better. http://charlesmunger.github.com/jpregel-aws/tutorial http://charlesmunger.github.com/jpregel-aws/tutorial &gt; nontrivial",
    "url" : "http://charlesmunger.github.com/jpregel-aws/"
  }, {
    "id" : 43,
    "title" : "[PDF] Automatically Finding Patches Using Genetic Programming",
    "snippet" : "  Hold on, shouldn't the concentration be on GP to simplify [wffs](http://en.wikipedia.org/wiki/Well-formed_formula) over the problem space (possibly [TPS](http://en.wikipedia.org/wiki/Automated_theorem_proving))? Identifying, resolving, and optimizing *processes* should be paramount, not trial/error on Pre/Post conditions and mutated code. Did I miss something? I don't know where *should* comes from. This research team decided they wanted to solve a problem and they (more or less) solved it. Are you arguing that they shouldn't have tried to solve it, that they didn't really solve it, that some other method would've done as well or better, or something else? I guess you might mean the third of those, to which the response (as always in research) is: let a thousand flowers bloom. And then benchmark them. Correct, but here I believe we have a classic example of over-fitting due to an unsupervised GP process.  No domain knowledge, nor best practices were invoked by GP during the trial processes on patching the methods - the lowest p-value wins rule in GP statistic - most likely invoking existential cases over universality, hence the switch statement fiasco referenced. Not sure if you read the paper now.\n\n&gt; a classic example of over-fitting\n\nOver-fitting refers to problems of modelling where the aim is to produce a model which generalises beyond training data. The patches found in this article don't need to generalise.\n\n&gt; No domain knowledge, nor best practices were invoked by GP\n\nI would've called the abstract syntax tree and the weighted path and the test cases \"domain knowledge\". \n\nRe best practices, not sure whether you mean GP best practices or software engineering best practices. If the latter, I agree. Again, you're entitled to say that using best practices might have helped in some way, but it's a weak argument given the success of the methods the authors chose. If the former, I disagree. The experimental methods in this paper are fine. \n\n&gt; the lowest p-value wins rule in GP statistic\n\nWhat page and line number are you referring to? Not sure whether you're trying to ridicule those papers which use $p$-values blindly or those which fail to use them. This paper reports success rates out of 100 runs and the lowest is 5.\n\n&gt; most likely invoking existential cases over universality\n\nVague idle speculation. If you have an argument make it.\n\n&gt; the switch statement fiasco referenced.\n\nThere's no fiasco. They succeeded!\n For making skim through this whitepaper again, I applaud you.  \n\nUnless I am truly missing something here, GP is a poor choice for this task *because* of \"over-fitting\" in the solution space, as stated\n\n&gt; Incremental compilation approaches and optimizations are left as future work\n\nThe solutions evolve by passing on the best fit weighted paths only, and I have a ~~great~~ overwhelming feeling they are Case based solutions or worse yet, redundant. From experience with GP, and by contradiction, consider \n\n    add(int a, int b) {\n        if (a &lt; 27) {\n            a + b;\n        } else if ((a &gt; 2349) || (b &lt; -23412)) { \n            b + a;\n        } else {\n            (b + (a * a * (a + b)/(a + b))) * (2 / (a * b) * (a * b)) - (2 * a * a) - b + a\n        }\n    }\n\nbecause no spooled solutions are presented as results in this paper. Oh cool, and R^2 = 1 solution which meets are quantitative quality conditions. Optimization aside; where's the domain knowledge here? How about a deterministic model so we can use what we learned? Nope! You're using GP pal. Hey, it converges, and it \"over-fits\" so well with near perfect R^2, but spool it out and the operator has no ideal what the hell is going on. I mean, where did \n\n&gt; (a &gt; 2349) || (b &lt; -23412)\n\ncome from? How was that locally best in iteration? It's [lost in time, like tears in rain](http://en.wikipedia.org/wiki/Blade_Runner).\n\nAgain, unless I'm truly missing something here, the fixing is trivial and the use of GP is really simplistic.  I would be more impressed if they approached the Pre/Post conditions of each method and utilized GP to fix **and** optimize the method, using measures of fit **and** computational complexity, or exploit some sort of previously unknown parallelism which improves performance, then publish that beast. &gt; For making skim through this whitepaper again, I applaud you.\n\nYour comments suggest you skimmed the wrong paper, or are spouting anti-GP buzzwords that you don't understand:\n\nThere is no over-fitting. There are no correlation values (*R*). \n\nOther comments suggest you completely missed the point: \n\nThe code they've produced looks nothing like what you wrote. You think the patches they produced can't be used, but they can. You think it would be better to use pre- and post-conditions, but that would require manual annotation of the source, when the whole point of the paper is an automated method of patching. Hold on, shouldn't the concentration be on GP to simplify [wffs](http://en.wikipedia.org/wiki/Well-formed_formula) over the problem space (possibly [TPS](http://en.wikipedia.org/wiki/Automated_theorem_proving))? Identifying, resolving, and optimizing *processes* should be paramount, not trial/error on Pre/Post conditions and mutated code. Did I miss something?  Ah, arms races. A few years ago, hackers discovered a way to automatically generate exploits based on revision changes in open source projects. I'd love to read more about this! any links you can share? Gah, I struggled to find the news article. I saw it on Reddit a few years ago. ",
    "url" : "http://www.ursuletz.com/~weimer/p/weimer-icse2009-genprog.pdf"
  }, {
    "id" : 44,
    "title" : "Women Bloggers",
    "snippet" : "Hi, looking for some women in CS and related fields blogs. I like to read about other women in school or in the workplace because it makes me feel like I am not alone.  If anyone knows any good or hell, even any terrible ones, I would appreciate it.\n\n\nThank you  Any topics in particular? Operating systems? Programming? Cryptography? Sysadmins? Research? Also, you're not alone :)  If it's programming that OP is looking for, awesome people &amp; awesome bloggers I'd recommend are CodeFrenzy, Kellabyte, Sharon Cichelli, Anne Epstein, and Iris Classon. They're all really active on twitter too, so if you've got programming questions you can reach out to them.  Oh wow, Iris Classon is impressive. Watch this interview if you guys/gals haven't: http://www.youtube.com/watch?v=6PHH-T41VdY . Pretty inspiring stuff, and I'm a male. What was the point with the last sentence? The 'inspiring' part, that I understand. But why'd you have to say you were male? I'm just confused. It doesn't change the fact that you're impressed, doesn't change the fact that she's skilled.  I think the intention behind it was that a woman might feel inspired by another woman's success where a man might not (\"so what?\"), but in this case she is genuinely impressive rather than \"pretty good for a girl\". I think that's sad, honestly. Maybe that's just me, but if you're being commended for your skills, it shouldn't matter who's giving you praise. I think that's kind of like saying \"because I am a man, my opinion matters more than if it came from a women\". Maybe I'm just reading too much into it... I think that's sad, honestly. Maybe that's just me, but if you're being commended for your skills, it shouldn't matter who's giving you praise. I think that's kind of like saying \"because I am a man, my opinion matters more than if it came from a women\". Maybe I'm just reading too much into it... What was the point with the last sentence? The 'inspiring' part, that I understand. But why'd you have to say you were male? I'm just confused. It doesn't change the fact that you're impressed, doesn't change the fact that she's skilled.  Yeah I know exactly where you're coming from, and I almost didn't include that sentence. The reason I did was because the OP is a female looking for other females in the field, so gender is already embedded in this discussion. I thought, as a result, some males might read this with the mindset that these compsci women might be inspiring from a psychological perspective of being a woman in a field dominated by men, and that they wouldn't get anything worthwhile from it. That is not the case obviously. So I guess it was just a cheap way to make those guys take notice of it and not ignore it. I'll pick my words a little better next time :) Any topics in particular? Operating systems? Programming? Cryptography? Sysadmins? Research? Also, you're not alone :)  Any topics in particular? Operating systems? Programming? Cryptography? Sysadmins? Research? Also, you're not alone :)   You are not alone! I am a woman in CS too. If you are still in school there are probably some clubs for women in computer science. I know at my university there is, and there's one for women in science too!  How large is your university?  Mine is about 10k students but only around 100 (give or take a few) CS Majors.  Of that, I'd say we have 3 or 4 women. I think only one is an upperclassman. Due to being the only girl in many CS classes and the general creeps who think it is okay to always hit on her, they usually switch majors.  \n\nEither my school has an absolutely shitty percentage of female majors or my department is a lot smaller than the one at your school. There aren't enough women in CS at my school to start a club (well I guess the 3 or 4 could start a club but it's below the minimum number of members needed to actually get $2000 from the school and be a \"real\" club) You are not alone! I am a woman in CS too. If you are still in school there are probably some clubs for women in computer science. I know at my university there is, and there's one for women in science too!  Women in science are nowhere near the minority that women in CS are, especially in life sciences.  Seems a bit weird that they have a club like that.  As the only girl in a CS class of 100 guys , I salute you.  Haha I was one of the 3 girls in my CS classes! Started off as 6 girls but 3 dropped out within the first few weeks. For me, there's been a big difference in the amount of women at my 4 year university vs my 2 year community college. Is that similar for you guys? At my community college there's me to 15 guys, and at the 4 year there are 5 women to 13 guys.    not a blog, but they have a blog. [Anita Borg Institute](http://anitaborg.org)\n\nAlso sounds like you might want to convince your boss to send you to the [Grace Hopper Convention](http://gracehopper.org/2012/)\n\nSun was really good to women. [Val](http://bubbva.blogspot.ca), [Liane](https://twitter.com/lpraza), [Deirdre](http://www.beginningwithi.com/comments/) from the Solaris group alone. 10gen ( the mongodb company ) is similarly gung-ho about women in technology.                 [the first computer programmer was a woman!](http://en.wikipedia.org/wiki/Ada_Lovelace) [Grace Hopper](http://en.wikipedia.org/wiki/Grace_Hopper) was pretty badass too. [the first computer programmer was a woman!](http://en.wikipedia.org/wiki/Ada_Lovelace) She dead.\n\n*Edit*:\n\n&gt;Died\t27 November 1852 \n\nCome on people. You clearly memorized her name as \"ooh a famous woman programmer!\" without caring enough to actually read about what she did and when she did it if you are surprised to hear she died 150 years ago. She dead.\n\n*Edit*:\n\n&gt;Died\t27 November 1852 \n\nCome on people. You clearly memorized her name as \"ooh a famous woman programmer!\" without caring enough to actually read about what she did and when she did it if you are surprised to hear she died 150 years ago. Not to mention, there is controversy over her [contributions](http://en.wikipedia.org/wiki/Ada_Lovelace#Controversy_over_extent_of_contributions). It seems that it was Babbage who wrote the first program. but she who wrote the first programming language. She dead.\n\n*Edit*:\n\n&gt;Died\t27 November 1852 \n\nCome on people. You clearly memorized her name as \"ooh a famous woman programmer!\" without caring enough to actually read about what she did and when she did it if you are surprised to hear she died 150 years ago. She dead.\n\n*Edit*:\n\n&gt;Died\t27 November 1852 \n\nCome on people. You clearly memorized her name as \"ooh a famous woman programmer!\" without caring enough to actually read about what she did and when she did it if you are surprised to hear she died 150 years ago.          Fellow female CS student here. I know the feeling. My university's program has ~300 CS undergrads and of that 15 or so are female.\n\nThanks for starting this thread. I've found some names I hadn't heard of prior. \n   ",
    "url" : "http://www.reddit.com/r/compsci/comments/10cm5e/women_bloggers/"
  }, {
    "id" : 45,
    "title" : "Nice resource for learning about Fourier analysis of Boolean functions.",
    "url" : "http://www.contrib.andrew.cmu.edu/~ryanod/?p=199"
  }, {
    "id" : 46,
    "title" : "What are computer scientists working on these days?",
    "snippet" : "I'm a collage undergraduate, working as a programmer. At first I find it enjoyable because I need to learn how to program and everything seems cool. Now when I'm doing the n project which is basically a cluster of my old projects I get bored. I've to do this work to get money, but I would like something more challenging. I want to go deeper in compsci, but I've no idea what computer scientists are doing these days. What are you working on ? How do you get paid? Where do you work? Are you only publishing papers? I would really appreciate if someone introduce me to this field. Thanks  Computer Scientists end up working everywhere on the spectrum from Academia to Industry on infinitely varied fields. \n\nCheck out the ACM digests for current research news from all over the CS community: http://technews.acm.org/ The statement is so true.   I found the question difficult to actually answer because it's so unconstrained. Probably best to just Google it for the full list.\n\nI've done Computer Vision, Numerical solutions for just about anything, Pattern Recognition, Chaotic Dynamics research etc.... How did you get to work on such varied stuff? I'm working on transitioning from \"Programmer\" to CS and hope to some day work on these things :) I've focused on Statistical Inference and the math behind everything which, can be applied to many fields, such as NLP, Computer Vision, Networks, etc. Each project at work has it's own focus but I get to span many of these projects. Some Computer Scientists decide they love one particular form of data and spend their entire career researching that. Find what really excited you and go balls out on that.  That was my plan - glad your experience concurs.\n\nCan you say who you work for? How did you get to work on such varied stuff? I'm working on transitioning from \"Programmer\" to CS and hope to some day work on these things :) Computer Scientists end up working everywhere on the spectrum from Academia to Industry on infinitely varied fields. \n\nCheck out the ACM digests for current research news from all over the CS community: http://technews.acm.org/   I'm familiar with the field of programming language theory. It's very diverse, with people considering the design of new programming language features, or new techniques to reason about existing programming languages (to find bugs, prove memory safety, prove security properties, improve tooling...). A sub-community in particular is very interested in type systems, which are a style of static analysis that has grown more and more powerful over time, and is also the basis of some \"proof assistants\" (languages to write mathematical proofs instead of computer programs).\n\nSome examples of open questions/problems:\n\n- There are a lot of different approaches to parallelism and concurrency, and we're not sure how to integrate them in programming languages. There is a rough consensus that we're not looking for \"one technique to rule them all\" anymore, but rather how to combine different techniques on different problem domains in a way that doesn't require writing ten different versions of the program.\n\n- The most powerful form of type system we know about are so-called \"dependent types\", where small pieces of programs are executed at type-checking time. They can do really powerful things (prove deep things about programs), but are also very complex (and pose hard constraints on language design). How to design a programming language that support them yet is simple enough to be used by programmers is an open question.\n\n- There is no agreement on what kind of programming language features is necessary to write large modular programs (\"programming in the large\"). Object-oriented techniques, ML module systems, formal concurrent calculi, there are a lot of different approaches, but none is both simple enough and powerful enough to be the clear \"right solution\" (and, as opposed to concurrency approaches, it's not clear that we couldn't find one good approach to fit all modularity needs).\n\n- We are not able to reason very easily on programs with side effects (eg. update of mutable state or communication with concurrent programs). Some of the most successful approaches are based on so-called \"separation logic\" that emphasize that some side-effects can be independent from each other and therefore handled in isolation. While we understand how to go about proving some well-understood programs by hand, or even checking those proofs mechanically, we're not able to integrate them in a system that is simple and convenient enough to be used by interested programmers. If we had this, we could for example reject all program with data races or deadlocks, because we would request that the programmer justifies why those can't happen. [deleted]  A pretty large field of research is Artificial Intelligence. If you want to know wether or not you like working in AI, you check out [Kaggle](http://kaggle.com) for a more practical approach. As for research, the field is still in its infancy so you won't have any problems finding things to study.      There's a Stack Exchange (Q&amp;A site) for theoretical computer scientists and researchers in related fields.  You can look there to see what kinds of things computer scientists are working on.  [Theoretical Computer Science](http://cstheory.stackexchange.com/)\n\nThere's also a site for questions about [Academia](http://academia.stackexchange.com/) in general, where you might find answers to your questions about how academics get paid, publish papers, etc.   So I just started working for a professor, but I can safely say 'only publishing papers' is the wrong approach. It may seem like it's underpaid and there won't be any real fame from it, but there may be other factors:\n\n1. I think that doing research is extremely more fun than writing [overly abstract classes for a decade-old program](http://static.springsource.org/spring/docs/2.5.x/api/org/springframework/aop/framework/AbstractSingletonProxyFactoryBean.html) (please don't hit me), as you will do much more diverse programming (I think).\n2. Coding without a user may seem unappealing but it is actually quite fun, as the only person you code for is yourself. You develop something you consider cool.\n3. I'd like to think that in ten to 15 years some big company comes by, finds that research, sees a market, implements it and has to write my name on their product.\n\nI should add that I'm currently still what you would consider an undergrad and working in the field of human-computer interaction, which is a very hands-on-y field, compared to theoritical computer science and the like.\n\n**tl;dr**: alnkpa rants about how cool and fun researching is.          [deleted] I don't understand the downvotes. I found this quite funny. Is this an instance of a meme I'm not aware of, that is downvoted because people are tired of it?  iOS development - doing random projects for my company and hopefully some games soon (computer science degree, smallish company called BNOTIONS)\n\nHere's a list of where my close friends have ended up (we all graduated with undergrad degrees around may of last year)\n\nAmazon - web back end dev (software engineer, a couple friends here)\nGoogle - idk, magical stuff (software engineers)\nStart up called Sweet Tooth (co owner and is a director of operations, software engineer)\nMicrosoft - windows phone 7 team ( cs degree)\nQualcomm - tools developer in c# (ca degree)\nMorgan Stanley - some sort of dev (comp eng)\nSome start up similar to mine doing assortment of random projects for random company's for iOS (comp eng)\n\n why the downvotes? SE != CS  I think you work as a programmer for a while and move to either management or software architect, meaning you design the entire project from a high level perspective and how the pieces works together.  This is what my friend told me anyway as his two options, after programming professionally for 10 years.  Maybe a master degree in either management or software architecture would be interesting?  Or you could do security, lots of money to be made there and its pretty challenging. NO! Although it is reiterated over and over again in this subreddit. Computer Science is not about programming! No offense, but I wouldn't say that your friend who has worked as a professional programmer is a \"computer scientist\". He is a software engineer or designer. The difference between these is like that between a mechanical engineer and a physicist. Engineers APPLY the principles of the field. Scientists STUDY them. There are computer scientists who don't program AT ALL for their work. \n\nThat said asking \"What computer scientists are doing?\" is a huge broad question that's hard to answer. Many work in research for industry in a variety of capacities. Some also work in academia. Just some of the myriad of subfields:\n\n* Machine Learning\n* Algorithms\n* Computability/Complexity\n* Natural Language Processing\n* Human Computer Interaction\n* Computer Vision\n* Security\n* Programming Languages\n\nWithin these sub-fields there are even more subdivisions. Seriously. Like you wouldn't ask: \"What are biologists doing these days\" would you?  thanks for your input. I know it's a huge broad question but I meant to ask it individually. What is that you do? and where?. I would like to know what different jobs are there excluding academia. Also, I would like to get more precise answer, perhaps not \"natural language processing\" but \"interpreting volume changes in natural languages\" or whatever subjects are there.  NO! Although it is reiterated over and over again in this subreddit. Computer Science is not about programming! No offense, but I wouldn't say that your friend who has worked as a professional programmer is a \"computer scientist\". He is a software engineer or designer. The difference between these is like that between a mechanical engineer and a physicist. Engineers APPLY the principles of the field. Scientists STUDY them. There are computer scientists who don't program AT ALL for their work. \n\nThat said asking \"What computer scientists are doing?\" is a huge broad question that's hard to answer. Many work in research for industry in a variety of capacities. Some also work in academia. Just some of the myriad of subfields:\n\n* Machine Learning\n* Algorithms\n* Computability/Complexity\n* Natural Language Processing\n* Human Computer Interaction\n* Computer Vision\n* Security\n* Programming Languages\n\nWithin these sub-fields there are even more subdivisions. Seriously. Like you wouldn't ask: \"What are biologists doing these days\" would you?  I find the physicist comparison severely lacking. I don't know any computer scientist who didn't have to program during his studies at least to a basic degree. I have yet to meet a physicist that has to perform an actual engineering task during his education.\n\nProgramming is part of computer science. It's not all but it has quite some part. Now, I agree with people who say one should maybe split up computer science and software engineering, but currently the latter is mostly considered part of the former. That's why I don't agree with this anti-programming attitude prevalent here. I also don't agree with the \"computer science is math\" attitude sometimes found here. There are many subfields of computer science that don't require a lot of math skills beyond some basic degree, and others that do, yet again others focus more on the engineering and management part.\n\nI am sorry, but all that is part of computer science. Not only what you or some other /r/compsci subsribers consider computer science. I'm curious which sub field you think doesn't require a lot of math? I'm curious which sub field you think doesn't require a lot of math? I would say programming language design, but it's not the only one... and as usual you can always pick something that *does* require a lot of math.\n\nAs I said, you *do* need math skills, but there is a limit to it. Yes, you need to be willing to learn math (if you completely hate formalisms and mathematics you will have a hard time) and CS *is* closer to math than other disciplines, but it's not the same as math. I would maybe agree that it's a very specific type of mathematics people in CS are usually interested in or good at. I personally don't consider formal languages, automata theory etc. to be on the same level as ... let's say calculus.\n\nI know quite a few people who were not good at the mandatory math courses and in college but eventually got their Master's degrees without having to rely on that knowledge very much (e.g. bad grades and dissatisfaction in calculus, but As and satisfaction in the CS classes that used parts of calculus). Yes, they sometimes had to learn a few new things about math and yes they had to be able to grasp some very specific math concepts they required for their research. But in the end they just used / applied what they found in some papers, maybe tweaked a little. They didn't have to propose and prove new mathematical theorems to be good at CS.\n\nCS is not just theoretical computer science, parts of it are also applied and sometimes even experimental (e.g. CHI). \n\n Wow...... Programming language design is one of THE most heavy math requiring fields, where are you getting this info. Programming design is largely a special application of [lambda calculus](http://en.wikipedia.org/wiki/Lambda_calculus), (invented btw by one of the fathers of CS, Alonzo Church) known as [Anonymous functions](http://en.wikipedia.org/wiki/Anonymous_functions)\n\nAlso I would agree that a lot of CS people dislike calculus, but I wouldn't say things like graph theory, info theory, or combinatorics are any LESS mathematics than calculus. In fact, many mathematicians hate calculus. In fact, the only people I know who LIKE calculus are physicists, engineers, and analysts (in both senses). \n\nI think most people attach \"math\" to calculus because of experiences in high school. But math people HATE this. It is really a very very small portion of the field.  1. You are confusing programming language design with programming language theory.\n\n2. You can understand anonymous functions without ever having heard of lambda calculus or deeply investigating it.\n\n3. Not all programming languages build on lambda calculus or require a deep undestanding of it.\n\nA lot of CS is *like* math, but it is a very specific collection of subfields of mathematics. That does not make it the same. I am not arguing against warning people that you need to work with math when you go into CS but saying \"CS is not programming, it's math\" is not true just as \"CS is all about programming\" isn't true.  I think you work as a programmer for a while and move to either management or software architect, meaning you design the entire project from a high level perspective and how the pieces works together.  This is what my friend told me anyway as his two options, after programming professionally for 10 years.  Maybe a master degree in either management or software architecture would be interesting?  Or you could do security, lots of money to be made there and its pretty challenging. I think you work as a programmer for a while and move to either management or software architect, meaning you design the entire project from a high level perspective and how the pieces works together.  This is what my friend told me anyway as his two options, after programming professionally for 10 years.  Maybe a master degree in either management or software architecture would be interesting?  Or you could do security, lots of money to be made there and its pretty challenging.",
    "url" : "http://www.reddit.com/r/compsci/comments/10c5la/what_are_computer_scientists_working_on_these_days/"
  }, {
    "id" : 47,
    "title" : "HTML5 Bird Flocks Simulation using KD-trees and binary heaps",
    "snippet" : "  People keep saying that JS is fast enough etc. But still it definitely feels like a slow. Eg on my system I can have about 70 boids before the performance begins to suffer (one core gets fully utilized). Meanwhile a [C implementation](https://github.com/GenTiradentes/tinyflock) of boids that has been submitted recently to reddit can have 1024 boids and do about 90 ticks per second (using only one core to make it more fair). And that C implementation is not probably the most optimal, afaik it doesn't use fancy data structures like this JS one etc.\n\nSure, TinyFlock uses OpenGL for graphics, which probably explains some of the performance difference. But on the other hand, I'd argue that HTML5 encourages to use inefficient graphics like `&lt;canvas&gt;`. Or that the design of `&lt;canvas&gt;` hasn't taken implementation performance into consideration enough. Is it possible that the bottleneck is more in the canvas rendering, rather than the js interpreter?\n\nEdit: Never mind. I must have skipped the last half of your comment... sorry. People keep saying that JS is fast enough etc. But still it definitely feels like a slow. Eg on my system I can have about 70 boids before the performance begins to suffer (one core gets fully utilized). Meanwhile a [C implementation](https://github.com/GenTiradentes/tinyflock) of boids that has been submitted recently to reddit can have 1024 boids and do about 90 ticks per second (using only one core to make it more fair). And that C implementation is not probably the most optimal, afaik it doesn't use fancy data structures like this JS one etc.\n\nSure, TinyFlock uses OpenGL for graphics, which probably explains some of the performance difference. But on the other hand, I'd argue that HTML5 encourages to use inefficient graphics like `&lt;canvas&gt;`. Or that the design of `&lt;canvas&gt;` hasn't taken implementation performance into consideration enough.    My computer can deal with Skyrim and does not hold that.  I wonder what the effects are of using the \"k nearest boids\" instead of just \"everything within x units of here\". Certainly the latter is simpler to write acceleration structures for - instead of using KD-trees you can just use square/hex bins, after all.\n\nI also wonder if there's anything interesting to say about boids. People say there's some interesting chaotic behaviour going on, that flocks might behave \"intelligently\" somehow, or they can be used to solve real problems, but I'm not really sure how much credence to give those ideas. At best it looks like an introductory programming exercise that everyone is inordinately proud to show off to the world. &gt; Certainly the latter is simpler to write acceleration structures for - instead of using KD-trees you can just use square/hex bins, after all.\n\nWhy? I could use square bins with the k-nearest boids as well, but the performance wouldn't be so good, since boids tend to gather in one place. Also, KD-trees can be easily modified to answer queries of the type \"everything within X units of here\", or even \"the closest K boids within X units of here\". In fact, it might improve performance slightly when K is big and X is small.\n\nAs for your second paragraph, I agree. I just thought that it's an impressive model that creates something that I enjoy to watch, so I implemented it. ",
    "url" : "http://chrisp.gr/projects/boids/"
  }, {
    "id" : 48,
    "title" : "Reduction of Horn-Clauses: Are there any standard methodes?",
    "snippet" : "Horn-clauses are DNF (Disjunctive Normal Forms) with at most one positive literal. They are separated into 3 kinds:  \n   \n* Goal: n negated literals, e.g. (not A or not B or not C) \n* Rule: 1 positive literal and n negated ones, for n&gt;0\n* Fact: 1 positive literal   \n\nNow you can \"reduce\" them. A reduced Horn-Clause is a close in which every [propositional symbol](http://www.reddit.com/r/compsci/comments/10c6ej/reduction_of_hornclauses_are_there_any_standard/c6c9js5) only used once. \nI have no idea how to do this though, are there any standard methodes to accomplish this? \nThe only information I found online is that \"every clause which contains a complimentary literal pair can be reduced\".\n\ntl;dr: How do I reduce Horn-Clauses?    The literals in a Horn clause are combined by disjunction, i.e. \"or\". So normal rules for propositional logic apply:\n\n\"not A or not A\" is equivalent to \"not A\". this means if a negated literal appears several times, you only have to keep one of them.\n\n\"A or not A\" is equivalent to \"True\". This means if a literal appears both positive and negated, you can remove all appearances of this literal.\n\n(\"A or A\" would reduce to \"A\" but this cannot appear in a Horn clause.)\n Thanks for this!\n\nIf I have something like (not A or A or not B or not C) I could write it down as (not B or not C). But this is not reduced, right? Since there are two \"not\" ? \nIs the sentence \"every clause which contains a complimentary literal pair can be reduced\" false then?   &gt; If I have something like (not A or A or not B or not C) I could write it down as (not B or not C).\n\nUnless I'm misreading horribly, no.  You could write it down as \"True\". Thanks for this!\n\nIf I have something like (not A or A or not B or not C) I could write it down as (not B or not C). But this is not reduced, right? Since there are two \"not\" ? \nIs the sentence \"every clause which contains a complimentary literal pair can be reduced\" false then?   &gt; If I have something like (not A or A or not B or not C) I could write it down as (not B or not C). \n\nyes. (**Edit:** as grayvedigga points out, this is actually equivalent to True, not to (not B or not C))\n\n&gt; But this is not reduced, right? Since there are two \"not\" ?\n\nAh, here is your problem: You said \"A reduced Horn-Clause is a close in which every expression symbol only used once.\" If that really is what your textbook/material says, it is at least misleading, if not false. A proper way to phrase would be: \"A reduced Horn-Clause is a clause in which every *literal* is only used once.\"\n\n\"not A or not B or not C\" is reduced, because the literals A,B,C each appear only once.\n\n \"A or not B or not A\" is not reduced, because the literal A appears twice (once negated, once positive). Thank You so much for helping me out! :)\n\nWe don't have a textbook at all, we where just told to research it and write a paper about it, I mostly worked with university papers published online, which use the word \"Aussagensymbol\" in this context, now I get that they probably mean literals plus T and contradiction. \n",
    "url" : "http://www.reddit.com/r/compsci/comments/10c6ej/reduction_of_hornclauses_are_there_any_standard/"
  }, {
    "id" : 49,
    "title" : "[PDF] Phonotactic Reconstruction of Encrypted VoIP Conversations: Hookt on fon-iks",
    "snippet" : " ",
    "url" : "http://www.cs.unc.edu/~fabian/papers/foniks-oak11.pdf"
  } ],
  "processing-time-source" : 94,
  "processing-result.title" : "compsci7_reddit.xml",
  "XmlDocumentSource.xml" : {
    "absolutePath" : "/Users/cole/code/cs467/r-topics/carrot2-cli-3.6.2/../data/compsci7_reddit.xml"
  }
}
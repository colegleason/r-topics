{
  "processing-time-total" : 1879,
  "clusters" : [ {
    "id" : 0,
    "size" : 6,
    "score" : 66.14853102106605,
    "phrases" : [ "Image" ],
    "documents" : [ 3, 21, 22, 29, 30, 44 ],
    "attributes" : {
      "score" : 66.14853102106605
    }
  }, {
    "id" : 1,
    "size" : 6,
    "score" : 57.61496516825705,
    "phrases" : [ "Related to Computer Science" ],
    "documents" : [ 4, 9, 24, 30, 35, 47 ],
    "attributes" : {
      "score" : 57.61496516825705
    }
  }, {
    "id" : 2,
    "size" : 5,
    "score" : 54.467316123721695,
    "phrases" : [ "Compsci" ],
    "documents" : [ 9, 13, 19, 41, 47 ],
    "attributes" : {
      "score" : 54.467316123721695
    }
  }, {
    "id" : 3,
    "size" : 5,
    "score" : 69.2275226796259,
    "phrases" : [ "Data Structures" ],
    "documents" : [ 4, 9, 24, 31, 49 ],
    "attributes" : {
      "score" : 69.2275226796259
    }
  }, {
    "id" : 4,
    "size" : 5,
    "score" : 88.56463361003941,
    "phrases" : [ "File Systems" ],
    "documents" : [ 13, 14, 24, 37, 43 ],
    "attributes" : {
      "score" : 88.56463361003941
    }
  }, {
    "id" : 5,
    "size" : 5,
    "score" : 47.33737002675308,
    "phrases" : [ "Good Look at the bottom Line" ],
    "documents" : [ 4, 15, 21, 24, 47 ],
    "attributes" : {
      "score" : 47.33737002675308
    }
  }, {
    "id" : 6,
    "size" : 4,
    "score" : 71.4194489744318,
    "phrases" : [ "Approximation Algorithms" ],
    "documents" : [ 21, 40, 45, 47 ],
    "attributes" : {
      "score" : 71.4194489744318
    }
  }, {
    "id" : 7,
    "size" : 4,
    "score" : 76.04814121288614,
    "phrases" : [ "Automatic" ],
    "documents" : [ 3, 25, 30, 39 ],
    "attributes" : {
      "score" : 76.04814121288614
    }
  }, {
    "id" : 8,
    "size" : 4,
    "score" : 55.21033299185706,
    "phrases" : [ "Program Memory" ],
    "documents" : [ 2, 24, 43, 47 ],
    "attributes" : {
      "score" : 55.21033299185706
    }
  }, {
    "id" : 9,
    "size" : 3,
    "score" : 93.22673974714266,
    "phrases" : [ "Engineer more than a Scientist" ],
    "documents" : [ 9, 15, 43 ],
    "attributes" : {
      "score" : 93.22673974714266
    }
  }, {
    "id" : 10,
    "size" : 3,
    "score" : 87.91941084369417,
    "phrases" : [ "List from r Books" ],
    "documents" : [ 4, 20, 49 ],
    "attributes" : {
      "score" : 87.91941084369417
    }
  }, {
    "id" : 11,
    "size" : 3,
    "score" : 54.65031864483495,
    "phrases" : [ "Media" ],
    "documents" : [ 9, 22, 42 ],
    "attributes" : {
      "score" : 54.65031864483495
    }
  }, {
    "id" : 12,
    "size" : 3,
    "score" : 65.31879427630139,
    "phrases" : [ "States at a Given Time" ],
    "documents" : [ 4, 9, 18 ],
    "attributes" : {
      "score" : 65.31879427630139
    }
  }, {
    "id" : 13,
    "size" : 2,
    "score" : 59.836698244722776,
    "phrases" : [ "Curve" ],
    "documents" : [ 3, 29 ],
    "attributes" : {
      "score" : 59.836698244722776
    }
  }, {
    "id" : 14,
    "size" : 2,
    "score" : 95.681055684923,
    "phrases" : [ "Hosted SQL Server" ],
    "documents" : [ 32, 43 ],
    "attributes" : {
      "score" : 95.681055684923
    }
  }, {
    "id" : 15,
    "size" : 2,
    "score" : 59.09905985209309,
    "phrases" : [ "Lecture Notes" ],
    "documents" : [ 14, 49 ],
    "attributes" : {
      "score" : 59.09905985209309
    }
  }, {
    "id" : 16,
    "size" : 2,
    "score" : 54.1691404429664,
    "phrases" : [ "Neumann Architecture" ],
    "documents" : [ 24, 43 ],
    "attributes" : {
      "score" : 54.1691404429664
    }
  }, {
    "id" : 17,
    "size" : 2,
    "score" : 84.99652238314192,
    "phrases" : [ "Neural Network" ],
    "documents" : [ 36, 47 ],
    "attributes" : {
      "score" : 84.99652238314192
    }
  }, {
    "id" : 18,
    "size" : 2,
    "score" : 40.94251962899984,
    "phrases" : [ "Proof and I can Break your Algorithm" ],
    "documents" : [ 1, 43 ],
    "attributes" : {
      "score" : 40.94251962899984
    }
  }, {
    "id" : 19,
    "size" : 2,
    "score" : 77.42947476008965,
    "phrases" : [ "Sample and Reference" ],
    "documents" : [ 3, 28 ],
    "attributes" : {
      "score" : 77.42947476008965
    }
  }, {
    "id" : 20,
    "size" : 2,
    "score" : 74.36430068214844,
    "phrases" : [ "Story" ],
    "documents" : [ 9, 12 ],
    "attributes" : {
      "score" : 74.36430068214844
    }
  }, {
    "id" : 21,
    "size" : 17,
    "score" : 0.0,
    "phrases" : [ "Other Topics" ],
    "documents" : [ 0, 5, 6, 7, 8, 10, 11, 16, 17, 23, 26, 27, 33, 34, 38, 46, 48 ],
    "attributes" : {
      "other-topics" : true,
      "score" : 0.0
    }
  } ],
  "processing-time-algorithm" : 1818,
  "documents" : [ {
    "id" : 0,
    "title" : "Determining equality of numbers constructed with a set of operations",
    "snippet" : "We can implement algorithms that compute equality of numbers constructed with 1, - and /, since that is exactly the rational numbers. We can also include ~~square roots~~ nth roots of polynomials, which gives us the algebraic numbers. We can compute equality for them. Can we add various trigonometric functions too? How about exponentials and logarithms?\n\nBasically, if we have to be able to determine equality of numbers in finite time, how many operations can we include?\n\nBefore you start talking about computable reals (that happened some time ago in /r/math), note that you can't compute equality for computable reals.   As long as you have an algorithm to reduce a number to some canonical form, you can compare equality. As you take more and more general functions, the canonical forms will get messier. To get through the trig functions your canonical form will (I am a little sleepy so excuse me if I am off) will be some linear combination of complex exponential. To get into weirder special functions, you can start expressing things in terms of hypergeometric series and such.   &gt; We can also include square roots, which gives us the algebraic numbers.\n\nI don't think rationals and square roots are sufficient to construct the algebraic numbers. (Could we even express 2^(1/3)?) Instead it's more natural to express an algebraic number as the nth root of some polynomial. This approach is used by [Mathematica](http://reference.wolfram.com/mathematica/ref/Root.html). &gt; We can also include square roots, which gives us the algebraic numbers.\n\nI don't think rationals and square roots are sufficient to construct the algebraic numbers. (Could we even express 2^(1/3)?) Instead it's more natural to express an algebraic number as the nth root of some polynomial. This approach is used by [Mathematica](http://reference.wolfram.com/mathematica/ref/Root.html).  [deleted] What do you mean? I don't understand. [deleted] [deleted]  See this: \n\nM.H. Escardo. Computing with real numbers represented as infinite sequences of digits in Haskell. CCA'2009 tutorial, August 2009, based on older stuff.\n\nDownload: zip file with Haskell programs and scripts. There is also a literate program derived from this used for a talk in Fun in the Afternoon (March 2011).\n\nhttp://www.cs.bham.ac.uk/~mhe/papers/index.html#ref:unpublished\n The problem is that this leaves us with all computable real numbers, which prevents us from computing equality. You compute equality up to any desired precision. This should suffice.  Looks like [this](http://www.jstor.org/discover/10.2307/2271358?uid=3739864&amp;uid=2129&amp;uid=2&amp;uid=70&amp;uid=4&amp;uid=3739256&amp;sid=21101568425801) article has some of the answers you are looking for. It is by Daniel Richardson, 1968. I'm sorry to link from the currently reviled JSTOR archive. According to [this](http://www.reddit.com/r/compsci/comments/16kcaq/determining_equality_of_numbers_constructed_with/c7x8yqd) comment, that allows some quantifiers (or something like that), which means that the theorems are weaker than needed to disprove the possibility of what I'm asking. I already knew that you don't have to get much stronger for it to become undecidable.",
    "url" : "http://www.reddit.com/r/compsci/comments/16kcaq/determining_equality_of_numbers_constructed_with/"
  }, {
    "id" : 1,
    "title" : "Polynomial Exact-3-SAT Solving Algorithm\n",
    "snippet" : "  I hope people aren't too harsh on you in the 'debunking'--- it's usually educational to dissect why a candidate algorithm doesn't really work as advertised and everyone can benefit in the process.  Replying to this comment so llucifer sees a few words of encouragement.\n\nOk, now the critique... This is not a proof and I can break your algorithm.\n\n&gt;I say, the total number of clauses ever added to the CNF will, if the CNF is unsolvable, never exceed the count of all possible clauses with 3 or less literals.\n\nYou define you cutoff as \nsum( i=1 to 3)\nn choose i \n\nor C(n,i) where n is the total number of variables. This grows in O(n!)\n\nAs for the rest of the paper. Your main idea of overlaying clauses isn't even a defined in your paper and I don't see any mention of this method on any of the 4 wikipedia articles you used as sources. The formula you used to do the run time analysis appears to have been pulled out of your ass.\n\n&gt;The origin of these formulas are my own considerations. They are rather easy to understand and I could not falsify them in tests, so it is very probable that the formulas are correct.\n\nJust because you couldn't find a counter example isn't a proof of correctness. Also don't cite wikipedia when you're presenting what you think is a resolution to one of the biggest questions in comp sci, and especially don't ONLY cite wikipedia.  &gt; You define you cutoff as sum( i=1 to 3) n choose i or C(n,i) where n is the total number of variables. This grows in O(n!)\n\nYes, but also in O(n^3 ).\n\n&gt; Your main idea of overlaying clauses isn't even a defined in your paper and I don't see any mention of this method on any of the 4 wikipedia articles you used as sources.\n\nIt looked like [resolution](http://en.wikipedia.org/wiki/Resolution_%28logic%29) to me. I hear it was already written on the stone tablets brought from the mountain by Moses. Replying to this comment so llucifer sees a few words of encouragement.\n\nOk, now the critique... This is not a proof and I can break your algorithm.\n\n&gt;I say, the total number of clauses ever added to the CNF will, if the CNF is unsolvable, never exceed the count of all possible clauses with 3 or less literals.\n\nYou define you cutoff as \nsum( i=1 to 3)\nn choose i \n\nor C(n,i) where n is the total number of variables. This grows in O(n!)\n\nAs for the rest of the paper. Your main idea of overlaying clauses isn't even a defined in your paper and I don't see any mention of this method on any of the 4 wikipedia articles you used as sources. The formula you used to do the run time analysis appears to have been pulled out of your ass.\n\n&gt;The origin of these formulas are my own considerations. They are rather easy to understand and I could not falsify them in tests, so it is very probable that the formulas are correct.\n\nJust because you couldn't find a counter example isn't a proof of correctness. Also don't cite wikipedia when you're presenting what you think is a resolution to one of the biggest questions in comp sci, and especially don't ONLY cite wikipedia.      Why is he asking us to check his proofs instead of solving all six remaining Clay Prize problems, or at least cracking one of the cryptography challenges? I really don't see why people seem to think that a constructive algorithmic proof of P=NP means that the resulting algorithm will give the constructor any more power than they already have.\ntl;dr His algorithm is (really) slow. O(n^(3)) that he claims is not \"really slow\" by any sane measure, and should not prevent him from grabbing all those prizes and everyone's RSA private keys as he goes. ",
    "url" : "http://www.louis-coder.com/P_NP_Problem_solved/P_NP_Problem_solved.htm"
  }, {
    "id" : 2,
    "title" : "Buffer overflow (attack), when only *reading* memory.",
    "snippet" : "A buffer overflow (attack) is normally associated with *writing* past the boundaries of a buffer.\n\nHowever, how about *reading* past said boundaries? Does this constitute a “buffer overflow”, and is there such a thing as a buffer overflow attack where only *reading* outside of the buffer occurs?\n  It _is_ a buffer overflow.\n\nA read overflow is not typically exploitable in the same way as a write, but may (emphasis on _may_) allow exploitation via exception handlers (either actual \"exception\" exceptions, or OS signal handlers), or an information leak that may allow exploitation through another vulnerability (a pointer leak frequently means an ASLR bypass).\n\nIn either case, the overflow is a vulnerability, but exploitation would almost certainly require additional vulnerabilities to be used as well (though this is increasingly required anyway) &gt; …or an information leak that may allow exploitation through another vulnerability (a pointer leak frequently means an ASLR bypass).\n&gt; In either case, the overflow is a vulnerability, …\n\nDo you know of any academic papers that describe how a pointer leak means an ASLR bypass? I understand perfectly what’s going on, but I’m writing a report and I need something to back this up in my references. In other words: *[citation needed]* ;-)\n\nAny help massively appreciated! ASLR means the location of a particular piece of data will vary from run to run of the same binary, but once you have a pointer to it, you know exactly where it is (at least for now). Also, if you know that a particular piece of data will always be at a certain offset within a memory segment, then a pointer to it allows you to infer the base address of the segment, which allows you to calculate the absolute address of anything that has a known fixed offset in that segment. Yes, I know what it means now, but I was asking for academic papers on the subject... Yes, I know what it means now, but I was asking for academic papers on the subject... Yes, I know what it means now, but I was asking for academic papers on the subject... Yes, I know what it means now, but I was asking for academic papers on the subject... &gt; …or an information leak that may allow exploitation through another vulnerability (a pointer leak frequently means an ASLR bypass).\n&gt; In either case, the overflow is a vulnerability, …\n\nDo you know of any academic papers that describe how a pointer leak means an ASLR bypass? I understand perfectly what’s going on, but I’m writing a report and I need something to back this up in my references. In other words: *[citation needed]* ;-)\n\nAny help massively appreciated! It _is_ a buffer overflow.\n\nA read overflow is not typically exploitable in the same way as a write, but may (emphasis on _may_) allow exploitation via exception handlers (either actual \"exception\" exceptions, or OS signal handlers), or an information leak that may allow exploitation through another vulnerability (a pointer leak frequently means an ASLR bypass).\n\nIn either case, the overflow is a vulnerability, but exploitation would almost certainly require additional vulnerabilities to be used as well (though this is increasingly required anyway) I understand that such a vulnerability can disclose information, but it seems like you're saying that one can exploit this further to run malicious code without writing to memory (*may allow exploitation via exception handlers*), if so, how?  Sort of. This sort of thing is actually a somewhat common bug. Names vary, but it could be considered a type of \"information disclosure\", or more precisely \"memory disclosure\" vulnerability.\n\nThe most common case comes up in *format string vulnerabilities*. Here, something like a printf is left with user controllable arguments. If an attacker does something like send a string \"%x%x%x%x%x%x\" etc, this will walk the stack, printing out values. (These format string vulnerabilities can also be exploited to allow an attacker to write to memory, though this is going away)\n\nThis is especially important because modern programs use randomization to move the program around, called ASLR (address space layout randomization). If you leak these addresses, attackers can usually figure out where the program was rebased, and may be able to use another vulnerability to easily exploit the program. (Stack cookies or other protections can also be defeated with proper memory disclosure bugs.)\n\ntl;dr Our of bounds reading is a security vulnerability. It is not called a buffer overflow attack. Without an additional vulnerability, out of bounds reading is not an exploitable bug (you cannot get arbitrary code execution with it). Format string vulnerabilities can also be used to write arbitrary data to memory, usually using the %n token. Sort of. This sort of thing is actually a somewhat common bug. Names vary, but it could be considered a type of \"information disclosure\", or more precisely \"memory disclosure\" vulnerability.\n\nThe most common case comes up in *format string vulnerabilities*. Here, something like a printf is left with user controllable arguments. If an attacker does something like send a string \"%x%x%x%x%x%x\" etc, this will walk the stack, printing out values. (These format string vulnerabilities can also be exploited to allow an attacker to write to memory, though this is going away)\n\nThis is especially important because modern programs use randomization to move the program around, called ASLR (address space layout randomization). If you leak these addresses, attackers can usually figure out where the program was rebased, and may be able to use another vulnerability to easily exploit the program. (Stack cookies or other protections can also be defeated with proper memory disclosure bugs.)\n\ntl;dr Our of bounds reading is a security vulnerability. It is not called a buffer overflow attack. Without an additional vulnerability, out of bounds reading is not an exploitable bug (you cannot get arbitrary code execution with it).     ",
    "url" : "http://www.reddit.com/r/compsci/comments/16j1kz/buffer_overflow_attack_when_only_reading_memory/"
  }, {
    "id" : 3,
    "title" : "Can you help me fit a Bezier curve to my sample- For Science! reference included for explanation",
    "snippet" : "   Moar information: I am trying to recreate an experiment that measures the change in curvature of the initial sample by applying a Bezier curve after determining the mean width of the canal before and instrumentation. after the curve is applied circles are placed on that curve and the radius is measured. results are the comparison of the length of radius.\nMy problem arises in that my program cannot afford the software used in this paper and even if they could dentists don't know the first thing about this stuff.\nI have access to MatLab, Image J, Photoshop and possibly other software. \nIs there someone out here who can explain some steps for me to do this or point me in the right direction to ask? As the paper indicates, the technique used by authors is as follows:\n\n- find the edges of the object (EDIT: and determine the axis)\n- sample the ~~edges~~ axis at small interval (1mm) to get control points\n- use control points (omitting some for simplicity?) to construct the Bezier curve approximating the ~~edge~~ axis.\n\nIf you have control points, drawing a Bezier curve is actually rather trivial, because it comes down to evaluating linear combination of third order [Bernstein polynomial](http://en.wikipedia.org/wiki/Bernstein_polynomial).\n\nEdit: need more caffeine.\n\nEdit 2: A control point of a Bezier curve allows to influence the shape of the curve. Only first and last points are guaranteed to be on the curve. It is much easier to understand if you look at how Bezier curves are [drawn](http://en.wikipedia.org/wiki/File:Bezier_3_big.gif).\n\nGiven the number of images to process, you need to do it automatically. The paper appears to use a more advanced method than just averaging the edges to find the axis. Thank you for the reply. I have zero experience with this. I will have to do this with 180-200 images.\nWhat software should I be using?\nWhat is a control point? is it the points along the curve that make up the line? in my case the mean of distance between edges?\n\n How experienced are you with something like MATLAB, Mathematica or programming in general? If you have a bunch of images, you could do some automated processing. If there's a high contrast between the curved object and the background (e.g. you took them all in front of a near-solid white backdrop), you can adjust the contrast of the images so that the foreground object is solid black and eventually use the continuous points to plot out a curve of the points that are directly in the middle of the rod. Find a MATLAB or Mathematica guru, and they maybe be able to crank something out for you that's only a few lines long. I have no experience just access. there is a PHD student who can help write the code but there is a strong language barrier and he was having trouble with the scope of the project. \nI can certainly put a contrast medium in the canal the remnants of which are seen in the linked photo. \nwould matlab be able to apply the points, circles and measure the radius?\n How experienced are you with something like MATLAB, Mathematica or programming in general? If you have a bunch of images, you could do some automated processing. If there's a high contrast between the curved object and the background (e.g. you took them all in front of a near-solid white backdrop), you can adjust the contrast of the images so that the foreground object is solid black and eventually use the continuous points to plot out a curve of the points that are directly in the middle of the rod. Find a MATLAB or Mathematica guru, and they maybe be able to crank something out for you that's only a few lines long. Thank you for the reply. I have zero experience with this. I will have to do this with 180-200 images.\nWhat software should I be using?\nWhat is a control point? is it the points along the curve that make up the line? in my case the mean of distance between edges?\n\n",
    "url" : "http://imgur.com/a/uUXB1"
  }, {
    "id" : 4,
    "title" : "Self-teaching basics of comp sci with absolutely no foundation. What to read, ect.",
    "snippet" : "Greetings fellow redditors.\nI seek advice on how to teach myself the basics of computer science in preparation for taking it as a class in the future.\nThe class would be an AP course.\nI am an avid learner, especially when I get along well with my teacher and am genuinely curious about the material as well. \n\nMy main issue is that I don't know where to begin. Many textbooks assume you know something. \nI can, essentially, use the internet, make powerpoints, and type. (Such is the computer education system here in the states)\n\nBook reccomendations would be best\nThanks in advance.  A list from /r/books. http://www.reddit.com/r/books/comments/ch0wt/a_reading_list_for_the_selftaught_computer/ A list from /r/books. http://www.reddit.com/r/books/comments/ch0wt/a_reading_list_for_the_selftaught_computer/  Honestly, you're better off reading things that will interest you and pave the mental road for good abstraction rather than class material.\n\nJust go with one of these:\n\n*Godel, Escher, Bach: An Eternal Golden Braid\n\n*Code: the Hidden Language of Computer Hardware and Software\n\nLet me explain why: first, using a computer as a 'user' (you mention powerpoint, the internet) has about as much to do with computer science as collecting gardening tools. Computer science, in fact, is not very much about actual computers - it's much more about the fact that there are patterns over which computations can be applied. Our use of *computing machines* to do this is just accidental. (By the way, Computer Science is not exactly science, either).\n\nSecond, and kind of following from the first point, what matters is not really how you interact with the medium you're given, it's the understanding of the principles that guide it. So, you see, when you're having a class, some people will be 'huh? Why? What?', cause they'll still be thinking about the wrong things. Having read some 'extracurricular' and 'not textbook' material, you'll have the foundations for understanding, well, everything!\n\nIn case you're wondering, what should matter for you (at least at first) is the set of things belonging to what I like to call the Three Big A's: Abstraction, Algorithms and All the data structures.\n\nP.S.: please note that both of the books have it's on 'subject' which might very well not be what you're after, its just that, the way they're written, they turn out to be so goddamn useful for inspiring and making you think about stuff! Keep in mind that this is an AP Computer Science class.\n\nI think that [Code: the Hidden Language of Computer Hardware and Software](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1358106744&amp;sr=1-1&amp;keywords=code+hidden+language+of+computer+hardware+and+software) by Charles Petzold is probably the best recommendation from your post for the OP.  Not only will this build on the basic foundations of how a computer functions, but it will also go into how and why programming languages exist.  I found that book to be a great introduction for folks who have interest in Computer Science and they want to get some necessary understanding. Keep in mind that this is an AP Computer Science class.\n\nI think that [Code: the Hidden Language of Computer Hardware and Software](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1358106744&amp;sr=1-1&amp;keywords=code+hidden+language+of+computer+hardware+and+software) by Charles Petzold is probably the best recommendation from your post for the OP.  Not only will this build on the basic foundations of how a computer functions, but it will also go into how and why programming languages exist.  I found that book to be a great introduction for folks who have interest in Computer Science and they want to get some necessary understanding.  http://www.udacity.com/ seems pretty good                I would try and find a book that goes through the basics of algorithms and data structures.You may be tempted to jump into coding right away, but going through some basics of discrete math will really help.\n\nA discrete math book like \"Discrete Mathematics and its Applications\" by Rosen really helped me to understand recurrence relations and basic algorithms. Oh god no, that book. Wikipedia can do a better job than that book. It's got like one star on Amazon for a reason. Agreed. Don't get me wrong, it's a fantastic book, but for OP I think online sources are more in line with his needs. I think in AP CS you learn Java, so I would get a book on object-oriented Java and start doing practice projects.\n\nFor theory I would start with a high level overview:\nhttp://en.wikipedia.org/wiki/Computer_science\n\nThen learn about searching and sorting, recursion, basic data structures, basic GUIs. This is most of the material in an intro CS class. That's true. Does the AP CS exam still test data structures? I know it did when I took it, but I knew that there were talks about changing it. I would try and find a book that goes through the basics of algorithms and data structures.You may be tempted to jump into coding right away, but going through some basics of discrete math will really help.\n\nA discrete math book like \"Discrete Mathematics and its Applications\" by Rosen really helped me to understand recurrence relations and basic algorithms. I will echo these sentiments.\n\nA book on discrete math will go a long way, and Rosen's is pretty much the best I've come across.\n\nAho and Ullman also offer a [free book](http://i.stanford.edu/~ullman/focs.html), \"Foundations of Computer Science\" which covers much of the same material. However, I would recommend Rosen's book first.\n\nBut most fundamental to CS is foundation of computing. You will want to read [Theory of Computation](http://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/0534950973) by Michael Sipser. It's a very easy read, and tries to make as little assumption about your background knowledge.\n\nBoth Sipser &amp; Rosen books combined will give you a fairly good start for any CS curriculum.\n\nAs for programming, I would recommend getting a very basic understanding of a language like Python or Ruby, which requires little investment in time and just start solving problems. Don't worry too much about understanding all the idioms of the programming language. Programming languages are a tool to solve problems using computers.\n\nAs for solving problems, you can try the challenges made available at [r/dailyprogrammer](http://www.reddit.com/r/dailyprogrammer) and also working through [Project Euler](https://projecteuler.net/) problems.\n\nThen after you have the basics of programming down, then you may want to consider looking at algorithms and data structures (and their analysis) more in depth. I would try and find a book that goes through the basics of algorithms and data structures.You may be tempted to jump into coding right away, but going through some basics of discrete math will really help.\n\nA discrete math book like \"Discrete Mathematics and its Applications\" by Rosen really helped me to understand recurrence relations and basic algorithms.   www.codecademy.com can teach you the basics of html/css, python, javascript, and ruby.  [deleted]  Google. Or try a reddit search. Do you think you're in a unique situation, one nobody has ever experienced before?  I really wish the rules on the sidebar about not posting these stupid topics were actually enforced by the admin.",
    "url" : "http://www.reddit.com/r/compsci/comments/16i33e/selfteaching_basics_of_comp_sci_with_absolutely/"
  }, {
    "id" : 5,
    "title" : "Network analysis and data mining in food science: Computational Gastronomy",
    "snippet" : " ",
    "url" : "http://www.flavourjournal.com/content/2/1/4"
  }, {
    "id" : 6,
    "title" : "A Github for data sets - What do you guys think?",
    "snippet" : "I've been toying with this idea for a little while. There is no place to collaborate and share data sets you've collected. Sure you can do it through Github but it's not made for it.\n\n\nFor example, I wanted to do a project recently where I would analyse reddit comments for various things. I looked for a repository of comments that had already been collected but found nothing. I had to mine www.reddit.com/comments myself and while I don't mind that, I too have no standard way to share with everybody the ~100k comment database I have been building up.\n\nImagine a place like Github where you can search through quality rated data sets on whatever you might want and find that someone has already gathered everything you need. The community can add to/curate the data just like an open source project and everybody wins.\n\nWhat do you think? Would you use something like this?   I would absolutely use this. As you mention, there are already a good number of datasets on github, but it's far from ideal for it. I've been using [this one](https://github.com/unitedstates/congress-legislators) in a project recently.  The [Open Science Framework](http://openscienceframework.org/) looks relevant. In fact, it's also based on git. Its scope is fairly wide (it's a place to share all aspects of scientific work) but it most definitely encompasses datasets.   I work at a large computer science R&amp;D facility and I cannot tell you how many times we've talked about building something like that just in the short time I've been there. Science is all about being able to share and collaborate yet there isn't an easy way for scientist to do this with data sets especially very large ones. If we could open source data the way we have open sourced code the results would be fantastic! I work at a large computer science R&amp;D facility and I cannot tell you how many times we've talked about building something like that just in the short time I've been there. Science is all about being able to share and collaborate yet there isn't an easy way for scientist to do this with data sets especially very large ones. If we could open source data the way we have open sourced code the results would be fantastic!   Plenty of these already exist if you look around, such as the [UCI repository](http://archive.ics.uci.edu/ml/datasets.html). I'm not sure I understand why you would want/need version control on something that is usually static for the sake of reproducibility. Data sets aren't entirely static, however; the concept of forking can be used to create, for example, different formats for the dataset; branching could be used to create updated, more recent versions. Good points about version control benefits/uses, although in practice they may not be entirely that useful.  From my experience, datasets are often serial/static releases with few \"raw\" formats made available. Also, large-scale datasets would probably be a burden in a dvcs like git.\n\nEdit: As an afterthought, I could definitely see it being quite interesting for crowd-sourced datasets! Git doesn't handle large binaries well on its own, but addons like git-annexe address this problem. And raw Git handles large text files just fine. I'll have to check those out, as I've never used git for anything larger than a moderate codebase. How large is large? Still seems infeasible for git to handle anything \"bigdata\", like a 100GB+ dataset with 100k+ files, and then have someone else pull and push back a heavily modified version of the dataset into a new branch, but I guess I've never tried it :) Data sets aren't entirely static, however; the concept of forking can be used to create, for example, different formats for the dataset; branching could be used to create updated, more recent versions. Plenty of these already exist if you look around, such as the [UCI repository](http://archive.ics.uci.edu/ml/datasets.html). I'm not sure I understand why you would want/need version control on something that is usually static for the sake of reproducibility.  You might be interested in [CKAN](http://ckan.org/), \"the world’s leading \nopen-source data portal platform\".  Very interesting idea. I see one problem that needs to be solved first: a generalisable data model. It's hard to predict what kind of data will be stored, so it's hard to come up with a model that'll cover all, or at least a few, possible uses. I think you seriously underestimate how hard that problem is and how many engineer-years (and philosopher-years!) have already been expended on it. Google \"semantic Web ontology\". I'm not underestimating anything. I know very well it's a problem impossible to solve completely.    There have been a number of unsuccessful attempts at a Github for data, and it's just not a very good idea.\n\nIn fact just googling for [the phrase](https://www.google.com/search?q=github+for+data&amp;oq=github+for+data) turns up plenty of discussion on the topic, including a post on the [sunlight lab blog](http://sunlightlabs.com/blog/2010/we-dont-need-a-github-for-data/).\n\nSo unless you've got a really compelling and unique take on the idea... i'd say skip it and move onto something else.       build something on top of github rather than replace it. then you can leverage their infrastructure and ecosystem  I don't know much about hosting issues, but wouldn't space and bandwidth be costly? That is what I would would fear the most if I started this project. Maybe some kind of torrent option for larger files I have been thinking about exactly this for a while now. Groups with big data can definitely afford to seed and there could be measures in places such as seeding ratios etc directly affecting access to data etc. Direct download could be a paid option, which would help with bandwidth and storage. That does rather lock out casual and home users. I have been thinking about exactly this for a while now. Groups with big data can definitely afford to seed and there could be measures in places such as seeding ratios etc directly affecting access to data etc. Direct download could be a paid option, which would help with bandwidth and storage.    http://datahub.com/ seems to be free.      I think torrents are the way to go, but this is no guarantee of clean tagged annotated just  data...  perhaps a central clearing house that data checksums where registered with would help...  using hdf5 protocol might be a standard to incorporate...    I think torrents are the way to go, but this is no guarantee of clean tagged annotated just  data...  perhaps a central clearing house that data checksums where registered with would help...  using hdf5 protocol might be a standard to incorporate...          ",
    "url" : "http://www.reddit.com/r/compsci/comments/16f4ru/a_github_for_data_sets_what_do_you_guys_think/"
  }, {
    "id" : 7,
    "title" : "Approximate Bayesian Computation",
    "snippet" : "  OK, so I'm now kind of confused.  Can someone explain how this is different from the way that JAGS or BUGS might work if I constructed a really hairy and nested formula linking several latent parameters to observations?  (I recognize that samples are never rejected in Gibbs, but aside from that, what's the difference?) ",
    "url" : "http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002803"
  }, {
    "id" : 8,
    "title" : "Introduction to Mezzo",
    "snippet" : "  Very cool project, though I wonder if this pushes ML types too far away from their sweet spot of \"human interpretable static analysis\". What do the signatures of functions with non-trivial side effects look like? Can you make errors intelligible?  What does Mezzo have over Haskell, Erlang, OCaml, F#, etc.? In which of these languages can you express the in-place `mswap` function described in the post?",
    "url" : "http://gallium.inria.fr/blog/introduction-to-mezzo/"
  }, {
    "id" : 9,
    "title" : "CS folk of Reddit, do you identify more as a scientist or an engineer?",
    "snippet" : "I've been thinking about this a lot over the past year, and I'm genuinely curious.\n\n*Do you identify more as an engineer or a scientist?*\n\nI think it would be kind of interesting to see how the largest CS community I know of identifies themselves.\n\nEDIT: This is a question about your own perception of your career. My personal perception is that engineer creates, scientist discovers, thus I'm more of an engineer.\n\nEDIT2: So, based on what everybody is saying, there still isn't a whole lot of consensus between engineer or scientist. On top of that, we're seeing a lot of people identifying as mathematicians, and a sprinkling of wizards.  You forgot mathematician. I personally wrap mathematician into the scientist category.\n\nMy view (at least):\n\nScientist: Someone who's focus is on learning new things with less focus on creating an end product.\n\nEngineer: Someone who's focus is on producing end products. I like to think of it as:\n\nScientist: Someone who's focus is on researching new ideas and concepts\n\nEngineer: Someone who's focus is on practical applications of those ideas and concepts\n\nI'm not saying one view is wrong or right, just different :) To me the difference boils down to discovery vs. invention.  A scientist discovers, an engineer invents. I like to think of it as:\n\nScientist: Someone who's focus is on researching new ideas and concepts\n\nEngineer: Someone who's focus is on practical applications of those ideas and concepts\n\nI'm not saying one view is wrong or right, just different :) It's not always so cut-and-dry in terms of research.  I do research in a physics department, doing fundamental science on polymer properties. However, most of the other labs we interact with are in chemical engineering and do very similar stuff. I like to think of it as:\n\nScientist: Someone who's focus is on researching new ideas and concepts\n\nEngineer: Someone who's focus is on practical applications of those ideas and concepts\n\nI'm not saying one view is wrong or right, just different :) I like to think of it as:\n\nScientist: Someone who's focus is on researching new ideas and concepts\n\nEngineer: Someone who's focus is on practical applications of those ideas and concepts\n\nI'm not saying one view is wrong or right, just different :) I personally wrap mathematician into the scientist category.\n\nMy view (at least):\n\nScientist: Someone who's focus is on learning new things with less focus on creating an end product.\n\nEngineer: Someone who's focus is on producing end products. I think there is a big difference between a scientist and a mathematician, though. A scientist discovers things using the scientific method and focuses on observable results. If a scientist can't describe an experiment to validate his hypothesis, he hasn't really said anything.\n\nA mathematician builds models, and studies how those models behave based on other models. There's no experimentation -- in fact, many of the \"objects\" mathematicians study *could not* exist, e.g. infinitely large or infinitely precise numbers (real numbers) and infinite processes (Turing machines, RE-sets). If a mathematician can't prove his hypothesis using formal logic, he hasn't really said anything (even if he can experimentally show it).\n\nThere is a great deal of intermixing, and most people are probably a mix of both, but to say that mathematicians are scientists is just not true.\n\nP.S. This is why I always describe myself as a *theoretical* computer scientist. I work in formal logic and automated reasoning, and I'm very much a mathematician. Some of my best friends are scientists and engineers, though. I think there is a big difference between a scientist and a mathematician, though. A scientist discovers things using the scientific method and focuses on observable results. If a scientist can't describe an experiment to validate his hypothesis, he hasn't really said anything.\n\nA mathematician builds models, and studies how those models behave based on other models. There's no experimentation -- in fact, many of the \"objects\" mathematicians study *could not* exist, e.g. infinitely large or infinitely precise numbers (real numbers) and infinite processes (Turing machines, RE-sets). If a mathematician can't prove his hypothesis using formal logic, he hasn't really said anything (even if he can experimentally show it).\n\nThere is a great deal of intermixing, and most people are probably a mix of both, but to say that mathematicians are scientists is just not true.\n\nP.S. This is why I always describe myself as a *theoretical* computer scientist. I work in formal logic and automated reasoning, and I'm very much a mathematician. Some of my best friends are scientists and engineers, though. I personally wrap mathematician into the scientist category.\n\nMy view (at least):\n\nScientist: Someone who's focus is on learning new things with less focus on creating an end product.\n\nEngineer: Someone who's focus is on producing end products. I personally wrap mathematician into the scientist category.\n\nMy view (at least):\n\nScientist: Someone who's focus is on learning new things with less focus on creating an end product.\n\nEngineer: Someone who's focus is on producing end products. I personally wrap mathematician into the scientist category.\n\nMy view (at least):\n\nScientist: Someone who's focus is on learning new things with less focus on creating an end product.\n\nEngineer: Someone who's focus is on producing end products. Honestly I'd say you're actually leaving out a ton of categories here.  Computers are actually a really broad field, but everyone tends to get grouped together.  I'd say you've at least got 9 categories.  Scientist, Engineer, Developer, Mathematician (or possibly theoretician), Security, Sys admin, Grp, Designer (project lead), Databases.  These are all related, but they do different jobs, and more importantly they approach the jobs differently.\n\nYou could actually break this up a lot more, and I'm sure I missed several groups, but the point is programming/computer science is a lot more divided than just scientist vs. engineer.  Personally I fancy myself more of a wizard. Technomage. Technomage. Personally I fancy myself more of a wizard. Although I'm sure you intended that to be a joke, I think a wizard is precisely what I would call myself. Not because I'm so good that it's unbelievable, but because I feel that CS lands so oddly between engineering and science that it seems like magic sometimes. Although I'm sure you intended that to be a joke, I think a wizard is precisely what I would call myself. Not because I'm so good that it's unbelievable, but because I feel that CS lands so oddly between engineering and science that it seems like magic sometimes. Personally I fancy myself more of a wizard. Personally I fancy myself more of a wizard. You're a wizard Harry! What? I thought it was kind of funny. Upvote. But seriously: Our jobs are to either research and/or create new laws that govern imaginary systems that effect the whole world, or engineer said systems for progress and pleasure. Can somebody please explain to me how that is not a wizard? That is a wizard. Wizards do that. But seriously: Our jobs are to either research and/or create new laws that govern imaginary systems that effect the whole world, or engineer said systems for progress and pleasure. Can somebody please explain to me how that is not a wizard? That is a wizard. Wizards do that. Hmm, I'd be more comfortable saying that the job is to create new policies (programs) that work with existing laws (compilers, frameworks, machines).\n\nWe rarely come up with new laws ourselves. Restrictions are placed on us before we start. That, I think works for the engineers side. For the scientists side, I dunno. I'm not a scientist. :) Yes, and if you hadn't replied, I WOULD HAVE DELETED MY POST. Asshole. What? I thought it was kind of funny. Upvote. What? I thought it was kind of funny. Upvote. Personally I fancy myself more of a wizard. Personally I fancy myself more of a wizard.   I have a degree in \"Computer Engineering. And a PhD. So I am 1/2 engineer, 1/2 scientist and 1/2 bad with fractions. A few high-level math classes will irreparably harm your ability to do basic arithmetic. \"Dad, what is 3+9?\" - \"That depends, what is the space, and how is '+' defined?\" A few high-level math classes will irreparably harm your ability to do basic arithmetic.   Definitely an engineer. I am all about the practical application of computer science concepts.\n\nHowever I hesitate to call myself an engineer in front of certain friends/colleagues because I did not receive an engineering degree. In Canada, engineers [participate in a special closed ritual and receive an iron ring](http://en.m.wikipedia.org/wiki/The_Ritual_of_the_Calling_of_an_Engineer) at graduation. My degree program (computer science) fell under the math faculty, so I did not participate in such a ceremony and may not be officially considered an engineer by some of my peers. It's not just some of your peers. Depending on your province, it might be by law as well. Here is Alberta, [APEGGA](http://www.apegga.org/) actually owns the right to title. Check out this legal bullshit:\n\n&gt; [From 3(1)(a)(ii)] No individual, corporation, partnership or other entity, except a professional engineer, licensee or permit holder entitled to engage in the practice of engineering, shall use the word “engineer” in combination with any other name, title, description, letter, symbol or abbreviation that represents expressly or by implication that the individual, corporation, partnership or other entity is a professional engineer, licensee or permit holder...\n\nI may have to switch to comp. sci. from comp. eng. because I can't stand the whole money-grabbing holier-than-thou thing...\n\nEDIT: Full document: [PDF WARNING!!!](http://www.apega.ca/About/ACT/pdf/2012/CA-C-2012%20ACT.pdf) for those interested. Computer engineering is a branch of electrical, and is a hardware focus.  Usually CS is the software side of things (except at wilfred laurier and a couple of others).\n\nAnd yes, claiming to be an engineer without a P.Eng is illegal with some exceptions (microsoft or cisco engineer is allowed because they are not claiming to be actual engineers).  The iron ring thing is irrelevant, the professional engineering accreditation and requirements (including a professional ethics exam) and membership in the professional society is what matters.   What do you mean by:\n&gt;...(except at wilfred laurier and a couple of others).\n\nOh, and it's Wilfrid not Wilfred Laurier CS is a much  more hardware focused school than most of the others in Canada.  That's usually because it grew out of their physics department, rather than out of a maths department  and has stuck to the hardware legacy.   \n\nThat's not a criticism, just an observation.  Some schools have more programming or more maths etc.  Laurier is more hardware.   The [Computing and Computer Electronics program](http://www.wlu.ca/page.php?grp_id=2&amp;p=31) focuses on hardware related topics, while the [Computer Science program](http://www.wlu.ca/page.php?grp_id=2&amp;p=32) offered at WLU focuses on software. Computer engineering is a branch of electrical, and is a hardware focus.  Usually CS is the software side of things (except at wilfred laurier and a couple of others).\n\nAnd yes, claiming to be an engineer without a P.Eng is illegal with some exceptions (microsoft or cisco engineer is allowed because they are not claiming to be actual engineers).  The iron ring thing is irrelevant, the professional engineering accreditation and requirements (including a professional ethics exam) and membership in the professional society is what matters.   What do you think of Ottawa's or westerns soft engineering? Computer engineering is a branch of electrical, and is a hardware focus.  Usually CS is the software side of things (except at wilfred laurier and a couple of others).\n\nAnd yes, claiming to be an engineer without a P.Eng is illegal with some exceptions (microsoft or cisco engineer is allowed because they are not claiming to be actual engineers).  The iron ring thing is irrelevant, the professional engineering accreditation and requirements (including a professional ethics exam) and membership in the professional society is what matters.   Maybe it varies, but in my university, software engineering is a branch of computer engineering. Sorry if that's not the norm; I was not aware. Also, you're right, the ring has nothing to do with it. Definitely an engineer. I am all about the practical application of computer science concepts.\n\nHowever I hesitate to call myself an engineer in front of certain friends/colleagues because I did not receive an engineering degree. In Canada, engineers [participate in a special closed ritual and receive an iron ring](http://en.m.wikipedia.org/wiki/The_Ritual_of_the_Calling_of_an_Engineer) at graduation. My degree program (computer science) fell under the math faculty, so I did not participate in such a ceremony and may not be officially considered an engineer by some of my peers. That may be more regional than anything else. My CS program down in the states is wrapped into the School of Science, but I've never heard of any of our engineers going through any similar rights.    Engineer.  But I apply the scientific method to my development processes.   To be fair, the scientific method and engineering methods are very similar. Correct, but to be fair I'm amazed at the number of \"engineers\" who don't use either.   I'd say I'm an artist. I view software development more as an art form than a science. I'd say I'm an artist. I view software development more as an art form than a science.  Being an undergrad, it really depends on what class I'm taking. OS or Languages and Compilers class? I'm an engineer. Numerical Analysis or Algorithms class? I'm a total scientist.\n\nI think this can be applied to the whole of CS, since, to me, Computer Science is a strange subject that incorporates both science and Engineering, so I would assume what people are working on are what they view themselves. Being an undergrad, it really depends on what class I'm taking. OS or Languages and Compilers class? I'm an engineer. Numerical Analysis or Algorithms class? I'm a total scientist.\n\nI think this can be applied to the whole of CS, since, to me, Computer Science is a strange subject that incorporates both science and Engineering, so I would assume what people are working on are what they view themselves. Being an undergrad, it really depends on what class I'm taking. OS or Languages and Compilers class? I'm an engineer. Numerical Analysis or Algorithms class? I'm a total scientist.\n\nI think this can be applied to the whole of CS, since, to me, Computer Science is a strange subject that incorporates both science and Engineering, so I would assume what people are working on are what they view themselves.  It depends on your background. Someone who really enjoys the programming side would see themselves as a engineer, and someone who enjoys the theory would see themselves as more of a scientist. Personally, I see myself as a scientist :P  That's what I'm thinking. \n\nStill, though, I believe it'd be fascinating to get a nice sampling of other /r/compsci folk's answers. Maybe even help unify the community's identity. It depends on your background. Someone who really enjoys the programming side would see themselves as a engineer, and someone who enjoys the theory would see themselves as more of a scientist. Personally, I see myself as a scientist :P   Personally, I see myself as an engineer more than a scientist.\n\nEDIT: Background. Right. CS major at Purdue University focusing on software engineering, graphics, and human-computer interaction. What year? Working through my sophomore year right now. Lots of data structures, algorithms, and hardware. Personally, I see myself as an engineer more than a scientist.\n\nEDIT: Background. Right. CS major at Purdue University focusing on software engineering, graphics, and human-computer interaction.   Linguist!\n\n*not that I identify as one, just wanted to hack your dichotomy with a humanities category*\n\nBut, actually, seriously... grammars are important to me, both formally (those developed by Chomsky, a linguist), and in coding, where I factor out different parts of code. It feels similar to, and probably uses the same brain centers as, rephrasing English sentences and paragraphs.\n\nAnd... in coding, I fancy myself a writer. I make stuff up *out of my head* that is worth something. Brooks compares thee to a poet:\n\n&gt; Finally, there is the delight of working in such a tractable medium. The programmer, like the poet, works only slightly removed from pure thought-stuff. He builds his castles in the air, from air, creating by exertion of the imagination. Few media of creation are so flexible, so easy to polish and rework, so readily capable of realizing grand conceptual structures. - *[Brooks](http://www.grok2.com/progfun.html)*  I see myself as a mathematician which I think falls under engineer more than scientist This is interesting. I don't have any evidence for this, but my instinct is to say that most mathematicians consider themselves scientists rather than engineers.\n\nMaybe the fact that computer science is in some sense applied mathematics is the difference? Engineering is also, in some sense, applied mathematics. personally I view [this kind of thing](http://xkcd.com/435/)\n\nTo me CS is a 'purer' science,.. under discrete math So where does engineering fit in on that chart?\n\nOff the top of my head, I can think of two interpretations:\n\n1. Science is applied \"pure math\" (to some degree), and engineering is applied \"science\".\n\n2. Engineering is a second axis, and you can have \"applied pure math\" (maybe some branches of CS) or \"applied chemistry\" (chemical engineering).\n\nWhat are your thoughts? \n\nNote that I'm not disputing what you say, I just think it's really fascinating the way all of us try to systematize language like this. This is interesting. I don't have any evidence for this, but my instinct is to say that most mathematicians consider themselves scientists rather than engineers.\n\nMaybe the fact that computer science is in some sense applied mathematics is the difference? Engineering is also, in some sense, applied mathematics. Engineers build things. Scientists discover things. You can always build new shit, but discovering is harder, slower, and less rewarding (in terms of personal achievement). \n\nDon't interpret that the wrong way; I think discovery is amazing. I just think it's less independent than engineering. Having said that, engineering wouldn't be possible without discovery by scientists.\n\nMathematicians, however  -- I don't know, but I've certainly never read a news story about or directly benefitted from a new way to divide by Pi.  [deleted]          I think the CS part of me is more Engineer, but I'd rather call myself a scientist because of the way I like to think I think.\n\nAm I allowed both?  Computer Science is engineering. CS research is about finding new algorithms or new methods to improve exsiting systems or create completely new ones- Science is about expanding our knowledge about the natural world. We expand out knowledge of the world we created, or about mathematics.\n\nI am an engineer.  There is something deeply natural about computation itself.  The Church-Turing theses states that all computational models that are at least Turing-complete are equivalent in a computability theory sense.  The natural world defines what is computable, not us!  Some physicists believe that the behavior of the universe is either determined by a big computation or can be perfectly simulated by one, given enough resources.  Questions like P =? NP are not really about \"some fantasy world we created.\"  Some of these questions are independent of the model of computation used.\n\nDiscovering the theoretical limits of computation by physical machine (\"computer\") definitely could qualify as a science.\n\nI think you may be thinking of software engineering/programming, which is much different from the pure CS research that I do.\n\nDo you consider math to be engineering?          Artist, coding means expressing your ideas using the medium of your programming language :) But if I had to choose, definitely engineer, you pick the right parts and make them work together.         I tend to see myself as both, depending on what I'm doing at the time.\n\nIf I'm building software, using preexisting software tools and libraries or my own code, then I'm an engineer. \n\nIf I'm learning about a new technique, language or framework then I'm a scientist. I even have a white coat for such an occasion. I don't think learning a new programming language or software framework makes you a scientist, per se.  Science is about expanding the realm of human knowledge.\n\nHowever, I don't want to ruin the moment or give you reason to believe you shouldn't wear that spiffy lab coat.              Computer Scientist, Hacker\n\nI consider \"Engineer\" a term reserved for people with actual engineering degrees (in Canada, with the la-de-da ring and all that).\n\nEngineers also tend to be professional.  I like to think of myself as more of a lunatic fringe mad scientist.         Despite the poorly chosen name of the field, Computer Science has *nothing* to do with Science.  I would argue that Computer Science is the opposite of Science.  Science is the process of discovering fundamental laws by inspecting the results of these laws.  Computer Science involves discovering what can result from fixed fundamental laws.  We know the rules that drive a computer at the *fundamental* (read: hardware) level, and we try to build things up from there.  In my opinion, all of computer science falls between the fields of Mathematics and Engineering.  Theoretical computer science is pure Math, whereas applied computer science falls under engineering.  Personally, I see myself as more of a mathematician.\n\n**Edit: TL;DR:** It's between Engineering and Math, not Engineering and Science.\n\nAlso, forgot to mention: Before the field of Computer Science existed, there was only Electrical Engineering and Math. All of my professors majored in one of those two fields.  The term Computer Science emerged later, and was not an apt term for the field.\n               ",
    "url" : "http://www.reddit.com/r/compsci/comments/16bjlw/cs_folk_of_reddit_do_you_identify_more_as_a/"
  }, {
    "id" : 10,
    "title" : "Can you determine an undirected graph from a two-step adjacent matrix? (x-post from r/math)",
    "snippet" : "I am trying to find a way to go from a two step adjacent matrix to an (undirected) graph. I have searched for a long time but can still only manage to solve it using trial and error.\n\nIs there any standardized approach to this problem for where the initial (one step) adjacent matrix is unavailable?  What exactly do you mean by a two-step adjacency matrix?  Is it a binary matrix with a 1 for every pair of vertices that are connected via some path of length 2?  If that's the case then it's not, in general possible.  Consider the graph\n\n    A-B\n    B-C\n\nThis is a wedge.  The two-step adjacency matrix of this graph is:\n\n       A B C\n     A 0 1 1\n     B 1 0 1\n     C 1 1 0\n\nSince everything is within two hops of everything else (the diag depends on how you handle self-loops).  However, this graph is indistinguishable from the triangle, i.e.:\n\n    A-B\n    A-C\n    B-C\n\nThis graph has the same two-step adjacency matrix (i.e. everything is connected to everything else), yet the original graphs are not isomorphic. This is exactly what I mean. Thank you very much for your answer! You're welcome :)!",
    "url" : "http://www.reddit.com/r/compsci/comments/16cdlq/can_you_determine_an_undirected_graph_from_a/"
  }, {
    "id" : 11,
    "title" : "Software architecture as a function of trust",
    "snippet" : "  ",
    "url" : "http://www.johndcook.com/blog/2011/05/26/software-architecture-and-trust/"
  }, {
    "id" : 12,
    "title" : "The story of Ping from its author",
    "snippet" : "  ",
    "url" : "http://ftp.arl.army.mil/~mike/ping.html"
  }, {
    "id" : 13,
    "title" : "Bittorrent doesn't handle small files efficiently. What is the state of the art for p2p distribution of multiple small files?",
    "snippet" : "Yes you could bundle many small files together into one large file and distribute that via bittorrent. But what if additional small files were being created at regular intervals? An example application might be p2p mirroring of a debian-based linux distribution's package archive. If you used bittorrent you would have to choose between distributing the recently created small files immediately but inefficiently, or withholding distribution of the recent small files until they could be bundled up into a larger group.\n\nHas anybody proposed any modifications to bittorrent (or alternative protocols) that handle the case of small files well?  Since you mentioned it specifically, the problem of using BitTorrent with Debian's repositories, where small files change regularly, has actually been solved: [Apt-P2P](http://www.camrdale.org/apt-p2p/). ah thanks - in fact a little research based on your link indicates there have been at least 3 attempts at this: [debtorrent](http://debtorrent.alioth.debian.org/), [apt-torrent](http://sianka.free.fr/) and Apt-P2P\n\nIn fact there is a paper by the author of Apt-P2P that goes into exactly the issues that I was thinking of in the OP http://www.camrdale.org/apt-p2p/motivation.pdf\n\nI think that's pretty much the question answered!  Not P2P, but rsync is probably the way to go. Funnily enough rsync tends to be a decent answer to a large number of data transfer problems. I have aliased `scp` to `echo Use rsync -az instead!` I have aliased `scp` to `echo Use rsync -az instead!` [deleted] Funnily enough rsync tends to be a decent answer to a large number of data transfer problems. Funnily enough rsync tends to be a decent answer to a large number of data transfer problems. You can trust rsynch to \"do the right thing.\"  This is showing my age a little, but Direct Connect? The protocol is pretty bad, but it does let you transfer lots of little files, uses hashing to find duplicates, and large files can be downloaded from multiple sources simultaneously.   bittorrent is used to distribute a certain version of the file(s). If you wish to have live updates to the files, all I can think of is dropbox et al.  Dropbox is not p2p. If the p2p requirement would be dropped, plain old http would do the trick. [deleted] bittorrent is used to distribute a certain version of the file(s). If you wish to have live updates to the files, all I can think of is dropbox et al.  dropbox doesn't scale! for a noobie like me:\nwhy not? As far as I understand dropbox, its a cloud based synchronisation system. So you upload stuff to the cloud, then it is served from the cloud to all your machines or anyone you share your files with.\n\nSo if I receive a file from dropbox I make a connection to their server, and that entire file is delivered to me. If a file became very popular, then tens of thousands of people might attempt to download the file at the same time. There is probably an upper limit on the amount of server bandwidth the dropbox can provide. And before it reaches that point you would expect download speed to decrease as more people are downloading the file simultaneously.\n\nBittorrent on the other hand is a peer to peer protocol. What that means is that instead of 5000 users with 5000 connections to the server downloading 5000 copies of the same file, each user has several connection to several other users and downloads parts of the file from several different users and in turn shares those parts with other users. This is often called a swarm. Its possible that very few of the users ever download from the original server/uploader, instead downloading the entire file from other users (or peers).\n\nContrasted with dropbox's model, bittorrent scales very well, because the more people there are trying to download the file, the more opportunities you have to find someone who has a part of the file that you want. for a noobie like me:\nwhy not? for a noobie like me:\nwhy not?  Just use email. haha has anyone tried tunnelling bittorrent over email yet? A more serious answer would be some DVCS like Git or Mercurial, where you may be able to sync small changes from anyone with an updated copy.  Was going to upvote but then I realized this is /r/compsci...so not so relevant.  You think that network protocol design isn't computer science? Doesn't seem to fit with what the sidebar says:\n&gt; topics such as algorithms, formal languages, automata, information theory, cryptography, machine learning, computational complexity, programming language theory, etc...\n\n\n...I suppose it had the potential to be theoretical, but it became a practical engineering-type discussion pretty quickly. More relevant to /r/networking, /r/sysadmin, /r/techsupport, etc.\n\n\nFrom the downvotes I guess anything computer-y is fair game here though.  Doesn't seem to fit with what the sidebar says:\n&gt; topics such as algorithms, formal languages, automata, information theory, cryptography, machine learning, computational complexity, programming language theory, etc...\n\n\n...I suppose it had the potential to be theoretical, but it became a practical engineering-type discussion pretty quickly. More relevant to /r/networking, /r/sysadmin, /r/techsupport, etc.\n\n\nFrom the downvotes I guess anything computer-y is fair game here though.  Doesn't seem to fit with what the sidebar says:\n&gt; topics such as algorithms, formal languages, automata, information theory, cryptography, machine learning, computational complexity, programming language theory, etc...\n\n\n...I suppose it had the potential to be theoretical, but it became a practical engineering-type discussion pretty quickly. More relevant to /r/networking, /r/sysadmin, /r/techsupport, etc.\n\n\nFrom the downvotes I guess anything computer-y is fair game here though. ",
    "url" : "http://www.reddit.com/r/compsci/comments/168va6/bittorrent_doesnt_handle_small_files_efficiently/"
  }, {
    "id" : 14,
    "title" : "Need a good website that posts video lectures on Operating Systems Engineering",
    "snippet" : "I'm taking the class this semester and I'm looking for a supplement to the lecture. I would prefer video but lecture notes would work too.\n\nI'm looking for something that covers the topics of: Processes and Threads, Process Synchronization, CPU Scheduling, Deadlocks, Memory Management and File Systems.\n\nThanks in advance and sorry if wrong subreddit please let me know if there is a better one to post to\n\nedit: Thanks everyone. Just to let you know I'm looking for something in the 400-level class range or comparable.  Here you go, start here: http://www.youtube.com/watch?v=XgQo4JkN4Bw Lecture notes to the above course available [http://inst.eecs.berkeley.edu/~cs162/fa10/](here). This page will also help you determine the videos you actually need to view. Oh the links to the videos in this page don't work. # **Fixed your link**\nI hope I didn't jump the gun, but you got your link syntax backward! Don't worry bro, I fixed it, have an upvote!\n\n- [here](http://inst.eecs.berkeley.edu/~cs162/fa10/)\n\n\n\n^Bot ^Comment  ^-  ^[ [^Stats ^&amp; ^Feeds](http://jordanthebrobot.com) ^] ^- ^[ [^Charts](http://jordanthebrobot.com/charts) ^] ^- ^[ [^Information ^for ^Moderators](http://jordanthebrobot.com/moderators) ^]      If books will do too, I'd suggest Tanenbaums Modern Operating Systems. One of the best textbooks I've ever read. ",
    "url" : "http://www.reddit.com/r/compsci/comments/167nur/need_a_good_website_that_posts_video_lectures_on/"
  }, {
    "id" : 15,
    "title" : "Believing in Computer Science «  Existential Type",
    "snippet" : "   Since when is incremental research in danger? All recent CS research that I've seen, in every subfield, has been entirely incremental. Academia has not, for at least two decades, been very much into revolutionary anything. Academia loves incremental research and is very very afraid of taking risks, of pursuing any direction that might not lead to guaranteed publications.\n\nAs a personal example: I wanted, for my PhD, to create my own programming language. Several professors have told me that I should not do this because this would make it very difficult to publish papers. Languages like LISP, Ocaml, Scheme, Haskell and SELF were borne out of academia, but today, you would have much difficulty publishing papers about language design to the top conference in our field: PLDI, whose name stands for Programming Language **D**esign and Implementation.\n\nMuch of the bold research that was done in the 1960s and 1970s could never take place today. The original CS papers published by Alan Turing would now be refused from most conferences. If anything, I would say that computer science is kind of stagnating right now, because nobody is really willing to take big risks and try radically new directions anymore. If you can't guarantee publications that definitely improve on past results, you can't guarantee funding. As a fellow \"PL major\", I want to talk about the whole \"design your own programming language\" thing.  I've done it.  And I can say: it *does* make it hard to publish papers.\n\nWhy?  Because one programming language can only have so many innovations beyond the research frontier, and the mere design of a language combining existing constructs is not really research.  For one thing, without an empirical engineering study (which is very difficult to do for an entirely new language), what is there to study or measure or even prove about your new language?\n\nThe problem with radical invention is that it's unmeasurable, and therefore difficult to publish.  It's also, you should realize, usually not as radical as you think. I was also told not to do a language for my phd, thankfully I did a language for my phd. \n\nYou don't publish languages, you publish features. It just happens that I have to design a language for each feature, it's not that hard actually. Innovative design papers are perfectly welcome at Onward! Btw....I'm not sure about pldi, it hasn't been a design conference for around 10 or 15 years.  I agree and I disagree.\n\nI agree in that everything you've said is true.\n\nI disagree in that actually coming up with new feature after new feature from the research frontier rather than following one line of research into its deepest depths requires rather much creativity for only one dissertation ;-).\n\nSo for example, I've got a types paper under submission to ECOOP at the moment.  My language uses lots of interesting features.  However, most of them have already been published as research, and thus I can't publish them.  The combination yields a good language, but I can't honestly claim it's *all* research, can I? Did you check out Crista Lopes' [great post](http://tagide.com/blog/2012/03/research-in-programming-languages/) on the topic? Also sometimes you have to wonder if you are doing research at all, maybe your contribution is design, innovation, invention. Design is not research, of course, but everything is very muddled in our field because we claim to be strictly a science.  I've read it, and I agree with much of it.  *However*, I think that Academic PL is partly at fault.  Contrast what she writes with what Bob Harper writes.  It is *plain* that Bob Harper is *almost entirely unconcerned* with writing actual programs.\n\nIn fact, he is often even unconcerned with the usefulness of programming languages.  He is *only* concerned with what interesting things he can prove about the types involved in the program.  Actually, even worse: he only cares about the kinds of types used in Type Theory Proper.  Type systems formalized just to talk about real languages are not his domain, especially if they've got the dreaded words \"object-oriented\" or \"Haskell\" on them.\n\nI can think of a PL department I really, truly admire.  That is not how they work.\n\nI think that PL research has a valid role, but a different valid role than its role now.  Its role now is merely to act as a Statler-and-Waldorf critic of actual programs and languages.  What it *needs* to do is explore the search space of possible languages, with the search-tree expansion being constrained by consistency. Bob Harper will be Bob Harper, it hardly matters much to us. There are many academic PL researchers that are doing useful stuff (PLT for one), and the PL community is big enough that we have our ideologies and factions (see Cook vs. Harper, for example). \n\nThere was a time when you could write a PLDI/OOPSLA paper that begin with \"This paper explores the design space of...\", Self comes to mind, and that wasn't too long ago. And to be honest, I really blame us (the ~3G of PL researchers)! We hardly compare to the PL researchers of yesterday who were out there doing \"big\" things, while we've become complacent with incremental innovation on Java and Haskell and such, and are risk adverse to doing more. There are a lot of open hard problems that need solving, and we just tend avoid those as too hard. \n\nWhat does it mean to be a PL researcher really? Are we designers? Innovators? Inventors? Scientists? Mathematicians? A little bit of each? Even we can't agree on this, even though we know something is wrong. And the problem is really the judge and jury, who judges you and/or issues your paycheck, what will they accept? I'm sort of lucky in my current role, but its a fragile luckiness. \n\n &gt;And to be honest, I really blame us (the ~3G of PL researchers)!\n\nWho's that?  My research notebook is filled with my internal debates and decisions over the design process, which I would love to actually share as published research... if someone considered design work worthy of publication. As a fellow \"PL major\", I want to talk about the whole \"design your own programming language\" thing.  I've done it.  And I can say: it *does* make it hard to publish papers.\n\nWhy?  Because one programming language can only have so many innovations beyond the research frontier, and the mere design of a language combining existing constructs is not really research.  For one thing, without an empirical engineering study (which is very difficult to do for an entirely new language), what is there to study or measure or even prove about your new language?\n\nThe problem with radical invention is that it's unmeasurable, and therefore difficult to publish.  It's also, you should realize, usually not as radical as you think. Programming languages, the concepts just seem to rotate through each language, but also, not really. \n\nI try to continue studying as many as I can, find new things if I can; really is quite impossible to even imagine new concepts, from my perspective at least. Perhaps I don't know enough about them, but I don't think that's necessarily the case. \n\nIt just seems too dependent on (so many) individuals adding incrementally to language constructs, bit by bit, that the changes are not really as profound as they might have been, when computer science was a smaller field. They are almost imperceptible, unless you really look *very* hard for them, which again is from my perspective. I don't know if I just manage to confuse myself often, but even so, it combines the work of many, many, many people, and I know that much to be true. \n\nProbably don't have a clue of what I am talking about though. Still enjoy the math behind them, though, although, maybe not.   You can definitely come up with a very inventive and well-designed language by combining features in an interesting way.  *However*, it's not necessarily *research*.  Much of the low-hanging fruit in PL, like most other fields, has already been picked. I agree that the low-hanging fruit has been picked, but what do you define as research?",
    "url" : "http://existentialtype.wordpress.com/2012/08/25/believing-in-computer-science"
  }, {
    "id" : 16,
    "title" : "Project idea for Discrete Mathematics",
    "snippet" : "I'm an Advanced Mathematics and Computer Science major at Michigan State and have a Discrete Mathematics class this semester. I talked to the professor about doing an Honors option (additional work/project) and was approved, now I just need to come up with a good topic to do work in. I'd love to delve into some hard proofs and learn their real world purpose and then give a presentation where I teach this to the class.\n\nDoes anyone have any good topics I should read about or possible proofs that would be good for this? Or just another interesting idea for this honors option  Perhaps getting into some of the newer methods of cryptography? A lot of that is mostly number theory. Elliptic-curve, functional, etc. Also very relevant to computer science and real life.    ",
    "url" : "http://www.reddit.com/r/compsci/comments/164qzo/project_idea_for_discrete_mathematics/"
  }, {
    "id" : 17,
    "title" : "timestamp puzzle",
    "snippet" : "I'm trying to reverse engineer a timestamp that an api uses.\n\n1972313a\n\n,\n\ne02717d2\n\n,\n\ncb0c295d\n\nThe first one was taken 6th Jan 2013, the second 7th Jan 2013, the third 8th Jan 2013. It seem to change once every day.\n\nAny ideas? I have tried converting the hex number to decimal and used that to convert to epoch time (seconds since 1970) but thats not right. Any help appreciated.\n\n(it's asian, GMT+8 if that matters).  They seem quite different witch may suggest it is a hash of something based on a timestamp. That makes sense, as its part of a url session string, so a bit secret. Unless a 6 year old made the website, you're not likely to figure it out (if it is supposed to be mildly secure). Not to be rude, but if the best attempt you had was converting hex to decimal, I doubt you have the experience to attack this problem efficiently. There are just too many easy ways to \"encrypt\", \"encode\", or hash a datestamp. What I might suggest, is taking a common time format (like you were doing above), and work backwards. Start applying various hashing algorithms and such to it and see if you get the numbers you're trying to reverse. Yeah, not to be rude but the first part does come off a bit confrontational. \n\nFor what its worth. The previous version of the api used a timestamp that was simply a hex representation of seconds not since epoch, but what I looked like an arbitrary date. I later found out it was due to the internal timestamp representation of a legacy database they where using.\n\n Sorry, I tried to think of another way to say it but I started a new job today and I'm rushing to fit in Reddit along with all my other things in half the time now :P\n\nThere's probably good odds the software isn't custom made, so if you read the very bottom of the web page maybe it says something like \"Powered By x\" and if you get a copy of that you could reverse-engineer that. That makes sense, as its part of a url session string, so a bit secret.  The relationship between the timestamp and the current time is obviously non-linear:\n\nt(6 Jan 2013) &lt; t(8 Jan 2013) &lt; t(7 Jab 2013)\n\nSo,as others said, it was likely hashed or encrypted in some way. It's 32 bits only, so I doubt it's a cryptographic function (SHA1, MD5, AES etc). CRC32 is one possibility, but that's just a wild guess. The relationship between the timestamp and the current time is obviously non-linear:\n\nt(6 Jan 2013) &lt; t(8 Jan 2013) &lt; t(7 Jab 2013)\n\nSo,as others said, it was likely hashed or encrypted in some way. It's 32 bits only, so I doubt it's a cryptographic function (SHA1, MD5, AES etc). CRC32 is one possibility, but that's just a wild guess. ",
    "url" : "http://www.reddit.com/r/compsci/comments/1653wt/timestamp_puzzle/"
  }, {
    "id" : 18,
    "title" : "Can NDTMs solve problems that DTMs can't solve (Turing Machines)?",
    "snippet" : "According to wikipedia, \"it is possible to simulate NTMs with DTMs\" so I'm assuming this means DTMs can't simulate **all** NTMs? Thanks.\n\nEDIT: I guess what I really want to know is, are there problems NTMs can solve that DTMs can not?  Nope. For any nondeterministic TM, there is a deterministic TM that simulates it. The idea is that we can describe one computation path of an NTM by the sequence of \"choices\" it makes; so our TM can first try all possible choices for the first step, then all possible sequences of two choices, and so on. It will eventually accept if and only if the original NTM had an accepting path.\n\nAn interesting follow-up question, though, is whether TMs are as \"fast\" as NTMs.... I might be wrong, but this follow-up question is exactly the P=NP problem, isn't it? Yep. The way I heard it explained, that finally made it click for me, is that it's very hard to find the answer to an NP problem, but once you do find the answer, it's trivial for a third party to verify that your answer really is correct.\n\nFor example, integer factorization - I take two large primes, multiply them together, and tell you the product. The fastest way we know of for you to find the two original primes is brute force. However, if you actually do the brute force search and find the primes, you can tell the answer to someone else and they could verify that you're correct simply by multiplying them together.\n\nThis is why non-determinism makes NP problems \"fast\". A NTM can \"check\" every possible answer at the same time - spinning off an infinite number of threads, if you will. Since each individual \"check\" can be done in polynomial time, the overall process on a NTM is polynomial. Doing the same thing on a DTM is \"single-threaded\", which makes it slower by a factor of the number of \"threads\" that the NTM used. Yep. The way I heard it explained, that finally made it click for me, is that it's very hard to find the answer to an NP problem, but once you do find the answer, it's trivial for a third party to verify that your answer really is correct.\n\nFor example, integer factorization - I take two large primes, multiply them together, and tell you the product. The fastest way we know of for you to find the two original primes is brute force. However, if you actually do the brute force search and find the primes, you can tell the answer to someone else and they could verify that you're correct simply by multiplying them together.\n\nThis is why non-determinism makes NP problems \"fast\". A NTM can \"check\" every possible answer at the same time - spinning off an infinite number of threads, if you will. Since each individual \"check\" can be done in polynomial time, the overall process on a NTM is polynomial. Doing the same thing on a DTM is \"single-threaded\", which makes it slower by a factor of the number of \"threads\" that the NTM used. &gt; This is why non-determinism makes NP problems \"fast\". A NTM can \"check\" every possible answer at the same time - spinning off an infinite number of threads, if you will. Since each individual \"check\" can be done in polynomial time, the overall process on a NTM is polynomial. Doing the same thing on a DTM is \"single-threaded\", which makes it slower by a factor of the number of \"threads\" that the NTM used.\n\nI haven't taken a theory class in a long time. I never quite understood how an NTM can check the solution space all at once.\n\nI recall NFAs where you can transition to multiple states in one step. Is this is what is meant by an NTM, in that you can be in multiple states of a DTM at a given time? Whereas a DTM must enumerate each state? &gt; This is why non-determinism makes NP problems \"fast\". A NTM can \"check\" every possible answer at the same time - spinning off an infinite number of threads, if you will. Since each individual \"check\" can be done in polynomial time, the overall process on a NTM is polynomial. Doing the same thing on a DTM is \"single-threaded\", which makes it slower by a factor of the number of \"threads\" that the NTM used.\n\nI haven't taken a theory class in a long time. I never quite understood how an NTM can check the solution space all at once.\n\nI recall NFAs where you can transition to multiple states in one step. Is this is what is meant by an NTM, in that you can be in multiple states of a DTM at a given time? Whereas a DTM must enumerate each state? Non-determinism doesn't exactly mean checking the solution space at once. What it really means is \"if there is a correct choice to make, I'll make it\". That is exactly why NP can be defined as both \"the class of languages that have a certificate that can be verified in polynomial time\" (e.g. for the Hamiltonian cycle problem the certificate is the actual cycle) and \"the class of languages accepted by a non-deterministic TM in polynomial time\".\n\nDeterminism on the other hand means that you are always forced to follow a single path. If you look at the formal definition for a DFA, you will see that the function delta of the current state and the current letter of the alphabet is a single state, whereas for an NFA the function delta maps a set of states and a letter of the alphabet to a set of states. At the end you accept if the set of states you are in contains an accept state. This is what is meant by \"being in multiple states\" at a given time. The definitions for TMs are slightly more complicated, but similar.\n\nHope this cleared things up for you! I appreciate the elaborate explanation. I just want to understand the fault in the explanation I presented about checking the solution space all at once.\n\nSolution space was incorrect since it assumes that all states checked are also accepting states. This was incorrect terminology.\n\nSo from my understanding of what you explained, an NTM can be constructed for a decision problem such that it will transition to all possible states to verify all possible certificates,  and will continue to transition along the paths for certificates which result in an accepting state or a YES to the problem instance.\n\nUnless I'm completely wrong, I would imagine that my understanding is still nuanced.\n Non-determinism doesn't exactly mean checking the solution space at once. What it really means is \"if there is a correct choice to make, I'll make it\". That is exactly why NP can be defined as both \"the class of languages that have a certificate that can be verified in polynomial time\" (e.g. for the Hamiltonian cycle problem the certificate is the actual cycle) and \"the class of languages accepted by a non-deterministic TM in polynomial time\".\n\nDeterminism on the other hand means that you are always forced to follow a single path. If you look at the formal definition for a DFA, you will see that the function delta of the current state and the current letter of the alphabet is a single state, whereas for an NFA the function delta maps a set of states and a letter of the alphabet to a set of states. At the end you accept if the set of states you are in contains an accept state. This is what is meant by \"being in multiple states\" at a given time. The definitions for TMs are slightly more complicated, but similar.\n\nHope this cleared things up for you! [deleted] &gt; This is why non-determinism makes NP problems \"fast\". A NTM can \"check\" every possible answer at the same time - spinning off an infinite number of threads, if you will. Since each individual \"check\" can be done in polynomial time, the overall process on a NTM is polynomial. Doing the same thing on a DTM is \"single-threaded\", which makes it slower by a factor of the number of \"threads\" that the NTM used.\n\nI haven't taken a theory class in a long time. I never quite understood how an NTM can check the solution space all at once.\n\nI recall NFAs where you can transition to multiple states in one step. Is this is what is meant by an NTM, in that you can be in multiple states of a DTM at a given time? Whereas a DTM must enumerate each state? I'm currently going through a big complexity book - it makes a big deal of not explaining P and NP through DTMs and NDTMs precisely because this bit is confusing, because NDTMs do not in fact exist and thus just introduce confusion, right at the most basic level.\n\nIt's much more interesting (imo) to look at P as being (as another poster said) the set of decision problems for which a solution can be found in polynomial time, and NP as being the set of problems for which a solution can be proved to be correct in polynomial time. (both on a standard turing machine). Obviously it gets more formal than this, but that's the gist. Uhm... You can build an NDTM in the same sense that you can build a DTM (i.e. both will end up being restricted to some finite size). You just need a lot more transistors to do it. I might be wrong, but this follow-up question is exactly the P=NP problem, isn't it? I might be wrong, but this follow-up question is exactly the P=NP problem, isn't it? Technically P=NP is only a very small subset of the question of how fast TMs are relative to NTMs... \n\nFor example, we actually know L=NL (that's logarithmic time equals non-deterministic logarithmic time) which is a pretty beautiful result.\n\nOn the other hand, it's also been proven that DTIME(n) &lt; NTIME(n). That is, there are actually problems that NTMs can solve in linear time that DTMs require *more* than linear time to solve. One example of this is the palindrome problem (accept iff input is a palindrome). Of course, this doesn't resolve P=NP, but it does show that sometimes TMs need more than a constant factor extra time than NTMs.\n\nThere's of course a whole slew more questions one could ask about this subject, very few of which have been resolved. Technically P=NP is only a very small subset of the question of how fast TMs are relative to NTMs... \n\nFor example, we actually know L=NL (that's logarithmic time equals non-deterministic logarithmic time) which is a pretty beautiful result.\n\nOn the other hand, it's also been proven that DTIME(n) &lt; NTIME(n). That is, there are actually problems that NTMs can solve in linear time that DTMs require *more* than linear time to solve. One example of this is the palindrome problem (accept iff input is a palindrome). Of course, this doesn't resolve P=NP, but it does show that sometimes TMs need more than a constant factor extra time than NTMs.\n\nThere's of course a whole slew more questions one could ask about this subject, very few of which have been resolved. Nope. For any nondeterministic TM, there is a deterministic TM that simulates it. The idea is that we can describe one computation path of an NTM by the sequence of \"choices\" it makes; so our TM can first try all possible choices for the first step, then all possible sequences of two choices, and so on. It will eventually accept if and only if the original NTM had an accepting path.\n\nAn interesting follow-up question, though, is whether TMs are as \"fast\" as NTMs.... I think oulipo covered us on that one! Slightly un-related but couldn't find on Google, do NTMs **always** contain a null transition, or can they not contain any? Or perhaps I am confusing myself with FSMs. I think oulipo covered us on that one! Slightly un-related but couldn't find on Google, do NTMs **always** contain a null transition, or can they not contain any? Or perhaps I am confusing myself with FSMs. I also want to add that converting a NTM to a TM can still be done in polynomial space (Savitch's theorem), which means that the issue is really an issue of speed, not of space.\n\nAnd even though you were confusing yourself with finite automata (and for every NFA there is a DFA that simulates it), I want to add that you can get an exponential blow-up in changing an NFA to a DFA even without epsilon transitions. Think for instance of the language that has a 0 k positions from the end. And additionally, even though the problem of minimizing DFAs can be done in polynomial time, if you were able to minimize an NFA in polynomial time, you would actually have P=NP. &gt; Think for instance of the language that has a 0 k positions from the end.\n\nDo you mean converting an NFA to a DFA which accepts the language: L = {Sigma* 0^k | k is a natural number}?\n\nI don't understand how an NFA can be constructed such that the DFA conversion (assuming Thompson's construction) will lead to exponential blow-up. Nope. For any nondeterministic TM, there is a deterministic TM that simulates it. The idea is that we can describe one computation path of an NTM by the sequence of \"choices\" it makes; so our TM can first try all possible choices for the first step, then all possible sequences of two choices, and so on. It will eventually accept if and only if the original NTM had an accepting path.\n\nAn interesting follow-up question, though, is whether TMs are as \"fast\" as NTMs.... [deleted] [deleted]  Already answered, but if you go a few steps back down the hierarchy of automata a parallel demonstration can help to convince:  every NFA can be simulated by a DFA created by a simple mechanical translation process.  Given that DTM = DFA + tape and NTM = NFA + tape, the correspondence is somewhat intuitive.   No. Any NTM O(f(n)) can be simulated by an O(fˆ2(n)) Turing Machine. Savitch's Theorem ",
    "url" : "http://www.reddit.com/r/compsci/comments/1608fx/can_ndtms_solve_problems_that_dtms_cant_solve/"
  }, {
    "id" : 19,
    "title" : "r/compsci, help me build a comprehensive list of Machine Learning survey papers",
    "snippet" : "    ",
    "url" : "http://www.mlsurveys.com/"
  }, {
    "id" : 20,
    "title" : "Books that tells the beauty of Computer Science and Mathematics to an undergrad",
    "snippet" : "  The best book by far that I have read on this theme is: [Godel, Escher, Bach](http://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach). But its tough for an undergrad :)\n Really? It's aimed at the (intelligent) general public. Started reading, but couldn't progress much :( Stay with it. It's pretty good. Metamagical Themas, which is a collection of articles he wrote when he had a column in Scientific American is also very interesting and is probably more accessible. \n\nAlso, two books that I would have expected on the list (they both came out in 2012) would be:\n1) Turing's Cathederal by George Dyson which is (like at least one of the other books on your list) about the history of computing, and the theory behind it. \n2) The Signal and the Noise: Why So Many Predictions Fail -- But Some Don't by Nate Silver. Exactly what it says on the tin. Very interesting, and has a comprehensive bibliography.  Stay with it. It's pretty good. Metamagical Themas, which is a collection of articles he wrote when he had a column in Scientific American is also very interesting and is probably more accessible. \n\nAlso, two books that I would have expected on the list (they both came out in 2012) would be:\n1) Turing's Cathederal by George Dyson which is (like at least one of the other books on your list) about the history of computing, and the theory behind it. \n2) The Signal and the Noise: Why So Many Predictions Fail -- But Some Don't by Nate Silver. Exactly what it says on the tin. Very interesting, and has a comprehensive bibliography.  Started reading, but couldn't progress much :( Really? It's aimed at the (intelligent) general public. But its tough for an undergrad :)\n But its tough for an undergrad :)\n The best book by far that I have read on this theme is: [Godel, Escher, Bach](http://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach).   [The Nature of Computation](http://www.nature-of-computation.org/) by Moore and Mertens is pretty great. heard its a good one. But not available in my place :(      ",
    "url" : "http://jestinjoy.github.com/thinkgeek/12-books-i-read-in-2012-and-found-interesting.html"
  }, {
    "id" : 21,
    "title" : "Super Resolution From a Single Image",
    "snippet" : "  Does anyone know if there already are available plug ins to use this? I'm also wondering about this, it's not the first time I've seen a paper on image enhancement with no download link :( why one of these hasn't made it into Photoshop yet baffles me. No kidding, I'm still waiting on a publicly available implementation of [this one](http://agl.unm.edu/melding/index.php). Does anyone know if there already are available plug ins to use this? Does anyone know if there already are available plug ins to use this?  I'm at work, so I haven't glossed over the paper yet. I see potential from the examples, but I do miss one simple comparison. Take a high resolution image, scale it down and scale it up again using the various methods. With this methodology you can see how it compares to the original picture, and are much better equipped to reason about the methods and the quality they produce (relative to what you're trying to achieve, assuming you want to be as faithful to the original picture as possible).  That is a deceptive comparison though. Human perception is a tricky thing. We don't necessarily see an image as \"more natural looking\" just because the delta is minimized. Certain artifacts that don't contribute much to the delta may stand out significantly to us. So optimizing for minimal delta can only be a first approximation to weed out the worst. I'm at work, so I haven't glossed over the paper yet. I see potential from the examples, but I do miss one simple comparison. Take a high resolution image, scale it down and scale it up again using the various methods. With this methodology you can see how it compares to the original picture, and are much better equipped to reason about the methods and the quality they produce (relative to what you're trying to achieve, assuming you want to be as faithful to the original picture as possible).  It seems as though that's what they did.  The example images are upscaled from downscaled 'ground truth' images at the end of the paper.  I think in the first few the \"Kim et. al\" look clearer to me, but then they got better and better and the eye chart blew me away. But Kim et.al is only in 2 shots: the first Shot #1 is definitely more natural to me whereas their SR result accentuates photo imperfections, and Shot #2 (statue) is only partial (browser bug maybe).\n\nI'm curious why haven't they compared with Kim et.al all the way through - does the algo that only works on specific facial patterns?\n\nBut quite impressive, when it will become standard in our web browsers? I think in the first few the \"Kim et. al\" look clearer to me, but then they got better and better and the eye chart blew me away.  Did anyone else say \"Enhance!\" in their head every time they clicked the blue \"Our SR Result\" button? Did anyone else say \"Enhance!\" in their head every time they clicked the blue \"Our SR Result\" button? All I was thinking was CSI Is REAL! :) All I was thinking was that we can't make fun of CSI anymore. All I was thinking was CSI Is REAL! :) But this is just replacing low resolution patches with higher ones from a database. The detail of the enlarged image can't be relied upon.  For instance, I wouldn't trust any wording that became clear once \"enhanced\". &gt; But this is just replacing low resolution patches with higher ones from a database.\n\nNot true.  Read the abstract:\n\n&gt; We further show how this combined approach can be applied to obtain super resolution from as little as a single image (with no database or prior examples).\n\nI'm not saying your main point was incorrect (that you can't trust the \"enhanced\" image), but your reason was wrong.    I'm going to question the authenticity of the eye-test image. \n\nTake a good look at the bottom line. There is just absolutely no way you can make out the letters there. The letters are about 3x4 pixels and completely unrecognizable, but after \"processing\" you end up with near-perfect letters in some cases. Sorry, I call bullshit on that one.\n\n[Screencap for the lazy](http://i.imgur.com/CQySP.png) Look at the paper. The very first illustration on the very first page explains how they do that - reoccurring patterns within the same image. So an image with repeating patterns that scale from large to small gains a lot. The large pattern is downscaled, the small scale result is searched for in the image and when found, replaced with the large pattern. Really ingenious. Look at the paper. The very first illustration on the very first page explains how they do that - reoccurring patterns within the same image. So an image with repeating patterns that scale from large to small gains a lot. The large pattern is downscaled, the small scale result is searched for in the image and when found, replaced with the large pattern. Really ingenious. Fascinating, so I guess this paves the way for using reoccurring patterns in a huge archive of HD photos?\n\nThis way details could be infered from the the most common real world occurences most of the time, a bit like what our brains are able to figure out/imagine detail by experience when looking at low-res pictures. For effective compression, this probably wouldn't be lossless. I think JPG is still more effective. This OP type compression would be really really slow, because it scans the whole picture times and times again, whereas JPG is just linear top to bottom rendering. So it's more probable that you'll just double the size of your archive drive. Don't think you're replying to correct comment. Maybe... Look at the paper. The very first illustration on the very first page explains how they do that - reoccurring patterns within the same image. So an image with repeating patterns that scale from large to small gains a lot. The large pattern is downscaled, the small scale result is searched for in the image and when found, replaced with the large pattern. Really ingenious. I'm going to question the authenticity of the eye-test image. \n\nTake a good look at the bottom line. There is just absolutely no way you can make out the letters there. The letters are about 3x4 pixels and completely unrecognizable, but after \"processing\" you end up with near-perfect letters in some cases. Sorry, I call bullshit on that one.\n\n[Screencap for the lazy](http://i.imgur.com/CQySP.png) I'm going to question the authenticity of the eye-test image. \n\nTake a good look at the bottom line. There is just absolutely no way you can make out the letters there. The letters are about 3x4 pixels and completely unrecognizable, but after \"processing\" you end up with near-perfect letters in some cases. Sorry, I call bullshit on that one.\n\n[Screencap for the lazy](http://i.imgur.com/CQySP.png) I'm going to question the authenticity of the eye-test image. \n\nTake a good look at the bottom line. There is just absolutely no way you can make out the letters there. The letters are about 3x4 pixels and completely unrecognizable, but after \"processing\" you end up with near-perfect letters in some cases. Sorry, I call bullshit on that one.\n\n[Screencap for the lazy](http://i.imgur.com/CQySP.png) I'm going to question the authenticity of the eye-test image. \n\nTake a good look at the bottom line. There is just absolutely no way you can make out the letters there. The letters are about 3x4 pixels and completely unrecognizable, but after \"processing\" you end up with near-perfect letters in some cases. Sorry, I call bullshit on that one.\n\n[Screencap for the lazy](http://i.imgur.com/CQySP.png) I'm going to question the authenticity of the eye-test image. \n\nTake a good look at the bottom line. There is just absolutely no way you can make out the letters there. The letters are about 3x4 pixels and completely unrecognizable, but after \"processing\" you end up with near-perfect letters in some cases. Sorry, I call bullshit on that one.\n\n[Screencap for the lazy](http://i.imgur.com/CQySP.png) True, if you compare the raw with the result. But compare the bicubic interpolation with their result and see that the difference is much less obvious. Also, as I understand, they use machine learning in their approach, so their program would know how a blurred out letter would look like. I'm going to question the authenticity of the eye-test image. \n\nTake a good look at the bottom line. There is just absolutely no way you can make out the letters there. The letters are about 3x4 pixels and completely unrecognizable, but after \"processing\" you end up with near-perfect letters in some cases. Sorry, I call bullshit on that one.\n\n[Screencap for the lazy](http://i.imgur.com/CQySP.png)   This approach is pretty amazing. It seems like the next step will be to apply methods like this to the entirity of a video. Frequently a video will have a much closer look at an object, so if you can do the extrapolation over time you could really crank out much higher resolutions.     The freckled girl actually looked like her face was burnt to me with their filter. Pretty good, but I never liked that kind of filter because of the watery-esque look it gives  Looks really nice! Will I be able to use it in my applications soon, i.e. will they release any algorithm? \n\nI glanced over the paper but all I've got was a bit of maths, which I'm not really good at. This paper really doesn't have that much math in it. Much of it is explained in a pretty clear way IMHO. &gt;Thus, each low-resolution pixel p = (x,y) in each\n&gt;low-resolution image Lj induces one linear constraint on\n&gt;the unknown high-resolution intensity values within the local \n&gt;neighborhood around its corresponding high-resolution\n&gt;pixel q ∈ H (the size of the neighborhood is determined by\n&gt;the support of the blur kernel Bj):\n&gt;\n&gt;Lj(p)=(H ∗Bj)(q) = Σqi∈Support(Bj)H(qi) Bj(qi−q)  (1)\n&gt;\n&gt;\n&gt;where {H(qi)} are the unknown high-resolution intensity\n&gt;value.\n\nSure thing there, buddy :)  Aaaaaand nobody will ever release an implementation of this to the public.",
    "url" : "http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html"
  }, {
    "id" : 22,
    "title" : "Image Analogies (NYU Media Research Lab)",
    "url" : "http://mrl.nyu.edu/projects/image-analogies/"
  }, {
    "id" : 23,
    "title" : "Princeton professor foresees computer science revolution (little old but still relevant)",
    "snippet" : "  ",
    "url" : "http://www.eurekalert.org/pub_releases/2006-02/pu-ppf021506.php"
  }, {
    "id" : 24,
    "title" : "Looking for a list of undergrad CS topics. Things that someone with a BS in CS should have seen at least once.",
    "snippet" : "I didn't go to school for CS (Physics major), but have been interested in CS my entire life and currently work as a software engineer. However, even though I've taken CS courses and have watched countless online lectures, I worry that may miss out on some fundamental material (like FSMs or Big O notation) just because I've never heard of it. I plan on going to graduate school for CS and want to have a solid foundation going in. I'm looking for things like \"Topics every CS major should know\" or \"Typical Undergrad CS Curriculum\". I've tried looking at the websites of local universities, but their course descriptions are often very vague.  \n\nEdit: I thought I should clarify what I mean why \"I've taken CS courses\". \n\nI've taken Data Structures, \n\nC Programming, \n\nObject-Oriented Programming with C++, \n\nUnix Administration and Programming, \n\nProgramming Languages, \n\nCryptography, \n\nDatabases (Relational Algebra and Oracle Administration), \n\nComputer Architecture, \n\nDiscrete Math,\n\nDesign and Implementation of Algorithms.\n\nEdit2: Thanks to everyone for their responses.  Just some that come to my mind:\n\n* Discrete mathematics, in particular graph theory\n* Fundamentals of algebra\n* a bit of number theory, fermat's little theorem, factorization problem, discrete logarithm problem, primality testing\n* Algorithmics: fundamental data structures (binary search trees, hash tables, priority heap), sorting algorithms, graph algorithms (dfs, bfs, dijkstra shortest path, A*, prim's, kruskal's, bellman-ford, floyd-warshall, topological sort, edmonds–karp)\n* Programming languages paradigms (imperative, functional, logic, object-oriented)\n* Compiler construction, parsing\n* Type systems, lambda calculus, curry-howard isomorphism\n* First Order Logic, inference\n* Formal languages and automata\n* Halting problem, turing-completeness, rice theorem, time/space hierarchy theorems, church-turing thesis\n* Complexity theory: asymptotic analysis, complexity classes\n* Information theory and cryptography: entropy, huffman coding, error detection, public key cryptography, symmetric key cryptography, hashes\n* Data base theory: relational algebra, SQL\n* Computer architecture and operating systems: von neumann architecture, memory hierarchy, processes/memory managements, file systems, I/O handling\n* Networks and protocols\n* artificial intelligence: local search, heuristics, answer set programming, planning, machine learning, bayesian networks\n\nI am sure I missing many, many important subjects :)\n\nEdit: grammar This is my favorite list so far.\n\nThe one thing I would probably add is some basic set theory. It seems to come up *everywhere*, and is very useful for getting the right mindset for working with more abstract ideas and thinking mathematically.\n\nA whole bunch of your suggested topics do overlap with set theory, so learning them will probably give you the fundamentals, but I think it's healthy to cover set theory by itself as well. Just some that come to my mind:\n\n* Discrete mathematics, in particular graph theory\n* Fundamentals of algebra\n* a bit of number theory, fermat's little theorem, factorization problem, discrete logarithm problem, primality testing\n* Algorithmics: fundamental data structures (binary search trees, hash tables, priority heap), sorting algorithms, graph algorithms (dfs, bfs, dijkstra shortest path, A*, prim's, kruskal's, bellman-ford, floyd-warshall, topological sort, edmonds–karp)\n* Programming languages paradigms (imperative, functional, logic, object-oriented)\n* Compiler construction, parsing\n* Type systems, lambda calculus, curry-howard isomorphism\n* First Order Logic, inference\n* Formal languages and automata\n* Halting problem, turing-completeness, rice theorem, time/space hierarchy theorems, church-turing thesis\n* Complexity theory: asymptotic analysis, complexity classes\n* Information theory and cryptography: entropy, huffman coding, error detection, public key cryptography, symmetric key cryptography, hashes\n* Data base theory: relational algebra, SQL\n* Computer architecture and operating systems: von neumann architecture, memory hierarchy, processes/memory managements, file systems, I/O handling\n* Networks and protocols\n* artificial intelligence: local search, heuristics, answer set programming, planning, machine learning, bayesian networks\n\nI am sure I missing many, many important subjects :)\n\nEdit: grammar Allow me to suggest some relevant stuff:\n\n* Euclidean/Manhattan/[Others?] Distance\n* Geometric properties\n* Convex hull\n* Sweep line\n* Simulation\n* Game theory  [A good list that usually pops up] (http://matt.might.net/articles/what-cs-majors-should-know/)  Off the top of my head:\n\n'real' CS: Formal languages, automata, core theoretical CS (halting problem, computability, stuff like that), algorithm design and analysis\n\nDiscrete math: Combinatorics, graph theory, logic\n\nmore math: Probability, calculus, linear algebra\n\nSystems: networks, OS concepts, computer architecture\n\nsome applications: software engineering, machine learning, computer vision, robotics, graphics, HCI, NLP, programming languages (enough to write a simple compiler) +1 for including linear algebra.  I believe that's the most overlooked math subject when it comes to computer science especially given its wide array of applications. Really? I have never seen a CS program skipping it, and most would agree on it's importance. +1 for including linear algebra.  I believe that's the most overlooked math subject when it comes to computer science especially given its wide array of applications.  Though it goes into an absurd amount of depth, [this ACM document](http://www.acm.org/education/education/education/curric_vols/cc2001.pdf) specifies a complete curriculum for an undergraduate education in computer science. I found it very informative when I was trying to make a list of topics I should know before I graduate!  \n  \n(To cut to the chase, look at page 17 in the document/page 21 in the PDF. That page outlines the \"computer science body of knowledge\" and highlights core topics in the subject.) Though it goes into an absurd amount of depth, [this ACM document](http://www.acm.org/education/education/education/curric_vols/cc2001.pdf) specifies a complete curriculum for an undergraduate education in computer science. I found it very informative when I was trying to make a list of topics I should know before I graduate!  \n  \n(To cut to the chase, look at page 17 in the document/page 21 in the PDF. That page outlines the \"computer science body of knowledge\" and highlights core topics in the subject.)      regular expressions?    Since no one has mentioned it, concurrency. Many problems are embarrassingly parallelizable.",
    "url" : "http://www.reddit.com/r/compsci/comments/15ukbb/looking_for_a_list_of_undergrad_cs_topics_things/"
  }, {
    "id" : 25,
    "title" : "Type-Directed Automatic Incrementalization",
    "snippet" : " ",
    "url" : "http://www.cs.cmu.edu/~joshuad/papers/incr/"
  }, {
    "id" : 26,
    "title" : "The om programming language: based on a minimalist philosophy.",
    "url" : "http://sparist.github.com/Om/"
  }, {
    "id" : 27,
    "title" : "John McCarthy, Father of AI",
    "snippet" : "  cool talk but awkward conversation.  Their knees are touching.  John McCarthy looks like colonel sanders in a good way.    ",
    "url" : "https://www.youtube.com/watch?v=Ozipf13jRr4"
  }, {
    "id" : 28,
    "title" : "I want to compare two audio samples of speech and score how similar they are",
    "snippet" : "Hello, \n\nI am working on an application where I have the user say a phrase, I record it as a wav file, then I compare it to a reference audio sample they were trying to repeat. I would love to get some advice on algorithms better algorithms I can use to do this, I'm sure this is at least a somewhat solved problem and so I can \"stand on the shoulders of giants\" rather than try and reinvent the wheel. \n\nBelow is my current primitive scheme that in practice has shown a weak, often times random correlation between the sample and reference speech signals.\n\n---\n\nThe wav file is sampled at some kHz rate in 16-bit PCM data format. So I take the FFT of both the reference and the sample. Then I try and find out how \"similar\" the shapes of the FFTs are by splitting up the sample and reference into some fixed number of regions and comparing the total power (i.e. x[n]^2) per region b/w sample and reference. The differences in power per region give me an error vector, and I take its magnitude and do appropriate scaling to give a score out of 100. \n\nI should note that for the signals I scale them appropriately by doing x[n] = x[n]/max(x[n]) and I make sure the FFTs are of the same length (if one sample is longer, I make the longer length the official length and pad the shorter one with zeros).\n\nThe reason I split the x[n] signals (with N samples) into a number of regions (say, 32) is because I notice that while the shapes of the FFT graphs are similar, their exact spacing is not. People speak in different \"octaves\" or whatever it's called when they hit the correct pitch but up or down the scale than the reference signal. I don't want that to be an error, so I allow for some shifting in frequency to not affect the score - the leeway created by the 32 regions. Also, when comparing the differences, I allow region i to compare to region i-1 and i+1 and pick the lowest possible difference score. \n\n---\n\nLike I said, my current algorithm kind of sucks, but does show weak correlation between saying the phrase similar to the reference. I know there must be some good way to do this because when I open the audio samples in Matlab there is always a good similarity in FFT waveform shapes between the reference and the sample. \n\nSpeaking of which, here's a pic: http://i.imgur.com/nae2u.png (reference on left, sample on right)\n\nLooking forward to your feedback, cheers.  You want to difference the MFCC's: http://en.wikipedia.org/wiki/Mel-frequency_cepstrum\n\nThat way you'll achieve more amplitude/tonal invariance than any naive method. It's the current state-of-the-art in a lot of audio processing. Just out of curiosity, how does this handle one phrase being said more slowly than the other or with a break between words, for example? Just out of curiosity, how does this handle one phrase being said more slowly than the other or with a break between words, for example? In addition to artoonie's good answers, one should read up on [dynamic time warping](http://en.wikipedia.org/wiki/Dynamic_time_warping). In addition to artoonie's good answers, one should read up on [dynamic time warping](http://en.wikipedia.org/wiki/Dynamic_time_warping). You want to difference the MFCC's: http://en.wikipedia.org/wiki/Mel-frequency_cepstrum\n\nThat way you'll achieve more amplitude/tonal invariance than any naive method. It's the current state-of-the-art in a lot of audio processing.  Have you looked into echonest.com?  I have no good ideas but am really interested to hear how you solve the problem. Would you report back? will do.          ",
    "url" : "http://www.reddit.com/r/compsci/comments/15s6zj/i_want_to_compare_two_audio_samples_of_speech_and/"
  }, {
    "id" : 29,
    "title" : "Question about resolution enhancement methods.",
    "snippet" : "What are the options for increasing the resolution of a low-res image sensor? \n\nI recently [put together some hardware](http://www.reddit.com/r/ECE/comments/15r7rw/remember_that_16x4_far_infrared_sensor_from_last/) to read a 16x4 pixel infrared heat sensor, but of course the resolution is extremely low.  \n\nI've considered stitching together multiple exposures, but the sensor already has a very wide field of view (60 degrees), so that doesn't really work.  Optics could narrow the FOV, but those would be very expensive (you can't use regular glass for far-infrared).\n\nI am aware of linear interpolation, which would give me a smoother display but would not actually provide any more information. \n\nWhat I would like to know, is if I can get more information (more pixels) by physically shifting the sensor 1/2 pixel and taking another exposure.  Intuitively, I think this wouldn't work, but I don't really know.\n\nIf you guys have any pointers to papers or tips on resolution enhancement, I've love to see them.  \n\nI'm thinking that some kind of non-linear interpolation (looking at the second derivative of the curve of 3 pixels, that is, concavity) would produce a better interpolation, but I'm not sure it's worth the effort (and processing power).\n\nThanks!\n  The concept is [superresolution](http://www.photoacute.com/tech/superresolution_faq.html).  I think it should work fine, but you'll want to make sure that the field of view doesn't overlap too much and that you don't take multiple measurements within the time it takes to re-read accurately. \n\nAs a test, I'd probably stick with just one sensor and have it scan around like an old CRT beam and calibrate spacing/timings for it to work well, then move up to using multiple sensors.    &gt; 16x4 pixel infrared heat sensor, but of course the resolution is extremely low. \n\nThis low resolution brings to mind the [single-pixel camera](http://dsp.rice.edu/cscamera). That didn't make much sense for light cameras given the availability of bazillions of megapixels in cheap cameras, but maybe it would for something like infrared.",
    "url" : "http://www.reddit.com/r/compsci/comments/15s4p4/question_about_resolution_enhancement_methods/"
  }, {
    "id" : 30,
    "title" : "Turing Drawings « Pointers Gone Wild",
    "snippet" : "     Seems to work better in chrome than safari.\n\nHere are some green waves: \nhttp://maximecb.github.com/Turing-Drawings/#6,4,4,3,0,4,1,2,3,3,2,1,3,2,3,1,1,5,2,2,1,3,3,1,1,3,3,1,2,1,2,2,3,2,2,5,1,2,1,2,2,1,3,1,1,1,2,4,2,1,2,2,1,4,1,1,1,2,2,2,3,3,0,1,0,3,1,2,5,3,2,4,1,3   It's been a good long while since I was excited about CS art. Thank you for this. My pleasure. I like to code up little experiments every once in a while, for the fun of it, to see what happens.\n\nAny CS art links you'd like to share? My pleasure. I like to code up little experiments every once in a while, for the fun of it, to see what happens.\n\nAny CS art links you'd like to share? My pleasure. I like to code up little experiments every once in a while, for the fun of it, to see what happens.\n\nAny CS art links you'd like to share? My pleasure. I like to code up little experiments every once in a while, for the fun of it, to see what happens.\n\nAny CS art links you'd like to share? I feel the need to write some code to search the randomly produced machines for output of different characteristics... have you considered expanding this to do such a thing?  For instance, I wanted to see something more colorful... but it seems that a high proportion of the animations produced with a larger number of symbols result in mostly random-looking noise.  It wouldn't be hard to take a measure of the entropy of a frame and reject any over a certain threshold in order to find more 'substantive' images/animations. I considered adding a filter to automatically reject machines with certain properties (e.g.: getting stuck in a loop early).  I think the more symbols and states you allow, the more complex the behavior of the machines tends to get, which makes it seem random more often than not. How would you measure the entropy of a frame?  Gah. I've got one that looks like snow but I can't select the shareable URL. :(\n\nEDIT: I was able to drag and drop the URL into a different browser. [Snow](http://maximecb.github.com/Turing-Drawings/#4,3,1,2,0,3,1,0,1,2,2,2,1,3,1,2,1,0,2,3,2,1,1,1,1,3,1,1,1,1,2,1,1,2,3,3,2,2) Which browser do you use? Firefox. Should be fixed now. Try to refresh the page and let me know if it works. Don't know about Fenyx, but the shareable URL text field doesn't work for me with Firefox 17 on Linux. I changed the field from \"disabled\" to \"readonly\" and that fixed it in my Firefox. Can you not select the text in the field? Did you refresh the page (if you visited it several hours ago)? Should be fixed now. Try to refresh the page and let me know if it works.   Freaking awesome.\n\n[awesome thing 1](http://maximecb.github.com/Turing-Drawings/#4,3,0,2,2,2,1,2,3,1,1,3,2,1,3,1,0,2,2,2,1,1,0,2,2,2,1,1,0,1,2,3,0,1,2,3,1,3)\n\n[awesome thing 2](http://maximecb.github.com/Turing-Drawings/#2,5,1,1,0,0,1,0,0,3,1,0,3,0,1,4,2,1,4,2,1,4,3,0,1,1,1,3,0,1,2,0)\n\n[green flashing thing](http://maximecb.github.com/Turing-Drawings/#4,4,1,1,0,3,2,1,1,1,3,0,3,0,3,3,1,0,1,3,2,1,3,3,3,0,0,2,0,1,3,1,3,3,1,3,2,0,1,2,1,1,2,3,0,1,3,2,3,0)\n\n[creeping black and white waves](http://maximecb.github.com/Turing-Drawings/#4,3,0,1,0,0,2,0,1,2,2,0,2,1,2,1,1,2,2,3,3,2,2,2,1,1,3,1,3,0,2,2,2,1,0,2,2,3)\n\n[I'm wasting so much time](http://maximecb.github.com/Turing-Drawings/#4,3,3,1,1,1,2,2,2,2,0,1,1,1,3,2,0,0,2,0,2,2,1,3,2,1,0,1,1,0,2,1,2,2,1,0,2,3) http://maximecb.github.com/Turing-Drawings/#6,6,1,1,1,5,3,0,1,2,0,1,3,0,5,4,0,2,3,0,0,3,1,4,1,1,0,1,0,1,2,3,1,4,0,2,5,2,0,3,2,3,3,2,5,5,1,5,4,1,5,2,0,0,3,2,0,1,2,3,5,3,2,2,2,5,5,1,5,2,3,0,2,0,3,4,1,2,5,0,4,1,1,4,5,1,0,5,3,0,2,2,0,1,1,2,3,1,2,2,1,5,5,1,0,1,0,3,4,3\n\nhttp://maximecb.github.com/Turing-Drawings/#7,4,3,2,1,1,1,2,3,3,2,1,1,0,1,2,2,2,2,2,2,2,3,6,3,1,0,2,0,6,3,2,0,1,0,0,2,1,3,1,1,1,2,0,0,3,3,2,2,3,2,1,0,0,1,1,6,2,3,0,2,3,4,1,3,3,2,0,3,3,0,6,2,0,1,2,2,6,1,0,4,3,1,2,3,2\n\nhttp://maximecb.github.com/Turing-Drawings/#7,4,4,1,3,3,2,1,5,2,2,3,1,2,6,1,2,2,3,3,1,3,2,5,1,1,1,2,3,3,2,0,5,1,0,2,1,3,6,2,3,6,1,1,6,2,2,3,1,0,6,3,1,2,1,1,0,1,3,3,1,0,3,1,0,1,3,1,3,1,1,1,2,0,1,1,1,4,2,0,1,1,0,2,3,1\n\nhttp://maximecb.github.com/Turing-Drawings/#5,4,1,3,1,0,1,1,3,3,1,3,3,3,3,1,1,4,1,3,2,2,1,2,1,3,2,1,0,0,2,3,1,2,2,2,2,3,1,1,2,0,2,3,4,3,3,1,1,2,4,3,0,0,1,2,3,2,2,2,3,1 [Spikey Flowing Thing](http://maximecb.github.com/Turing-Drawings/#4,3,2,2,1,1,1,3,3,2,2,1,2,0,0,1,0,3,1,0,2,2,2,0,2,0,1,1,2,0,2,0,1,2,1,3,2,1)  I want to put this on a raspberry behind a framed, square monochrome OLED display, and hang it on the wall to iterate forever and ever.  I want to put this on a raspberry behind a framed, square monochrome OLED display, and hang it on the wall to iterate forever and ever.  Heh, similar was the reason I got a raspberry pi.  Not for this specifically obviously, but because I often play around with different kinds of generative art and I wanted to be able to build a thing such as you describe which displays the different little graphical apps I've done.  It's probably entirely overkill but I've been intending to write an absolutely-bare-bones OS for the purpose... good excuse to learn about OS stuff and ARM assembly.   Very nice content and cool idea, though the insanely dismissive first paragraph really bothered me. \"[Turing Machines] They’re a very useful tool for computer science proofs... Beyond this, however, Turing machines seem relatively useless in terms of “real-world” applications...\"\n\nOh come on, any CS professional will agree that it is important to study and really understand FSM, DFS, etc. I wrote my own compiler and knowing this background information dramatically helped with development. Knowing how to use a turing machine and what it represents helps enormously with software design and algorithmic complexity measurements. These are two of countless examples.  He's right, they are useless in *application*. Nobody out there make chips of little turing machines :P I believe the author is a \"she\", but not sure. Regardless, I respectfully disagree :-) There are tons of applications of turing machines in code. Not in the literal sense (i.e. tape and a read/write head virtualization), but as equivalent state machines, such as virtual-machines (for languages). Very nice content and cool idea, though the insanely dismissive first paragraph really bothered me. \"[Turing Machines] They’re a very useful tool for computer science proofs... Beyond this, however, Turing machines seem relatively useless in terms of “real-world” applications...\"\n\nOh come on, any CS professional will agree that it is important to study and really understand FSM, DFS, etc. I wrote my own compiler and knowing this background information dramatically helped with development. Knowing how to use a turing machine and what it represents helps enormously with software design and algorithmic complexity measurements. These are two of countless examples.   Anyone care to explain what's going on here? I'm a second year Computer Engineering student, so I know some things, but this is lost on me. It's a Turing machine with a finite 2D tape instead of a linear one. Each cell of the tape corresponds to a pixel in the canvas. As it runs, it produces something we can interpret as a 2D animation (the symbols written are encoded as pre-selected colors). How do you initialize the tape? How do you choose the states? Do you just generate all possible states and select them randomly? Do you sanitize the selection (e.g. first state can't be a halting state, no halting states...)?\n\nI'm going to dig through the code, these results are amazing.\n\nI'd wish I could let it go slower. I understand it, but I can't really see how it's progressing.  The grid is initialized with all symbol 0 (red color). There are no halting states, as I thought having halting states would cause too many machines to halt very early. The machines technically can't write symbol 0, only read it. This constraint is not necessary, but it makes it possible to see where the machine has written. Thanks for the explanation. That's interesting.\n\nI've pulled the code and made it go slower. The images which look like static, or which seem to move as a whole look rather different at a slower pace. It's cool to see how they get their moves.    This reminds me of [Life](http://en.wikipedia.org/wiki/Conway's_Game_of_Life).  Can it be modeled as a cellular automaton?  One that may be the [Sierpinsky thing](http://maximecb.github.com/Turing-Drawings/#4,3,2,2,0,1,2,2,3,1,1,3,2,2,0,1,3,2,1,1,3,1,1,3,2,1,0,1,2,3,2,3,0,2,0,1,2,0) the author mentions. Very similar, thanks for posting. Someone else found [this one](http://maximecb.github.com/Turing-Drawings/#6,4,5,2,3,2,2,3,1,3,2,2,2,3,4,1,2,3,3,0,5,1,2,1,3,2,5,3,0,0,2,3,3,1,3,3,3,3,4,1,2,3,1,1,2,3,3,3,3,0,5,3,1,2,2,3,1,2,0,5,3,1,3,3,0,2,2,1,1,2,2,3,3,0). I like yours better :D Oh, haha, and thanks for posting a link to my Wang Tile project on your twitter :) One that may be the [Sierpinsky thing](http://maximecb.github.com/Turing-Drawings/#4,3,2,2,0,1,2,2,3,1,1,3,2,2,0,1,3,2,1,1,3,1,1,3,2,1,0,1,2,3,2,3,0,2,0,1,2,0) the author mentions. Another [Sierpiński triangle](http://maximecb.github.com/Turing-Drawings/#5,4,4,2,1,1,3,2,4,3,1,2,2,3,1,2,1,3,2,0,2,2,3,2,3,0,2,3,2,4,2,2,0,2,0,1,1,0,2,3,0,1,2,1,2,3,3,3,2,0,1,1,3,2,2,0,2,2,3,3,2,0) (only 2 visible colors/symbols)       [Rain on a window](http://maximecb.github.com/Turing-Drawings/#4,3,3,1,0,2,2,3,2,1,3,1,1,1,2,1,3,0,2,0,3,2,2,2,1,3,2,1,0,0,2,1,1,1,3,0,1,2)\n\nAbsolutely mesmerized by this one.    Could anyone explain why there's such a high occurrence of programs that produce pictures that somehow use the diagonal, and those that just fill the canvas with random noise, endlessly? The Turing machine can only read/write one pixel at a time. It can also only move left/right/up/down one pixel at each time step. The limited movement directions makes it fairly likely that it will move in shapes that are horizontal/vertical/diagonal.\n\nNoise is essentially random pixels. The machines are generated at random. If they move/read/write in sufficiently unpredictable ways, you won't be able to see a pattern in what they produce, and it will look like noise to you. If you did a rigorous statistical analysis on the \"noise\" though, you would probably find it's not \"good quality\" randomness, it has some patterns in it, because the machine producing it obeys fixed rules.\n\nSince the machines are randomly generated, it makes sense that most of them wouldn't produce interesting patterns. I'm actually quite pleased with how many of them do produce visually interesting things though! Oh, that makes sense, thanks for responding :) Is there any way I could go about understanding the programs that get randomly generated?     [Traveling Grass](http://maximecb.github.com/Turing-Drawings/#2,5,1,2,2,0,2,1,0,4,1,1,4,0,0,1,1,0,3,2,1,3,3,0,2,1,1,3,0,0,1,3)\n\n[Computation](http://maximecb.github.com/Turing-Drawings/#2,5,1,1,2,1,3,0,1,4,2,1,2,1,1,4,2,0,2,0,0,2,2,0,3,1,1,2,3,0,3,2)\n\n[Creating the Oceans](http://maximecb.github.com/Turing-Drawings/#3,6,0,1,0,2,5,1,2,5,2,1,5,2,0,4,2,0,4,0,2,2,0,1,1,1,2,4,1,2,2,0,1,3,3,0,5,3,0,1,3,0,1,3,0,3,2,0,4,0,1,4,2,1,2,1)\n\n[Packaging](http://maximecb.github.com/Turing-Drawings/#3,6,0,3,3,1,3,0,2,4,3,2,1,3,1,1,3,2,2,2,1,4,3,0,4,3,2,3,2,2,4,1,2,2,1,2,2,0,2,5,1,2,4,1,0,1,3,1,4,2,0,4,1,1,3,3)\n\n[Rapids](http://maximecb.github.com/Turing-Drawings/#3,6,2,2,3,2,4,0,0,1,0,2,1,2,1,1,0,1,2,3,2,3,0,2,1,0,2,5,3,2,5,2,2,4,1,1,5,0,2,4,3,0,4,0,0,1,1,2,1,3,2,1,0,2,2,0)\n\n[Factories](http://maximecb.github.com/Turing-Drawings/#3,8,0,5,2,2,1,1,2,6,2,0,5,0,0,5,2,1,5,1,2,7,0,1,7,1,0,4,0,1,6,3,1,7,3,0,6,3,2,2,3,1,6,1,2,2,2,2,4,2,1,5,1,1,2,1,1,5,3,2,5,2,2,5,2,1,7,2,2,4,1,2,4,2)   Too cool\n[1](http://maximecb.github.com/Turing-Drawings/#4,4,3,2,3,2,1,1,1,3,2,2,1,0,1,3,3,1,1,0,1,3,3,3,2,1,0,2,0,0,1,1,1,3,2,1,3,3,0,1,2,0,1,1,2,1,0,0,1,1)\n[2](http://maximecb.github.com/Turing-Drawings/#2,3,1,1,1,0,1,3,0,2,0,0,2,0,1,1,2,1,2,1)\n[3](http://maximecb.github.com/Turing-Drawings/#2,7,0,2,0,0,6,3,0,5,2,0,6,3,0,4,1,0,3,0,0,6,3,0,1,2,0,6,0,0,4,3,1,5,0,0,2,1,0,1,2,0,5,0)\n[4](http://maximecb.github.com/Turing-Drawings/#2,6,0,4,0,0,5,2,1,5,3,0,1,2,0,2,3,1,4,2,0,3,1,1,1,2,1,3,0,1,4,1,0,4,0,1,4,2)\n[5](http://maximecb.github.com/Turing-Drawings/#3,6,0,2,3,0,3,0,0,1,2,2,1,0,0,1,0,2,5,3,2,2,2,2,5,1,0,3,1,0,5,2,0,4,2,2,5,1,1,4,2,2,4,2,1,1,2,1,3,2,0,4,1,1,5,1)\n[6](http://maximecb.github.com/Turing-Drawings/#2,3,1,2,2,0,1,1,1,1,1,1,2,2,0,2,3,1,2,2)\n[7](http://maximecb.github.com/Turing-Drawings/#3,3,0,2,2,2,1,0,2,2,2,1,1,3,0,1,3,1,2,0,1,1,0,0,2,3,1,2,3)\n[8](http://maximecb.github.com/Turing-Drawings/#7,3,3,2,1,1,1,1,0,1,1,5,2,2,3,1,2,4,2,3,2,1,0,2,2,1,5,1,1,3,1,0,4,2,2,1,1,3,5,1,2,5,1,1,3,2,2,4,2,1,3,2,0,1,2,3,0,2,2,0,1,0,3,1,2)\n[9](http://maximecb.github.com/Turing-Drawings/#)  [deleted]    It be used for image compression, just find the mapping from image to TM?    ",
    "url" : "http://pointersgonewild.wordpress.com/2012/12/31/turing-drawings/"
  }, {
    "id" : 31,
    "title" : "Probabilistic Data Structures, Part I - Bloom Filters",
    "url" : "http://billlaboon.com/probabilistic-data-structures-part-i-bloom-filters/"
  }, {
    "id" : 32,
    "title" : "Ideas for ensuring synchronicity across multiple hosts doing parts of the same repetitive task?",
    "snippet" : "Assume I have four hosts, and a set of biological stats which are recorded regularly (heart rate, blood pressure etc.). Every five minutes the hosts need to check certain subsets of the total stats and, if there are high or low values, trigger an alarm.\n\nHow do I ensure that all four hosts:\n\n1. Manage to run all the alarm checks every five minutes between all four of them. (n alarms must be completed every 5 minutes)\n2. Avoid check repeats for the same time slot (for example, host 1 and host 2 should not run the same check for the same time slot; however, host 1 may do time slot x, and host 2 can do x+1).\n3. If a host should fail or die or take too long, there should be a process to redistribute the alarms to the other hosts to avoid metric gaps.\n\nI'm currently using a MySQL database to store the biological metrics. Any ideas how to ensure the above?\n\nThanks in advance  Wouldn't using a message queue solve this easily? You could of course use DB tables to create a message queue-like system yourself, assuming you're using MySQL with InnoDB or another ACID-compliant MySQL storage engine.\n\nBasically, your queue will distribute the different tasks. The four hosts will each register which task they're working on, and the time they start working on it. When a host is done with its task, it asks for a new task -- either one that hasn't been assigned yet, or that's been assigned to a host already, but too long ago. (A message queue generally takes care of this rescheduling of failed tasks.)\n\nIf you don't want to use an extra system, you could, as I said above, do this yourself with DB tables, or code such a system. This shouldn't be too hard to code I think, you just need to make sure there's no mistakes in your synchronization/locking.\n\n\nOr am I missing something? Thanks: this is very similar to how I've implemented it:\n\n1. The host updates a \"last-seen\" value in the host table.\n2. The host takes a portion of the tasks (dependant on number of alarms / number of recently seen hosts) which have not been processed in the last 5 minutes, and puts these alarms in a \"working\" table.\n3. The host checks these alarms, updates them and sets the \"last-processed\" value to the current time.\n4. The host removes its own alarms from the \"working\" table, as well as clearing any old alarms (thus allowing other hosts to take them up).\n\nI think this should work. Thank you. If this project has end users or will be used in any kind of production environment then I strongly suggest you just grab any off-the-shelf message queue solution.     \n\nIf this is just for research/learning then more power to you.       Why can't you check the same time slot twice? That makes it more complicated and I don't see the purpose.\n\nWhere is the SQL server? Does each host have its own db, does the server run on one of the four hosts, or is their a fifth machine involved? Because, the alarm service I use doesn't deal with duplicates, so if two hosts alarm, it'll come through as double the metric count which starts making unsense.\n\nIt's a hosted SQL server, which the four machines link to. There is a fifth machine which sends the metrics through to the server, but it's not really relevant at the moment. I wouldn't try to synchronize the checking, since that will add extra complexity, and bugs could cause the alarm to not be triggered. Just synchronize the alarm.\n\nYou can synchronize it by sending packets to the other three hosts, telling them you want to send the alarm. Then wait for the responses - there are four possible responses:\n\n* OK, go ahead (this will happen 99% of the time)\n* don't do that, I already sent the alarm\n* network error, could not send to host (equivalent to OK)\n* no response (will have to wait for timeout, but this case is extremely rare)\n\nAfter all other hosts have responded, or after a timeout period, stop waiting for responses. If none of the responses were \"don't do that\", then signal the alarm. I wouldn't try to synchronize the checking, since that will add extra complexity, and bugs could cause the alarm to not be triggered. Just synchronize the alarm.\n\nYou can synchronize it by sending packets to the other three hosts, telling them you want to send the alarm. Then wait for the responses - there are four possible responses:\n\n* OK, go ahead (this will happen 99% of the time)\n* don't do that, I already sent the alarm\n* network error, could not send to host (equivalent to OK)\n* no response (will have to wait for timeout, but this case is extremely rare)\n\nAfter all other hosts have responded, or after a timeout period, stop waiting for responses. If none of the responses were \"don't do that\", then signal the alarm. Because, the alarm service I use doesn't deal with duplicates, so if two hosts alarm, it'll come through as double the metric count which starts making unsense.\n\nIt's a hosted SQL server, which the four machines link to. There is a fifth machine which sends the metrics through to the server, but it's not really relevant at the moment.    ",
    "url" : "http://www.reddit.com/r/compsci/comments/15pq0z/ideas_for_ensuring_synchronicity_across_multiple/"
  }, {
    "id" : 33,
    "title" : "Efficient non-linear SVM learning with approximate kernels",
    "snippet" : "  I'm wondering if the Nyström approximation run-time is always 'significantly' less than the run-time of the vanilla RBF kernel. I'm currently working on ~10 million rows of training data so these techniques might help out. I'm wondering if the Nyström approximation run-time is always 'significantly' less than the run-time of the vanilla RBF kernel. I'm currently working on ~10 million rows of training data so these techniques might help out. ",
    "url" : "http://peekaboo-vision.blogspot.com/2012/12/kernel-approximations-for-efficient.html"
  }, {
    "id" : 34,
    "title" : "Spanner: Google's Globally-Distributed Database",
    "snippet" : " ",
    "url" : "http://research.google.com/archive/spanner.html"
  }, {
    "id" : 35,
    "title" : "What blogs related to computer science do you read?",
    "snippet" : "I'm interested in blogs that are related to computer science. So what do you read?         ",
    "url" : "http://www.reddit.com/r/compsci/comments/15n6i2/what_blogs_related_to_computer_science_do_you_read/"
  }, {
    "id" : 36,
    "title" : "Improving neural networks by preventing co-adaptation of feature\n  detectors",
    "snippet" : " ",
    "url" : "http://arxiv.org/abs/1207.0580"
  }, {
    "id" : 37,
    "title" : "A Short Introduction to Operating Systems",
    "snippet" : "  If you want a radically different OS.  http://www.sparrowos.com\n\n I've followed your work for quite some time. Most people talk shit about your OS. Let me be one of the first to say that I think it is awesome. I'm a programmer working on an Electrical Engineering degree and your OS is inspiring. Let's see the OS of those who mock you..  Wow thanks! I have a really nice free book in pdf form on operating systems and middleware. Pity Reddit doesn't allow you to attach files. We always have the [_data_](https://en.wikipedia.org/wiki/Data_URI_scheme) URI scheme. ;-)",
    "url" : "http://www.iu.hio.no/~mark/os/os.html"
  }, {
    "id" : 38,
    "title" : "A list of Alan Turing's major contributions to computer science",
    "snippet" : "  And who knows what else could have been on that list if not for his untimely death.   I take issue with the language in #4.\n\nParticularly as I am a philosophy nerd. I take issue with the language in #4.\n\nParticularly as I am a philosophy nerd. I meant that it avoids certain philosophical arguments -- it certainly raises others :) ",
    "url" : "http://levreyzin.blogspot.com/2012/12/turings-revolution.html"
  }, {
    "id" : 39,
    "title" : "Automatic Complexity Analysis [pdf]",
    "url" : "http://www2.imm.dtu.dk/~fnie/Papers/NNS02_AA.pdf"
  }, {
    "id" : 40,
    "title" : "Which NP-complete problems are actually easy?",
    "snippet" : "Some NP-complete problems seem to resist good approximate solutions, whereas others have strong polynomial-time approximation algorithms. What differentiates an 'easy' from a 'hard' NP-complete problem? Also, are the approximable problems always the ones we can solve quickly in practice? I know certain 3SAT solvers have started scaling to millions of variables--- is this sort of 'in practice' tractability possible for all NP-complete problems?   I think your first question is \"Which NP-complete problems can be efficiently approximated within a small distance from optimal solutions?\" to which I don't know the answer (hopefully someone else here does). I know however that some of the most central NP-complete problems (among them the optimization version of 3-SAT) have strong inapproximability results presented here http://www.nada.kth.se/~johanh/optimalinap.pdf\n\nFor your second question the answer is that there are actually several known families of CNF-formulas that are very hard for any current SAT-solver. One of them is a straight-forward formula that encodes the pigeon hole principle for n holes and n+1 pigeons. One notion of an \"easier\" NP-complete problem is if a PTAS (polynomial time approximation scheme) exists for it. This basically means that for all small eps &gt; 0, we can get within a factor of (1+eps) to the optimal solution in time polynomial in n and constant eps. An example of this is Euclidean TSP.\n\nFor an even stronger notion,  we have FPTAS (fully PTAS), which is much more useful because we have the same optimality bound, but the running time is in poly(n, 1/eps). An example of this is the knapsack problem. These are much harder to come by.\n\nIn practice though, I think the fastest solvers for geometric TSP, SAT, etc. use a combination of some (modified for the domain) max-finding algorithm with lots of heuristics and just go with it without worrying about whether it's the optimal.  I think your first question is \"Which NP-complete problems can be efficiently approximated within a small distance from optimal solutions?\" to which I don't know the answer (hopefully someone else here does). I know however that some of the most central NP-complete problems (among them the optimization version of 3-SAT) have strong inapproximability results presented here http://www.nada.kth.se/~johanh/optimalinap.pdf\n\nFor your second question the answer is that there are actually several known families of CNF-formulas that are very hard for any current SAT-solver. One of them is a straight-forward formula that encodes the pigeon hole principle for n holes and n+1 pigeons. While there might be known families of CNF-formulas that are hard to solve for current SAT-solvers, I think it's still fair to say that SAT is being solved quite well in practice, since current algorithms scale very well for a lot of industrial applications . Your reply \"the answer is\" is very dependent on perspective, and arguably not the answer :) The whole point of it being NP-hard is that there will always be hard instances out there.  Thanks for all the interesting replies. A converse question: are any problems known to be particularly tricky? (i.e. where neither approximation schemes nor optimized exact solvers have reasonable running times for small real-world input instances).   The notion you are looking for is called [hardness of approximation](http://en.wikipedia.org/wiki/Hardness_of_approximation).\n\nThe holy grail of approximation is a full PTAS, or fully polynomial time approximation sceme. jrupac talks about this in his post.\n\nOn the other hand, there are problems that exist such that approximating them within a certain level is itself NP-hard. Some examples:\n\n* Approximating the solution to an instance of CLIQUE better than O(n^(1-epsilon)) for any epsilon is NP-hard.\n* Approximating the solution to an instance of SET-COVER better than O(log n) is NP-hard.\n* Approximating the solution to an instance of VERTEX-COVER better than 1.3606 is NP-hard.\n* Approximating the solution to an instance of INDEPENDENT-SET for some constant c (a function of the maximum degree of the graph) times the optimal solution is NP-hard. I don't remember what the function for c is.\n* Approximating the solution to an instance of machine scheduling better than 7/6 is NP-hard.\n   There exist some very fast SAT solvers ([MiniSAT](http://minisat.se/) for example). Since SAT is an NP-Complete problem, we can reduce all problems in NP to SAT in polynomial time. So in terms of what you referred to as 'in practice' speed, yes it is possible for all NP-complete problems. However, these fast SAT solvers are just exponential time algorithms with lots of optimizations for specific SAT cases. And a lot of times, reducing problems to SAT results in an exponential blow up in the number of clauses.\n\nAlso, SAT solvers are not approximations, they give actual solutions. &gt; And a lot of times, reducing problems to SAT results in an exponential blow up in the number of clauses.\n\nUnless you're using exponential informally, that doesn't make any sense. To create 2^n new clauses takes O(2^n ) time, and would thus not be a polynomial-time reduction.   I have always loved the title of this paper [\"Where are the hard knapsack problems?\"](http://www.dcs.gla.ac.uk/~pat/cpM/JChoco/knapsack/papers/hardInstances.pdf). \n I have always loved the title of this paper [\"Where are the hard knapsack problems?\"](http://www.dcs.gla.ac.uk/~pat/cpM/JChoco/knapsack/papers/hardInstances.pdf). \n   There are three relevant computational complexities to your questions: FPTAS ⊂ [PTAS](https://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme) ⊂ [APX](http://en.wikipedia.org/wiki/APX).\n\nAPX are problems that have a constant factor polynomial approximations, PTAS has polynomial approximations for *any* constant factor, and FPTAS approximations are also polynomial time in the factor of approximation.\n\nTSP is not approximable at all, i.e. not in APX (although there are many special cases of TSP that do have good approximations).\n\nAny APX-hard problem does not have a PTAS. Read at least the last paragraph of the APX wiki page. The maximum satisfiability problem has a 30% approximation, but is APX-complete, so has no PTAS.\n\nAnything with a PTAS I would call easy to approximate.\n\n[Here](http://www.cs.princeton.edu/~wayne/cs423/lectures/approx-alg-4up.pdf) is a set of slides on some specific approximation algorithms if you want to read it.\n\n-------------------------------------\n\nYou also asked about 3SAT - if you haven't already, you would probably appreciate reading about the phase transition. [Here](http://cs.famaf.unc.edu.ar/~careces/cordoba08/Bib/gent94sat.pdf) is an interesting read that google showed up. You might be able to find better though, I just picked a random paper.    If your problem instance reduces to a specific 3SAT problem instance that is solved quickly, then yes, your problem instance will be solved quickly.\n\nThe issue is this: certain problem instances are easy to solve.  But for a problem to be easy, *all problem instances* must be easy.  \n\nI'm not sure what 3SAT solvers you are mentioning, but their success or failure on particular problem instances tells us nothing about the complexity of the problem, only that instance of the problem.   ",
    "url" : "http://www.reddit.com/r/compsci/comments/15kvzc/which_npcomplete_problems_are_actually_easy/"
  }, {
    "id" : 41,
    "title" : "The SAT Phase Transition",
    "snippet" : "  Keep up the good posts cypherx; r/compsci needs them.",
    "url" : "http://cs.famaf.unc.edu.ar/~careces/cordoba08/Bib/gent94sat.pdf"
  }, {
    "id" : 42,
    "title" : "Social media data may help predict exam scores",
    "snippet" : " ",
    "url" : "http://www.aabgu.org/media-center/news-releases/students-social-networks.html"
  }, {
    "id" : 43,
    "title" : "Computers: It's Time to Start Over: Computer scientist Robert Watson, putting security first, wants to design with a “clean slate”",
    "snippet" : "  Good read, thanks for sharing.\n\nHow practical do you think a ground-up OS redesign/reconceptualization is? I've been a fan of the idea for quite some time now, but I haven't seen a ton of progress (granted I haven't been looking too hard). People make new OSs all the time.  The biggest issue is that any currently popular OS will have a huge number of programs already written and working for it, and it would take an absolutely monumental amount of work to get all of that functionality on a new OS. Yepp one needs to write an compatibility layer of some sort. Problem with that is a compatibility layer will compromise security by allowing buggy and insecure legacy code to run on the new OS, and allow the compatibility layer itself to become infected with malware and allow access to data by exploit. If the compatibility layer prevents malware and prevents the access to data by exploit, then it won't be compatible enough to run the software people need to do business out there.\n\nAMA I used to write custom business software, I had to port legacy code to new platforms and put in HTML and SQL sanitizing of all inputs and do quality control checks. That is before 'good enough' became the standard for quality control checks and they don't sanitize inputs anymore nor check for exploits anymore in most custom business software. I am not talking about COTS (Commercial Off The Shelf) software, but what business develop themselves in-house.\n\nFor example they will want the new OS to be able to run Visual C# and Visual BASIC programs, most of the in-house stuff is developed on that or legacy Visual Studio languages. If they can't compile their old Visual Studio code on it, they won't want to use it. So this new OS is going to have to not have a compatibility layer, but should have programing languages that can compile the in-house custom business apps. Compatiblity layer such as [KeyTECH](http://cap-lore.com/CapTheory/KK/UnixOnMicroKernel/) was for Unix ontop of KeyKOS not \"Win XP Compatiblity Mode\" kind of crap we are seeing in Win7 and Wait.\n\nAnd I agree, the recompile without major modification of the sourcecode of the in-house custom business apps is a way to go.\n\n(Heck I heard some custom business apps written in Rexx are still running somewhere.) Is the KeyKOS KeyTECH Unix compatibility layer able to run Unix apps without the Unix exploits and data exploits? It sort of looks like the WIN-OS2 compatibility layer for OS/2 2.X, that led to the downfall of OS/2 because who wants to develop native OS/2 apps when it runs DOS and Windows apps in the compatibility layer? For example what apps exist for the KeyKOS native code and not just the Unix code? It looks like it has been out since 1991, and I never heard of it until you mentioned it. Reminds me of Topview/Desqview that ran a DOS compatibility layer as well. Yepp one needs to write an compatibility layer of some sort. People make new OSs all the time.  The biggest issue is that any currently popular OS will have a huge number of programs already written and working for it, and it would take an absolutely monumental amount of work to get all of that functionality on a new OS. People make new OSs all the time.  The biggest issue is that any currently popular OS will have a huge number of programs already written and working for it, and it would take an absolutely monumental amount of work to get all of that functionality on a new OS. Nah, just takes money, marketing and something popular to run it on. iOS isn't that old. Nah, just takes money, marketing and something popular to run it on. iOS isn't that old. Nah, just takes money, marketing and something popular to run it on. iOS isn't that old. Good read, thanks for sharing.\n\nHow practical do you think a ground-up OS redesign/reconceptualization is? I've been a fan of the idea for quite some time now, but I haven't seen a ton of progress (granted I haven't been looking too hard). Good read, thanks for sharing.\n\nHow practical do you think a ground-up OS redesign/reconceptualization is? I've been a fan of the idea for quite some time now, but I haven't seen a ton of progress (granted I haven't been looking too hard). Remember how people were expecting Linux to take over on the Desktop? About as long as it took for that to work. That's OK because it has dominated nearly everything else. Except in the consumer world. Linux struggles even with techies as a desktop OS.\n\nEDIT: Strictly talking about the desktop OS. I know Linux is everywhere. I think that's what he just said... it dominates everything except personal computers.\n\nAs for the consumer world... The consumer market consists of more than just PCs. Android and iOS pretty well dominate sales in the smartphone market at the moment at about ~90% combined. A market that surpassed PCs in sales almost exactly two years ago. These OSes have Linux and Unix at their bases respectively. As an interesting note, these numbers count tablets as PCs. The numbers would be even more skewed if this were taken into account.\n\nAs for everything else, Linux/Unix dominate Servers, Routers/networking gear, TVs, medical equipment, cash registers, microwaves, potatoes (Oh linux humor, you're not funny at all), and pretty much anything else with a microchip in it. What microwaves run Linux? Pick one?\n\nI believe Linux has ~50% of the embedded OS market share, and ~30% of RT. How can I pick one? Clearly I don't know, that's where I'm asking you where you've seen such a thing. I'm curious about at what feature level you'd need an OS like Linux in microwave, as opposed to, say, a fridge running touch screen apps. Except in the consumer world. Linux struggles even with techies as a desktop OS.\n\nEDIT: Strictly talking about the desktop OS. I know Linux is everywhere. Linux is everywhere in the consumer world. TVs, set-top boxes, WiFi routers, and cell phones, to name a few. I think you'd be hard pressed to find a home that didn't have at least one Linux device.  I was replying to a post mocking the Year of the Linux Desktop. Of course Linux is everywhere, but as a desktop OS, it failed to reach critical mass.\n\nRight now, my server and router are both on Linux, and so does my roommate's two computers. I agree with what you say, but it wasn't my point. Remember how people were expecting Linux to take over on the Desktop? About as long as it took for that to work. Who was expecting Linux to take over on the desktop? Everyone on Slashdot circa 2000. No, it was a running joke even before 2000. Some people really didn't get the memo on that one.... Hi, I'm Mark Shuttleworth. What memo? Good read, thanks for sharing.\n\nHow practical do you think a ground-up OS redesign/reconceptualization is? I've been a fan of the idea for quite some time now, but I haven't seen a ton of progress (granted I haven't been looking too hard). See the L4 microkernel and what it's gone through. Perhaps you would know; is there a good point to start from if one were to look through the Pistachio code base to get a handle on how it works? Read the academic papers on L3 and L4 to get a feel for the architecture and the platform-specific code needed to make context switching and message passing fast.  Good idea, never going to happen. Is there actually a new idea in there? I kept skimming forward when he started explaining what everyone has already been doing but couldn't find what I expected given the title. Am I wrong to take this as a simple call to arms? When I read the tittle all I thought was that I have heard this before.  And then I recall my school work, and how much sound theory and mathematical proof surrounded logic design and algorithm correctness, and then when you get to security topics you have all these wishey washey design ideas that are far less formalized coupled with hard core hacking that tends to be dominated by minds far different from mere mortals who tend to think in the aforementioned formalisms.\n\nSo yeah.  The security field us fast moving and the more people we have trying to make this happen the better, but I get the feeling we're a ways off before we abandon all the security unconscious hardware and software out there.\n\nI might read the article now.\n\n It's a phone interview that includes an explanation of buffer overflows for layman.\n\nSome of the problem with secure programming is purely social. The high stakes and computational complexity engender an environment where thoughtful caution can bleed into egotistical paranoia. I've read usenet threads that were poluted by some \"X Files\" warrior so hell bent on being king of the hill that he openly thought other posters were socially engineering the discussion. Yet, he wasn't contributing patches.\n\nAt the same time there are coders like George Hotz who are so eagle eyed they can craft a bootloader exploit out of a hex dump that involves subverting a circuit board trace with a needle. Holy shit. What music did his mother listen to while pregnant?\n\nSheer indiffence comes into play too. I chatted with a programmer I met in a bar recently who worked for a B2B site. He was excited about an internal project all his own that would hammer a domain looking for security holes. It's biggest flaw was that when telling a client their site was insecure many would respond, \"We don't care.\" Is there actually a new idea in there? I kept skimming forward when he started explaining what everyone has already been doing but couldn't find what I expected given the title. Am I wrong to take this as a simple call to arms? The \"new idea\" was compartmentalization (like the tabs on Chrome) with very granular permissions and access control for applications (similar to Android) built into the core of the OS.\n\nAlso, abandoning the Neumann architecture (program binary === data) in favor of, say, the Harvard architecture (separate program memory and data memory). Is there actually a new idea in there? I kept skimming forward when he started explaining what everyone has already been doing but couldn't find what I expected given the title. Am I wrong to take this as a simple call to arms? Good idea, never going to happen.  I cannot help but think for an interview discussing a fundamental redesign it kinda seems too focused on the less difficult problem... I mean compartmentalization has by and large \"won\" and app environments that don't have to worry about a huge legacy (iOS, the droids, chromeos, and i'd even add, say, heroku to this mix) only give you a private, limited view into the real host device. All well and good. Yes the sandbox has holes in it sometimes, but I really think we are, at this point, closer to the point of simply patching all the holes rather then  doing something new from scratch. This is largely thanks to the fact that system designers have (wisely) limited the amount of software that needs to run outside the sandbox, or (generally speaking) have direct access to the underlying iron. Buffer overflows are the mother of all security holes and you can essentially forget about them if you are writing in a memory-managed environment, which in turn, is something you can absolutely do for almost all user level applications. Yes yes, sometimes three are exploits to be had in the environment interpreter/manager itself, but again, the point is, we have drastically reduced the amount of software that needs to be written \"without mistakes\". I agree with the OP that it is impractical to try and write software in the general case \"without mistakes\" but it can absolutely be done for a small handful of cases. \n\nTo me, the much more troubling \"fundamental problem\" is turning out to be the sharing of data and resources when we actually want it. This keeps coming back to ye olde confirmation dialog which has proven time and time again to be extremely ineffective. From sudo prompts to UAC to facebook confirmation pages, apps just can't be trusted to not over-demand their privileges, and non-technical users (or even highly technical users in a hurry) are not an appropriate arbiter of how adequate these privilege requests really are. That one is, at least to me, worth considering re-doing a bunch of our systems from scratch - though I suspect it's much more of a cultural change then a technical one. Not only that, but any new system will have millions of details to get right. It may be that it's security model will be better in theory, but there will still be done. Our current systems, while flawed in many ways, have already gone through a lengthy security vetting process. Any new system will have to go through the same decades-long process all over again. &gt;Any new system will have to go through the same decades-long process all over again.\n\nIt would be much quicker than decades-long because the security principles are outlined. It's the same situation as with verifying an existing C program. It takes far far longer because you assume nothing and construct the verification proof from scratch. With code that's based on an existing proof or code that comes with a proof (no matter how partial or incomplete) you're starting with something and it should be quicker to get through it. Except that the verification process itself isn't free. Fully verified code is expensive, which is why it's always been limited to the niches where people could die from software bugs. I cannot help but think for an interview discussing a fundamental redesign it kinda seems too focused on the less difficult problem... I mean compartmentalization has by and large \"won\" and app environments that don't have to worry about a huge legacy (iOS, the droids, chromeos, and i'd even add, say, heroku to this mix) only give you a private, limited view into the real host device. All well and good. Yes the sandbox has holes in it sometimes, but I really think we are, at this point, closer to the point of simply patching all the holes rather then  doing something new from scratch. This is largely thanks to the fact that system designers have (wisely) limited the amount of software that needs to run outside the sandbox, or (generally speaking) have direct access to the underlying iron. Buffer overflows are the mother of all security holes and you can essentially forget about them if you are writing in a memory-managed environment, which in turn, is something you can absolutely do for almost all user level applications. Yes yes, sometimes three are exploits to be had in the environment interpreter/manager itself, but again, the point is, we have drastically reduced the amount of software that needs to be written \"without mistakes\". I agree with the OP that it is impractical to try and write software in the general case \"without mistakes\" but it can absolutely be done for a small handful of cases. \n\nTo me, the much more troubling \"fundamental problem\" is turning out to be the sharing of data and resources when we actually want it. This keeps coming back to ye olde confirmation dialog which has proven time and time again to be extremely ineffective. From sudo prompts to UAC to facebook confirmation pages, apps just can't be trusted to not over-demand their privileges, and non-technical users (or even highly technical users in a hurry) are not an appropriate arbiter of how adequate these privilege requests really are. That one is, at least to me, worth considering re-doing a bunch of our systems from scratch - though I suspect it's much more of a cultural change then a technical one. Yes, I'm kinda confused -- the guy really did not say anything besides \"they are trying to compartmentalize stuff, it's hard, a new approach is needed you fools\". So, what's the new approach? Am I missing something from only skimming the interview?\n\n&gt; This keeps coming back to ye olde confirmation dialog which has proven time and time again to be extremely ineffective.\n\nI don't know, isn't it obvious: virtualize instead of restricting? When an app says \"I need access to your SD card, all of it!\", all right, give it access to a virtual SD card where you can mount some parts of the real one if you want. The same for every other demand, if it asks for anything -- never reject, give it a virtual empty internet, a virtual camera that is seeing a pic of someone's ass, a virtual GPS that reports you as walking around the pentagon.\n\nThe idea that you either agree to every demand of a freshly downloaded application (which are traditionally coarse as fuck by the way) or _don't use it at all_ is absolutely, totally retarded.  The alternative to the Von Neumann architecture where code and data are in entirely separate stores is called the Harvard architecture. I think we should move to such an architecture for greater security. In cases where we want to output machine code, there should be a secure hardware mechanism for enabling this, but it should not be available by default.  Do you realize how ridiculously hard this would be to implement? Designing the hardware would be relatively easy, but almost *none* of the software we have now would be portable.\n\nALL modern natively compiled languages assume a Von Neumann architecture. C, C++, Objective-C (everything that's commonly used to write native apps) assumes that code and data are both accessible in the same address space. Dynamically compiled languages like Java and Python are also designed for a Von Neumann architecture. On a Harvard machine, JIT compilation would basically be impossible as we do it now.\n\nFor simple embedded devices like Atmel AVRs, which are weakly Modified Harvard (Harvard but with the ability to read/write code memory via special instructions), the compiler has to emulate all sorts of shit in a way that I think is downright dirty. For example, let's say that you want to compile some code on-the-fly for maximum optimization. On a Von Neumann machine (a desktop PC) you can just do this.\n\n1. Compile code and save instructions into an array with starting address \"array_address\"\n2. Call ((void(*)())array_address)() //or something like that\n\nOn a Harvard machine, you must do the same as above, but with the extra step of storing to and reading from code memory after compilation. You also have to worry about address space ambiguity, which is my main issue with devices like AVRs and compiling \"C\" for them. On a Harvard machine, 0xFFFF1234 means two things at once. So on an AVR, if you saved your dynamically compiled program at 0x1234 and called ((void(*)())0x1234)(), guess what... you're not executing what you think you're executing. This ambiguity actually bit me in the ass one time. I accidentally read from RAM when I should have been reading from program memory or vice versa. You don't have to worry about that on a Von Neumann. \n\nAnyway, because of all this stuff, using a Harvard machine would completely break most modern languages that use things like function virtualization or dynamic compilation.\n\nAnd really, I question the security benefit. We already have DEP on all modern processors, and technologies like ASLR are making traditional attacks that leverage the flexibility of Von Neumann machines less practical. &gt; ALL modern natively compiled languages assume a Von Neumann architecture. C, C++, Objective-C (everything that's commonly used to write native apps) assumes that code and data are both accessible in the same address space.\n\nHmm, I don't think this is true. Many C/C++ programs assume that code is *addressable*, but I don't think they assume that code and data live in the *same* address space. If they did, incrementing a pointer to a Foo could yield a pointer to a function.\n\nOnly C extensions, like GNU labels-as-values directly address code like this. &gt;but I don't think they assume that code and data live in the same address space.\n\nBut they do. How do you think shellcode injection works? When you write the following:\n\n    function();\n\nThe compiler reduces this to\n\n    preserve registers, save return address and frame address, etc.\n    &amp;function =&gt; I.P.\n    restore registers\n\n\"Function\" could point to anywhere in memory (including a string), and as long as that memory page is set to executable, whatever is in memory will be executed.\n\n&gt;If they did, incrementing a pointer to a Foo could yield a pointer to a function.\n\nIndeed, it could, and it sometimes does. &gt; The compiler reduces this to ...\n\nIt's irrelevant what the compiler reduces it to. What matters is the abstract semantics presented by the language. There's nothing in the standard that depends on von Neumann semantics that I can think of off the top of my head. Casting to a function pointer from any other kind of pointer is undefined.\n\n&gt; Indeed, it could, and it sometimes does.\n\nThis behaviour is undefined in the standard. It's a mistake if your platofmr independent code depends on it. &gt;It's irrelevant what the compiler reduces it to. \n\nI disagree, since this is pretty dang import when considering the practicality of porting software.\n\n&gt;There's nothing in the standard that depends on von Neumann semantics that I can think of off the top of my head. \n\nIt's a very important feature of C that data and code can be treated the same. I'm not sure if this is part of the C standard, but it sure would break a hell of a lot of programs if this no longer worked.\n\n&gt;It's a mistake if your platofmr independent code depends on it.\n\nThis may be true, but it would still be a practical nightmare. Obviously a LOT of things rely on code-data interchangeability.\n\nIf we didn't have to worry about the dirty intricacies of low-level programming, I would agree with you. Unfortunately, we do. &gt; This may be true, but it would still be a practical nightmare. Obviously a LOT of things rely on code-data interchangeability.\n\nLike what? JITs are the only programs that really depend on this. Have you ever done kernel or embedded programming? A lot of stuff gets abstracted away so that high-level programmers never have to touch it, but trust me when I say that data execution and code modification are pretty damned important in a lot of places.\n\nAnd it's not like JITs are a trivial subset of programs. JITs are probably some of the most significant programs in use today. The alternative to the Von Neumann architecture where code and data are in entirely separate stores is called the Harvard architecture. I think we should move to such an architecture for greater security. In cases where we want to output machine code, there should be a secure hardware mechanism for enabling this, but it should not be available by default.  Is he really suggesting different physical storage? Or reiterating the current memory protection architecture where pages can be marked no-execute? \n\n(op;dr)  Yes, I'm not really sure where either stcredzero or Robert Watson are saying they want to go with this.\n\nAMD has the NX bit, Intel has the XD bit, and ARM has the XN bit, which mark pages as non-executable.  These have been around for quite a while.\n\nAlso, most CPUs have separate code and data caches at the L1 level, making them \"Harvard architectures\" from the hardware perspective.  It's just that this separation isn't (currently) preserved at lower levels of the memory hierarchy.\n\n[Wikipedia has a little NX Bit overview](http://en.wikipedia.org/wiki/NX_bit) for those who are interested.\n\n The alternative to the Von Neumann architecture where code and data are in entirely separate stores is called the Harvard architecture. I think we should move to such an architecture for greater security. In cases where we want to output machine code, there should be a secure hardware mechanism for enabling this, but it should not be available by default.  Say you have downloaded a new application or an update. That is data - in the Updater app of course, but the architecture does not differ. Now you need a bridge to allow that data to move it to the other side, to get write access to program storage. It's irrelevant if that is in pages, different file system or separate storage device. To make that crossing you would need hardware to secure access, in simplest form a switch, in other forms encryption and digital signatures, basically treating any executable code as a careful firmware update. How to ensure nothing else goes across?  The alternative to the Von Neumann architecture where code and data are in entirely separate stores is called the Harvard architecture. I think we should move to such an architecture for greater security. In cases where we want to output machine code, there should be a secure hardware mechanism for enabling this, but it should not be available by default.  The alternative to the Von Neumann architecture where code and data are in entirely separate stores is called the Harvard architecture. I think we should move to such an architecture for greater security. In cases where we want to output machine code, there should be a secure hardware mechanism for enabling this, but it should not be available by default.  I'm not sure I'm understanding your comment.  Are you suggesting a similar approach to how Haskell treats IO with monads with respect to... machine code? Not sure I follow.  I remember when Intel and ms proposed tpcm or tcpm or whatever it was. I remember being against it because of privacy concerns. But the idea of shifting some security to the hw layer is a good one. Curious about what exactly he has in mind.       Uh sure, but any new secure OS is going to lack applications.\n\nYou just know that most businesses won't run it because it does not run their in-house custom business apps, nor supports a programming language they use like ones in Visual Studio because it too has exploits and security issues.\n\nLook at Plan 9, that is what happened when Bell Labs started over with Unix to a brand new \"clean slate\" operating system. Or look at how iOS was a total failure because of the lack of initial software... Oh, wait.      If Robert Watson has kept up with the release notes of FreeBSD then he should be familiar with something called Capscium being enabled by default in version 10.\n\nBasicly it is very flexible compartimentalization based on ideas first inclined by a paper by Dennis And Van Horn.\n\nWhat the hardware could do is make context switching and inter process communication very very cheap instead of how it is on x86 nowdays. Umm ... you might want to catch up on your reading. Robert Watson [created and implemented Capsicum](http://www.cl.cam.ac.uk/research/security/capsicum/people.html). He knows a thing or two about BSD and security. \n\nAlso, x86 context switching can be [extremely fast (8 ns)](http://www.cs.kent.ac.uk/pubs/2006/2522/content.pdf). There are many ways in which current hardware can be used with a different kind of kernel. Umm ... you might want to catch up on your reading. Robert Watson [created and implemented Capsicum](http://www.cl.cam.ac.uk/research/security/capsicum/people.html). He knows a thing or two about BSD and security. \n\nAlso, x86 context switching can be [extremely fast (8 ns)](http://www.cs.kent.ac.uk/pubs/2006/2522/content.pdf). There are many ways in which current hardware can be used with a different kind of kernel. ",
    "url" : "http://spectrum.ieee.org/podcast/computing/software/computers-its-time-to-start-over"
  }, {
    "id" : 44,
    "title" : "Locating interesting parts of an image  | IP-TECH Group",
    "url" : "http://www.iptech-group.com/node/492"
  }, {
    "id" : 45,
    "title" : "The empirical science of machine learning: evaluating RBMs ",
    "snippet" : "  Machine learning is not purely an empirical science though. It's also possible to prove certain things mathematically. One example is that they proved just a few months ago that (roughly speaking) [RBMs are NP-hard to approximately evaluate (PDF)](http://dejanseo.com.au/research/google/36361.pdf).\n\nThis means that in some cases, the IAS algorithm presented in this article will need an exponential number of steps to get close to the real partition function. Empirical science on only the MNIST dataset might never encounter such cases and one could draw the wrong conclusions. But almost any problem of interest is at least NP-hard and even among those many seem to be inapproximable. That still doesn't stop us from register allocation, loop optimization, constrained route planning, efficient circuit layouts. There is often some simpler underlying structure to real-world problems that allows us to outperform worst-case analyses. Perhaps an average-case analysis with a realistic distribution would map better to our experiences?",
    "url" : "https://hips.seas.harvard.edu/blog/2012/12/24/the-empirical-science-of-machine-learning-evaluating-rbms/"
  }, {
    "id" : 46,
    "title" : "Weighted Random numbers",
    "snippet" : "Just finished a stats course. I'm wondering how I I transform a sequence of \"random numbers\" that conforms to some probability distribution?\n\nFor example, let x be a random number on [0,1], then f(x) = a + (x *(b-a)) gives us a uniform distribution. Then f(x) gives a uniform distribution over the interval [a,b]. This was trivial because x was already uniform over [0,1].\n\nIs possible to transform the uniform distribution [0,1] to a more complicated distribution?   The [cumulative distribution function](http://en.wikipedia.org/wiki/Cumulative_distribution_function) *P(x)* for your distribution is bounded between 0 and 1. If you have a uniform distribution, and get *u* from it, then you can get a random number *x* from your distribution by finding *x* such that *P(x) = u*. Wikipedia describes this in more detail at [inverse transform sampling](http://en.wikipedia.org/wiki/Inverse_transform_sampling).\n\n\nBut getting the inverse of the cdf isn't always easy. So you can always do something like [rejection sampling](http://en.wikipedia.org/wiki/Rejection_sampling). The [cumulative distribution function](http://en.wikipedia.org/wiki/Cumulative_distribution_function) *P(x)* for your distribution is bounded between 0 and 1. If you have a uniform distribution, and get *u* from it, then you can get a random number *x* from your distribution by finding *x* such that *P(x) = u*. Wikipedia describes this in more detail at [inverse transform sampling](http://en.wikipedia.org/wiki/Inverse_transform_sampling).\n\n\nBut getting the inverse of the cdf isn't always easy. So you can always do something like [rejection sampling](http://en.wikipedia.org/wiki/Rejection_sampling).       ",
    "url" : "http://www.reddit.com/r/compsci/comments/15dzpw/weighted_random_numbers/"
  }, {
    "id" : 47,
    "title" : "What is the enlightenment I am supposed to attain after studying finite automata?",
    "snippet" : "  Reducible complexity.\n\nAlso how to deal with many interacting states without going mad.  This post is not a self.reddit question, it is a link to a StackExchange discussion which already has [a truly excellent answer](http://cstheory.stackexchange.com/a/14818/4984) posted. You should definitely read this answer before commenting here.  Finite state automata are completely central to computer science.  They pop up everywhere.  For example:\n\n* FSAs are fundamental tools in building and understanding formal languages (including all programming languages).  Understanding FSAs will help you understand what a programming language *is*\n\n* All computers are FSAs (albeit with a LOT of states). Thus, any fundamental limitations of FSAs apply to your computer as well.\n\n* A Turing machine is basically just a FSA plus an infinite memory tape.   I disagree with all your points. \n\n1. Programming languages are not the kind of languages that FSA work on. Some minor parts of programming language theory and design (say, parser generators that generate parsing automatons) rely on knowledge of automatons, but even when you write a compiler from end to end there is very little about FSAs.\n\n2. You can model computers with FSAs, but that doesn't tell you much about computers. I don't think it is true to say, for example, that important theorems of automata theory tell you important things about computer as computing devices. That might be true for very specific kind of embedded devices that work with a small constant amount of memory, but doesn't accurately model programs running general-purpose operating systems.\n\n3. Despite using FSAs to model the control part of turing machine, I actually know of little results about Turing Machine that use important amount of automata theory. I'm really no expert of Turing machines so I may very well be wrong here (example welcome). I don't get your objection with point 2. Computers have finite amount of memory, ergo they are FSA's. Whether or not results from FSA's are \"useful\" when applied to general computers doesn't negate the fact that computers are instances of FSA's. I don't get your objection with point 2. Computers have finite amount of memory, ergo they are FSA's. Whether or not results from FSA's are \"useful\" when applied to general computers doesn't negate the fact that computers are instances of FSA's. Computers are devices from the real world. FSAs, or Turing Machines, are formal objects. You may choose to *model* computers with these formal objects, or others, but just saying \"This formal object can model computers\" is not necessarily pertinent. I can model the state space of a computer as a function from time and input to states, but is that an useful model? What kind of problems about computer will thinking in this way help me solve?\n\nI'm saying that while, technically, you can model general-purpose computers as FSAs, I'm not aware of many meaningful applications of automaton theory to problems about computers in general. So that's actually a relatively dubious argument to FSAs being \"central\" to computer science. I see what you're saying. However, there is one big result that may end up being important in the future: the halting problem is solvable under a FSA, and therefore classes of problems that are impossible for a Turing machine may actually be possible for computers (every time you see someone say X is impossible because halting problem is misapplying it).\n\nSolution to halting problem: Take an algorithm and run it, making note of its memory state at each step. Either the algorithm will halt, or it will repeat a memory state and thus run forever. &gt; every time you see someone say X is impossible because halting problem is misapplying it\n\nI'm not sure I agree. The boundary between \"impossible\" and \"impractical\" is so thin in this case that I'd rather find a remark about \"but memory is finite in practice blah blah\" needlessly pedantic.\n\nYou could always say that the phenomenon you're saying formal thigs about about is not \"I run my program on this machine\" but \"I run an instrumented version of my program that, each time it runs out of memory, stops and prints a specific error, then I go buy more memory, recode the OS if need be, and re-run the program\". Will this process terminate?\n\nNote that this kind of reasoning, \"let's exhaust all the state space\", is the basis for [Model Checking](http://en.wikipedia.org/wiki/Model_checking), a formal method which has had some successes, again not in the field of general-purpose programs (but that may be coming indeed, at least under the approximated form of white-box fuzzing), but more specialized settings like hardware verification, verification of bounded communication protocols, etc. My statement about *every* time the halting problem is cited was certainly too broad, but I think its valid for the usual applications of it that only serve to short-circuit an actual analysis of the problem.\n\nThe way people generally cite the halting problem is less about the impracticality of exhausting state-space, and more about short-circuiting any sort of analysis because why bother if its obviously impossible. But the proof of something being impossible under an ideal generalization says nothing about its practicality in practice. This line of reasoning is ignoring very useful subsets of the problem space that in practice, may be very important. Aside from the trivial subset of programs that run on finite memory, we also have subsets of programs that do not self-modify (which turns out to be the vast majority of them), or programs that are non-recursive, etc. I see no reason why very important results in program verification won't come from these subsets (and probably already have, model checking looks like one of them).    Well a lot actually, automata theory is a problem solving discipline and it governs the behavior of languages, or anything that can be modelled as a language and allows you to define its logical constraints. \n\nOne thing I realized after covering regular languages is that the human brain is made of a neural network much more complex than a deterministic finite automata! This is since we can speak and envision languages that are not regular. \n\nYou can model tons of things as languages\nDiets\nWardrobes\nTv show schedules\nChess games \nGo games\nGames games lols\nEtc\n\n\nI hope I helped ;) Well technically, the fact that you can envision a language that is not regular doesn't mean your thinking device is not \"regular\". Computers can solve turing decidable problems even if they are only FSMs in practice. you need be careful  when you say FSM. What I said that our neural networks were not deterministic finite automata (DFAs)! \n\n\" brain is made of a neural network much more complex than a deterministic finite automata \"\n\nAn FSM is simply a machine with finite states and describing something by merely saying it has a finite number of states doesn't really say that much about its capabilities. The reason some FSMs are more powerful than others is because some of them have storage areas, stacks and input tapes and printing tapes some have an infinite number of tapes---which is impossible if you believe the universe is of bounded size. \n\nWhat makes our brains NOT DFAs is the fact that neurons have little thresholds with which to bias signals, this is one example of the storage property the other example is well our memory!\n\nAnd how on earth can you learn to speak a context-free language like english if your brain is a deterministic finite automata with no memory? its impossible! well you would be able to recognize the parts of the language that are regular but very simple sentences would be impossible to recognize, any where recursion takes place would be a good example.\n\nEnvisioning/creating a language REQUIRES you to be able to classify whether something is inside the language or not, in other words you need to recognize it. If you want to create/envision a non-regular language your need a device that is capable of atleast recognizing a non-regular language. With FSM, DFA or NFA (which is really the same) is assumed.\n\nPlus, you can simulate memory with state, which is what your computer does. Your computer doesn't have any problem telling English from Latin :P\n\nAlso, English is not context-free :) Observe: I would neither go home nor to school. Okay I was incorrect about the whole English being context free it turns out it doesn't behave so well under intersection. But this only helps my argument, the fact that its even not context free means you require an even more powerful machine to recognize it! A Turing machine. And if our brains are not Turing machines then how on earth are u understanding what's written here?\n\nAnd NO, all DFAs and NFAs are FSMs but all FSMs are not DFAs or NFAs! \n\nBesides being able to store information , our brains have the ability to grow and change AUTONOMOUSLY depending on the input it recieves.  I've already told you that to understand English you don't need turing machines. Computers can do it just fine not being turing machines...\n\nAnd yes, *NOBODY* calls PDA \"finite state automaton\". I don't know where you got that :/\n\nAnd if you don't believe me, read: **Finite Automata and Their Decision Problems, Rabin, Scott** these are the people who pretty much *invented* them. That paper is also one of the foundation papers of modern compsci, so you should read it anyway :) Lols soz okay im not going to debate the whole english thing it was merely a bad example. My argument in short is this:\n\nThe human brain is more powerful than a deterministic finite automaton\nBecause it is capable of creating and thus recognizing languages that are not regular.\nIn saying that it is more powerful, I also declare that it is not a DFA.\n And I say that you have no proof that it can in fact generate and recognize language that is not regular :) In fact, it MOST CERTAINLY can't recognize that. If I gave you a sentence with 2^40 recursive clauses I'm pretty sure you don't have enough *atoms* in your brain to store that information.\n\nIn fact, by this very argument I can *decisively* show that your brain is *at most* DFA.\n\nFact that you can speak a language simply means that you have enough states to work with it on a \"day to day\" level. Nobody ever uses more than 5 nested clauses in everyday language, so it's no mistery you can comprehend it. If someone unleashed the full potential of even CFL you would be utterly lost. Okay I was incorrect about the whole English being context free it turns out it doesn't behave so well under intersection. But this only helps my argument, the fact that its even not context free means you require an even more powerful machine to recognize it! A Turing machine. And if our brains are not Turing machines then how on earth are u understanding what's written here?\n\nAnd NO, all DFAs and NFAs are FSMs but all FSMs are not DFAs or NFAs! \n\nBesides being able to store information , our brains have the ability to grow and change AUTONOMOUSLY depending on the input it recieves.  Well a lot actually, automata theory is a problem solving discipline and it governs the behavior of languages, or anything that can be modelled as a language and allows you to define its logical constraints. \n\nOne thing I realized after covering regular languages is that the human brain is made of a neural network much more complex than a deterministic finite automata! This is since we can speak and envision languages that are not regular. \n\nYou can model tons of things as languages\nDiets\nWardrobes\nTv show schedules\nChess games \nGo games\nGames games lols\nEtc\n\n\nI hope I helped ;)     For language development: Finite State Automaton = Context Free Grammar = Regular Expression",
    "url" : "http://cstheory.stackexchange.com/questions/14811/what-is-the-enlightenment-im-supposed-to-attain-after-studying-finite-automata"
  }, {
    "id" : 48,
    "title" : "Turning theory into algorithms.",
    "snippet" : "  Interesting post, I wonder why there are so many downvotes. Is this subreddit getting more math-averse with time?  Interesting post, I wonder why there are so many downvotes. Is this subreddit getting more math-averse with time?  As far as I can tell, the main purpose of this subreddit is to ask what a highshooler should study before undergrad, not to share interesting papers, blog posts, and questions about topics such as algorithms, formal languages, automata, information theory, cryptography, machine learning, computational complexity, programming language theory, etc... We seem to have taken a wrong turn at some point. Any ideas how to improve the atmosphere around here?  We seem to have taken a wrong turn at some point. Any ideas how to improve the atmosphere around here? ",
    "url" : "https://hips.seas.harvard.edu/blog/2012/12/21/turning-theory-into-algorithms/"
  }, {
    "id" : 49,
    "title" : "What are some good topics in theoretical computer science that an undergrad could study?",
    "snippet" : "I am in my second year of university, studying CS. I am very interested in theoretical CS, particularly computability and complexity. I also hope to complete a summer research project this year (just getting my application together now!) However, the most exposure to \"theory\" I've had thus far in my studies is one course on data structures and algorithms this past term, and one course on applied logic this coming term. I've also done a bit of independent reading from books suggested to me by a professor (like Hopcroft). My question is that if I were to try an independent study in theory at some point next year, which topics might be \"on my level\"? That is, what might I be able to tackle at this point without having taken any higher-level courses in theory? (I've read about topics like Turing Machines and P ?= NP, but those still seem out of my reach at this point. However, topics like those seem like they would interest me most.)  First of all: Congratulations to the choice of sub field in CS. While a bit further away from practice than topics such as AI or Software Engineering, Algorithmics and Complexity Theory are invaluable tools for most paths down the road.\n\nIn CS, you are in the good position, that there is a lot of great material available for free on the internet. Any books linked to below should be available at your university's library. Alternatively, you should be able to get good deals for used copies on [Abe](www.abebooks.com).\n\nI had a course on theoretical CS in my second year (Germany) which covered formal languages (centered around but not exclusive to the Chomsky hierarchy) and eventually led to the P/NP problem definition. I guess that the problem here is not your ability to understand this but merely a good presentation of the topic. Sadly the book we used is only available in Germany (and it was somewhat terse without a lecture).\n\nHowever, I just did a quick Google search and [these lecture notes](http://math.tut.fi/~ruohonen/FL.pdf) appears to cover the material from our lectures. Also, they appear to be somewhat descriptive. My recommendation is to search for lecture notes covering similar content that also have excercises that you can solve. Make sure that the lecture is offered in the first 1-2 years of a CS education (I can only speak for German universities but it should be similar in the rest of Europe).\n\nIf you are interested in the theory of automatons, look for more extensive material on the Turing Machine, Register Machine, RAM, or Cellular Automatons. From my impression, these are some more prevalent theoretical models. There are more exotic ones, but the Turing Machine is established as the canonical model, the RAM and register machine are \"quite\" close to real CPUs. Cellular Automatons are a bit exotic but surprisingly interesting. Google for the Firing Squad Problem, for example.\n\nFor algorithms, the [CLRS](http://www.amazon.com/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844) is one of the best written introductory books. Also, it covers a lot of quite advanced material. You should be able to get it from any library since the price is somewhat hefty. A good book somewhat closing the gap to practice is [Algorithms and Data Structures - The Basic Toolbox](http://www.mpi-inf.mpg.de/~mehlhorn/ftp/Mehlhorn-Sanders-Toolbox.pdf).\n\nKnuth's [Concrete Mathematics](http://en.wikipedia.org/wiki/Concrete_Mathematics) is a challenging book giving a solid foundation of tricks for computational complexity computation. That one takes a *lot* time for reading, though.\n\nOh, and get a good book on Computer Architecture (e.g. [http://www.amazon.com/Computer-Architecture-Quantitative-Approach-Edition/dp/0123704901](Hennessy and Patterson) and Systems Architecture (e.g. the [http://www.amazon.com/Modern-Operating-Systems-Andrew-Tanenbaum/dp/0136006639/](Tanenbaum)). This will help you to understand how your algorithms work on real hardware. [Introduction to the Theory of Computation](http://www.amazon.de/Introduction-Theory-Computation-Michael-Sipser/dp/0619217642/ref=sr_1_1?ie=UTF8&amp;qid=1356304258&amp;sr=8-1) by Sipser should be added to the list of books to read. I'm given to understand that it covers almost everything a novice needs to know about theoretical CS [Introduction to the Theory of Computation](http://www.amazon.de/Introduction-Theory-Computation-Michael-Sipser/dp/0619217642/ref=sr_1_1?ie=UTF8&amp;qid=1356304258&amp;sr=8-1) by Sipser should be added to the list of books to read. I'm given to understand that it covers almost everything a novice needs to know about theoretical CS Hm, it's been a while but IIRC it was also recommended to me for newbies. I started to read it before beginning my studies but I somehow did not keep through. I skipped over the material after my theoretical CS lecture and I found it too shallow. Also I think the price tag is quite hefty. However, that's just my opinion.\n\nFor the record: I have the first edition.   Does concurrency and multi processor programming fit under theoretical CS? It's a little closer to something practical and implementable and is definitely something an undergrad can study.   There's a ton of interesting stuff in the field of artificial intelligence. Check out the n-queens problem or fuzzy logic. There's a ton of interesting stuff in the field of artificial intelligence. Check out the n-queens problem or fuzzy logic.  I would recommend the subfields of Artificial Intelligence, and Quantum Computing, that's what I'm studying right now, with only 1 year left to go til graduation (I hope) woot! How in-depth is the Quantum Computing course at your school?      ",
    "url" : "http://www.reddit.com/r/compsci/comments/15ccy8/what_are_some_good_topics_in_theoretical_computer/"
  } ],
  "processing-time-source" : 61,
  "processing-result.title" : "compsci2_reddit.xml",
  "XmlDocumentSource.xml" : {
    "absolutePath" : "/Users/cole/code/cs467/r-topics/carrot2-cli-3.6.2/../data/compsci2_reddit.xml"
  }
}
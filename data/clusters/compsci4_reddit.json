{
  "processing-time-total" : 1580,
  "clusters" : [ {
    "id" : 0,
    "size" : 10,
    "score" : 29.788555535029413,
    "phrases" : [ "Write" ],
    "documents" : [ 0, 1, 3, 7, 15, 17, 22, 23, 32, 39 ],
    "attributes" : {
      "score" : 29.788555535029413
    }
  }, {
    "id" : 1,
    "size" : 6,
    "score" : 46.93623917862138,
    "phrases" : [ "Areas in Computer Science" ],
    "documents" : [ 17, 23, 32, 36, 39, 44 ],
    "attributes" : {
      "score" : 46.93623917862138
    }
  }, {
    "id" : 2,
    "size" : 6,
    "score" : 45.9815955259918,
    "phrases" : [ "Greatly Differ" ],
    "documents" : [ 1, 17, 18, 22, 36, 44 ],
    "attributes" : {
      "score" : 45.9815955259918
    }
  }, {
    "id" : 3,
    "size" : 6,
    "score" : 45.44332842391501,
    "phrases" : [ "Operations" ],
    "documents" : [ 22, 30, 40, 41, 43, 45 ],
    "attributes" : {
      "score" : 45.44332842391501
    }
  }, {
    "id" : 4,
    "size" : 6,
    "score" : 110.55319928086661,
    "phrases" : [ "Type Systems" ],
    "documents" : [ 1, 17, 18, 25, 38, 44 ],
    "attributes" : {
      "score" : 110.55319928086661
    }
  }, {
    "id" : 5,
    "size" : 5,
    "score" : 55.86937244518777,
    "phrases" : [ "Gives us a Function" ],
    "documents" : [ 0, 1, 18, 29, 45 ],
    "attributes" : {
      "score" : 55.86937244518777
    }
  }, {
    "id" : 6,
    "size" : 5,
    "score" : 48.755093540158306,
    "phrases" : [ "Pdf" ],
    "documents" : [ 3, 10, 33, 37, 42 ],
    "attributes" : {
      "score" : 48.755093540158306
    }
  }, {
    "id" : 7,
    "size" : 3,
    "score" : 30.267727461714415,
    "phrases" : [ "Fields and Subfields of Computer Science" ],
    "documents" : [ 17, 32, 36 ],
    "attributes" : {
      "score" : 30.267727461714415
    }
  }, {
    "id" : 8,
    "size" : 3,
    "score" : 68.86021739263713,
    "phrases" : [ "Functionally Complete Set" ],
    "documents" : [ 18, 29, 45 ],
    "attributes" : {
      "score" : 68.86021739263713
    }
  }, {
    "id" : 9,
    "size" : 3,
    "score" : 105.74810270444016,
    "phrases" : [ "Functions Programs should Halt" ],
    "documents" : [ 1, 18, 29 ],
    "attributes" : {
      "score" : 105.74810270444016
    }
  }, {
    "id" : 10,
    "size" : 3,
    "score" : 75.91588030255853,
    "phrases" : [ "Good Explanation" ],
    "documents" : [ 1, 9, 18 ],
    "attributes" : {
      "score" : 75.91588030255853
    }
  }, {
    "id" : 11,
    "size" : 3,
    "score" : 61.22407486381713,
    "phrases" : [ "Rendering" ],
    "documents" : [ 24, 28, 35 ],
    "attributes" : {
      "score" : 61.22407486381713
    }
  }, {
    "id" : 12,
    "size" : 3,
    "score" : 123.14144319156604,
    "phrases" : [ "Turing Machine" ],
    "documents" : [ 1, 18, 34 ],
    "attributes" : {
      "score" : 123.14144319156604
    }
  }, {
    "id" : 13,
    "size" : 2,
    "score" : 93.47468736582378,
    "phrases" : [ "Game that never Ends" ],
    "documents" : [ 16, 28 ],
    "attributes" : {
      "score" : 93.47468736582378
    }
  }, {
    "id" : 14,
    "size" : 2,
    "score" : 69.11177379489277,
    "phrases" : [ "High School" ],
    "documents" : [ 11, 16 ],
    "attributes" : {
      "score" : 69.11177379489277
    }
  }, {
    "id" : 15,
    "size" : 2,
    "score" : 47.60146335346667,
    "phrases" : [ "Instructions" ],
    "documents" : [ 0, 25 ],
    "attributes" : {
      "score" : 47.60146335346667
    }
  }, {
    "id" : 16,
    "size" : 2,
    "score" : 47.6703982927742,
    "phrases" : [ "Open" ],
    "documents" : [ 13, 18 ],
    "attributes" : {
      "score" : 47.6703982927742
    }
  }, {
    "id" : 17,
    "size" : 2,
    "score" : 98.5720202147964,
    "phrases" : [ "Paper is under Peer Review" ],
    "documents" : [ 3, 32 ],
    "attributes" : {
      "score" : 98.5720202147964
    }
  }, {
    "id" : 18,
    "size" : 17,
    "score" : 0.0,
    "phrases" : [ "Other Topics" ],
    "documents" : [ 2, 4, 5, 6, 8, 12, 14, 19, 20, 21, 26, 27, 31, 46, 47, 48, 49 ],
    "attributes" : {
      "other-topics" : true,
      "score" : 0.0
    }
  } ],
  "processing-time-algorithm" : 1521,
  "documents" : [ {
    "id" : 0,
    "title" : "Why not compile intermediate byte-code (say Java byte code) to native code as \"installation\"?",
    "snippet" : "I don't know much about compiling, but a thought occured to me.\n\nImagine compiling your high-level code into an intermediate language, similar to Java byte code or Microsofts CIL. But in stead of having a virtual machine interpret that byte code, you use some software that automatically compiles the intermediate code into native code. That is, just compile it once; sort of like installing by compiling.\n\nDoes this exists already, and if so, why isn't it used more? Wouldn't that give you the \"write-once-run-everywhere\"-advantage, while retaining the speed of native code?  Both the JVM and the .NET virtual machine JIT compile the bytecode (see other comments).\n\nIt is possible to ahead-of-time (AOT) compile bytecode.  The Mono implementation of .NET does this (http://www.mono-project.com/Mono:Runtime:Documentation:AOT).  Microsoft research has experimented with this (http://en.wikipedia.org/wiki/Bartok_(compiler)).\n\nAside from the fact that JIT compilation seems to be satisfactory for most uses, you lose a couple advantages going to ahead-of-time compilation:\n\n* Compiling at install time requires an installation :-) With a VM, only the VM is installed, and applications are download-and-run\n* If a VM has a bug in the JIT, upgrading the VM fixes all programs that hit the bug.  If you compile at installation, upgrading your Java/.NET installation requires recompiling all of your installed programs using that platform.\n* Some optimizations are available to a JIT that are difficult to do with ahead-of-time compilation, like dynamically optimizing a hot path through multiple functions based on the current application's workload.\n\nOf course, there are advantages to ahead-of-time, too, like not paying the cost of JIT compiling every time you run a program. &gt; Of course, there are advantages to ahead-of-time, too, like not paying the cost of JIT compiling every time you run a program.\n\nhm, need the JIT to maintain a cache across executions. that'd be neat. Both the JVM and the .NET virtual machine JIT compile the bytecode (see other comments).\n\nIt is possible to ahead-of-time (AOT) compile bytecode.  The Mono implementation of .NET does this (http://www.mono-project.com/Mono:Runtime:Documentation:AOT).  Microsoft research has experimented with this (http://en.wikipedia.org/wiki/Bartok_(compiler)).\n\nAside from the fact that JIT compilation seems to be satisfactory for most uses, you lose a couple advantages going to ahead-of-time compilation:\n\n* Compiling at install time requires an installation :-) With a VM, only the VM is installed, and applications are download-and-run\n* If a VM has a bug in the JIT, upgrading the VM fixes all programs that hit the bug.  If you compile at installation, upgrading your Java/.NET installation requires recompiling all of your installed programs using that platform.\n* Some optimizations are available to a JIT that are difficult to do with ahead-of-time compilation, like dynamically optimizing a hot path through multiple functions based on the current application's workload.\n\nOf course, there are advantages to ahead-of-time, too, like not paying the cost of JIT compiling every time you run a program.  As others have noted, this is essentially just doing JIT compilation further ahead of time, which is an idea that does exist.\n\nThis may actually result in slower code.  If you have a function with an expensive loop that uses a value passed as a parameter, the JIT compiler can kick in and compile an optimized version of the loop for the particular value that was actually passed to this invocation of the function.  It can do constant propagation, and maybe optimize out a whole bunch of stuff from the loop body.\n\nIf we do the compilation ahead of time, the compiler does not, in general, know the value that is passed for the parameter, and cannot perform this sort of optimization. Your argument doesn't really hold too much water. HotSpot-type optimizations could be baked into the executable with AOT compilation (see profile-guided optimization).\n\nFurthermore, most JIT compilers do not currently work much at the argument level - adaptive online optimizers tend to do things like look for dead code and remove it, removing extraneous NULL checks, inline code to remove function call overhead, eliminate superfluous locks and allocations, copy propagation &amp; redundant loads, switch short-term allocations from the heap to the stack, etc. All of these can be done ahead of time.\n\nHowever, really the best arguments against doing offline compilation (e.g. NGEN) is that most runtimes for JIT'd code have precompiled caches which are going to be warmer than just binary loading. This makes a significant difference when it comes to loading the binary. In practice, both AOT and JIT executables are reasonably efficient and there's nothing much to gain.   I don't understand what's going on here -- why not just compile the source code straight to native code? For really dynamic languages that's just not possible. Think about eval(). For really dynamic languages that's just not possible. Think about eval(). Writing a simple interpreter for language core is simple, and possible to embed in a runtime. I would really like to see a language like ruby or python to be compiled to machine code... So what [psyco](http://psyco.sourceforge.net/) did? Or what [PyPy](http://pypy.org/download.html#with-a-jit-compiler) does now?\n\nThese do compile to machine code instructions, they just do it at runtime, rather than compiling to a native executable. Essentially including the eval() in the runtime. Yes, but with compiling to a native executable aot. :)\nWhy? I'm not sure, maybe because one can? [llvm-lua](http://code.google.com/p/llvm-lua/) is both a jit and static lua compiler. I don't understand what's going on here -- why not just compile the source code straight to native code?      Would that even make a major difference?  I was under the impression that most of the overhead from Java comes from features like garbage collection, not having POD types, expensive function calls, etc, rather than the JIT.  &gt; Does this exists already\n\nYes it does.\n\nhttp://en.wikipedia.org/wiki/Just-in-time_compilation\n With JIT there is still a VM running. What I am trying to describe is a piece of software that takes the byte code, compiles the entirety of it to native code, exits and is then never run again (unless, maybe, during an update, or something). Sort of like an installer (in Windows) that compiles the code as it installs the program. With JIT there is still a VM running. What I am trying to describe is a piece of software that takes the byte code, compiles the entirety of it to native code, exits and is then never run again (unless, maybe, during an update, or something). Sort of like an installer (in Windows) that compiles the code as it installs the program.  This does exist (see JIT compilation).  However, it's a lot easier to write a VM for an arbitrary platform than it is to write a JIT compiler, which would require advanced knowledge of the architecture. Any interpreter can become a compiler by the first Futamura projection.\n\nBy writing the VM, you are using the instructions necessary to encode the VM behaviour.",
    "url" : "http://www.reddit.com/r/compsci/comments/1484zf/why_not_compile_intermediate_bytecode_say_java/"
  }, {
    "id" : 1,
    "title" : "What's the most fascinating computational concept you know of?",
    "snippet" : "  [Diffie-Hellman Key Exchange](http://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange)\n\nBasically, two parties (with no pre-shared information), can agree upon a secret key. With all of their communications intercepted by a third party.\n\nAnd that third party does not learn the key.\n\nIt's freaking magic. (There's a cool youtube video explaining it with color mixing if you have trouble with the concept). I love that video.\n\nHere it is-  \n\nMans voice - http://www.youtube.com/watch?v=YEBfamv-_do\n\nWomans voice - http://www.youtube.com/watch?v=6NcDVERzMGw I love that video.\n\nHere it is-  \n\nMans voice - http://www.youtube.com/watch?v=YEBfamv-_do\n\nWomans voice - http://www.youtube.com/watch?v=6NcDVERzMGw [Diffie-Hellman Key Exchange](http://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange)\n\nBasically, two parties (with no pre-shared information), can agree upon a secret key. With all of their communications intercepted by a third party.\n\nAnd that third party does not learn the key.\n\nIt's freaking magic. (There's a cool youtube video explaining it with color mixing if you have trouble with the concept). [Diffie-Hellman Key Exchange](http://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange)\n\nBasically, two parties (with no pre-shared information), can agree upon a secret key. With all of their communications intercepted by a third party.\n\nAnd that third party does not learn the key.\n\nIt's freaking magic. (There's a cool youtube video explaining it with color mixing if you have trouble with the concept). [Diffie-Hellman Key Exchange](http://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange)\n\nBasically, two parties (with no pre-shared information), can agree upon a secret key. With all of their communications intercepted by a third party.\n\nAnd that third party does not learn the key.\n\nIt's freaking magic. (There's a cool youtube video explaining it with color mixing if you have trouble with the concept). Actually a [Man in the Middle Attack](http://en.wikipedia.org/wiki/Man-in-the-middle_attack) can compromise communications based on Diffie-Hellman Key Exchange  Topological optimization or genetic algorithms. Took a machine learning course in college. We wrote genetic algorithms to solve the traveling salesman problem (optimal route going through n points). Watching the algorithm I wrote converge on an optimal solution blew my mind.\n\nSimilarly, in that same machine learning course, I wrote a perceptron-based neural network (back propagation) and watched it train and learn from data then use what it learned to accurately predict outcomes it has never seen before (we used medical outpatient data to predict cardiac risk factors).\n\nMachine learning is amazing because although you write code, it begins to take on a life of its own as it literally adapts and learns.  I've always been fascinated by how many radically different models of computation are Turing complete. While any given model--like a Turing machines or some programming language--is arbitrary and artificial, the class of problems it can handle is somehow fundamental and natural. \n\nThis is the fact that made me finally realize how some of the CS stuff I was learning actually reflected some underlying universal truth rather than just something made up by humans. (That is, there is some emergent internal consistency.)\n\nAlso, more recently, I've learned about mathematical duality and how seemingly disparate things are often, in a certain sense, inverses. A great example would be product types (tuples) and sum  (disjoint unions). But this is more of a math idea than a computation one.  I've always been fascinated by how many radically different models of computation are Turing complete. While any given model--like a Turing machines or some programming language--is arbitrary and artificial, the class of problems it can handle is somehow fundamental and natural. \n\nThis is the fact that made me finally realize how some of the CS stuff I was learning actually reflected some underlying universal truth rather than just something made up by humans. (That is, there is some emergent internal consistency.)\n\nAlso, more recently, I've learned about mathematical duality and how seemingly disparate things are often, in a certain sense, inverses. A great example would be product types (tuples) and sum  (disjoint unions). But this is more of a math idea than a computation one.  Could you expand on product types and sums being inverses? Could you expand on product types and sums being inverses?  The Busy Beaver function grows faster than any computable function. I tried googling it but couldn't understand any explanation of it. ELI5? This is going to be rough, but here goes:\n\nEvery computer program (in this case, a Turing machine) has a \"size\" - the number of symbols or tokens it takes to write it down.\n\nFor every size, say 5, there is a finite number of programs.\n\nSome of these programs go into infinite loops.  Some stop after a certain number of steps.\n\nWe can look at all the programs of a given size (say 5), throw away the ones that run forever, and take the one that runs for the largest number of steps and call that the \"maximally productive program of size 5\".  We call these maximally productive programs Busy Beavers.\n\nThis gives us a function: `BB(n) = p`, where `n` is program size and `p` is maximum productivity.  If you plot this function you get a curve that gets steeper and steeper as `n` increases.\n\nComputer scientists like to classify curves like this by their rate of growth:  linear, quadratic, polynomial, exponential etc.  You might come across this as *computational complexity analysis* or \"Big-O\" notation.\n\nThis curve can't be classified in such a way.  Why?  For a start, `BB(n)` is *uncomputable*.  You cannot write a program to calculate successive values, because the step \"throw out all the programs that run forever\" cannot be performed by a computer (see: Halting Problem).\n\nRates of growth (Big-O) are generally (almost by definition) expressed in terms of computable functions (like `O(n), O(n^2), O(e^n), O(n!)`).  The thing is, any computable function can be encoded in a program of finite length.  If `BB(n)` were bounded by a computable function (call it `M(n)`), we could use that function to *solve the Halting Problem*.  This has been well proven to be impossible, so obviously there is no such function `M(n)`.\n\nThe solution to the halting problem would look like this:\n\n  * determine the length of the input program, call it `n`\n  * calculate `M(n)`\n  * run the input program for `M(n)` steps, or until it halts\n  * if it hasn't halted, it never will *Very* good explanation! Many thanks, and what a cool function! This is going to be rough, but here goes:\n\nEvery computer program (in this case, a Turing machine) has a \"size\" - the number of symbols or tokens it takes to write it down.\n\nFor every size, say 5, there is a finite number of programs.\n\nSome of these programs go into infinite loops.  Some stop after a certain number of steps.\n\nWe can look at all the programs of a given size (say 5), throw away the ones that run forever, and take the one that runs for the largest number of steps and call that the \"maximally productive program of size 5\".  We call these maximally productive programs Busy Beavers.\n\nThis gives us a function: `BB(n) = p`, where `n` is program size and `p` is maximum productivity.  If you plot this function you get a curve that gets steeper and steeper as `n` increases.\n\nComputer scientists like to classify curves like this by their rate of growth:  linear, quadratic, polynomial, exponential etc.  You might come across this as *computational complexity analysis* or \"Big-O\" notation.\n\nThis curve can't be classified in such a way.  Why?  For a start, `BB(n)` is *uncomputable*.  You cannot write a program to calculate successive values, because the step \"throw out all the programs that run forever\" cannot be performed by a computer (see: Halting Problem).\n\nRates of growth (Big-O) are generally (almost by definition) expressed in terms of computable functions (like `O(n), O(n^2), O(e^n), O(n!)`).  The thing is, any computable function can be encoded in a program of finite length.  If `BB(n)` were bounded by a computable function (call it `M(n)`), we could use that function to *solve the Halting Problem*.  This has been well proven to be impossible, so obviously there is no such function `M(n)`.\n\nThe solution to the halting problem would look like this:\n\n  * determine the length of the input program, call it `n`\n  * calculate `M(n)`\n  * run the input program for `M(n)` steps, or until it halts\n  * if it hasn't halted, it never will This is going to be rough, but here goes:\n\nEvery computer program (in this case, a Turing machine) has a \"size\" - the number of symbols or tokens it takes to write it down.\n\nFor every size, say 5, there is a finite number of programs.\n\nSome of these programs go into infinite loops.  Some stop after a certain number of steps.\n\nWe can look at all the programs of a given size (say 5), throw away the ones that run forever, and take the one that runs for the largest number of steps and call that the \"maximally productive program of size 5\".  We call these maximally productive programs Busy Beavers.\n\nThis gives us a function: `BB(n) = p`, where `n` is program size and `p` is maximum productivity.  If you plot this function you get a curve that gets steeper and steeper as `n` increases.\n\nComputer scientists like to classify curves like this by their rate of growth:  linear, quadratic, polynomial, exponential etc.  You might come across this as *computational complexity analysis* or \"Big-O\" notation.\n\nThis curve can't be classified in such a way.  Why?  For a start, `BB(n)` is *uncomputable*.  You cannot write a program to calculate successive values, because the step \"throw out all the programs that run forever\" cannot be performed by a computer (see: Halting Problem).\n\nRates of growth (Big-O) are generally (almost by definition) expressed in terms of computable functions (like `O(n), O(n^2), O(e^n), O(n!)`).  The thing is, any computable function can be encoded in a program of finite length.  If `BB(n)` were bounded by a computable function (call it `M(n)`), we could use that function to *solve the Halting Problem*.  This has been well proven to be impossible, so obviously there is no such function `M(n)`.\n\nThe solution to the halting problem would look like this:\n\n  * determine the length of the input program, call it `n`\n  * calculate `M(n)`\n  * run the input program for `M(n)` steps, or until it halts\n  * if it hasn't halted, it never will I think you need to leave out the *O* notation because the implicit constants mean that you can't be sure that you have run the machine for long enough. Or is there some simple argument to show that the constants must be computable when the bounding function is? This is going to be rough, but here goes:\n\nEvery computer program (in this case, a Turing machine) has a \"size\" - the number of symbols or tokens it takes to write it down.\n\nFor every size, say 5, there is a finite number of programs.\n\nSome of these programs go into infinite loops.  Some stop after a certain number of steps.\n\nWe can look at all the programs of a given size (say 5), throw away the ones that run forever, and take the one that runs for the largest number of steps and call that the \"maximally productive program of size 5\".  We call these maximally productive programs Busy Beavers.\n\nThis gives us a function: `BB(n) = p`, where `n` is program size and `p` is maximum productivity.  If you plot this function you get a curve that gets steeper and steeper as `n` increases.\n\nComputer scientists like to classify curves like this by their rate of growth:  linear, quadratic, polynomial, exponential etc.  You might come across this as *computational complexity analysis* or \"Big-O\" notation.\n\nThis curve can't be classified in such a way.  Why?  For a start, `BB(n)` is *uncomputable*.  You cannot write a program to calculate successive values, because the step \"throw out all the programs that run forever\" cannot be performed by a computer (see: Halting Problem).\n\nRates of growth (Big-O) are generally (almost by definition) expressed in terms of computable functions (like `O(n), O(n^2), O(e^n), O(n!)`).  The thing is, any computable function can be encoded in a program of finite length.  If `BB(n)` were bounded by a computable function (call it `M(n)`), we could use that function to *solve the Halting Problem*.  This has been well proven to be impossible, so obviously there is no such function `M(n)`.\n\nThe solution to the halting problem would look like this:\n\n  * determine the length of the input program, call it `n`\n  * calculate `M(n)`\n  * run the input program for `M(n)` steps, or until it halts\n  * if it hasn't halted, it never will This is going to be rough, but here goes:\n\nEvery computer program (in this case, a Turing machine) has a \"size\" - the number of symbols or tokens it takes to write it down.\n\nFor every size, say 5, there is a finite number of programs.\n\nSome of these programs go into infinite loops.  Some stop after a certain number of steps.\n\nWe can look at all the programs of a given size (say 5), throw away the ones that run forever, and take the one that runs for the largest number of steps and call that the \"maximally productive program of size 5\".  We call these maximally productive programs Busy Beavers.\n\nThis gives us a function: `BB(n) = p`, where `n` is program size and `p` is maximum productivity.  If you plot this function you get a curve that gets steeper and steeper as `n` increases.\n\nComputer scientists like to classify curves like this by their rate of growth:  linear, quadratic, polynomial, exponential etc.  You might come across this as *computational complexity analysis* or \"Big-O\" notation.\n\nThis curve can't be classified in such a way.  Why?  For a start, `BB(n)` is *uncomputable*.  You cannot write a program to calculate successive values, because the step \"throw out all the programs that run forever\" cannot be performed by a computer (see: Halting Problem).\n\nRates of growth (Big-O) are generally (almost by definition) expressed in terms of computable functions (like `O(n), O(n^2), O(e^n), O(n!)`).  The thing is, any computable function can be encoded in a program of finite length.  If `BB(n)` were bounded by a computable function (call it `M(n)`), we could use that function to *solve the Halting Problem*.  This has been well proven to be impossible, so obviously there is no such function `M(n)`.\n\nThe solution to the halting problem would look like this:\n\n  * determine the length of the input program, call it `n`\n  * calculate `M(n)`\n  * run the input program for `M(n)` steps, or until it halts\n  * if it hasn't halted, it never will This is going to be rough, but here goes:\n\nEvery computer program (in this case, a Turing machine) has a \"size\" - the number of symbols or tokens it takes to write it down.\n\nFor every size, say 5, there is a finite number of programs.\n\nSome of these programs go into infinite loops.  Some stop after a certain number of steps.\n\nWe can look at all the programs of a given size (say 5), throw away the ones that run forever, and take the one that runs for the largest number of steps and call that the \"maximally productive program of size 5\".  We call these maximally productive programs Busy Beavers.\n\nThis gives us a function: `BB(n) = p`, where `n` is program size and `p` is maximum productivity.  If you plot this function you get a curve that gets steeper and steeper as `n` increases.\n\nComputer scientists like to classify curves like this by their rate of growth:  linear, quadratic, polynomial, exponential etc.  You might come across this as *computational complexity analysis* or \"Big-O\" notation.\n\nThis curve can't be classified in such a way.  Why?  For a start, `BB(n)` is *uncomputable*.  You cannot write a program to calculate successive values, because the step \"throw out all the programs that run forever\" cannot be performed by a computer (see: Halting Problem).\n\nRates of growth (Big-O) are generally (almost by definition) expressed in terms of computable functions (like `O(n), O(n^2), O(e^n), O(n!)`).  The thing is, any computable function can be encoded in a program of finite length.  If `BB(n)` were bounded by a computable function (call it `M(n)`), we could use that function to *solve the Halting Problem*.  This has been well proven to be impossible, so obviously there is no such function `M(n)`.\n\nThe solution to the halting problem would look like this:\n\n  * determine the length of the input program, call it `n`\n  * calculate `M(n)`\n  * run the input program for `M(n)` steps, or until it halts\n  * if it hasn't halted, it never will Someone once said that everything, be it as complex as it will, can be explained by someone who completely understood it. You sir are such a someone. Thanks for the great explanation. This is going to be rough, but here goes:\n\nEvery computer program (in this case, a Turing machine) has a \"size\" - the number of symbols or tokens it takes to write it down.\n\nFor every size, say 5, there is a finite number of programs.\n\nSome of these programs go into infinite loops.  Some stop after a certain number of steps.\n\nWe can look at all the programs of a given size (say 5), throw away the ones that run forever, and take the one that runs for the largest number of steps and call that the \"maximally productive program of size 5\".  We call these maximally productive programs Busy Beavers.\n\nThis gives us a function: `BB(n) = p`, where `n` is program size and `p` is maximum productivity.  If you plot this function you get a curve that gets steeper and steeper as `n` increases.\n\nComputer scientists like to classify curves like this by their rate of growth:  linear, quadratic, polynomial, exponential etc.  You might come across this as *computational complexity analysis* or \"Big-O\" notation.\n\nThis curve can't be classified in such a way.  Why?  For a start, `BB(n)` is *uncomputable*.  You cannot write a program to calculate successive values, because the step \"throw out all the programs that run forever\" cannot be performed by a computer (see: Halting Problem).\n\nRates of growth (Big-O) are generally (almost by definition) expressed in terms of computable functions (like `O(n), O(n^2), O(e^n), O(n!)`).  The thing is, any computable function can be encoded in a program of finite length.  If `BB(n)` were bounded by a computable function (call it `M(n)`), we could use that function to *solve the Halting Problem*.  This has been well proven to be impossible, so obviously there is no such function `M(n)`.\n\nThe solution to the halting problem would look like this:\n\n  * determine the length of the input program, call it `n`\n  * calculate `M(n)`\n  * run the input program for `M(n)` steps, or until it halts\n  * if it hasn't halted, it never will Hey I read your explanation again, but there was something I now realized: wouldn't it be possible for a computable function to have a bigger O, just that we can't possibly confirm that it actually is that way. No ... a computable function, by definition, can be encoded in a (finite) Turing machine - this would give us an escape hatch from the Halting Problem, so contradiction strikes and we're back at the beginning.\n\nI think the implied second part of your question can work for other non-computable functions:  given a computable function `F` and a non-computable function `G` it may not be possible to know whether `F(x)` grows faster than `G(x)` - but in this case the definition of the Busy Beaver function means that if `F(x)` is computable, `BB(x)` will *always*, eventually, out-grow `F(x)`.  Another way to look at it is that at some point when `x` exceeds the size of a Turing Machine representation of `F`, it is possible to make a Turing Machine that encodes a function which grows faster than `F(x)`.\n\n*as someone already pointed out, my use of `O()` is misleading so I've refrained from mentioning it here, sticking to the more verbose but correct \"rate of growth (of a function)\".* But the function couldn't be used to escape the Halting Problem, that's what I said.\n\nFor example: we have this function `F(x) = x^x^x^x^x^x^x^x^x^x`, we can't know if it actually grows faster than the `BB()` function, but if we asked some all knowing god then we would get the answer that \"yes, it does.\"\n\nSince we can't talk to this all knowing god then the function `F()` will still be useless, as it may not be an upper bound for `BB()`.\n\nI just don't see how this would go against the Halting Problem which was the only proof you said that such a function can't exist. Okay, so say your function does grow faster than `BB`:  That is to say, there is some `a` such that `\\forall x &gt; a`,  `BB(x) &lt; F(x)`.  Let's use this to solve the Halting Problem.\n\nFirst, let's just deal with Turing machines of size `&lt; x`.  There is a finite number of these - we can just build a list of them and store it statically in our Halting oracle.  If you're not convinced we can do that, we can observe that `BB` is strictly nondecreasing on a large enough scale and declare that `BB(s) &lt;= BB(s+k)` for some known `k` and thus `BB(s) &lt;= F(s+k)`.  Ask me to elaborate if any of that's not clear, but it's the next part that's more interesting:\n\nFor Turing machines of size `&gt;= x`, we know that if they terminate, they must do so within `F(x)` steps.  So our Oracle Machine simply calculates `F(x)` and then simulates the input machine for `F(x)` steps.  If it doesn't halt within this time, we know that the input is a non-terminating program.\n\nThe only property of `F(x)` that matters here is computability, which means that it can be calculated by a Turing machine -- our hypothetical Oracle machine -- in finite time. &gt;For Turing machines of size &gt;= x, we know that if they terminate, they must do so within F(x) steps.\n\nHow would we be able to know if it terminates if we don't even know if `F` is the upped bound for `BB`? We know some functions aren't growing faster than `BB`, `f(x) = x^2` for example, but the rest are unknown if they are growing faster. And of course we can never know what function is growing faster than `BB`, for that would solve the Halting Problem.\n\nWhat I'm saying is that there exists computable functions that grow faster than `BB`, but that we can never know which functions are a part of that group. Therefore they do not break the Halting Problem. Hmm this seems to be moving on philosophy.  If such a function exists, whether or not we know of it, the machine I described can be built.  That's not a property of our knowledge, but a property of the function's existence.  Similarly, the fact that we cannot solve the Halting Problem for Turing Machines is not because we do not know how, but that the nature of Turing Machines does not admit a solution to exist. I tried googling it but couldn't understand any explanation of it. ELI5? [Here](http://www.scottaaronson.com/writings/bignumbers.html) is a good explanation. The Busy Beaver function grows faster than any computable function. [deleted] What do you mean by saying it doesn't exist?  There is some arbitraryness in the initial definitions, but once they're made, there is a definite function.  They've even calculated the first few values. Well, I mean it's non-computable. \n\nIt's non-computable in an interesting way, but I found the Halting problem more general and more interesting.\n\nEDIT: I retract what I said above, the Busy Beaver function is interesting. I don't think it's as interesting as the Halting problem, but that's definitely very subjective. Anything that can be defined exists. Define exists first, then we'll see if this statement is true.\n\n(Hint: PI doesn't exist in reality)  Philosopher-computer scientist here; I think this is on the right track, and probably what winsurfer means is things which are not possible (i.e. Things that have logically contradictory properties). It is not particularly controversial to say that abstract things exist (though some groups deny this). So colorless blue frogs can be defined (as I just did), but cannot exist. I haven't defended this much here, but I suppose the \"bumper sticker\" version would be \"saying something doesn't make it exist\". I'd like to hear anyone's thoughts on it, though. As is probably clear, I'm in the group that says abstract things don't exist.  I think the question really comes down to \"do logical results exist independent of a mind to discover them\"? My answer would be (tentatively) no, aside from certain fundamental rules like causality. While logical rules cannot be violated regardless of the existence of an intelligence to verify them, the rules are themselves a product of our understanding. This is akin to the question of whether math is invented or discovered. I don't think we're going to break new ground here. Define exists first, then we'll see if this statement is true.\n\n(Hint: PI doesn't exist in reality) Hint: Keep your patronising shit to yourself. Anything that can be defined exists. Surely there are lots things you can define which don't exist, like square circles.\n\nBut in general I'll leave that debate to the philosophers. How do you define a square circle? Presumably, a line forming a closed loop following these two conditions:\n\n1. There exists a \"center point\" such that every point on the line is equidistant from the center.\n2. The line consists exactly of four straight edges and four corners. The Busy Beaver function grows faster than any computable function.  The Curry-Howard correspondence. When I was explained how a ~~type inference~~ typed program (see tactics's response) is equivalent to a logical proof, my mind exploded. My favorite part of the Curry Howard correspondence is this:\n\nParadoxes correspond to infinite loops.\n\nPhilosophers and logicians have spent centuries scratching their heads over things like the liar's paradox. But what is liar's paradox? It's a program that gets stuck in an infinite loop.\n\nParadoxes don't seem so deep after you realize this. I hadn't thought of it like that, but it makes sense. My favorite part of the Curry Howard correspondence is this:\n\nParadoxes correspond to infinite loops.\n\nPhilosophers and logicians have spent centuries scratching their heads over things like the liar's paradox. But what is liar's paradox? It's a program that gets stuck in an infinite loop.\n\nParadoxes don't seem so deep after you realize this. I tried googling this, but the search led me nowhere. Could you elaborate on the relation between infinite loops and paradoxes as a result of the CH correspondence? The Curry-Howard correspondence. When I was explained how a ~~type inference~~ typed program (see tactics's response) is equivalent to a logical proof, my mind exploded. The Curry-Howard correspondence. When I was explained how a ~~type inference~~ typed program (see tactics's response) is equivalent to a logical proof, my mind exploded. &gt; When I was explained how a type inference is equivalent to a logical proof, my mind exploded.\n\nThat's not quite right.\n\nType inference is figuring out *from the proof* what the *theorem* you proved was.\n\nThe program (the function and other definitions) are the proofs. The types are the theorems.\n\nTypechecking is verification of a proof.\n The Curry-Howard correspondence. When I was explained how a ~~type inference~~ typed program (see tactics's response) is equivalent to a logical proof, my mind exploded. I would really like to beef up my knowledge on this subject. Can you recommend any good books/papers that would be digestible by a busy, working software engineer who has a 7 month old baby to handle? I would really like to beef up my knowledge on this subject. Can you recommend any good books/papers that would be digestible by a busy, working software engineer who has a 7 month old baby to handle?   Bubble sort is still dope.  Bubble sort is still dope.   Undecidability.  What amazes me is that something so seemingly esoteric actually has significant effects on real problems.  For instance, certain type systems (e.g. with dependent types) have very desirable properties but can't be automatically checked because when a type system gets sufficiently powerful, it becomes undecidable.   I love algorithms. I think some of the best algorithms are these: http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations\n\nDid you know that the long multiplication algorithm you learnt in elementary school is slow (O(n^2 )), and that there is a much faster algorithm (O(nlogn) or so) for multiplying numbers? I find it astonishing that such a universally taught algorithm can be improved in such a dramatic fashion. I know what you're saying, but I think it's pretty common for an \"intuitive\" algorithm to not be the \"best\" in terms of time complexity. For example, when sorting something (say, a stack of papers), most people would probably naturally tend toward a selection or insertion type of sort, but those both grow quadratically in the average case, whereas sorting can be done in O(n log n) time in the worst case. I love algorithms. I think some of the best algorithms are these: http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations\n\nDid you know that the long multiplication algorithm you learnt in elementary school is slow (O(n^2 )), and that there is a much faster algorithm (O(nlogn) or so) for multiplying numbers? I find it astonishing that such a universally taught algorithm can be improved in such a dramatic fashion. I love algorithms. I think some of the best algorithms are these: http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations\n\nDid you know that the long multiplication algorithm you learnt in elementary school is slow (O(n^2 )), and that there is a much faster algorithm (O(nlogn) or so) for multiplying numbers? I find it astonishing that such a universally taught algorithm can be improved in such a dramatic fashion.  [tarski's undefinability theorem](http://en.wikipedia.org/wiki/Tarski%27s_undefinability_theorem)/[godel's incompleteness theorems](http://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems)    Recursion.  :)  [Chaitin's constant](https://en.wikipedia.org/wiki/Chaitin%27s_constant)\n\nIt's a number that\n\n - is definable but not computable (you can't write a program that will output its nth digit, like you can with pi or any other familiar constant)\n - is computably enumerable (you can make a non-halting program that outputs a sequence of numbers approaching the constant)\n - is algorithmically random (essentially this means the digits are random/incompressible. A slightly innacurate definition: the shortest program you can write that would output the first n bits of the number is going to be the ~n bit program \"print x.xxxxxxxxxxxxxx\")\n\nI would actually like to see a proof of the third point (I'm just trusting wikipedia), if anyone has a link or knows a book with the proof, let me know. [Algorithmic randomness](https://en.wikipedia.org/wiki/Algorithmically_random_sequence) only applies to infinite sequences, and means that there is no finite program that will output any given digit after some finite time, so it's basically the same thing as uncomputability.    [Solomonoff induction](http://wiki.lesswrong.com/wiki/Solomonoff_induction).      hash consing - and the concept of a full computing stack based on \"hons\" ( -&gt; [shameless link to my reddit](http://www.reddit.com/r/entrelacs))   Hamming code and some other assembly algorithms. It blew my mind how simple, easy to learn and understand and effective they were.      [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) ~~decomposition~~. It's incredible how many use cases are there, from text indexing to face recognition through recommendation systems.\n\n\nIt took some time to understand what it does, and still the algebra behind looks hard to me, it's like a spell which in some cases shows interesting properties of data and allow to reduce the number of dimensions preserving information. Singular value decomposition decomposition decomposition is even cooler.  Visible Light Communication\n\nhttp://en.wikipedia.org/wiki/Visible_light_communication So, how does this work? as in, how is data transmitted? sorry, if this is basic. ",
    "url" : "http://www.reddit.com/r/compsci/comments/143mu2/whats_the_most_fascinating_computational_concept/"
  }, {
    "id" : 2,
    "title" : "December PLT Games Challenge - Turing Tarpits",
    "snippet" : "  My submition: https://github.com/JD557/Wolf\n\nI'm afraid it might be a little bit too powerful for a Turing Tarpit. Do you guys think I should remove the call stack (C and P commands)? ",
    "url" : "http://pltgames.com/"
  }, {
    "id" : 3,
    "title" : "Timelapse Writing of a CS Research Paper",
    "snippet" : "  Do we get to read the paper? Author here. The paper is under peer review. After review, if it is accepted, there will certainly be more edits based on the comments of the reviewers. \n\nIt's usually not a good idea to release non peer reviewed science. &gt; It's usually not a good idea to release non peer reviewed science.\n\narxiv.org would like to have a word with you Author here. The paper is under peer review. After review, if it is accepted, there will certainly be more edits based on the comments of the reviewers. \n\nIt's usually not a good idea to release non peer reviewed science. I think it's OK if you acknowledge it as needing review. Maybe a reader with have helpful comments. To me the whole point of peer review is that peers should be able to see it, not just designated reviewers. Well the designated reviewers are definitely peers. There is a certain level of confidence and weight attached to peer review in a journal or conference.\n\nPS - the paper is about Reddit. Well the designated reviewers are definitely peers. There is a certain level of confidence and weight attached to peer review in a journal or conference.\n\nPS - the paper is about Reddit. Will it be in a freely available journal? Will it be in a freely available journal? Do we get to read the paper? I had a quick look on a couple of databases and found a bunch of papers by the author who posted the video, Tim Weninger, but this specific one eluded me. If you find it, let me know!   I am wondering what was used to write this paper. Was it word? pages? or Latex? If if it was Tex how was the time lapse captured? &gt;how did you﻿ record this?\n\n\n&gt;I didn't actually \"record\" anything. I wrote a computer program that﻿ converted the pdf files into images with the page layout, and then used another program called ffmpeg to stitch the pages together at 10 frames per second. So you used latex? Or did you just export to PDF when a timer next to your desk went off? Not the author, but that paper is almost certainly laid out using LaTex. Most people use a version control system to edit latex papers. My guess is that the author just wrote a script to go back up the repository version by version, compile each version to pdf, and export each pdf to the imageset. \n\n(Note: it may have been easier to just check in the pdf to the repository, but most people don't do this because pdfs compiled on different machines are not identical. Also, being mostly binary, pdfs do not merge well which will cause merge conflicts). The way I would do it would be to have a command in my Makefile that copies a timestamped (in the filename) version of the pdf to a separate directory every time I do \"make\" to re-compile the latex.  I'd be pretty surprised if he actually logged 463 commits on a single paper; I just wrote an 8 page paper using git with 3 other authors actually working on the text and we logged something like 80 commits between all of us (wrote the paper over a period of ~5 days). Though I could definitely believe other people commit a lot more frequently than we do. The way I would do it would be to have a command in my Makefile that copies a timestamped (in the filename) version of the pdf to a separate directory every time I do \"make\" to re-compile the latex.  I'd be pretty surprised if he actually logged 463 commits on a single paper; I just wrote an 8 page paper using git with 3 other authors actually working on the text and we logged something like 80 commits between all of us (wrote the paper over a period of ~5 days). Though I could definitely believe other people commit a lot more frequently than we do. Not the author, but that paper is almost certainly laid out using LaTex. Most people use a version control system to edit latex papers. My guess is that the author just wrote a script to go back up the repository version by version, compile each version to pdf, and export each pdf to the imageset. \n\n(Note: it may have been easier to just check in the pdf to the repository, but most people don't do this because pdfs compiled on different machines are not identical. Also, being mostly binary, pdfs do not merge well which will cause merge conflicts). So you used latex? Or did you just export to PDF when a timer next to your desk went off? So you used latex? Or did you just export to PDF when a timer next to your desk went off? I did this once with similar results. I simply wrote a small loop that generated the pdf, stored it with the revision number and checked out the previous version before repeating. Then I used mupdf's command line utility to turn the pdf files into images and imagemagick's convert utility to make it into an animation.\n Can you post it too? I am wondering what was used to write this paper. Was it word? pages? or Latex? If if it was Tex how was the time lapse captured? Latex was used. Everytime I compiled the source to inspect it, my IDE fired a script that copied the pdf to a separate folder.\n\nI'm thinking that I may have compiled the paper more than I normally would because I wanted a smooth video, so 463 versions is probably inflated.  The music makes it look like something tragic is about to happen while you write.  Scumbag latex.  Delete one line and suddenly the paper is half a page shorter. Scumbag latex.  Delete one line and suddenly the paper is half a page shorter. This drives me crazy! I knew I would find you if I started browsing /r/compsci Haha, congrats on 2nd place in mechatronics, we saw you guys over on the second floor balcony. I was pretty out of it after submitting my last 50 page problem set. &gt;50 page\n\nWhat   ",
    "url" : "http://youtube.com/watch?v=hNENiG7LAnc"
  }, {
    "id" : 4,
    "title" : "Canadian scientists create a functioning, virtual brain",
    "snippet" : "    ",
    "url" : "http://www.canada.com/news/Canadian+scientists+create+functioning+virtual+brain/7628440/story.html"
  }, {
    "id" : 5,
    "title" : "10 Free online resources for learning Algorithms and Data Structures",
    "snippet" : "  haven't actually read it but I bookmarked this a while back \n\nhttp://www.cs.berkeley.edu/~vazirani/algorithms.html   ",
    "url" : "http://www.techvyom.com/free-online-resources-learning-algorithms-data-structures.html"
  }, {
    "id" : 6,
    "title" : "Discrete Differential Geometry - Helping Machines (and People) Think Clearly about Shape",
    "snippet" : "  I'm really disappointed this isn't getting voted higher--- I think it's a well delivered lecture about a really fascinating topic with pretty visualizations. What more can you ask for?  That was the coolest thing I've seen in months, personally.  I don't check r/compsci very often, so I almost missed it.\n\nThe reason I say \"was\" is that then I read his [paper](http://users.cms.caltech.edu/~keenan/pdf/spinxform.pdf) on the spin transformations stuff for conformal transforms, which is easily the coolest thing I've seen all year.  Dude is a beast.\n",
    "url" : "http://www.youtube.com/watch?v=Mcal5Cy7r4E"
  }, {
    "id" : 7,
    "title" : "Any advice on note taking in comp sci courses?",
    "snippet" : "I'm a junior comp sci major at a small liberal arts college. Recently, I've found that my notes for upper-level com sci courses are inadequate, and often don't capture the essential parts of topics in these courses. Generally, I just copy (with pen and paper) what the professor writes on the board, verbatim. I was wondering if there were any better strategies for note taking, be they specific to comp sci courses or not. Any advice on the organization of notes would also be greatly appreciated.  I used to write down everything verbatim, but I found that I was missing key parts of lecture because I was trying to scribble everything down before the slide changed rather than paying attention to what was being said *about* that slide. Since my lecture slides get posted online in all of my classes, I rarely write down what's on them anymore, unless it requires explanations. I usually just sit and pay close attention rather than spend my time writing and missing what's being lectured. I'll try to write down an idea or two per slide  or every 30 seconds or so but in my own words - quickly taking my understanding of the material and getting it down to help remember it. \n\nHope this helps. I used to mind map lectures as they went along. How did points construct together,cool examples and how they relate. \n\nNo more than an a5 art pad for a lecture By pad I mean page by reply I mean edit I was still getting used to my tablet, and the \"reddit is fun\" app isn't exactly obvious how to edit a post. It was vastly easier to reply.  #firstworldproblem We're on reddit, talking about computer science. That alone puts our problems in the hilarious firstworldproblems section.  I disagree.\n\nI spent most of the year going through 3rd world countries and access to the internet was ubiquitous access to tablets was not. Internet is pretty ubiquitous if your a tourist from a developed world in a developing nation. \n\nBesides, the vast majority of internet enabled devices in developing nations are mobile devices.  by reply I mean edit same hsit shit shit Well played? I used to write down everything verbatim, but I found that I was missing key parts of lecture because I was trying to scribble everything down before the slide changed rather than paying attention to what was being said *about* that slide. Since my lecture slides get posted online in all of my classes, I rarely write down what's on them anymore, unless it requires explanations. I usually just sit and pay close attention rather than spend my time writing and missing what's being lectured. I'll try to write down an idea or two per slide  or every 30 seconds or so but in my own words - quickly taking my understanding of the material and getting it down to help remember it. \n\nHope this helps. I used to write down everything verbatim, but I found that I was missing key parts of lecture because I was trying to scribble everything down before the slide changed rather than paying attention to what was being said *about* that slide. Since my lecture slides get posted online in all of my classes, I rarely write down what's on them anymore, unless it requires explanations. I usually just sit and pay close attention rather than spend my time writing and missing what's being lectured. I'll try to write down an idea or two per slide  or every 30 seconds or so but in my own words - quickly taking my understanding of the material and getting it down to help remember it. \n\nHope this helps.  If the teacher uses slides, try getting the slides before the lesson and writing on top of / next to the slides any additional notes. \n\nBesides that, you really just need to figure out which bits are the most important. Things you'll ACTUALLY need to look back on later. Maybe before writing something down think to yourself: will this be something I won't look at a second time?  I dont take notes. I listen in class and do my homework extremely well.                  ",
    "url" : "http://www.reddit.com/r/compsci/comments/13zake/any_advice_on_note_taking_in_comp_sci_courses/"
  }, {
    "id" : 8,
    "title" : "Overview of deep neural networks and their connection to other machine learning techniques",
    "url" : "http://arxiv.org/abs/1206.5538"
  }, {
    "id" : 9,
    "title" : "Looking for a good (if possible easy) explanation for the BDD Synthesis..",
    "snippet" : "..does anyone know where i can find it? \n(does this post belong to r/askcomputerscience? idk..)     ",
    "url" : "http://www.reddit.com/r/compsci/comments/13yboi/looking_for_a_good_if_possible_easy_explanation/"
  }, {
    "id" : 10,
    "title" : "For anyone interested in Artificial Neural Networks; here's an informative introductory paper written by my AI Professor.[PDF]",
    "snippet" : "  LaTeX &gt; Word  This is an outdated presentation of neural networks-- I would recommend anyone interested in the field to check out the recent Coursera class or lecture slides from Geoff Hinton's intro classes. \n\nedit: apparently I use hyphens too much, also thanks to abyzmic for the links.  Some links for the lazy:\nhttps://www.coursera.org/course/neuralnets/\nhttp://www.cs.toronto.edu/~hinton/ ",
    "url" : "http://uhaweb.hartford.edu/compsci/nnintro.pdf"
  }, {
    "id" : 11,
    "title" : "Cross-post from programming: CS in HS: Support CS education in high school!",
    "url" : "http://www.reddit.com/r/programming/comments/13w5dt/cs_in_hs_support_cs_education_in_high_school/"
  }, {
    "id" : 12,
    "title" : "Looking for computer architecture videos",
    "snippet" : "I found some videos a few months ago that had a free online book and video lectures that taught computer architecture. I believe it was an Israeli university. Does anyone know the website? I believe they build the whole computer from the ground up. The ALU, CPU, Registers etc... and teach assembly.  You are probably looking for: http://www.nand2tetris.org/ This is exactly what I was looking for. Thanks so much! That book is really good. You're in for a treat.   Videos aren't the best way.\n\nThere is a lot to learn about computer architectures, and videos are a very slow way to absorb information. Fun and easy to concentrate on, but much slower than picking up a book and reading it. This. I reccomend reading Code: The Hidden Language of Computer Hardware and Software; By Charles Petzhold Thanks for the advice. I'm 2/3rds of the way through code and really enjoy it. ",
    "url" : "http://www.reddit.com/r/compsci/comments/13ut2d/looking_for_computer_architecture_videos/"
  }, {
    "id" : 13,
    "title" : "Changing the past in open systems",
    "url" : "http://dynamicaspects.org/blog/2012/08/15/changing-the-past-in-open-systems/"
  }, {
    "id" : 14,
    "title" : "Dealing with the data deluge: embedded analytics help researchers understand results as they come in. crossposted to /r/computing",
    "url" : "http://www.deixismagazine.org/2012/11/filling-in-the-blanks/"
  }, {
    "id" : 15,
    "title" : "A new subreddit for educational data mining.",
    "snippet" : "Hello everyone, I just created [EDMit](http://www.reddit.com/r/EDMit/), a subreddit dedicated to the application of computer science to the education domain.\n\nPlease subscribe and post if you are interested!\n\nIf you are particularly passionate about this idea, send me a PM and I will add you as a mod :)    [deleted] Uhh.. other than this being wildly unacceptable by this community, there is no need to learn how to make a web crawler for a specific purpose, such as mining personal information..\n\nIf you know how to crawl a webpage, you can apply it to any scenario. Sure there may be authenticated requests you may have to handle, but the routine of scraping information is pretty simple..\n\nFor educational purposes, the Simple HTML DOM Parser is a useful library\n\nhttp://simplehtmldom.sourceforge.net/\n\nEDIT: OP deleted comment, it was a request for how to build a personal information data mining application. It looks like you spent a bit of time writing up your post, but I seriously doubt this guy wants anything more than a application written for him, with a text field and a button:\n\nEnter Web Address: ________________\n[Go]\n\n [deleted] [deleted]",
    "url" : "http://www.reddit.com/r/compsci/comments/13rous/a_new_subreddit_for_educational_data_mining/"
  }, {
    "id" : 16,
    "title" : "I'm interested in building a robust Monopoly board game simulator, anyone familiar with previous work done in this area, or if something similar already exists?",
    "snippet" : "As a hobby I'm considering building a program to simulate Monopoly games to test strategies. I know that there are many models out there that simply calculate the likelihood of landing on a particular space. My goal would be to build something a bit more detailed to gather some more interesting data about the game. I'd like to incorporate agent based learning into the model to test and develop strategies for playing. I'm wondering if anyone has seen a similar project done in the past. I spent about an hour searching on google and I found a couple of things:\n\n*Monopoly Nerd's blog:\n\nhttp://monopolynerd.wordpress.com/\n\nThis guy built a web-based simulator to produce winning percentages based on starting conditions.\n\n*ESTIMATING THE PROBABILITY THAT THE GAME OF MONOPOLY NEVER ENDS\n\nhttp://www.informs-sim.org/wsc09papers/036.pdf\n\nThis is a paper that a couple of guys from Cornell wrote about their analysis on the game and determining the likelihood of having a game that \"never ends\". The model they used had some severe limitations (only two players, limited to no trading, etc).\n\n*Agent Based Simulation, Negotiation, and Strategy Optimization of Monopoly\n\nhttp://www.tjhsst.edu/~rlatimer/techlab08/LoffredoPaperQ2-08.pdf\n\nThis is the only thing that comes close to what I want to do, but the paper is extremely vague as it doesn't include any results so I don't know if the project was even completed. There is no institution or organization listed but the author's name is there.\n\nLet me know if any of you have stumbled across something related to Monopoly simulations on the internet. I know it's probably not likely but it's worth a shot. Thanks  Don't forget to factor in the banker stealing from the bank  I'd like to introduce myself to everyone on here.  My name is John Gruska, and I am the creator of the Monopoly Nerd blog over at Wordpress.\n\nThe Monopoly simulator that I have developed (and still am developing) was a direct result of my recent efforts to better quantify and map out a strategy that takes into account the situational nature of the game.  Like poker, Monopoly is a unique blend of chance and decision making.\n\nWhen researching the strategy and known mathematical models behind the game earlier this year, I found that beyond a relatively basic Markov Chain based approach to estimating long-term landing probability for each space, little had been done to take the intricacies of the game into account.\n\nThe most complex part of Monopoly lies within the various trading opportunities throughout the course of a game.  In my opinion, today's \"best\" Monopoly AI's pale in comparison to human players.  There is much room for improvement.\n\nDeveloping a relatively sophisticated Monopoly AI has been and continues to be a goal of mine, and I'd love to work and/or collaborate with anyone else interested. This was the response I was looking for. I completely agree with you, that currently the models/simulations completed and performed so far have plenty of room for improvement. At this point, I am just getting started with the project.  I'd like to introduce myself to everyone on here.  My name is John Gruska, and I am the creator of the Monopoly Nerd blog over at Wordpress.\n\nThe Monopoly simulator that I have developed (and still am developing) was a direct result of my recent efforts to better quantify and map out a strategy that takes into account the situational nature of the game.  Like poker, Monopoly is a unique blend of chance and decision making.\n\nWhen researching the strategy and known mathematical models behind the game earlier this year, I found that beyond a relatively basic Markov Chain based approach to estimating long-term landing probability for each space, little had been done to take the intricacies of the game into account.\n\nThe most complex part of Monopoly lies within the various trading opportunities throughout the course of a game.  In my opinion, today's \"best\" Monopoly AI's pale in comparison to human players.  There is much room for improvement.\n\nDeveloping a relatively sophisticated Monopoly AI has been and continues to be a goal of mine, and I'd love to work and/or collaborate with anyone else interested. How did you find this post? I'd like to introduce myself to everyone on here.  My name is John Gruska, and I am the creator of the Monopoly Nerd blog over at Wordpress.\n\nThe Monopoly simulator that I have developed (and still am developing) was a direct result of my recent efforts to better quantify and map out a strategy that takes into account the situational nature of the game.  Like poker, Monopoly is a unique blend of chance and decision making.\n\nWhen researching the strategy and known mathematical models behind the game earlier this year, I found that beyond a relatively basic Markov Chain based approach to estimating long-term landing probability for each space, little had been done to take the intricacies of the game into account.\n\nThe most complex part of Monopoly lies within the various trading opportunities throughout the course of a game.  In my opinion, today's \"best\" Monopoly AI's pale in comparison to human players.  There is much room for improvement.\n\nDeveloping a relatively sophisticated Monopoly AI has been and continues to be a goal of mine, and I'd love to work and/or collaborate with anyone else interested. Only in reddit  I'd like to introduce myself to everyone on here.  My name is John Gruska, and I am the creator of the Monopoly Nerd blog over at Wordpress.\n\nThe Monopoly simulator that I have developed (and still am developing) was a direct result of my recent efforts to better quantify and map out a strategy that takes into account the situational nature of the game.  Like poker, Monopoly is a unique blend of chance and decision making.\n\nWhen researching the strategy and known mathematical models behind the game earlier this year, I found that beyond a relatively basic Markov Chain based approach to estimating long-term landing probability for each space, little had been done to take the intricacies of the game into account.\n\nThe most complex part of Monopoly lies within the various trading opportunities throughout the course of a game.  In my opinion, today's \"best\" Monopoly AI's pale in comparison to human players.  There is much room for improvement.\n\nDeveloping a relatively sophisticated Monopoly AI has been and continues to be a goal of mine, and I'd love to work and/or collaborate with anyone else interested. I'd be very interested to see what sort of ratio of luck vs decision making affects the outcome of each game, which is something I believe these sorts of simulations would be able to come up with.  As an absolute devout board gamer I find monopoly really dry, but I think it definitely could be statistically interesting.  Concerning your last link, TJHSST is a science and tech high school in Virginia.  What you're looking at is the paper created by a senior (of the class of 08 if I'm parsing the link right) after around 2 quarters of research.   Thank you. This helps.    Um, if you are trying to answer the question of if a monopoly game can go on forever, a simulator will not help you answer the question.  A simulator can provide you with lots of examples of games ending, but will never provide you wit h an example of a game that never ends.\n\nYou will have to resort to a proof to claim that its possible for a Monopoly game to never end.  ",
    "url" : "http://www.reddit.com/r/compsci/comments/13rmj6/im_interested_in_building_a_robust_monopoly_board/"
  }, {
    "id" : 17,
    "title" : "Which specific areas in computer science do you think will become more important in the future?",
    "snippet" : "I remember reading a post in a computer science subreddit that outlined some areas in computer science that will have more relevance and importance in the future, such as parallel computing for processing large amounts of data and such. I couldn't find it, so I'm asking you guys. Which field in computer science do you think will be important to know in the future?\n\nEDIT: Yeah, I'm getting a LOT of various fields... it would be nice if you could explain why you think it's important   Which areas aren't going to be important?  I'm having a hard time coming up with something reasonable. Not *not important*, but less important (at least, relatively speaking).\n\nPerhaps databases? They have been a pretty big thing so far, but I can't see it being the forefront of research. Definitely incremental improvements, but I think for the most part what we have does not leave as much to be desired as some other fields.\n\nAs for what I think will be more important: language design, especially as applied to parallel computing. For the coder of average skill, writing and reasoning about complex parallel code isn't an easy task. I think most coders can do a decent job of writing well performing single threaded code these days, but multi-threaded is another story.\n\nWe aren't going to get much faster cores, so all programmers will need to be able to write parallel code. I've been teaching Chapel in my courses.  Maybe parallel languages will look like that in the future.  \n\nAbout databases, I can't say.  I don't know what current problems look like there.  I think data aggregation is still a big thing... OT: what classes do you teach that you teach Chapel? Is it a scientific computing class? I have taught it in Programming Languages (I \"made up\" an HPC paradigm) and Analysis/Design of Algorithms. That's awesome! Oh, thanks! :)  It's really a great language for many purposes, though it's certainly still in development.\n\nWe have actually added parallel programming to 8 of our core courses: CS0, CS1, CS2, PL, Algos, Software Design, Organization and Hardware.  It seems like we're doing the right thing.\n\nHow did you come across Chapel? :) Do you mind if I ask where you teach? That sounds like a very interesting curriculum! \n  \n  I came across Chapel while doing my regular surfing of the [Wikipedia list of programming languages by category](http://en.wikipedia.org/wiki/List_of_programming_languages_by_category). Programming languages and PL design are very much my interests in CS and will be my focus in my MS CS, so I often search for new and interesting languages. I teach at Wittenberg University in Ohio.  \n\nI totally found out about Chapel on accident.  I happened to be in a talk about Chapel at SC 2009 and my ears perked up because I recognized someone's name.  By the end of the talk I thought, \"I should teach this in PL next semester.\"\n\nNot a great reason to pick up a language, for sure.  It's worked out very well, though! Not *not important*, but less important (at least, relatively speaking).\n\nPerhaps databases? They have been a pretty big thing so far, but I can't see it being the forefront of research. Definitely incremental improvements, but I think for the most part what we have does not leave as much to be desired as some other fields.\n\nAs for what I think will be more important: language design, especially as applied to parallel computing. For the coder of average skill, writing and reasoning about complex parallel code isn't an easy task. I think most coders can do a decent job of writing well performing single threaded code these days, but multi-threaded is another story.\n\nWe aren't going to get much faster cores, so all programmers will need to be able to write parallel code. Which areas aren't going to be important?  I'm having a hard time coming up with something reasonable. FORTRAN programming?\n\n Not a subfield of computer science.  FORTRAN is just a tool, like a particular brand of telescope is to an astronomer. FORTRAN programming?\n\n    Paralellism and verification. verification as in verification of data transmission in communications, verification of the accuracy of stochastic results, or what? I'm guessing verification of correctness of computer programs   Cryptography why People don't realize how important cryptography is, we use it every time we access the secured version of a website, its just that our browser handles everything for us behind the scenes.\n\nSpecifically, research in crypto algorithms safe from quantum computers will see interest as some algorithms we currently use can be broken in reasonable time given a reasonably fast quantum computer.\n\n\nAnother area is lightweight crypto, crypto algorithms that are reasonably secure but have very low processing power requirements.They are required in standalone devices with very small batteries like wireless sensor networks, rfid based devices and their likes. People don't realize how important cryptography is, we use it every time we access the secured version of a website, its just that our browser handles everything for us behind the scenes.\n\nSpecifically, research in crypto algorithms safe from quantum computers will see interest as some algorithms we currently use can be broken in reasonable time given a reasonably fast quantum computer.\n\n\nAnother area is lightweight crypto, crypto algorithms that are reasonably secure but have very low processing power requirements.They are required in standalone devices with very small batteries like wireless sensor networks, rfid based devices and their likes.   We will see a more widespread use of expert systems in medical diagnostics and other various fields as computing speeds and knowledge databases continue to grow. That being said no one but us will have any idea what an expert system is. It will be everywhere but not many people will know that it is everywhere. Watson?     - Faster and denser hard drives. SSDs with terabytes of storage space, holographic disks? The permanent decline of DVDs and CDs.\n\n- Artificial intelligence for human-computer relationships such as Siri as a personal assistant.\n\n- Artificial intelligence for self-programming of applications. You tell the programs what they should do and they should be smart enough to create or modify themselves to do what you want.\n\n- Encrypted private mesh networks to overcome government firewalls. I think AI is basically going to go from\n\n1. Not important in our daily lives (today)\n2. Existent, but not in our daily lives (sort of like awesome prosthetics that are crazy expensive and experimental right now)\n3. Just about everywhere.\n\nin very discrete steps. AI algorithms are very important to our daily lives today.  The difference is they are \"weak\" AI systems which basically means they are just a subset of human intelligence and won't talk to you.  If you use Google, or you go to the hospital (that has an expert system to help with diagnosis) you have benefited from AI.  Hell if you use Siri you've really benefited. Well, I suppose that's true. By that definition, however, one could argue that pretty much any algorithm is a form of AI, which is not a leap I'd like to take.    The growing intersection of user interfaces and programming experience.\n\nWhen most aspects of our live will be supported by digital technology, people will interact with machines all the time -- partly as users, partly as programmers. From the specialist trying to maintain and improve hyper-complex code bases to the end-user giving requests to an fairly independent and context-sensitive system, the more power we have the better we need to *express* it.    Web development. \nNative apps will always have a place, but the browser is becoming the OS. Google apps, mobile sites, e-commerce, social networking, cloud computing, etc. all happen in the browser. Good web developers will be vital as the migration continues. We're not talking your nephew writing a static HTML page in his basement, or a crotchety C programmer who only writes printer drivers. We need developers who have a firm grasp on the HTTP protocol and understand how many technologies fit together to create a web app. These will be needed in the near and far future. Web development. \nNative apps will always have a place, but the browser is becoming the OS. Google apps, mobile sites, e-commerce, social networking, cloud computing, etc. all happen in the browser. Good web developers will be vital as the migration continues. We're not talking your nephew writing a static HTML page in his basement, or a crotchety C programmer who only writes printer drivers. We need developers who have a firm grasp on the HTTP protocol and understand how many technologies fit together to create a web app. These will be needed in the near and far future. Web development is not really computer science...\n\nThere *is* a fair amount of research related to the web as a whole, but, it's more often network infrastructure, compiler, and provisioning type stuff. I think they're both going to go hand in hand. Because it's so important to the consumer, I think networking is going to become the most researched piece of hardware. In twenty years, I see a very small chance we'll have native apps, and the network is going to have to accommodate this. It will also, I believe, tie into cybersecurity.  Computer Vision.  Real-time programming.  Swarm death robots. I'm curious of why you think Computer Vision (unless you're kidding). One of my favorite professors researches it and I don't really know if I understand its applications. I'm curious of why you think Computer Vision (unless you're kidding). One of my favorite professors researches it and I don't really know if I understand its applications.   Deep Knowledge and AI   ",
    "url" : "http://www.reddit.com/r/compsci/comments/13rs9u/which_specific_areas_in_computer_science_do_you/"
  }, {
    "id" : 18,
    "title" : "Are the results of theoretical computer science truly fundamental to the universe?",
    "snippet" : "I recently read about the halting problem and several other related problems. I initially thought that there would be some sort of mathematical axiom or something that lead to such results. On closer inspection, I couldn't find any axiom dependance of the halting problem (and those problems that are reduced from it). None at all. Is this just because I might have missed something (i.e., would the answer be different in a different mahematical universe)? Or is the solution of the halting problem truly universal, fundamental and independent of axioms?  \n\nEdit: I hope the following is a better explanation of my question.  \n\nI see the connection between CS results and mathematical theorems, but most CS results (the ones I've seen) rely on the undecidability of the halting problem (which to me didn't seem to depend on an axiom), whereas mathematical theorems rely on set axioms, as a result of which, I didn't consider them universal truths. Please do note that I'm only a beginner and my definition of 'universal' might greatly differ from yours. I don't consider mathematical theorems universal because their axioms can always be altered, leading to other truths (stuff like hyperbolic geometry, for example). CS results seemed universal to me because they were derived from the undecidability of the halting problem, which I felt was not quite modifiable.   The axiom you're looking for is called the [Church-Turing Conjecture](http://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis). It basically states that anything that can be computed using some algorithm, or computing device, or substrate can also be computed using a Turing machine (or lambda-calculus, or recursion, all three of which can compute the same set). \n\nWe can then prove the impossibility of solving the halting problem for a turing machine, and then extend that proof to all types of computing machines using the conjecture.\n\nNow, people treat the CTC like some sort of theorem, but it is vague enough that it cannot be proved (or at least, not to my knowledge). There is a non-zero chance that it is incorrect, I suppose, and there exists some machine that somehow can solve the halting problem for the general case of Turing machines*, but this is widely accepted as not the case.\n\n\n\\* People have studied an extension of Turing machines which have a \"halting oracle\", a device which solves the halting problem for any Turing machine *without* such an oracle. And then, of course, people have studied machines with an oracle that can solve the halting problem for machines with the first type of oracle. And again for the next level, etc. I don't think what is commonly called \"the halting problem\" (ie. that termination of arbitrary turing machines) depends on the Church-Turing Hypothesis. Extending it to arbitrary notions of computation might, but not for notions that have been proven equivalent to Turing Machines (for example, the Lambda Calculus which is equally fundamental and actually more convenient to study programming languages).\n\nSo yes, to me the undecidability of the Halting Problem is an universal result that is true in an universal sense, as most theorems of mathematics. The \"results of the theoretical computer science\" are most often essentially mathematics result, but mathematics of a kind we maybe weren't as interested in before, say, the beginning of the 20th century (Brouwer, Hilbert, Gödel, Church, Turing..). I see the connection between CS results and mathematical theorems, but most CS results (the ones I've seen) rely on the undecidability of the halting problem (which to me didn't seem to depend on an axiom), whereas mathematical theorems rely on set axioms, as a result of which, I didn't consider them *universal* truths. Please do note that I'm only a beginner and my definition of 'universal' might greatly differ from yours. I don't consider mathematical theorems *universal* because their axioms can always be altered, leading to other truths (stuff like hyperbolic geometry, for example). CS results seemed *universal* to me because they were derived from the undecidability of the halting problem, which I felt was not quite modifiable. I hope I have properly conveyed what I wanted to say. CS results depend on axioms just as mathematical theorems do; you cannot do *anything* because axioms because the mere idea of \"proving\" something relies on some pre-supposed way of reasoning. I don't think you can say anything interesting if you reject the fact of relying on axioms.\nAmong the axiom used in mathematics, some are reasonable and accepted by everyone, and some appear to be more arbitrary and are matter of debate about foundational mathematicians, philosophers and logicians.\n\nI suspect your point of view actually comes from not having learned enough mathematics. For example, your example of non-euclidian geometry is telling: there is nothing distinctly unconvincing about theorems of euclidian geometry if your understand them as saying \"if your geometry is euclidian, then P holds\" rather than just \"P holds all the time\" (which would be false in a non-euclidian setting). Could you list some underlying axioms used in TCS (other than the Church-Turing hypothesis)? Could you list some underlying axioms used in TCS (other than the Church-Turing hypothesis)? Could you list some underlying axioms used in TCS (other than the Church-Turing hypothesis)? Could you list some underlying axioms used in TCS (other than the Church-Turing hypothesis)? CS results depend on axioms just as mathematical theorems do; you cannot do *anything* because axioms because the mere idea of \"proving\" something relies on some pre-supposed way of reasoning. I don't think you can say anything interesting if you reject the fact of relying on axioms.\nAmong the axiom used in mathematics, some are reasonable and accepted by everyone, and some appear to be more arbitrary and are matter of debate about foundational mathematicians, philosophers and logicians.\n\nI suspect your point of view actually comes from not having learned enough mathematics. For example, your example of non-euclidian geometry is telling: there is nothing distinctly unconvincing about theorems of euclidian geometry if your understand them as saying \"if your geometry is euclidian, then P holds\" rather than just \"P holds all the time\" (which would be false in a non-euclidian setting). CS results depend on axioms just as mathematical theorems do; you cannot do *anything* because axioms because the mere idea of \"proving\" something relies on some pre-supposed way of reasoning. I don't think you can say anything interesting if you reject the fact of relying on axioms.\nAmong the axiom used in mathematics, some are reasonable and accepted by everyone, and some appear to be more arbitrary and are matter of debate about foundational mathematicians, philosophers and logicians.\n\nI suspect your point of view actually comes from not having learned enough mathematics. For example, your example of non-euclidian geometry is telling: there is nothing distinctly unconvincing about theorems of euclidian geometry if your understand them as saying \"if your geometry is euclidian, then P holds\" rather than just \"P holds all the time\" (which would be false in a non-euclidian setting). The axiom you're looking for is called the [Church-Turing Conjecture](http://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis). It basically states that anything that can be computed using some algorithm, or computing device, or substrate can also be computed using a Turing machine (or lambda-calculus, or recursion, all three of which can compute the same set). \n\nWe can then prove the impossibility of solving the halting problem for a turing machine, and then extend that proof to all types of computing machines using the conjecture.\n\nNow, people treat the CTC like some sort of theorem, but it is vague enough that it cannot be proved (or at least, not to my knowledge). There is a non-zero chance that it is incorrect, I suppose, and there exists some machine that somehow can solve the halting problem for the general case of Turing machines*, but this is widely accepted as not the case.\n\n\n\\* People have studied an extension of Turing machines which have a \"halting oracle\", a device which solves the halting problem for any Turing machine *without* such an oracle. And then, of course, people have studied machines with an oracle that can solve the halting problem for machines with the first type of oracle. And again for the next level, etc. It should be emphasized that there are considerably many variations of the Church-Turing thesis.  A good portion of Hofstadter's famous *Gödel, Escher, Bach*, for instance, is devoted to the discussion of what some versions are and how they relate to one another (and to artificial intelligence).\n\n&gt; Now, people treat the CTC like some sort of theorem, but it is vague enough that it cannot be proved (or at least, not to my knowledge).\n\nSome versions around the idea that \"any physically realizable computation can be simulated on a Turing machine\" are precise enough that they are amenable to proof (depending on various assumptions on the laws of physics, of course) or, at least, to discussion.  One paper along those lines is [here](http://arxiv.org/abs/1102.1612).\n\n&gt; And again for the next level, etc.\n\nAnd the three letters \"etc\" hide a very rich subject: see [what I wrote on the subject](http://www.reddit.com/r/compsci/comments/12jfh9/had_discussion_about_theoretical_computation_and/c6vrb5o) a few weeks ago. The axiom you're looking for is called the [Church-Turing Conjecture](http://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis). It basically states that anything that can be computed using some algorithm, or computing device, or substrate can also be computed using a Turing machine (or lambda-calculus, or recursion, all three of which can compute the same set). \n\nWe can then prove the impossibility of solving the halting problem for a turing machine, and then extend that proof to all types of computing machines using the conjecture.\n\nNow, people treat the CTC like some sort of theorem, but it is vague enough that it cannot be proved (or at least, not to my knowledge). There is a non-zero chance that it is incorrect, I suppose, and there exists some machine that somehow can solve the halting problem for the general case of Turing machines*, but this is widely accepted as not the case.\n\n\n\\* People have studied an extension of Turing machines which have a \"halting oracle\", a device which solves the halting problem for any Turing machine *without* such an oracle. And then, of course, people have studied machines with an oracle that can solve the halting problem for machines with the first type of oracle. And again for the next level, etc. The axiom you're looking for is called the [Church-Turing Conjecture](http://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis). It basically states that anything that can be computed using some algorithm, or computing device, or substrate can also be computed using a Turing machine (or lambda-calculus, or recursion, all three of which can compute the same set). \n\nWe can then prove the impossibility of solving the halting problem for a turing machine, and then extend that proof to all types of computing machines using the conjecture.\n\nNow, people treat the CTC like some sort of theorem, but it is vague enough that it cannot be proved (or at least, not to my knowledge). There is a non-zero chance that it is incorrect, I suppose, and there exists some machine that somehow can solve the halting problem for the general case of Turing machines*, but this is widely accepted as not the case.\n\n\n\\* People have studied an extension of Turing machines which have a \"halting oracle\", a device which solves the halting problem for any Turing machine *without* such an oracle. And then, of course, people have studied machines with an oracle that can solve the halting problem for machines with the first type of oracle. And again for the next level, etc. The axiom you're looking for is called the [Church-Turing Conjecture](http://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis). It basically states that anything that can be computed using some algorithm, or computing device, or substrate can also be computed using a Turing machine (or lambda-calculus, or recursion, all three of which can compute the same set). \n\nWe can then prove the impossibility of solving the halting problem for a turing machine, and then extend that proof to all types of computing machines using the conjecture.\n\nNow, people treat the CTC like some sort of theorem, but it is vague enough that it cannot be proved (or at least, not to my knowledge). There is a non-zero chance that it is incorrect, I suppose, and there exists some machine that somehow can solve the halting problem for the general case of Turing machines*, but this is widely accepted as not the case.\n\n\n\\* People have studied an extension of Turing machines which have a \"halting oracle\", a device which solves the halting problem for any Turing machine *without* such an oracle. And then, of course, people have studied machines with an oracle that can solve the halting problem for machines with the first type of oracle. And again for the next level, etc. The axiom you're looking for is called the [Church-Turing Conjecture](http://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis). It basically states that anything that can be computed using some algorithm, or computing device, or substrate can also be computed using a Turing machine (or lambda-calculus, or recursion, all three of which can compute the same set). \n\nWe can then prove the impossibility of solving the halting problem for a turing machine, and then extend that proof to all types of computing machines using the conjecture.\n\nNow, people treat the CTC like some sort of theorem, but it is vague enough that it cannot be proved (or at least, not to my knowledge). There is a non-zero chance that it is incorrect, I suppose, and there exists some machine that somehow can solve the halting problem for the general case of Turing machines*, but this is widely accepted as not the case.\n\n\n\\* People have studied an extension of Turing machines which have a \"halting oracle\", a device which solves the halting problem for any Turing machine *without* such an oracle. And then, of course, people have studied machines with an oracle that can solve the halting problem for machines with the first type of oracle. And again for the next level, etc.   Well lets try looking at this from an entirely mathematical point of view.  First we need to know what an axiom is, and to get a good understanding of that we need to talk about the structure of math.  This structure is usually called a mathematical universe, since in that subject we can only talk about things inside this structure.\n\nThere are really three main things in a universe definitions, axioms, and theorems.  Definitions are easy, they define something, and do nothing else.  Axioms limit what we can say about a universe.  It's been said here that you can't really say anything interesting without axioms.  I'd disagree with that.  The real problem is that you can say EVERYTHING interesting without axioms.  2+2=fish?  sure why not?  We haven't said this is a problem yet.  Finally theorems are statements about the universe that can be proved from definitions and axioms.\n\nOk, so let's give a little context here.  We'll start with set theory.  Axiom 1: A set is a collection of elements.  What's an element?  No clue, but they seem to like hanging out in sets.  What's a set?  I just told you, a collection of elements.  Can I say anything more?  Well, sure, but given what I have, it wouldn't be a very rich universe if we left it there.  Right now we only have one axiom, so our universe is pretty wide open.\n\nGeometry is another good example.  There's an axiom in geometry that 2 points determine a line.  What's a line? No clue, but it's got two points.  What are points?  Well they could be pretty much anything, chickens, numbers, half of your face. (oh yeah, well your face determines a line!  See? it works.)  We don't know what any of these things are, but any system satisfying our axiom is valid.  Good axioms are ones that seem reasonable, and that limit the universe in meaningful ways.\n\nNow that we know what an axiom is, let's look at the halting problem from a math perspective.  This is difficult right now, since the halting problem really doesn't have anything to do with math.  So let's restate this as a theorem.\n\nTheorem: Given an arbitrary Turing Machine, determining if that machine will eventually halt is undecidable.\n\nSo, let's break this down.  We have a few terms to deal with.  Turing Machine, halt, and undecidable all need to be defined.  These are all well defined mathematically, and I don't want to try defining them here, since I'll probably screw something up, but there's something important here.  These machines are in their own mathematical universe.  we have basic definitions, like state transition function, alphabet, tape, and decidable.  We have some simple axioms like a TM is in one state at a time, rules for halting, and rules for decidability.  Furthermore Turing Machines are defined using sets, so all of the axioms of set theory apply to Turing Machines.\n\nSo the short answer is that yes, there are axioms that underlie theoretical computer science.  We know this because there are thing that aren't true in this field.  It looks like the theory doesn't have any, but this is simply because the axioms are built into the definitions of the various machines.\n\nI hope this helps, or makes sense.  I'm off to go invent the mathematical theory of face geometry.  If you have ever done any philosophy, you might have come across the concepts of necessary and contingent truths. Necessary truths are those that are true in all possible worlds, and contingent truths are those that are true in our world but could be falsified in another possible world. It is very widely held that all mathematical truths are necessary. Since computer science is a subset of mathematics, then all truths of computer science are necessarily true, which is what I think you mean by \"fundamental to the universe\". No universe could exist in which these truths are not true. If you have ever done any philosophy, you might have come across the concepts of necessary and contingent truths. Necessary truths are those that are true in all possible worlds, and contingent truths are those that are true in our world but could be falsified in another possible world. It is very widely held that all mathematical truths are necessary. Since computer science is a subset of mathematics, then all truths of computer science are necessarily true, which is what I think you mean by \"fundamental to the universe\". No universe could exist in which these truths are not true. [deleted] He asked a philosophical question so I gave a philosophical answer. I'm a computer scientist with a strong interest in philosophy because of questions like these.\n\n1. If you don't think philosophy has authority in philosophical matters then what does?\n\n2. A truth is either necessary or contingent and not both. That's not very arbitrary. If you don't like my characterization in terms of possible worlds then think of it as what can be possibly false and what cannot. For example, 1=1 cannot be possibly false whereas the fact that we are having this discussion could.\n\n3. Just because you think you can imagine something doesn't mean it is possible. Think about a universe where ¬(1=1) and tell me that you can imagine it. If you analyze what you think you are imagining enough then you will no longer be able to imagine it if you recognize a contradiction.\n\n4. I only said \"it is widely held\" so as not to make an absolute claim. It is possible for someone to be a skeptic about anything, so you can never say that something is universally accepted. However, that mathematical truths are necessary truths is so obvious to most people that if you are arguing with someone who will not take that claim then you might as well stop arguing. If you like it more, I will make the claim that mathematical truths are necessary, but I think saying that it is widely accepted give more credit to the claim than me just stating it. 1! already equals 1 last I checked. [deleted]  This dude goes trough a lengthy list of examples that support the idea that Ｐ≠ＮＰ is not only a pure math thing, but also a fundamental principle of physics:\nhttp://www.youtube.com/watch?v=8bLXHvH9s1A\n\nAmazing talk! This dude goes trough a lengthy list of examples that support the idea that Ｐ≠ＮＰ is not only a pure math thing, but also a fundamental principle of physics:\nhttp://www.youtube.com/watch?v=8bLXHvH9s1A\n\nAmazing talk!     Is it even possible to have a \"truth\" without depending on *some* axiom or another?  If you keep digging down you have to eventually hit some sort of axiom, or all you have is meaningless.\n\nThe reason many people consider mathematics *more* universal than just about anything else is that it has very precisely defined axioms, much more precise than \"well, stuff in the universe seems to work this way most of the time when we look\". This is an interesting topic, one that logicians and mathematicians have investigated seriously for at least the past 100 years or so. But the general belief that is the answer to your question is: yes, there is a semantic notion of truth that does not seem to depend on (and likely cannot be formulated completely in terms of) a set of axioms. [For more information.](http://en.wikipedia.org/wiki/G%C3%B6del's_incompleteness_theorems) Ah, good point.  I never considered that Godel's theorems imply there is some kind of notion of \"truth\" outside any set of axioms. [Tarski's work](http://en.wikipedia.org/wiki/Tarski's_undefinability_theorem) takes it a bit further and perhaps deals with this particular question more directly: If defining truth in a certain logical system requires a more powerful one, and defining truth in that system requires an even more powerful one, what does that mean? Is it turtles all the way down?   Well, this is nothing but speculative navel gazing, but I'm of the opinion that we simply haven't created a computer complex enough to solve NP complete-type problems *of non trivial size*. And I don't mean \"not enough power, needs more RAM.\" I mean the architecture and design of current computers is too simple; something radically different has to happen.\n\n*Edit*: I'm an idiot and misspoke., you guys are right. My point was that were still at the horse and buggy phase of computing, in my opinion. It's not just better parts, its whole new designs that we need.  What do you mean? Of course we have computers that can solve such problems. We can solve every single NP complete problem trivially. It will take a very long time to do so for inputs of a medium-to-large size, but the issue isn't \"Oh god, how do we solve NP-complete problems\", the issue is \"Oh god, how do we solve non-trivially-small NP-complete problems within a reasonable amount of time\". The definition for \"non-trivially-small\" has changed dramatically over time. \n\nEDIT:\nAnd actually, NP-Complete problems aren't even that interesting outside of theory. The interesting problems are those without a polynomial-time verifier (something that, given an answer, says whether it is correct in a polynomial amount of time, the existence of which is the defining characteristic of problems in NP). Optimization problems are NP-Hard, not in NP. Well, this is nothing but speculative navel gazing, but I'm of the opinion that we simply haven't created a computer complex enough to solve NP complete-type problems *of non trivial size*. And I don't mean \"not enough power, needs more RAM.\" I mean the architecture and design of current computers is too simple; something radically different has to happen.\n\n*Edit*: I'm an idiot and misspoke., you guys are right. My point was that were still at the horse and buggy phase of computing, in my opinion. It's not just better parts, its whole new designs that we need.  We can solve NP complete problems just fine... Not really.  Our \"solutions\" that require non-exponential time are merely approximations. What's wrong with taking exponential time? Who said anything about time? Not really.  Our \"solutions\" that require non-exponential time are merely approximations. You're confusing 'cannot solve' with 'cannot solve in polynomial time'. Well, this is nothing but speculative navel gazing, but I'm of the opinion that we simply haven't created a computer complex enough to solve NP complete-type problems *of non trivial size*. And I don't mean \"not enough power, needs more RAM.\" I mean the architecture and design of current computers is too simple; something radically different has to happen.\n\n*Edit*: I'm an idiot and misspoke., you guys are right. My point was that were still at the horse and buggy phase of computing, in my opinion. It's not just better parts, its whole new designs that we need.  Well, this is nothing but speculative navel gazing, but I'm of the opinion that we simply haven't created a computer complex enough to solve NP complete-type problems *of non trivial size*. And I don't mean \"not enough power, needs more RAM.\" I mean the architecture and design of current computers is too simple; something radically different has to happen.\n\n*Edit*: I'm an idiot and misspoke., you guys are right. My point was that were still at the horse and buggy phase of computing, in my opinion. It's not just better parts, its whole new designs that we need.  ",
    "url" : "http://www.reddit.com/r/compsci/comments/13rccg/are_the_results_of_theoretical_computer_science/"
  }, {
    "id" : 19,
    "title" : "Your opinion on transferring colleges for Computer Science and Math.",
    "snippet" : "I am a freshman at the Georgia Institute of Technology, which I chose last year over the University of Maryland (College Park) for Computer Science. However, not only did I find that I don't like the south or the university, but now, Maryland has proven to be an even better computer science school than its ranking (14 to GT's 10) suggests by winning the ACM NorthEast finals. \n\nI am really interested in theory, which I think (because of UMD's strong  Math program) should be just as good at Maryland. I was also newly considering double-majoring in Math, which is tremendously easy at UMD and nigh- impossible here at Tech. To top it off, I'm also dreadfully homesick and I miss the north, where I plan to live when I'm older. However, I'm scared that I'd regret losing the \"name\" and the superior funding and tight- knit CS community that is here at Georgia Tech.\n\nPeople have said it's come down to where my heart is, and in the end, it will come down to that, when I figure out what it is that I want. Until then, could you please give me a view on the academic side of things? Much obliged.   I doubt ACM results mean anything. The success of a team is much more dependant on the dynamics of the team and how it trains than the school it is at.\n\nLook towards the program, not some number like that. ACM coding contests usually involve teams of 3 to 4 students who volunteer for the competition. It reflects only the students that participated, not the school.   The issue with Georgia Tech is what field of CS you want to study.  They've had more than a few \"genocides\". What do you mean by that?  when choosing what school I wanted to go to, GT was kinda the thing to do. I tried my absolute hardest to avoid going there if I could, but for the money (free for me as a GA resident) it was the best school around. Luckily I got a scholarship to an out of state engineering school that tends to be seen as lesser in value than GT, but I am extremely happy here.\n\nI have friends at GT but I know that most of them are nowhere near as happy as I am. I have a good working relationship with many of the professors in my department due to smaller classes and a better focus on undergraduate teaching rather than graduate teaching. From what I understand, this tends to go on a lot at Tech. Also, I have found that I don't really compete with any grad students to do research where I am now. I know the same is not quite as true at tech. \n\nI do plan on getting my masters at tech, but I just dont see it as that great of an undergraduate school.\n\nTLDR: Go to tech for grad school, not undergrad. ",
    "url" : "http://www.reddit.com/r/compsci/comments/13sudk/your_opinion_on_transferring_colleges_for/"
  }, {
    "id" : 20,
    "title" : "Gaussian Distributions form Monoids",
    "url" : "http://izbicki.me/blog/gausian-distributions-are-monoids"
  }, {
    "id" : 21,
    "title" : "1-dimensional bin packing: FFD vs Optimal",
    "snippet" : "I hope this is alright to ask here. This was posed as extra credit for my algorithms class, with any and all resources allowed, including the internet.\n\nThe assignment is to give an example of a case where the First Fit Decreasing solution to the 1-dimensional bin packing problem is exactly 71/42 OPT. Some additional information was given to us. First, this is an undergrad algorithms class, and nobody in the graduate class was able to solve it, so there must be some required techniques that we have not covered in class. Second, we were given a hint: in the TA's office posted on the door, there is a photo of the door itself with two Eyes of Horus. Third, the TA proved this a few years ago, but it was never published.\n\nCan anyone lead me in the right direction?   I'm probably misunderstanding something about your question, or maybe I don't know what FFD is, because this example is pretty easy to come up with:\n\n1 bin of size 71.\n3 objects of size 42, 36, 35.\nFFD will put in the object of size 42, while the optimal solution is 36+35. Sorry for the confusion. FFD is first-fit decreasing, which is what you did in your example. However, your example actually has the FFD solution equal to the optimal solution, because the thing we are interested in is the number of bins used to store the entire set of items. In this case, both algorithms use 2 bins, so FFD = OPT. So what I'm really looking for is an instance where FFD uses 71x bins, and OPT uses 42x bins.\n\nIt's also worth noting that there is a loose upper bound of 2. FFD will not leave two bins half-full, since if one bin is half-full, it will still be able to take more items that would have gone into the second bin.\n  How is this going? Any results?",
    "url" : "http://www.reddit.com/r/compsci/comments/13rkr0/1dimensional_bin_packing_ffd_vs_optimal/"
  }, {
    "id" : 22,
    "title" : "Question on the behavior of L1 cache given transactional memory...",
    "snippet" : "(If you know of a better subreddit for me to ask this, let me know)\n\nLet's say two threads T1 and T2 are running on the same CPU and therefore of course share the same L1 cache. If T1 brings in 0xbeef, and then T1 gets switched out for T2 and T2 also needs 0xbeef, does T2 get to access this data directly, or is the L1 swapped out and T2 re-requests it from again from L2?\n\nI would assume the former, but this picture gets muddier when then considering what happens with transactional memory. Same situation, but T1 speculatively modifies 0xbeef, and then T2 gets swapped in before T1 commits. Wouldn't T2 (incorrectly) see the uncommitted value, assuming the previous case? This is what led me to believe that T1 and T2 must see different state for 0xbeef in the L1. Either that or on a context switch L1 cache gets swapped out, but I have a hard time believing this to be the case.\n\nPerhaps processors that support transactional memory just keep extra state for each cache line in the L1 to say that the line only belongs to certain threads? But then doesn't this imply that a thread ID is stored with the speculatively changed cache line? Seems like a lot of overhead.\n\nThanks for any insight here.  In the non-transactional case you describe, if T1 writes the data to the L1 cache and then T2 reads that value, it will get the value from the L1 cache (since the data is stored there it will hit on a cache read).\n\nThe transactional case is more complicated for a number of reasons. The biggest is: what do you mean by swapping threads here? If T1 does not commit by the time a timer interrupt or something occurs, the behavior of hardware transactional memory is implementation specific. Some implementations would abort the transaction, rolling back T1 to before the transaction began, before executing T2. So the new value is discarded and T2 will read the old value. [This paper](https://agora.cs.illinois.edu/download/attachments/6848874/16_sosp2007-fp56-rossbach.pdf) proposes a way to integrate handling transactional memory into the operating system by pushing transaction state onto a transaction stack of sorts; I imagine that in this case T2 would still read the old value since T1 did not commit.\n\nIn any case, it is implementation specific what will happen on thread switches on a single processor. The answer is clear that T2 will read the pre-transaction state if it is running on a separate processor. I think the paper I linked to before and [this paper](http://www.cs.brown.edu/~mph/HerlihyM93/herlihy93transactional.pdf) linked to from the Wikipedia article can explain some of these cases. Great, thanks. Do you know much about [Load-Link/Store-Conditional](http://en.wikipedia.org/wiki/Load-link/store-conditional)? It seems the simple abort scenario where you force an abort on a context switch would work, but there could also be some optimization where a cache line is marked as having been load-linked, and therefore if it is stored to by another thread without a store-conditional then unmark that line, and therefore cause the store-conditional of the original thread to fail. In this case there is no need to keep track of which thread has performed the load-linked I don't believe. In the system used by Intel, IBM and Sun, the transaction state is tracked by adding two bits per hw thread into the tags for each line in the L1 cache. Switching out T1 does not immediately fail the transaction. When T2 requests the line used by T1, not only will the system compare addresses, it also compares the transaction state. Since they differ, there is no L1 hit, and the value is fetched from L2, and occupies another way in the L1 cache.\n\nShould T2 proceed to fetch 7 (in the Intel implementation) more aliasing addresses from the L2, the line with transaction bits set would be evicted, and this would cause a txn failure. Do you know where I can find a document describing the implementation details like this? I would like to know more about that. Afaik, you cannot find it in any publicly released documents yet. Any idea when (or if) those details will be publicly released? Great, thanks. Do you know much about [Load-Link/Store-Conditional](http://en.wikipedia.org/wiki/Load-link/store-conditional)? It seems the simple abort scenario where you force an abort on a context switch would work, but there could also be some optimization where a cache line is marked as having been load-linked, and therefore if it is stored to by another thread without a store-conditional then unmark that line, and therefore cause the store-conditional of the original thread to fail. In this case there is no need to keep track of which thread has performed the load-linked I don't believe.   I'm no expert on transactional hardware memory but I'm pretty sure that the design should prevent situation like the one you described. \n\nFor transactional hardware memory it is not sufficient to adapt the memory...processor, bus and cache coherence protocoll should be able to prevent a thread from being preempted while in a transaction. It is perhaps possible to keep two versions, but as you mentioned the overhead is quite extensive. So its better to not swap it out while the transaction is not commited. I'm no expert on transactional hardware memory but I'm pretty sure that the design should prevent situation like the one you described. \n\nFor transactional hardware memory it is not sufficient to adapt the memory...processor, bus and cache coherence protocoll should be able to prevent a thread from being preempted while in a transaction. It is perhaps possible to keep two versions, but as you mentioned the overhead is quite extensive. So its better to not swap it out while the transaction is not commited.  From what I just looked at, consumer hardware doesn't support it, it's implemented in software... so either the OS or code engine like the java vm would implement memory in a way to basically accomplish this.  \n\nhttp://en.wikipedia.org/wiki/Transactional_memory The new Intel Haswell microarchitecture will support Restricted Transactional Memory (RTM) as well as Hardware Lock Elision (HLE). \n\n[http://en.wikipedia.org/wiki/Transactional_Synchronization_Extensions](http://en.wikipedia.org/wiki/Transactional_Synchronization_Extensions)",
    "url" : "http://www.reddit.com/r/compsci/comments/13okv1/question_on_the_behavior_of_l1_cache_given/"
  }, {
    "id" : 23,
    "title" : "Could we work together to compile a listing of computer science professors, their subfield, affiliation, and past degrees?",
    "snippet" : "I'm the maintainer of the computer science [best paper awards page](http://jeffhuang.com/best_paper_awards.html) that occasionally pops up on this subreddit, e.g. [here](http://www.reddit.com/r/compsci/comments/ngx06/best_papers_in_computer_science_research_from_the/) and [here](http://www.reddit.com/r/compsci/comments/ethtg/every_best_paper_award_from_top_computer_science/).\n\nI think there could be tremendous value to the community in building a new resource: a spreadsheet/wiki of computer science faculty, and information about them. For example, [I did the one for University of Washington](http://sdrv.ms/WmYZgW) and their subfield, hiring year, title, and schools they went to earn their prior degrees.\n\nWhy is this useful?\n\n* Graduate applications: Students who want to apply to grad schools in Networking can immediately look up the active faculty in that area and their seniority (untenured vs tenured). It's also easy to see which universities have a presence in less-ubiquitous subfields like HCI or Information Retrieval.\n* We can find out academic hiring trends for different subfields in computer science: which subfields have top computer science departments hired in the past 5 years? How was academic hiring affected by the economy?\n* Department rankings: When DepartmentA hires a faculty from DepartmentB, it indicates that DepartmentA recognizes the quality of this student from DepartmentB. We can compute the PageRank for CS departments treating DepartmentA-&gt;DepartmentB as a link, giving us an objective ranking of CS departments. The current gold standard, US News, is [controversial](http://en.wikipedia.org/wiki/Criticism_of_college_and_university_rankings_(North_America\\)) because of it's based on subjective department chair surveys with poor response rates; this could be a USNews Killer, and include universities outside the United States.\n* Tons of other stuff on CS trends, correlation findings like: \"on average phd students will get a faculty job at a department 10 ranks below their phd department, but doing a postdoc will bump that up by 15 ranks.\"\n\nIf we had complete listings for the top 100 computer science departments, this would be amazingly useful already. I imagine it takes an hour or two to fill out the information for each department (which have 10-50 professors). The information is already out there on department websites, on faculty personal homepages, or in faculty CVs and bios. This means if a couple dozen people contributed a few departments each, we'd have a complete listing by the end of the year.\n\nIs this something people in this subreddit would be willing to contribute some time for? This could be a useful resource for the community, but I need your help.\n\n**Update:** This is going to happen! So far I've recruited half a dozen people, and sent PMs to those who commented. Reply back with your email so I can share the document with you.   Maybe link to their thesis supervisor as well and you have a family tree. You mean like the [Mathematics Genealogy Project](http://genealogy.math.ndsu.nodak.edu/)?\n\nPS: on the original proposal, it felt a bit too US-centered to me. (For a start, the concept of \"university CS department\" doesn't necessarily make sense in countries that have research facilities outside universities). well, we have research facilities outside universities in the US too -- i think OP just didn't think of them. I see no reason they couldn't be added  This sounds like a job for Mechanical Turk. That was my original plan, but I calculated that it would take a decent chunk of change, especially if we want to include some redundancy for better coverage.\n\nThere may also be some specialized knowledge required (like interpreting what subfield a professor is in, what is a postdoc, etc.).\n\nUpdate: I went and did [one university](http://sdrv.ms/WmYZgW) and indeed there is a lot of specialized knowledge needed to put together. People write their bios and CVs in a very non-standard format and I think it would be difficult for a non-academic to do this. There's plenty of work on resumes in the information extraction literature. Have you looked there at all? \n\nThere's also work on discovering research trends and researcher networks that might lead to some ready-made coding solutions for what you want to do here.       Jeff, how come your list doesn't have any of the computer architecture conferences (ISCA, MICRO, HPCA, ASPLOS)? ISCA is probably the most competitive of the top-tier architecture conferences. good question, ferromagnificent. there's no particular reason -- I just haven't compiled the lists. if someone emails me the awards lists, I'm happy to format and post I would be particularly interested in seeing these lists!     Most of these people will already be on linkedin - any way to mine the info from there?  Microsoft seems to have done this already:\n\nhttp://academic.research.microsoft.com/RankList?entitytype=2&amp;topDomainID=2&amp;subDomainID=0  ",
    "url" : "http://www.reddit.com/r/compsci/comments/13ntq4/could_we_work_together_to_compile_a_listing_of/"
  }, {
    "id" : 24,
    "title" : "MathJax for reddit?",
    "snippet" : "Wouldn't r/compsci and math related subreddits benefit from [MathJax](http://www.mathjax.org/) support? I would love to be able to do something like $x\\^2$ etc. and have it render correctly.  **From /r/math:**\n\n\nUsing LaTeX\n\n[Greasemonkey plugin](http://userscripts.org/scripts/show/92758)\n\n[This plugin](https://chrome.google.com/webstore/detail/mbfninnbhfepghkkcgdnmfmhhbjmhggn) works better on Chrome.\n\nPlugin based on TeX The World and modified by wsdenker.\n[; e^{\\pi i}+1=0 ; ]\n\nPost the equation above like this:\n`[; e^{\\pi i}+1=0 ; ]`\nYou may need to add four spaces before or put backticks around math fragments.  ",
    "url" : "http://www.reddit.com/r/compsci/comments/13n2sg/mathjax_for_reddit/"
  }, {
    "id" : 25,
    "title" : "Looking for statistics on assembly instruction breakdown per benchmark programs; or methods to do so",
    "snippet" : "I am looking for breakdown and statistics of the types of assembly instructions that occur running certain popular programs/benchmarks, e.g. \n\nProgram: Angry Birds (lol I don't know); CPU usage:\n\n* ADD/MUL/SUB/LOGICAL - 50%\n\n* LOAD - 10%\n\n* STORE - 10%\n\n* FLOATING POINT - 5%\n\n* CONDITIONAL JUMP - 15%\n\n* UNCONDITIONAL JUMP - 10%\n\n\nGiven that these may or may not be available for various software, what are some techniques and tools I can use to run a program, save what instructions were executed, and then analyze it myself after? \n\nIn terms of platforms, I'm mainly interested in ARM and other RISC architectures, but x86 microcode would be cool as well and might be easier to obtain I think. \n\nThanks   You can use [objdump](http://en.wikipedia.org/wiki/Objdump) and basic text processing to do this.  For example, \n\n$ objdump -d executable | grep mov | wc\n\nwill output the number of mov instructions in the executable file.\n\nKeep in mind that this will output *all* instances of the instructions as they exist statically.  If you wish to get an actual count of the frequency of instructions executed with particular input, I suggest you look into using gdb or valgrind. You can use [objdump](http://en.wikipedia.org/wiki/Objdump) and basic text processing to do this.  For example, \n\n$ objdump -d executable | grep mov | wc\n\nwill output the number of mov instructions in the executable file.\n\nKeep in mind that this will output *all* instances of the instructions as they exist statically.  If you wish to get an actual count of the frequency of instructions executed with particular input, I suggest you look into using gdb or valgrind. You can use [objdump](http://en.wikipedia.org/wiki/Objdump) and basic text processing to do this.  For example, \n\n$ objdump -d executable | grep mov | wc\n\nwill output the number of mov instructions in the executable file.\n\nKeep in mind that this will output *all* instances of the instructions as they exist statically.  If you wish to get an actual count of the frequency of instructions executed with particular input, I suggest you look into using gdb or valgrind. Where \"executable\" is one of those gcc-compiled C files? I think it gets a name like \"a.out\" or something.      [deleted] Why not? [deleted] [deleted] What a terrible question. Not only do you not make it clear what you are looking for, but you are insinuating that the OP's question is of little value. If you want clarification on something just ask for it. Analyzing instruction frequencies very common in hardware research and quite interesting. There is no reason to belittle the OP. [deleted] Have you never profiled your code?  If so, then you suck at what you do and you're also a dick.  OP's question is interesting.  Given that on different platforms, different instructions have varying cycle counts, this information could be useful in optimizing certain algorithms.  I guarantee the data that OP is looking for is something that people at nVidia, Intel, AMD, Microchip, etc have all sought out and used to implement compiler optimizations. [deleted]",
    "url" : "http://www.reddit.com/r/compsci/comments/13n3kh/looking_for_statistics_on_assembly_instruction/"
  }, {
    "id" : 26,
    "title" : "What's the most exciting paper or project you've recently seen?",
    "snippet" : "  [Halide](http://people.csail.mit.edu/jrk/halide12/) is a cool domain-specific language for image processing applications.  (From SIGGRAPH 2012)\n [Halide](http://people.csail.mit.edu/jrk/halide12/) is a cool domain-specific language for image processing applications.  (From SIGGRAPH 2012)\n [Halide](http://people.csail.mit.edu/jrk/halide12/) is a cool domain-specific language for image processing applications.  (From SIGGRAPH 2012)\n [Halide](http://people.csail.mit.edu/jrk/halide12/) is a cool domain-specific language for image processing applications.  (From SIGGRAPH 2012)\n [Halide](http://people.csail.mit.edu/jrk/halide12/) is a cool domain-specific language for image processing applications.  (From SIGGRAPH 2012)\n [Halide](http://people.csail.mit.edu/jrk/halide12/) is a cool domain-specific language for image processing applications.  (From SIGGRAPH 2012)\n  If you're into computer graphics \n\nReal time raytracing in game simulations\n\n[Paper](http://igad.nhtv.nl/~bikker/files/thesis_jbikker.pdf)\n\n[Demo] (http://igad.nhtv.nl/~bikker/)\n Yes, his raytracer is very nice, possibly the nicest realtime demo available with source. Unfortunately its not FOSS. I once asked him in an email about releasing it under FOSS. He was rather against it, mainly against it being used commercially; apparently some companies were *already* using it (apparently illegally). It is however free for non-commerical use, under a non standard license.      This probably fits more to engineering, but I was at the Southern California Conference for Undergraduate Research (SCCUR) last weekend and saw someone who was looking into how the brain reacts when you move. He is looking to find a way to make prosthetic limbs be more workable like a real human one.    [deleted] Dude, that's just a game... [deleted] Dude, that's just a game...",
    "url" : "http://www.reddit.com/r/compsci/comments/13lo53/whats_the_most_exciting_paper_or_project_youve/"
  }, {
    "id" : 27,
    "title" : "Today we remember Jacques de Vaucanson, one of the first automaton inventors",
    "snippet" : " ",
    "url" : "http://en.wikipedia.org/wiki/Jacques_de_Vaucanson"
  }, {
    "id" : 28,
    "title" : "Particle Object Programming",
    "snippet" : "What sort of keywords would I research regarding treating everything in a three dimensional world as connected particles, rather than conventional primitives?\n\nI am thinking toward persistent destructable environments and sprites. I assume that each particle would be an object with its own attributes... But that seems overly resource intensive...\n\n------------------------------\nAfter some initial reading, voxel raycasting seems very similar to what I'm imagining. I need to research some more, but allow me to bounce some thoughts off of the community for a moment:\n\nMy musings treat the entire environment as connected voxels, each one with attributes that define how it interacts with its neighbors. Essentially, I consider what I'm attempting to describe analogous to atoms, albeit on a larger scale. This approach is entirely separate from the rendering of the world. I'm merely describing how things exist in order to allow exciting, dynamic gameplay. Then, to render this world to a players display, I would refer to the raycasting. Vectors would be calculated from the point of viewing and terminate at objects bearing visual data...\n\n------------------------------\nWhat if, using voxel octrees, you increase the resolution of an object anytime there is a physical imperative? For example, a tree would have its resolution increased when there is an axe chopping into it.\n\n------------\n------------\n------------\n\nTL;DR - I'm interested in a completely destructible game environment and want to find out what's going on in the field in regards to such an end.  You are effectively looking at voxels (like in minecraft) or particle/point clouds.  Incredibly interesting. I believe I was shown a video of this group who is doing some sort of polygonless modeling. I wish I could remember more about it, but it is being done. Please respond if you find anything out. http://www.youtube.com/watch?v=Q-ATtrImCx4\n\nUnlimited Detail Technology. Even with the explanation the guy gives in his video description, I'm not sure I quite believe it. Seems mighty wishy washy to me. http://www.youtube.com/watch?v=Q-ATtrImCx4\n\nUnlimited Detail Technology. Even with the explanation the guy gives in his video description, I'm not sure I quite believe it. Seems mighty wishy washy to me.  ",
    "url" : "http://www.reddit.com/r/compsci/comments/13l066/particle_object_programming/"
  }, {
    "id" : 29,
    "title" : "Non-trivial language with a decidable halt",
    "snippet" : "  That was less interesting than I had hoped, but still somewhat interesting. They can identify, in finite time, the set of functions that ignore their arguments. That was less interesting than I had hoped, but still somewhat interesting. They can identify, in finite time, the set of functions that ignore their arguments.  By the way, there are two reverse problems that bother me: is there a language with an undecidable halt that is not powerful enough to solve TM-halting problem, and is there a non-trivial language with a universal function that is not Turing-complete?\n\nThe first one seems to be the Post's problem, do I understand correctly that proofs of existence are non-constructive? &gt;is there a language with an undecidable halt that is not powerful enough to solve TM-halting problem\n\nWouldn't this just be any Turing-complete language? &gt;is there a language with an undecidable halt that is not powerful enough to solve TM-halting problem\n\nWouldn't this just be any Turing-complete language? I think he might mean a language with undecidable halt that is not turing-complete? Hmm... That's actually quite interesting... the (classic) halting problem proof is constructed around the fact that the supposed oracle is also supposed to be a TM and hence can generate paradoxical self-references.  I don't know how you'd do that if the supposed oracle is a TM but the input is *not*, since you can't make a strange loop that way. As far as I remember it doesn't rely on the oracle being a turing-machine, it only relies on the input provided to it (and the program it's wrapped in, which is the same thing, of course) being able to loop indefinitely.    See [Total functional programming](http://en.wikipedia.org/wiki/Total_functional_programming), which mentions [epigram](http://en.wikipedia.org/wiki/Epigram_(programming_language), a language which is not Turing complete, but basically allows you to program anything you can prove will halt. The truth is, all our functions/programs should halt! Except maybe an outer event loop, but in general we as programmers should know that our functions halt, or exactly when they don't. Otherwise we have a bug!\n\nSo in reality, IMO, this should be the ideal. Our current class of programming languages lets us hang ourselves by giving us enough rope.\n\n\n See [Total functional programming](http://en.wikipedia.org/wiki/Total_functional_programming), which mentions [epigram](http://en.wikipedia.org/wiki/Epigram_(programming_language), a language which is not Turing complete, but basically allows you to program anything you can prove will halt. The truth is, all our functions/programs should halt! Except maybe an outer event loop, but in general we as programmers should know that our functions halt, or exactly when they don't. Otherwise we have a bug!\n\nSo in reality, IMO, this should be the ideal. Our current class of programming languages lets us hang ourselves by giving us enough rope.\n\n\n",
    "url" : "http://okmij.org/ftp/Computation/decidable-halt.html"
  }, {
    "id" : 30,
    "title" : "Chained operations on sequences with two operators",
    "snippet" : "  The question is all over the place, I suggest narrowing it to one refined point in order to encourage more responses. ",
    "url" : "http://cs.stackexchange.com/q/6790/2755"
  }, {
    "id" : 31,
    "title" : "What are the main differences between the different deep learning approaches?",
    "url" : "http://metaoptimize.com/qa/questions/11439/what-are-the-main-differences-between-the-different-deep-learning-approaches"
  }, {
    "id" : 32,
    "title" : "How to peer review scientfic work",
    "snippet" : "  Wow, I've spent nearly the whole evening on this guy's web site. Great resource.  &gt;I've written a few hundred peer reviews.\n&gt;There have been times when it was mathematically impossible to review only one paper per day. \n\nIt takes me about a week to make a comprehensive peer review. Just sayin... Look a bit who the author is before implying he's lying. I'm not trying to imply he's lying, just that maybe he doesn't expend *that* much effort in his reviews if he can perform them in under a day. In particular verifying proofs of mathematical statements is both tedious and indispensable.\n\nI also find arguments from authority to be meretricious. Just because he is an excellent researcher does not vouch for his integrity or innate skill at reviewing. On a side note, I do agree with most of his advice in this case. What area do you work in, and how long do papers tend to be?\n\nEven accounting for those factors, not all CS papers are heavily laden with mathematical statements and proofs.\n\nAlso, for someone on a PC that receives hundreds of paper submissions, it would literally be impossible to not review more than one paper a day at some times. If you spent one week on each paper, you likely wouldn't be able to meet your obligations for a single conference, even if that was the only thing you did for months. Formal methods and theory (type theory, rewriting), papers are approximately 15-20 pages long minus appendices. In the last 3 papers I have reviewed, I have found mathematical mistakes in the details, some of which required significant effort to resolve. It seems to me that with such short review times, mathematical mistakes are sure to slip through. This is not necessarily dramatic.\n\nIt's interesting to hear your take on the issue. I guess I was under the mistaken impression that PCs were more involved in the coordination of the many reviews rather than carrying them out themselves. I'm sure the former activity necessarily entails a fair amount of the latter though. I'm not trying to imply he's lying, just that maybe he doesn't expend *that* much effort in his reviews if he can perform them in under a day. In particular verifying proofs of mathematical statements is both tedious and indispensable.\n\nI also find arguments from authority to be meretricious. Just because he is an excellent researcher does not vouch for his integrity or innate skill at reviewing. On a side note, I do agree with most of his advice in this case. I'm not trying to imply he's lying, just that maybe he doesn't expend *that* much effort in his reviews if he can perform them in under a day. In particular verifying proofs of mathematical statements is both tedious and indispensable.\n\nI also find arguments from authority to be meretricious. Just because he is an excellent researcher does not vouch for his integrity or innate skill at reviewing. On a side note, I do agree with most of his advice in this case. Look a bit who the author is before implying he's lying. Who's Matt Might?  That name doesn't have any recognition with me. You can look him up, but he's pretty well known IMHO. Okay, I looked him up.  There doesn't appear to be a Wikipedia article on him, so it stands to reason that he isn't noteworthy enough to have one or not enough people have interest in him that someone has taken the time to document him (once again, noteworthy).  He writes a bunch of articles that seem pretty much programmer oriented, with his most popular being the Illustrated Guide to a PhD, which is not exactly groundbreaking research.\n\nYou know someone who is pretty well known?  Terry Tao.  At only 37 this guy has a section on Wikipedia just to list his awards.  If one were to make a call to authority as p4bl0 (and tangentially you) did, one should point to a figure like Terry Tao, not to someone best known for their illustrative comic.\n\nI don't mean to cut at Matt Might, he's doing a fine job.  But his name alone isn't some sort of stamp for \"this is good\".  So there's no blind review in CS? In computer science, the overwhelming majority of reviews are blind in the sense that authors do not know who their reviewers are. Double-blind reviews, in which neither the reviewer nor the reviewee(s) knows the identity of the other, are overwhelmingly rare, because of tradition and the fact that it is ofter quite easy to determine the identity of the author of a paper. In my field (a subfield of machine learning) almost all conferences are double-blind, and almost all journals are blind in your first sense. In computer science, the overwhelming majority of reviews are blind in the sense that authors do not know who their reviewers are. Double-blind reviews, in which neither the reviewer nor the reviewee(s) knows the identity of the other, are overwhelmingly rare, because of tradition and the fact that it is ofter quite easy to determine the identity of the author of a paper. Programming languages have moved almost entirely to double-blind. Well shucks I'm going to have to take back my comment. I've never encountered it I must admit. In AI/Optimization it's still fairly mixed. Some conferences are single blind, some double. I just submitted papers to two single blind conferences, so I think CS is still a fairly mixed field. No need to recant your comment. So there's no blind review in CS?",
    "url" : "http://matt.might.net/articles/how-to-peer-review/"
  }, {
    "id" : 33,
    "title" : "On Evaluation Contexts, Continuations,\nand the Rest of the Computation [pdf]",
    "url" : "http://www.cs.bham.ac.uk/~hxt/cw04/danvy.pdf"
  }, {
    "id" : 34,
    "title" : "The Toaster-Enhanced Turing Machine",
    "snippet" : " ",
    "url" : "http://www.scottaaronson.com/blog/?p=1121"
  }, {
    "id" : 35,
    "title" : "Question on using MPI and Blender for parallel rendering",
    "snippet" : "I am thinking about trying to use MPI and blender to create a \"render farm\" on my universities cluster system.\n\nMy question here is will MPI have issues with me using multiple instances of blender to render individual frames on their own processes. I'm afraid each process will have idle time until the first process finishes rendering, or will they all render on individual processes in parallel like they're suppose to.  I'm obviously not well informed on MPI and I would like to try to become familiar with it. \n\nAlso, the only way I can figure out how to use blender to render in this way is to use a sysop call to blenders command line arguments.  Is this the proper way to do it?\n\nThanks for your help,\nRobb  MPI is just a message passing interface. How the rendering happens is entirely up to your application.\n\nYou will need to determine a way to load balance all your nodes, and you can than use MPI to pass the data between each node. If blender is doing some form of raycasting or volume rendering all the nodes will have to render each frame in lockstep since the nodes will have to communicate between each other to determine things ray casting intersections.\n\nWork has been done on using a map reduce framework for volume rendering, and it does show promise, but it does require a more complex implementation of volume rendering since you are not projecting a point through the entire scene. Raycasting is something I would like to implement but volume rendering isn't something I've ever had to work with.\n\nYour reply is extremely helpful.  Have you had any experience in this?    Can't a simple mapreduce like system be used for that. Well yes it could but the cluster system at my university is set up specifically for mpi. That's what I have to work with, unfortunately.\n\nThanks for the input though ",
    "url" : "http://www.reddit.com/r/compsci/comments/13j73g/question_on_using_mpi_and_blender_for_parallel/"
  }, {
    "id" : 36,
    "title" : "Map of fields and subfields of computer science?",
    "snippet" : "Does anyone know where I can find a visual map of the \"tree\" that is the fields and subfields of computer science (if one does exist)?\n\nFor example, [this](http://www-rohan.sdsu.edu/~gawron/fundamentals/course_core/lectures/historical/pie2.gif), but for the different branches of computer science.\n\nOr, more exactly, I'm looking for a more visual form of [this](http://en.wikipedia.org/wiki/Outline_of_computer_science), but possibly more in-depth.\n\nAny help is appreciated!\n\n\nEdit: I found what I was looking for. The Association for Computing Machinery released [this](http://dl.acm.org/ccs.cfm?id=0&amp;lid=0&amp;CFID=205255578&amp;CFTOKEN=68139105) wonderful guide. While it's not as visual as I would have liked, it's still well-defined.  I don't know if a tree is the best way to see this, perhaps a directed graph. It's often going to be the case that one area of CS draws from more than one area, and influences more than one area. agreed. Gephi would be a great way to vis this up. Maybe I have a new project for the Thanksgiving break... I don't know if a tree is the best way to see this, perhaps a directed graph. It's often going to be the case that one area of CS draws from more than one area, and influences more than one area. If you wanted to make a tree out of that digraph I'd guess you could group all the strongly connected components into nodes and try to give each node a name. Though you might just get only one strongly connected component.... maybe weight the edges by how much one field is related to another and only use edges of a certain weight or higher in computing the strongly connected components.  I don't know if a tree is the best way to see this, perhaps a directed graph. It's often going to be the case that one area of CS draws from more than one area, and influences more than one area. Perhaps....a linked list? or you could just throw it all into a heap  Hash tables are the way to go. (Except not really; they're pretty much the opposite of what we're looking for. But you guys took all the good ones!) [deleted] You're spending too much time here, then! I promise not to contribute to any more \"theme\" threads (like pun threads, song threads, \"As an NPC, it seems to be in my program to upvote this\" threads, data structure threads, etc.) in this subreddit. (Or in general. I usually dislike them. I liked this one because it was relatively original.) Are they common here? or you could just throw it all into a heap  Perhaps....a linked list?    ",
    "url" : "http://www.reddit.com/r/compsci/comments/13hfqj/map_of_fields_and_subfields_of_computer_science/"
  }, {
    "id" : 37,
    "title" : "Idioms are oblivious, arrows are meticulous,\nmonads are promiscuous [pdf]",
    "url" : "http://homepages.inf.ed.ac.uk/wadler/papers/arrows-and-idioms/arrows-and-idioms.pdf"
  }, {
    "id" : 38,
    "title" : "Abstract interpretation and type systems",
    "url" : "http://lambda-the-ultimate.org/node/2208"
  }, {
    "id" : 39,
    "title" : "NLP Research Suggestions Anyone?",
    "snippet" : "Hey All,\n\nI am interested in learning about some broad areas of NLP (or anything else you think might help) that can point me in the right direction with a project I'm starting for work.\n\nEssentially, we have a massive database of 1 to 5 paragraph length natural language strings that describe issues from a specific problem domain, mainly computer networks.\n\nGiven this huge corpus of data, we would like to be able to determine when two paragraphs are in fact describing the same thing so that we can  filter through the data and do some aggregation.\n\nI'm not looking for any depth here, but if you know of any particular research projects taking place that I could investigate further, I would appreciate it.\n\nI'm already looking at quite a bit of stuff, including comparing trees produced using context free grammars, LSA, stochastic methods, keyword extraction, etc, but I thought I would ask around and see if maybe someone with more experience in the area had any suggestions.\n\nThanks for any help!  The tl;dr of NLP for practical applications goes like this:\n\nCan your problem be solved by n-gram frequency based methods?\n\n* Yes --&gt; Do that\n* No --&gt; Do nothing\n\nSo basically, I'd write a program that did TF-IDF similarity of the paragraphs, and look at its output. If it works, use it. If it's crap then throw up your hands and declare the problem too hard. The tl;dr of NLP for practical applications goes like this:\n\nCan your problem be solved by n-gram frequency based methods?\n\n* Yes --&gt; Do that\n* No --&gt; Do nothing\n\nSo basically, I'd write a program that did TF-IDF similarity of the paragraphs, and look at its output. If it works, use it. If it's crap then throw up your hands and declare the problem too hard. please define when should we consider an approach 'works' If it does something better than nothing. For instance, if using a TF-IDF similarity measure to display 10 candidate records that might be identical to a given record allows a human to quickly delete duplicates, then that works.\n\nIf you needed an evaluation measure, you'd pay someone on freelancer.com or elance.com etc to annotate a bunch of records for duplicates, and then evaluate how your system's true positive and false negative rates are looking. Ultimately you're going to care about how quickly the system lets a human deduplicate your records, which can also be measured (and is going to be determined largely by the TP/FN rates).\n\nIt's often difficult to design a system that does usefully better than random at this sort of task. Sometimes, it's totally infeasible given current technologies.\n\nOh, and if your application context is fault-intolerant AND putting a human in the loop is infeasible, then NLP simply won't be useful. Errors are absolutely inevitable given current technology.\n    &gt; Given this huge corpus of data, we would like to be able to determine when two paragraphs are in fact describing the same thing so that we can filter through the data and do some aggregation.\n\nYou basically want to find out if two piece of texts are **semantically** related to each other. The difficulty of this task depends on multiple factors -\n\n1) Is your text clean? By clean, I mean does it contain short forms or are they formatted clean English text.\n\n2) Are you able to detect paragraphs? Trivial but important.\n\n3) Do the words in your text fall into a vocabulary resource? By vocabulary resource I mean, do you have a dictionary for the words in your texts? This resource can be specific to your domain of computer networks. \n\nIf all the above points are true, you can look at various methods. The simplest I can think of is clustering using Point-wise Mutual Information scores between n-grams of paragraphs [0]. This should be a nice starting point. It is important to note that the more domain information resources you can include the better your technique would perform.\n\n[0] http://www.ling.uni-potsdam.de/~gerlof/docs/npmi-pfd.pdf   Hey man, I'm a big fan of NLP, but it is my understanding that this is a computer science subreddit, not psychology related. Hey man, I'm a big fan of NLP, but it is my understanding that this is a computer science subreddit, not psychology related.",
    "url" : "http://www.reddit.com/r/compsci/comments/13fh75/nlp_research_suggestions_anyone/"
  }, {
    "id" : 40,
    "title" : "Booth's multiplication method and the final “correction” adding",
    "snippet" : "I have a question regarding the second method of Booth's multiplication algorithm (at least that's how it's called in the script I'm using - I mean the one when we base our operations on analyzing single bits from the multiplier as opposed to analyzing pairs in the first method). I get it, but there is a disclaimer at the end reading:\n\n    \"Please note that there may be a need to add a correction (equal to multiplicand/2) to the final result. Whether we need it or not is dictated by the sign.\"\n\nProblem is, I don't know which sign. Sign of the result? Of one of the factors? When do we do it - before the final shift to the left, after it? I read many examples and nowhere have I found any involving the said correction adding. Could someone please clarify?  Instead of looking for a prepackaged example or waiting for someone to drop by and explain it to you, why don't you take the correct approach by working through a couple calculations on your own, seeing when the correction is necessary, and reasoning out why it was necessary based on your calculations?",
    "url" : "http://www.reddit.com/r/compsci/comments/13ef5u/booths_multiplication_method_and_the_final/"
  }, {
    "id" : 41,
    "title" : "Hadamard matrix rank",
    "snippet" : "Is a Hadamard matrix's rank always equal to the number of it's rows?\n\nIf not, what about a Hadamard matrix with zeros instead of -1?\n\nI couldn't find good answers on google.  According to Mathworld, HH' = nI is a definition of the conditions for a Hadamard matrix H. Since I is full rank and rank(AB) &lt;= min(rank(A), rank(B)), then rank(H) for such a matrix must be n. You don't even need to concern yourself with the inequality; by definition, all rows are orthogonal to each other and are therefore linearly independent. According to Mathworld, HH' = nI is a definition of the conditions for a Hadamard matrix H. Since I is full rank and rank(AB) &lt;= min(rank(A), rank(B)), then rank(H) for such a matrix must be n.  \"A Hadamard matrix is a square matrix whose entries are either +1 or −1 and whose rows are mutually orthogonal.\"\n\nBecause the rows are orthogonal that implies they will be linearly independent therefore it will always be full rank. \n\nReplacing -1's with 0's is equivalent to the following transformation: \n\nLet H be a Hadamard matrix. \n\nLet H' be the same Hadamard maxrix with 0's instead of -1's. \n\nTherefore H' = (H + 1)./2  /* *This bumps the 1's to 2's and the -1's to 0's. Then divides by 2 to change the 2's back to 1's. 0's are unaffected.* */\n\nThese operations do not effect the rank of H. \n\nTherefore rank(H') = rank(H) = rows(H) = cols(H). \\qed The operation does change the rank of the matrix since you could obtain a row with all 0's - consider the Hadamard matrix [-1]. Ah, true. But that is a special case. Let me amend the proof with a condition: If there is no row or column of all -1's. \n\nThen again, if your goal is to replace -1's it might be alright to 'interpret' the 0 vector as a valid linearly independent vector. (of course you better be careful to check that properties still hold, you can forget about dot products).  ",
    "url" : "http://www.reddit.com/r/compsci/comments/13czl4/hadamard_matrix_rank/"
  }, {
    "id" : 42,
    "title" : "Generic monadic constructs for embedded languages [pdf]",
    "snippet" : "  I'm one of the authors of the paper and I'm flattered to see it posted here on /r/compsci. I'd be happy to answer any question you might have on the paper.\n\nOne thing I've come to realize about the paper is that some of the types can be hard to understand. We're using the syntactic library to represent our AST and it uses a lot of type level hackery to achieve it's magic. I'd be happy to help clarify things for confused readers.\n\nI'd also like to mention that the paper won the Peter Landin prize for best paper at IFL. 1) Was my summary below any good? I'm not sure if I did the paper justice. \n\n2) How tied together are feldspar and syntactic? Did the library grow from developing the language? How easy do you think it would be to use syntactic for dramatically different semantics (i.e. message passing actors or typed representations of assembly or something else crazy)\n\n3) How does syntactic compare to the [typed tagless DSLs](http://okmij.org/ftp/tagless-final/index.html) that OlegK and others have written about?  ",
    "url" : "http://www.cse.chalmers.se/~josefs/publications/paper21_cameraready.pdf"
  }, {
    "id" : 43,
    "title" : "A More Interesting \"We the People\" Petition: Implement a Policy for Declassifying Aging and Unused NSA Discoveries (x-post from r/math)",
    "snippet" : "  In accordance with Executive Order 13526, published January 5, 2010, documents are automatically declassified 25 years after the day they are originally classified.  \n\nSource:  Wikipedia, and me (I've classified some documents for the U.S. government in my day)\n\nI understand 25 years is a long time, and a shorter period would be nice, but I just wanted to point out that there is in fact already a \"policy for declassifying aging and unused NSA discoveries.\" Very good point. But they might not be following it. From [Executive Order 13526](https://docs.google.com/viewer?a=v&amp;q=cache:cPJ7L9m5C1oJ:www.archives.gov/isoo/pdf/cnsi-eo.pdf+&amp;hl=en&amp;pid=bl&amp;srcid=ADGEEShN81wh-XUtuNTUkCYyDZXacfvSjmDm0sPs7yFcun323Z7izsy8twGHR_l7UK735yaYHIEh3GIJkUvn_bj0mUACDRdMDTXlnaxQpOVMAFR0SJbpe6KuHLzy-SCo0h5dRpzS1r-g&amp;sig=AHIEtbTgFUginZpwTf-WjVyentzzP2giZw):\n&gt;Sec. 1.5. Duration of Classification. (a) At the time of original classification, the original classification authority shall establish a specific date or event for declassification based on the duration of the national security sensitivity of the information. **Upon reaching the date or event, the information shall be automatically declassified. Except for information that should clearly and demonstrably be expected to reveal the identity of a confidential human source or a human intelligence source or key design concepts of weapons of mass destruction, the date or event shall not exceed** [**25 years**, emphasis mine] the time frame established in paragraph (b) of this section.\n\nFrom the [NSA Website section on Declassification and Transparency](http://www.nsa.gov/public_info/declass/index.shtml):\n&gt;Under the provisions of Executive Order 13526, (Classified National Security Information), dated 29 December 2009, NSA **reviews for declassification** all permanently classified documents 25 years or older. \n\nIt could be argued that the review process is in keeping with the exception clause, but compliance, and what constitutes clearly and demonstrably, is reviewed internally. They've also chosen the maximum possible time frame in which to release the information. And new discoveries should be pre-reviewed before, not after, the mandatory declassification deadline. I agree. As I pointed out below, I think we should be petitioning a change to the existing policy, not a brand new one. Demonstrating to those that would read this petition that we have a firm understanding of the current policy may help us to be taken more seriously.  In accordance with Executive Order 13526, published January 5, 2010, documents are automatically declassified 25 years after the day they are originally classified.  \n\nSource:  Wikipedia, and me (I've classified some documents for the U.S. government in my day)\n\nI understand 25 years is a long time, and a shorter period would be nice, but I just wanted to point out that there is in fact already a \"policy for declassifying aging and unused NSA discoveries.\" I see your point, but I think it can be fairly argued that, when it comes to cryptography, a 25-year policy is essentially no policy at all. It only applies to results which are obsolete in the vast majority of cases (are there any counterexamples? results which were first published because of the 25-year deadline that were actually novel and useful?). I see your point, but I think it can be fairly argued that, when it comes to cryptography, a 25-year policy is essentially no policy at all. It only applies to results which are obsolete in the vast majority of cases (are there any counterexamples? results which were first published because of the 25-year deadline that were actually novel and useful?). If a technique or discovery is useful there is little sense in releasing it to the public.  Even if the technique is discovered by third parties at some later date it is very useful to keep the NSAs knowledge of the technique secret especially if that means that our opponents operate in the belief that old secrete have not been lost yet.  \n\nIn the end this whole idea that the NSA or for that matter any other branch of the government should be forced to give up what in effect are trade secrets is asinine.  The techniques developed there cost tax payers millions and should not be given away on a whim especially when such developments can be used against us.  ",
    "url" : "http://wh.gov/XqC8"
  }, {
    "id" : 44,
    "title" : "Do most US cities use genetic algorithms for optimizing traffic?",
    "snippet" : "  I work on software related to this field. The short answer is no.\n\nLet me take you through what makes up our traffic light systems here in the United States. Most cities, if they can afford it, hire [traffic engineers](http://en.wikipedia.org/wiki/Traffic_engineering_(transportation\\)). These engineers don't usually use software to figure out timing plans, just mostly theorems and equations they learn in school (about modeling traffic given road width, speed limit, connectedness, etc). The field itself is not as digital as you'd like. It's getting there though. Jobs in city planning are only about as technical as being the one guy who knows how to use [ArcGIS](http://en.wikipedia.org/wiki/Arcgis) and script it, or being the guy who changes the timing plan configurations for an intersection.\n\nTraffic lights are controlled by computers known as traffic light controllers. Each light controller will typically control only one intersection, but some cities will use a controller for 2 or more intersections for cost savings or because the traffic pattern is complex enough that they need both intersections aware of the other. There are somewhere on the order of 15-20 different light controller manufacturers, each with different feature sets (wapiti, econolite, siemens, etc). Old light controllers require that a traffic engineer go to the intersection they want to change, and make their timing changes there. The newer light controllers, on the other hand, all have network interfaces, to allow them to be wired up to some centralized location. Some of these controllers are read only, i.e. if you want to change their timing, you still have to physically go to the intersection. Most of the new ones these days allow remote modification to their timing plans.\n\nTo facilitate these modifications, traffic management systems (produced by siemens, transcore, mccain, etc) were created to interface with a bunch of different kinds of light controllers to provide a centralized location to manage timing plans. If you've seen the [Italian Job](http://www.youtube.com/watch?v=r4IBPd7-LlQ&amp;feature=fvwrel), the stuff in the movie is only barely possible, and only in big cities. I've seen the systems used, and they're no where near as fancy as in the movie. The light controllers are usually on a private network that goes straight to the central location too, not much room for hacking.\n\nSo, the long answer is that while some computer systems are in place, they aren't always available, the software is pretty crappy, and the cities haven't adopted a workflow which takes the data they can get and uses it in their process (in a way that computer science could help with). Welcome to real world software :/. However, if you're interested in this sort of thing, do some research on the subject. There are some programs in academia that are trying to push forward traffic optimization. I've even heard of a professor working with a town, who have given him some 10 intersections to globally optimize, to eventually include all the controllers in the town. Can't remember the name at the moment, but there are people starting to do this out there.\n\nI'm not going to say what I work on, but I will say that if a commercial (or academic) entity that you work for is interested in receiving data about the status of traffic lights in your city, private message me. Really?  Even cities like LA, with tons of cash and a huge incentive to do better, don't do this? As befits a city the size of LA, LA does things differently. They have their own custom made traffic management system. I don't know the details of the system, or what its capabilities are. I've heard that they're well-funded, so it's certainly possible that they do adopt an analytic approach to their traffic.\n\nNew York just gives up I believe. I've heard that all of their traffic signals are pre-timed and that they're not particularly on top of the digitization of traffic. Again, hearsay because I don't have intimate knowledge of the particulars.\n\nSomething to keep in mind is that cities, in general, are very keen on eliminating any liability they may have, especially in something as accident prone as traffic. They're really hesitant, in my opinion, to do anything like a dynamic (on-the-fly) traffic management system for fear of liability.  There's already an optimized algorithm for traffic, it is called roundabouts.  I wouldn't even dare touch those things without some mathematical proof on whether or not it's possible for traffic to get stuck in an infinite loop there. [deleted] I wouldn't even dare touch those things without some mathematical proof on whether or not it's possible for traffic to get stuck in an infinite loop there. I wouldn't even dare touch those things without some mathematical proof on whether or not it's possible for traffic to get stuck in an infinite loop there. I wouldn't even dare touch those things without some mathematical proof on whether or not it's possible for traffic to get stuck in an infinite loop there.  hahahaha. My city certainly doesn't. I think they have 50 or so monkeys pressing the buttons randomly.    \nFeel free to downvote me once this conversation starts.\n Unfortunately you are pretty close to correct.  Except instead of 50 monkeys its just one overpaid one.\n\nMy town is only about 60,000 people but the lights are no where near optimized: Not with each other or even within one single intersection.  As an algorithm geek, I can't help but just weep at the lost productivity and gas as I sit at red lights. Unfortunately you are pretty close to correct.  Except instead of 50 monkeys its just one overpaid one.\n\nMy town is only about 60,000 people but the lights are no where near optimized: Not with each other or even within one single intersection.  As an algorithm geek, I can't help but just weep at the lost productivity and gas as I sit at red lights. Unfortunately you are pretty close to correct.  Except instead of 50 monkeys its just one overpaid one.\n\nMy town is only about 60,000 people but the lights are no where near optimized: Not with each other or even within one single intersection.  As an algorithm geek, I can't help but just weep at the lost productivity and gas as I sit at red lights. hahahaha. My city certainly doesn't. I think they have 50 or so monkeys pressing the buttons randomly.    \nFeel free to downvote me once this conversation starts.\n  I don't know about _most_ but I've heard that LA uses some level of sophistication to figure these things out (doesn't mention genetic algorithms, but it sounds like they have a simulator which is a prerequisite to running a GA): http://ladot.lacity.org/tf_Traffic_Signal_Timing.htm\n\nI also have read a study (can't find the link at the moment) that basically shows that you can not locally optimize this.  That is, if you had a traffic signal with really great sensors (say it could use a camera to determine queue length at every entry to the intersection, detect grid lock, etc) and tried to make signaling decisions based on that information, it would do a bad job due to interference with nearby signal timings.  They went on to show that with a mesh like communication structure in which you only can communicate with adjacent (in the graph theoretic sense) signals you could come close to the globally optimal flow.\n\nI'm sure there are lots of commonalities between CS type problems (think computer networks) and traffic signaling, and I know that modeling and simulation play a large part in the disciplines of civil engineering concerned with things like signal timing, but I've always wished for a more \"intelligent\" system.  Around where I live it seems like they do traffic studies pretty frequently (the tube across the rode which counts all entries and exists amongst a series of major intersections), and I assume the data collected is used to adjust light timings, but that seems like a labor intensive and poorly adaptive approach.  I've always imagined the system could be much more intelligent.\n\nSuffice it to say this subject has been well studied for quite some time:\nhttp://trid.trb.org/view.aspx?id=390323 I don't know about _most_ but I've heard that LA uses some level of sophistication to figure these things out (doesn't mention genetic algorithms, but it sounds like they have a simulator which is a prerequisite to running a GA): http://ladot.lacity.org/tf_Traffic_Signal_Timing.htm\n\nI also have read a study (can't find the link at the moment) that basically shows that you can not locally optimize this.  That is, if you had a traffic signal with really great sensors (say it could use a camera to determine queue length at every entry to the intersection, detect grid lock, etc) and tried to make signaling decisions based on that information, it would do a bad job due to interference with nearby signal timings.  They went on to show that with a mesh like communication structure in which you only can communicate with adjacent (in the graph theoretic sense) signals you could come close to the globally optimal flow.\n\nI'm sure there are lots of commonalities between CS type problems (think computer networks) and traffic signaling, and I know that modeling and simulation play a large part in the disciplines of civil engineering concerned with things like signal timing, but I've always wished for a more \"intelligent\" system.  Around where I live it seems like they do traffic studies pretty frequently (the tube across the rode which counts all entries and exists amongst a series of major intersections), and I assume the data collected is used to adjust light timings, but that seems like a labor intensive and poorly adaptive approach.  I've always imagined the system could be much more intelligent.\n\nSuffice it to say this subject has been well studied for quite some time:\nhttp://trid.trb.org/view.aspx?id=390323 Cracked has a [good article](http://www.cracked.com/article_19754_5-computer-hacks-from-movies-you-wont-believe-are-possible.html) about traffic controllers in LA.  Shit no! That's way too fucking intelligent!\n\nSorry, kinda drunk.\n\nWould be really surprised if there was a major metropolitan area that did this. Although they are incredibly similar in the abstract, urban planning and computer science are fields that have almost zero connection in the real world. The reality is, most of us who get CS degrees get hired into the private sector doing something like app development or web development right away--not only is there huge money there, but the people who need us know where to look. Even if an urban planner or whoever programs traffic lights came up with the idea of hiring a computer scientist to use genetic programming to optimize the traffic system, where would they go? To an university? Shit no! That's way too fucking intelligent!\n\nSorry, kinda drunk.\n\nWould be really surprised if there was a major metropolitan area that did this. Although they are incredibly similar in the abstract, urban planning and computer science are fields that have almost zero connection in the real world. The reality is, most of us who get CS degrees get hired into the private sector doing something like app development or web development right away--not only is there huge money there, but the people who need us know where to look. Even if an urban planner or whoever programs traffic lights came up with the idea of hiring a computer scientist to use genetic programming to optimize the traffic system, where would they go? LA has pretty well timed lights. Not sure what they use though.  LA has pretty well timed lights. Not sure what they use though.  I'm from L.A. myself. I'm pretty sure it's just a mixture of speed limit combined with \"how quickly do we need to cycle them so cars won't pile up and gridlock into the last intersection?\"  There is a light in my town that on occasion gives the green light to one set of lanes while the other set of lanes still on yellow.  There is a light in my town that gives the red light to all lanes and pedestrian crossings simultaneously. It has been like that for years. There is a light in my town that gives the red light to all lanes and pedestrian crossings simultaneously. It has been like that for years. There is a light in my town that on occasion gives the green light to one set of lanes while the other set of lanes still on yellow.  I know of one intersection that has no pause between a red light in one direction and a green in the next.\n\nOK, that was about as clear as mud.  Let me try again:\n\nUsually, a 4-way traffic light works like this:\n\n1. Red North/South, Green East/West (for some period of time).\n2. Red North/South, Yellow East/West (briefly).\n3. Red in all directions (briefly).\n4. Green North/South, Red East/West (for some possibly different period of time). \n5. Yellow North/South, Red East/West (briefly).\n6. Red in all directions (briefly).\n\nThe light I'm thinking of skips #3 and #6 entirely.  One of the two streets involved is moderately larger than nearby parallel streets. I know of one intersection that has no pause between a red light in one direction and a green in the next.\n\nOK, that was about as clear as mud.  Let me try again:\n\nUsually, a 4-way traffic light works like this:\n\n1. Red North/South, Green East/West (for some period of time).\n2. Red North/South, Yellow East/West (briefly).\n3. Red in all directions (briefly).\n4. Green North/South, Red East/West (for some possibly different period of time). \n5. Yellow North/South, Red East/West (briefly).\n6. Red in all directions (briefly).\n\nThe light I'm thinking of skips #3 and #6 entirely.  One of the two streets involved is moderately larger than nearby parallel streets.  Anecdote: Before Bill Gates started Microsoft, he was trying to sell traffic management software to cities. Anecdote: Before Bill Gates started Microsoft, he was trying to sell traffic management software to cities.   You don't always find the optimum solution with a genetic algo... aka \"hill climbing\"\n\nBut probably not. Traffic simulations are a difficult (costly) thing to judge their accuracy.  Yes, you should definitely use one of those guaranteed-global-optimum algorithms, like simulated annealing (given enough processing power...)\n\nIn all seriousness though, there are modern general-purpose optimization algorithms which give you more bang (optimum) for your buck (CPU time) than genetic algorithms.    ",
    "url" : "http://www.reddit.com/r/compsci/comments/139xlu/do_most_us_cities_use_genetic_algorithms_for/"
  }, {
    "id" : 45,
    "title" : "How did they find out that NAND and NOR gates are universal gates?",
    "snippet" : "I read in a book that you can use a NAND gate to build all the other gates. And by testing it out, I figured out how to do that. But I'm just wondering, is there any way to derive it from axioms? What properties does a gate need to have to be considered universal?  \nI don't know if my question is clear, I'll put it in an other way: Can you look at a nand-gate and decide that this is a universal gate, without trying to build all the other gates with it?  The property of being 'universal' is more formally known as *functional completeness*, and a *set* of operations is functionally complete iff you can express all boolean functions using only operations within that set. You can find that a set of operations is functionally complete by showing that you can create another, also functionally complete set (commonly AND, OR, NOT) out of it. But that's what you're doing already. \n\nIf you don't want to try to do that... a set of operations is functionally complete unless all of the operations within it have one of these properties:\n\n1. truth preservation. These are operations where setting all inputs to true sets the output to true. T NAND T = F,  so NAND isn't truth preserving.  \n\n2. falsity preservation, which is the reverse of the above -- setting all inputs false should set the output to false. F NAND F = T, so NAND isn't falsity preserving.\n\n3. monotonicity. If an operation is monotonic, it can't go from true to false without one of its inputs being changed from true to false. T NAND F = T, but T NAND T = F, so NAND isn't monotonic.\n\n4. affineness. These are operations which either *always ignore* an input, or *always depend* on it. IFF has this property. In Q = A iff B, setting A to true means Q=B, and setting it to false gives Q=not(B), and either way it depends on B. NAND does *not* have this property: Letting Q = A nand B, if A = True, Q = not(b), but if A=false, then Q = true, regardless of B. \n\n5. Self-duality. In these operations, flipping the value of every input will flip the value of the output. NOT is the trivial example. NAND is not an example: T NAND F = T, F NAND T = T. \n\nSince NAND has none of the properties that lead to functional incompleteness, it alone is a functionally complete set of operations. {AND, OR, NOT} is also functionally complete because although each one fits into at least one of these categories, they don't all fit into the *same* one. \n\n***\n\nThese properties were proven via [Post's Lattice](http://en.wikipedia.org/wiki/Post%27s_lattice) The property of being 'universal' is more formally known as *functional completeness*, and a *set* of operations is functionally complete iff you can express all boolean functions using only operations within that set. You can find that a set of operations is functionally complete by showing that you can create another, also functionally complete set (commonly AND, OR, NOT) out of it. But that's what you're doing already. \n\nIf you don't want to try to do that... a set of operations is functionally complete unless all of the operations within it have one of these properties:\n\n1. truth preservation. These are operations where setting all inputs to true sets the output to true. T NAND T = F,  so NAND isn't truth preserving.  \n\n2. falsity preservation, which is the reverse of the above -- setting all inputs false should set the output to false. F NAND F = T, so NAND isn't falsity preserving.\n\n3. monotonicity. If an operation is monotonic, it can't go from true to false without one of its inputs being changed from true to false. T NAND F = T, but T NAND T = F, so NAND isn't monotonic.\n\n4. affineness. These are operations which either *always ignore* an input, or *always depend* on it. IFF has this property. In Q = A iff B, setting A to true means Q=B, and setting it to false gives Q=not(B), and either way it depends on B. NAND does *not* have this property: Letting Q = A nand B, if A = True, Q = not(b), but if A=false, then Q = true, regardless of B. \n\n5. Self-duality. In these operations, flipping the value of every input will flip the value of the output. NOT is the trivial example. NAND is not an example: T NAND F = T, F NAND T = T. \n\nSince NAND has none of the properties that lead to functional incompleteness, it alone is a functionally complete set of operations. {AND, OR, NOT} is also functionally complete because although each one fits into at least one of these categories, they don't all fit into the *same* one. \n\n***\n\nThese properties were proven via [Post's Lattice](http://en.wikipedia.org/wiki/Post%27s_lattice) Fascinating. The property of being 'universal' is more formally known as *functional completeness*, and a *set* of operations is functionally complete iff you can express all boolean functions using only operations within that set. You can find that a set of operations is functionally complete by showing that you can create another, also functionally complete set (commonly AND, OR, NOT) out of it. But that's what you're doing already. \n\nIf you don't want to try to do that... a set of operations is functionally complete unless all of the operations within it have one of these properties:\n\n1. truth preservation. These are operations where setting all inputs to true sets the output to true. T NAND T = F,  so NAND isn't truth preserving.  \n\n2. falsity preservation, which is the reverse of the above -- setting all inputs false should set the output to false. F NAND F = T, so NAND isn't falsity preserving.\n\n3. monotonicity. If an operation is monotonic, it can't go from true to false without one of its inputs being changed from true to false. T NAND F = T, but T NAND T = F, so NAND isn't monotonic.\n\n4. affineness. These are operations which either *always ignore* an input, or *always depend* on it. IFF has this property. In Q = A iff B, setting A to true means Q=B, and setting it to false gives Q=not(B), and either way it depends on B. NAND does *not* have this property: Letting Q = A nand B, if A = True, Q = not(b), but if A=false, then Q = true, regardless of B. \n\n5. Self-duality. In these operations, flipping the value of every input will flip the value of the output. NOT is the trivial example. NAND is not an example: T NAND F = T, F NAND T = T. \n\nSince NAND has none of the properties that lead to functional incompleteness, it alone is a functionally complete set of operations. {AND, OR, NOT} is also functionally complete because although each one fits into at least one of these categories, they don't all fit into the *same* one. \n\n***\n\nThese properties were proven via [Post's Lattice](http://en.wikipedia.org/wiki/Post%27s_lattice) The property of being 'universal' is more formally known as *functional completeness*, and a *set* of operations is functionally complete iff you can express all boolean functions using only operations within that set. You can find that a set of operations is functionally complete by showing that you can create another, also functionally complete set (commonly AND, OR, NOT) out of it. But that's what you're doing already. \n\nIf you don't want to try to do that... a set of operations is functionally complete unless all of the operations within it have one of these properties:\n\n1. truth preservation. These are operations where setting all inputs to true sets the output to true. T NAND T = F,  so NAND isn't truth preserving.  \n\n2. falsity preservation, which is the reverse of the above -- setting all inputs false should set the output to false. F NAND F = T, so NAND isn't falsity preserving.\n\n3. monotonicity. If an operation is monotonic, it can't go from true to false without one of its inputs being changed from true to false. T NAND F = T, but T NAND T = F, so NAND isn't monotonic.\n\n4. affineness. These are operations which either *always ignore* an input, or *always depend* on it. IFF has this property. In Q = A iff B, setting A to true means Q=B, and setting it to false gives Q=not(B), and either way it depends on B. NAND does *not* have this property: Letting Q = A nand B, if A = True, Q = not(b), but if A=false, then Q = true, regardless of B. \n\n5. Self-duality. In these operations, flipping the value of every input will flip the value of the output. NOT is the trivial example. NAND is not an example: T NAND F = T, F NAND T = T. \n\nSince NAND has none of the properties that lead to functional incompleteness, it alone is a functionally complete set of operations. {AND, OR, NOT} is also functionally complete because although each one fits into at least one of these categories, they don't all fit into the *same* one. \n\n***\n\nThese properties were proven via [Post's Lattice](http://en.wikipedia.org/wiki/Post%27s_lattice) The property of being 'universal' is more formally known as *functional completeness*, and a *set* of operations is functionally complete iff you can express all boolean functions using only operations within that set. You can find that a set of operations is functionally complete by showing that you can create another, also functionally complete set (commonly AND, OR, NOT) out of it. But that's what you're doing already. \n\nIf you don't want to try to do that... a set of operations is functionally complete unless all of the operations within it have one of these properties:\n\n1. truth preservation. These are operations where setting all inputs to true sets the output to true. T NAND T = F,  so NAND isn't truth preserving.  \n\n2. falsity preservation, which is the reverse of the above -- setting all inputs false should set the output to false. F NAND F = T, so NAND isn't falsity preserving.\n\n3. monotonicity. If an operation is monotonic, it can't go from true to false without one of its inputs being changed from true to false. T NAND F = T, but T NAND T = F, so NAND isn't monotonic.\n\n4. affineness. These are operations which either *always ignore* an input, or *always depend* on it. IFF has this property. In Q = A iff B, setting A to true means Q=B, and setting it to false gives Q=not(B), and either way it depends on B. NAND does *not* have this property: Letting Q = A nand B, if A = True, Q = not(b), but if A=false, then Q = true, regardless of B. \n\n5. Self-duality. In these operations, flipping the value of every input will flip the value of the output. NOT is the trivial example. NAND is not an example: T NAND F = T, F NAND T = T. \n\nSince NAND has none of the properties that lead to functional incompleteness, it alone is a functionally complete set of operations. {AND, OR, NOT} is also functionally complete because although each one fits into at least one of these categories, they don't all fit into the *same* one. \n\n***\n\nThese properties were proven via [Post's Lattice](http://en.wikipedia.org/wiki/Post%27s_lattice) The property of being 'universal' is more formally known as *functional completeness*, and a *set* of operations is functionally complete iff you can express all boolean functions using only operations within that set. You can find that a set of operations is functionally complete by showing that you can create another, also functionally complete set (commonly AND, OR, NOT) out of it. But that's what you're doing already. \n\nIf you don't want to try to do that... a set of operations is functionally complete unless all of the operations within it have one of these properties:\n\n1. truth preservation. These are operations where setting all inputs to true sets the output to true. T NAND T = F,  so NAND isn't truth preserving.  \n\n2. falsity preservation, which is the reverse of the above -- setting all inputs false should set the output to false. F NAND F = T, so NAND isn't falsity preserving.\n\n3. monotonicity. If an operation is monotonic, it can't go from true to false without one of its inputs being changed from true to false. T NAND F = T, but T NAND T = F, so NAND isn't monotonic.\n\n4. affineness. These are operations which either *always ignore* an input, or *always depend* on it. IFF has this property. In Q = A iff B, setting A to true means Q=B, and setting it to false gives Q=not(B), and either way it depends on B. NAND does *not* have this property: Letting Q = A nand B, if A = True, Q = not(b), but if A=false, then Q = true, regardless of B. \n\n5. Self-duality. In these operations, flipping the value of every input will flip the value of the output. NOT is the trivial example. NAND is not an example: T NAND F = T, F NAND T = T. \n\nSince NAND has none of the properties that lead to functional incompleteness, it alone is a functionally complete set of operations. {AND, OR, NOT} is also functionally complete because although each one fits into at least one of these categories, they don't all fit into the *same* one. \n\n***\n\nThese properties were proven via [Post's Lattice](http://en.wikipedia.org/wiki/Post%27s_lattice) IIRC, redstone has all these properties. The property of being 'universal' is more formally known as *functional completeness*, and a *set* of operations is functionally complete iff you can express all boolean functions using only operations within that set. You can find that a set of operations is functionally complete by showing that you can create another, also functionally complete set (commonly AND, OR, NOT) out of it. But that's what you're doing already. \n\nIf you don't want to try to do that... a set of operations is functionally complete unless all of the operations within it have one of these properties:\n\n1. truth preservation. These are operations where setting all inputs to true sets the output to true. T NAND T = F,  so NAND isn't truth preserving.  \n\n2. falsity preservation, which is the reverse of the above -- setting all inputs false should set the output to false. F NAND F = T, so NAND isn't falsity preserving.\n\n3. monotonicity. If an operation is monotonic, it can't go from true to false without one of its inputs being changed from true to false. T NAND F = T, but T NAND T = F, so NAND isn't monotonic.\n\n4. affineness. These are operations which either *always ignore* an input, or *always depend* on it. IFF has this property. In Q = A iff B, setting A to true means Q=B, and setting it to false gives Q=not(B), and either way it depends on B. NAND does *not* have this property: Letting Q = A nand B, if A = True, Q = not(b), but if A=false, then Q = true, regardless of B. \n\n5. Self-duality. In these operations, flipping the value of every input will flip the value of the output. NOT is the trivial example. NAND is not an example: T NAND F = T, F NAND T = T. \n\nSince NAND has none of the properties that lead to functional incompleteness, it alone is a functionally complete set of operations. {AND, OR, NOT} is also functionally complete because although each one fits into at least one of these categories, they don't all fit into the *same* one. \n\n***\n\nThese properties were proven via [Post's Lattice](http://en.wikipedia.org/wiki/Post%27s_lattice) In fact, going back to the metatheory of logic, only the *not* and any binary operator can be used to construct all of the others.\n\nDon't ask me which 20th century logician proved that though.  I drank too much when I was taking that class. I don't think that's correct. For example, not and iff shouldn't be able to construct the others. (Both are affine operations, to tie it back to the earlier post.)\n\nAnd then you've got the constant binary operators, like (a,b) -&gt; True, which obviously aren't going to be enough.  Take a look at a truth table for NAND gates.\n\nA NAND can be used as a not by connecting the two inputs together. \nSo using two NANDS can become a NOT.\n\nThree NANDs can be used for OR, by using two to NOT the two imports, and the third as a standard NAND.  !(!A AND !B) = A OR B\n\nA and B are inputs, !Q is the final output. \n\n    A   B    !A   !B   (Q = !A AND !B)             !Q   \n\n    1    1    0    0        0                   1\n\n    1    0    0    1        0                   1\n\n    0    1    1    0        0                   1\n\n    0    0    1    1        1                   0\n\n\n\nTake a look at De Morgans Law, it's quite a useful thing to understand if you are working with logic gates.\n Tabled for you\n\n| A | B | !A | !B | Q=!A AND !B | !Q |\n|:--|---|----|----|-----------------|---:|\n| 1 | 1 | 0 | 0 |      0      | 1 |\n| 1 | 0 | 0 | 1 |      0      | 1 |\n| 0 | 1 | 1 | 0 |      0      | 1 |\n| 0 | 0 | 1 | 1 |      1      | 0 | Take a look at a truth table for NAND gates.\n\nA NAND can be used as a not by connecting the two inputs together. \nSo using two NANDS can become a NOT.\n\nThree NANDs can be used for OR, by using two to NOT the two imports, and the third as a standard NAND.  !(!A AND !B) = A OR B\n\nA and B are inputs, !Q is the final output. \n\n    A   B    !A   !B   (Q = !A AND !B)             !Q   \n\n    1    1    0    0        0                   1\n\n    1    0    0    1        0                   1\n\n    0    1    1    0        0                   1\n\n    0    0    1    1        1                   0\n\n\n\nTake a look at De Morgans Law, it's quite a useful thing to understand if you are working with logic gates.\n       What do you mean by \"Find Out\".  Its a mathematical proof. find out == discover. How did someone come across this idea and then decide to prove it?\n\nAt least, that's how I read it, not being a pendant. ;) Being a _pedant_, that's a [pendant](https://encrypted.google.com/search?tbm=isch&amp;q=pendant) :) &amp;#3232;\\_&amp;#3232;  Why more use NAND rather than AND/OR? ELI5 please. Economies of scale. It is cheaper to produce a chip with all of the same type of gate, even if that means more gates than strictly necessary. This means you can use the same machine for producing the entire chip, and the costs go down. Even without this, NAND gates are typically simpler to produce than AND and OR gates.  In CMOS, NAND gates use 4 transistors, while AND and OR use 6. Right, but it takes several NAND gates to produce one AND or NOT or OR gate, so if it weren't for economies of scale, this would still not be productive, since we're still using more transistors than those 3 gates really need, to implement them. Typically, it's fairly simple to convert an AND/OR Boolean expression to NAND/NOR with De Morgan's Law rather than directly making AND/OR gates with NAND/NOR gates. So the primary reason is that it take fewer transistors.  Economies of scale. It is cheaper to produce a chip with all of the same type of gate, even if that means more gates than strictly necessary. This means you can use the same machine for producing the entire chip, and the costs go down. Economies of scale. It is cheaper to produce a chip with all of the same type of gate, even if that means more gates than strictly necessary. This means you can use the same machine for producing the entire chip, and the costs go down.      By using both halves of their brain.\n\nSeriously, this is not one of the more difficult results in logic.\n\n**Edit:** And you should feel bad for even asking about it. Right, the OP stated he was able to build all of the basic gate types.\n\nBut that is not a proof.  He asked if it could be proven using axioms.  So please publish your proof if you have one. \n\nI have forgotten, where is that Reddit rule that says we must include a silly disparaging insult in our posts?",
    "url" : "http://www.reddit.com/r/compsci/comments/138jrk/how_did_they_find_out_that_nand_and_nor_gates_are/"
  }, {
    "id" : 46,
    "title" : "Brian Harvey: Why 'Structure and Interpretation of Computer Programs' (SICP) matters",
    "snippet" : "  I took SICP with Brian Harvey and I LOVED it.  It was my first college CS class, and it definitely solidified my decision to major in CS.\nThe semester after I took it, Harvey stopped teaching it, and they switched the language to Python.  I'm really glad I got to take it with Harvey in Scheme\n John DeNero's new CS61A (the Scheme replacement with Python) is extraordinarily well thought-out. There are a lot of people taking that course that won't stay in Computer Science; for these people, I argue that learning Python is more useful than Scheme. For those that plan to stay in CS, I'm conflicted, but I think switching to Python was a good choice. John DeNero's new CS61A (the Scheme replacement with Python) is extraordinarily well thought-out. There are a lot of people taking that course that won't stay in Computer Science; for these people, I argue that learning Python is more useful than Scheme. For those that plan to stay in CS, I'm conflicted, but I think switching to Python was a good choice. And for this reason I strongly believe that there needs to exist a separate programs from Computing Science that focus on the trade of computing and not the theory and fundamentals. And for this reason I strongly believe that there needs to exist a separate programs from Computing Science that focus on the trade of computing and not the theory and fundamentals. I took SICP with Brian Harvey and I LOVED it.  It was my first college CS class, and it definitely solidified my decision to major in CS.\nThe semester after I took it, Harvey stopped teaching it, and they switched the language to Python.  I'm really glad I got to take it with Harvey in Scheme\n So in other words, it didn't survive his retirement? I took SICP with Brian Harvey and I LOVED it.  It was my first college CS class, and it definitely solidified my decision to major in CS.\nThe semester after I took it, Harvey stopped teaching it, and they switched the language to Python.  I'm really glad I got to take it with Harvey in Scheme\n  Have you read your daily SICP? ",
    "url" : "http://www.eecs.berkeley.edu/~bh/sicp.html"
  }, {
    "id" : 47,
    "title" : "Speeding algorithms by shrinking data: \nA new approach to processing ‘big data’ creates succinct representations of huge data sets, so that existing algorithms can handle them efficiently\n",
    "url" : "http://web.mit.edu/newsoffice/2012/compressed-gps-data-1113.html"
  }, {
    "id" : 48,
    "title" : "Abstract Types with Isomorphic Types",
    "url" : "http://homotopytypetheory.org/2012/11/12/abstract-types-with-isomorphic-types/"
  }, {
    "id" : 49,
    "title" : "Pushing the power envelope: efficient exascale computers also could mean more powerful hand-helds with longer-lasting batteries",
    "snippet" : "  ",
    "url" : "http://ascr-discovery.science.doe.gov/exascale/exa_power1.shtml"
  } ],
  "processing-time-source" : 59,
  "processing-result.title" : "compsci4_reddit.xml",
  "XmlDocumentSource.xml" : {
    "absolutePath" : "/Users/cole/code/cs467/r-topics/carrot2-cli-3.6.2/../data/compsci4_reddit.xml"
  }
}
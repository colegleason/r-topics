{
  "processing-time-total" : 1847,
  "clusters" : [ {
    "id" : 0,
    "size" : 10,
    "score" : 33.28264106677249,
    "phrases" : [ "Set Number" ],
    "documents" : [ 14, 18, 25, 27, 29, 32, 39, 42, 43, 44 ],
    "attributes" : {
      "score" : 33.28264106677249
    }
  }, {
    "id" : 1,
    "size" : 8,
    "score" : 41.283136609976886,
    "phrases" : [ "Problems that can be Solved" ],
    "documents" : [ 4, 6, 18, 25, 27, 35, 44, 47 ],
    "attributes" : {
      "score" : 41.283136609976886
    }
  }, {
    "id" : 2,
    "size" : 5,
    "score" : 90.84039478407739,
    "phrases" : [ "Memory Spaces" ],
    "documents" : [ 16, 18, 22, 32, 42 ],
    "attributes" : {
      "score" : 90.84039478407739
    }
  }, {
    "id" : 3,
    "size" : 5,
    "score" : 76.10078738156695,
    "phrases" : [ "Start with Java" ],
    "documents" : [ 12, 29, 34, 35, 48 ],
    "attributes" : {
      "score" : 76.10078738156695
    }
  }, {
    "id" : 4,
    "size" : 4,
    "score" : 64.57504314396184,
    "phrases" : [ "Quantum" ],
    "documents" : [ 17, 18, 39, 47 ],
    "attributes" : {
      "score" : 64.57504314396184
    }
  }, {
    "id" : 5,
    "size" : 4,
    "score" : 53.00440514482398,
    "phrases" : [ "Really Fast" ],
    "documents" : [ 12, 22, 32, 39 ],
    "attributes" : {
      "score" : 53.00440514482398
    }
  }, {
    "id" : 6,
    "size" : 3,
    "score" : 70.12929241652795,
    "phrases" : [ "Basic Logic" ],
    "documents" : [ 18, 34, 39 ],
    "attributes" : {
      "score" : 70.12929241652795
    }
  }, {
    "id" : 7,
    "size" : 3,
    "score" : 61.6973653421224,
    "phrases" : [ "Cache is SRAM" ],
    "documents" : [ 13, 22, 32 ],
    "attributes" : {
      "score" : 61.6973653421224
    }
  }, {
    "id" : 8,
    "size" : 3,
    "score" : 57.3222768668133,
    "phrases" : [ "Concurrency" ],
    "documents" : [ 10, 24, 46 ],
    "attributes" : {
      "score" : 57.3222768668133
    }
  }, {
    "id" : 9,
    "size" : 3,
    "score" : 45.856011211194605,
    "phrases" : [ "Formal" ],
    "documents" : [ 9, 18, 19 ],
    "attributes" : {
      "score" : 45.856011211194605
    }
  }, {
    "id" : 10,
    "size" : 3,
    "score" : 47.03227326948231,
    "phrases" : [ "Great Game" ],
    "documents" : [ 3, 44, 48 ],
    "attributes" : {
      "score" : 47.03227326948231
    }
  }, {
    "id" : 11,
    "size" : 3,
    "score" : 69.36720633671618,
    "phrases" : [ "Listening" ],
    "documents" : [ 25, 28, 30 ],
    "attributes" : {
      "score" : 69.36720633671618
    }
  }, {
    "id" : 12,
    "size" : 3,
    "score" : 76.67304770921882,
    "phrases" : [ "Proof System" ],
    "documents" : [ 9, 18, 39 ],
    "attributes" : {
      "score" : 76.67304770921882
    }
  }, {
    "id" : 13,
    "size" : 3,
    "score" : 83.84316223675883,
    "phrases" : [ "Research Students" ],
    "documents" : [ 5, 25, 37 ],
    "attributes" : {
      "score" : 83.84316223675883
    }
  }, {
    "id" : 14,
    "size" : 3,
    "score" : 91.95286994609594,
    "phrases" : [ "Turing Machines" ],
    "documents" : [ 3, 18, 34 ],
    "attributes" : {
      "score" : 91.95286994609594
    }
  }, {
    "id" : 15,
    "size" : 2,
    "score" : 96.03629176500965,
    "phrases" : [ "Image Compression" ],
    "documents" : [ 12, 42 ],
    "attributes" : {
      "score" : 96.03629176500965
    }
  }, {
    "id" : 16,
    "size" : 2,
    "score" : 82.06996509260166,
    "phrases" : [ "Learn Category Theory" ],
    "documents" : [ 35, 46 ],
    "attributes" : {
      "score" : 82.06996509260166
    }
  }, {
    "id" : 17,
    "size" : 2,
    "score" : 57.176318926735625,
    "phrases" : [ "Natural Language Processing" ],
    "documents" : [ 18, 40 ],
    "attributes" : {
      "score" : 57.176318926735625
    }
  }, {
    "id" : 18,
    "size" : 2,
    "score" : 78.44863942880704,
    "phrases" : [ "Ranking even though I Sort by Top" ],
    "documents" : [ 4, 37 ],
    "attributes" : {
      "score" : 78.44863942880704
    }
  }, {
    "id" : 19,
    "size" : 2,
    "score" : 44.153029496754634,
    "phrases" : [ "Weekend" ],
    "documents" : [ 25, 31 ],
    "attributes" : {
      "score" : 44.153029496754634
    }
  }, {
    "id" : 20,
    "size" : 2,
    "score" : 60.5976691992707,
    "phrases" : [ "Wtf is up with Reddit" ],
    "documents" : [ 22, 25 ],
    "attributes" : {
      "score" : 60.5976691992707
    }
  }, {
    "id" : 21,
    "size" : 17,
    "score" : 0.0,
    "phrases" : [ "Other Topics" ],
    "documents" : [ 0, 1, 2, 7, 8, 11, 15, 20, 21, 23, 26, 33, 36, 38, 41, 45, 49 ],
    "attributes" : {
      "other-topics" : true,
      "score" : 0.0
    }
  } ],
  "processing-time-algorithm" : 1775,
  "documents" : [ {
    "id" : 0,
    "title" : "In dire need of help for a school project ",
    "snippet" : " as a higschool senior have been tasked with writing a paper. However, a secondary objective is to also seek out someone we don't know in the work force that can share info on my topic. My topic is the hardware and software advancements of consoles and PCs and how they relate. Its written in an article format. \n\nAll I would need from someone is to email me. I would then send a short List of questions to be responded to to include in my article as a q and a. I will then email you a final copy.  Also, my teacher will email you a link to evaluate me in three category's, on a 1-10 point system. \n\nAs I had several teacher changes through the year, this assignment is counting for 60% of my semester grade, and I have scholarship things coming up. The outside persons interaction accounts for 30% of it, so it is quite needed.  \n\nIf anyone is interested please post your emails here or PM them to me and I will contact you either between noon and one PST or after 5pm PST since I have to work today.\n\nThanks for your time /r/compsci :D\n\nPS, wrote this on my phone, pardon spelling and typos please :D    Cut the horseshit and do your own damned homework. Did you even read the post? Yes, I even read the the post. What's you point? What he point is, is \"The outside persons interaction accounts for 30% of it [his semester grade]\" The real point is that you're being lied to. No high school grades students on the basis of your quote. Don't be such a dumbass, thank you. The real point is that you're being lied to. No high school grades students on the basis of your quote. Don't be such a dumbass, thank you. The real point is that you're being lied to. No high school grades students on the basis of your quote. Don't be such a dumbass, thank you. Cut the horseshit and do your own damned homework. Give the fucking kid a break. There's really no reason to be a dick.  Cut the horseshit and do your own damned homework.",
    "url" : "http://www.reddit.com/r/compsci/comments/ucjiw/in_dire_need_of_help_for_a_school_project/"
  }, {
    "id" : 1,
    "title" : "Can anyone recommend a good methodology for Software Review?",
    "snippet" : "For our final year R&amp;D project we need to review some existing software against a set of required features to decide which software to fork and modify (or if an entirely new application needs to be built). Our lecturer said we needed to use a methodology to do it.  \n\nWe have never been introduced to such a methodology and our google-fu is failing us.\n\nDoes anyone have any recommendations that we can do some research into?\n\nThanks in advance :)  Googling \"software review methodology\" brings up quite a lot.\n\nThe method I used to use involved three basic principles.\n\n1) Pick a percentage of code to review: 10-35% is plenty if you target high usage and high risk areas for the review.\n\n2) The review is only to identify violations of software development practices and to ID bugs: performance or otherwise. It's not an opportunity to rewrite the code the way the reviewer would have done it. If you feel there is something important to say, discuss it outside of the review.\n\n3) Have someone strong in charge of the review to make sure #2 happens or it devolves in to \"this class should be called 'MarsRover' not 'Rover.'\" And 12 hours later, you haven't gotten anywhere. Two things I think have to be addressed:\n\nHow do you determine high usage and high risk areas?  ",
    "url" : "http://www.reddit.com/r/compsci/comments/ucwly/can_anyone_recommend_a_good_methodology_for/"
  }, {
    "id" : 2,
    "title" : "Java Native Interface / Multi-Level DLL Calls",
    "snippet" : "So I'm vaguely familiar with JNI and calling DLLs in C/C++. However, sometimes relating data types from Java to C/C++ is a pain. So to avoid this data type compatibility (from JNI straight to given DLL) I'm posing a question to have one resource given (with .lib, .dll, and .h files). To reference this .dll implicitly from another piece of C/C++ code. Then to have a Java program calling functions from the 2nd layer C/C++ code. All the exporting stuff can get a bit tedious (JNIEXPORT and __declspec(dllimport)), but it seems to be easier than trying to relate a Java type to a C/C++ type (especially when that data type is opaque... in my case -&gt; pcap_t in winpcap library).\n\nJava -&gt; MyDLL -&gt; GivenDLL\n\nReal Purpose:\nFor a side project at work I have a bunch of GUIs written that use sockets in Java. I need to use the same GUIs but using winpcap libraries in C/C++ so that I can communicate with a device through a straight ethernet cord connection (computer to device) using a packet driver library specific to the piece of equipment.\n\nI'm not asking anyone to implement it for me. But I would like to ask if anyone thinks there is a better solution than my proposed one? Or links to helpful articles that may help with this?\n\nThanks in advance.  [deleted]",
    "url" : "http://www.reddit.com/r/compsci/comments/ubld5/java_native_interface_multilevel_dll_calls/"
  }, {
    "id" : 3,
    "title" : "Manufactoria: A little game that makes DFA construction fun and easy to learn!",
    "snippet" : "  A great game, not so easy but very compelling ;)\n\nIf you like the genre, you can also check out SpaceChem which is similar but more polished. (although I found Manufactoria to have a more powerful paradigm). A great game, not so easy but very compelling ;)\n\nIf you like the genre, you can also check out SpaceChem which is similar but more polished. (although I found Manufactoria to have a more powerful paradigm). SpaceChem is preposterously difficult. SpaceChem is preposterously difficult. A great game, not so easy but very compelling ;)\n\nIf you like the genre, you can also check out SpaceChem which is similar but more polished. (although I found Manufactoria to have a more powerful paradigm).   Nit: They're actually not DFAs--they're a variety of Turing machine.  (DFAs don't have tape they can read from/write to--they just get the input and destructively process it. Turing machines have tape you can read to/from and move left/right on. You can only move right on the tape in this game, so it's not quite the same as a TM, but I believe it has the same computational power [everything this kind of machine can compute, a TM can compute, and vice-versa].) Indeed! It's a Turing Machine! I assigned this as a lab exercise to introduce TMs in a course entitled \"Great Ideas in Computer Science\" last summer... It was a class for non-majors, but they seemed to enjoy the game quite a bit! Are you Scott Aaronson? I'm not, sorry! then you must be canadian Why must this be? Though I do live in Buffalo, so maybe that's close enough, eh? ;p Nit: They're actually not DFAs--they're a variety of Turing machine.  (DFAs don't have tape they can read from/write to--they just get the input and destructively process it. Turing machines have tape you can read to/from and move left/right on. You can only move right on the tape in this game, so it's not quite the same as a TM, but I believe it has the same computational power [everything this kind of machine can compute, a TM can compute, and vice-versa].)   Freakiest part of this game is that when I was browsing the forums looking for optimization help, I came across a post from the author of the game saying that he hadn't actually found a solution for the later levels, merely proved them solvable using queueing theory. It's not only possible to beat it, but it's possible to beat without using crossovers (2 conveyors placed in a + shape).\n\n(Hold Shift to place a crossover.) It's not only possible to beat it, but it's possible to beat without using crossovers (2 conveyors placed in a + shape).\n\n(Hold Shift to place a crossover.) I was remarking more on the fact that he didn't bother to actually solve them than that it seemed impossible.\n\nHow do you know that you can solve it without crossovers, though?  That's new to me. Freakiest part of this game is that when I was browsing the forums looking for optimization help, I came across a post from the author of the game saying that he hadn't actually found a solution for the later levels, merely proved them solvable using queueing theory.    I gave up when I got a \"select\" tool and it didn't tell me how to use it.  Shame though, seems like a cool game. I gave up when I got a \"select\" tool and it didn't tell me how to use it.  Shame though, seems like a cool game.",
    "url" : "http://www.kongregate.com/games/PleasingFungus/manufactoria"
  }, {
    "id" : 4,
    "title" : "How *Not* To Rank Items - This Bothers Me",
    "snippet" : "  I disagree that Urbandictionary's model (model #1) is wrong.\n\nIt is wrong if the premise is you want to rank by percentage of audience who approve.  But I don't think that's what Urbandictionary wants.\n\nThey want to rank \"popular\" items higher even if these items are more \"controversial\".  They want to rank items that get very little attention lower regardless of their positive/negative swing.\n\nTo Urbandictionary, an item with +200 and -180 is *supposed* to rank higher than an item with +6 and -0 and that's the way I'd want it if I ran that site too.  Clearly they have different requirements to a shopping site.  Urbandictionary want the more notorious/well-known ranked higher, not the more well-liked.  It's supposed to paint a picture of what the higher number of people associate the word with, right or wrong.  Hence a definition that is rated +200 and -180 is much more important than one that is +6 and -0 in representing that.\n\nHowever, I agree with the second section - wrong solution #2 - this is a big problem all over the web. I do think urbandictionary's model is incorrect. But not for the reasons stated in the article.\n\nThey way it is basically makes it impossible for new submissions to become anywhere near the top *even if they are better*.\n\nThe +200 -180 item is probably not more controversial, but has just been there longer. Keep in mind that nearly half of people that see this one hate it.\n\nAnd the entry that is new, has +6 -0 is so far down the list no one sees it and will never get voted on. But everyone that saw it liked it. And it will never move up as long as twice as many people see the 50% popularity post than the 100% popularity post.\n\nThat being said, the article's solution doesn't solve this either. Almost need something similar to reddit's algorithm. &gt; They way it is basically makes it impossible for new submissions to become anywhere near the top even if they are better.\n\nThat is definitely true - I agree with you there.\n\nBut I'm not sure that it's altogether an undesirable thing for Urbandictionary specifically as they don't really need fresh content all the time.  If the existing definitions are popular ones and have more upvotes than downvotes - they have found their footing well - then maybe it's a good thing that they stay at the top.  Submitters should probably look for words that have poorer definitions, or fewer definitions, and submit their new definitions to those words instead where they are more needed.\n\nThe issue of new definitions being invisible only really begins to be a issue when there is more than about 3 or 4 definitions, and the newer ones are pushed down too low for most people to encounter them.\n\nIt is a tendency for urbandictionary definitions to be downvoted ruthlessly if people disagree with them (part of that site's culture) or even if the examples no longer seem funny.  It's not unusual for there to be quite a lot of definitions with a highly negative score (by that I mean, more downvotes than upvotes - what others might call \"less than 50% approval\").  Some people even clearly find it fun to scroll through lots of definitions and downvote *all* the ones they dislike.  As more definitions are pushed far into the negative, newer ones with few votes aren't necessary left at the bottom but gradually find their rank among ones that people like slightly better and those they hate slightly more.\n\nSo you typically get an equilibrium of about 10 definitions with a positive score, a few new ones that haven't aquired much of a score yet, then 10 or 20 with a highly negative score.  And they are moving all the time, gradually though, as public opinion about them changes.  One with a score of +2, -0 might gradually inch above one with +4, -1 if it gets a couple of downvotes. I disagree that Urbandictionary's model (model #1) is wrong.\n\nIt is wrong if the premise is you want to rank by percentage of audience who approve.  But I don't think that's what Urbandictionary wants.\n\nThey want to rank \"popular\" items higher even if these items are more \"controversial\".  They want to rank items that get very little attention lower regardless of their positive/negative swing.\n\nTo Urbandictionary, an item with +200 and -180 is *supposed* to rank higher than an item with +6 and -0 and that's the way I'd want it if I ran that site too.  Clearly they have different requirements to a shopping site.  Urbandictionary want the more notorious/well-known ranked higher, not the more well-liked.  It's supposed to paint a picture of what the higher number of people associate the word with, right or wrong.  Hence a definition that is rated +200 and -180 is much more important than one that is +6 and -0 in representing that.\n\nHowever, I agree with the second section - wrong solution #2 - this is a big problem all over the web.  &gt;http://evanmiller.org/rating-equation.png\n\nI don't think I'm the right fit for this subreddit. Hahaha it comes in time! Promise!\n\nMy first time reading an academic paper, I quit halfway through and cried. Then I picked it back up and kept going. Seven months later, I've pretty much figured out what all the squiggles meant. Hahaha it comes in time! Promise!\n\nMy first time reading an academic paper, I quit halfway through and cried. Then I picked it back up and kept going. Seven months later, I've pretty much figured out what all the squiggles meant. Hahaha it comes in time! Promise!\n\nMy first time reading an academic paper, I quit halfway through and cried. Then I picked it back up and kept going. Seven months later, I've pretty much figured out what all the squiggles meant. &gt;My first time reading an academic paper, I quit halfway through and cried. Then I picked it back up and kept going. Seven months later, I've pretty much figured out what all the squiggles meant.\n\nThen a few months later you realise the authors don't really understand the squiggles.  [deleted] [deleted]  &gt; Sites that make this mistake: Amazon.com\n\nI guarantee you that is not how Amazon's default ranking works. Their developers are not stupid. Amazon's ranking is influenced by potential profit given the expected probability of an order.\n\nAnd secondly, if you select rank by \"Avg. Customer Review\" then your results will be sorted by average customer review. **Duh.** Average review is implemented by averaging the reviews. &gt; Sites that make this mistake: Amazon.com\n\nI guarantee you that is not how Amazon's default ranking works. Their developers are not stupid. Amazon's ranking is influenced by potential profit given the expected probability of an order.\n\nAnd secondly, if you select rank by \"Avg. Customer Review\" then your results will be sorted by average customer review. **Duh.** Average review is implemented by averaging the reviews. Amazon's listings are so awful I wouldn't be surprised if they hired a guy to throw darts at the wall in order to organize the listings.\n\nNo.  Screw that.\n\nAmazon's listings are so bad it would take someone with great skill intentionally working for the sole purpose of creating the worst listings imaginable in order to achieve the horror that exists.\n\nSomewhere, there's an Elon Musk of making bad listings.   And you still think the sort order is optimized to improve your experience?\n\nNaive. No.\n\nThe sort order is designed in order to maximize customer purchasing and otherwise increase Amazon profits.  There've been a few articles on the matter.\n\nIt's still terrible *from the perspective of a customer*, and since I'm not  a large shareholder in Amazon I don't give a flying fuck whether it's better for them.   That was incredibly informative.  Nice article! \n\nI remember an article a few days back showing that Reddit also follows the Wilson confidence interval for the frontpage ranking.  Do you have a link for that article? Do you have a link for that article? There's a [link](http://amix.dk/blog/post/19588) to the article right on the frontpage of this subreddit!   Out of curiosity what algorithm does reddit use to rank? The one featured in the article! It was on the front page just this week (which is very probably the source of this post). I dont think that is true. This is what Reddit uses to sort comments, not articles. \n\nI think Reddit sorts articles by log(# of upvotes) + some time factor.   I remember last time I saw this thread (it wasn't in this subreddit) there was the criticism that having a hard to understand algorithm discourages voting because you can't directly see the effect.\n\nI know a lot of times that I have seen comments with less score and are newer be higher up in ranking even though I sort by top. That makes me wonder what effect my votes really have. http://amix.dk/blog/post/19588\n\nI posted this not long ago and it shows how your votes are counted..it kinda makes a lot of sense, to me at least. &gt; it kinda makes a lot of sense, to me at least.\n\nYes if I were to sit down 5 minutes and read through that I probably would too, but that's not what matters.\n\nWhat matters is if the *end user* feels the difference when they vote, and they aren't going to nor should be expected to read through a lot of math. Has your experience with less score and newer been recent? \n\nAccording to the link (which I'm trusting), posting time has an effect on links, but not comments. Kinda makes sense. I'm not sure if it's always been that way though. Otherwise, I don't see how that could happen really...and there aren't enough comments on this thread for me to see it in action much. \n\nBUT, I can understand how not really feeling it sucks.  Hmm, I just realized I don't know how the \"top\" rating *actually* works. I always assumed it was \"more score, higher up\" but that's obviously not right from this: http://i.imgur.com/hDZKr.png\n\nBut when I sort by \"hot\" neon_overlord's comment is above yours. Weird, it's almost as if the descriptions should be switched between them. I mean \"hot\" sounds like it will sort by comments that have recently been added and have many replies and \"top\" will only care about the score, but that's obviously not the case. I remember last time I saw this thread (it wasn't in this subreddit) there was the criticism that having a hard to understand algorithm discourages voting because you can't directly see the effect.\n\nI know a lot of times that I have seen comments with less score and are newer be higher up in ranking even though I sort by top. That makes me wonder what effect my votes really have. A much easier to understand compromise that I've seen some sites adopt is:\n\n- For any item with fewer than 5 votes, don't show the rating - instead show \"&lt;5 votes (add your vote!)\".  And when sorting by rating, just exclude these (or put them down the bottom, or in the middle, whatever).\n\nThe premise here is that anything with only 1 to 4 votes isn't accurate enough to give a meaningful score.  The benefit is that this rule is easily understood by visitors and voters.  The downside is that 5 votes is seen as just as reliable as 500 votes.  So you could spam an item to the top with only 5 IP addresses or accounts.  But at least that's different from just a single 5-star vote pushing an item to the top.  Another downside is that the first four voters don't see their vote making any difference.\n\nEdit:\n\nOne variant of this that I adopted for a web application of mine (many years ago, might do it different now) was to seed every item with 5 votes' worth of \"50%\" ratings (and just subtract 5 from the vote count shown).  So that even when nobody had voted yet, it was at 50%, and then it'd take quite a few votes to raise that significantly.  The reason I don't think I'd do that again is that it reduces transparency - users won't understand why new items already have 50% rankings and the first 5-star (100% rating) only raises it to 58%.   Is the Wilson score similar to bayesian sorting?\n\nhttp://andrewgelman.com/2007/03/bayesian_sortin/     This should be common sense to any programmer.  I didn't even read the rest of the article. You can't compare two numbers when those numbers are not normalized to a common range. The examples state right there that one has 60% positive ratings, and the other has 55% positive ratings.  No one would assume it would be (positive) - (neg) = score. &gt;No one would assume it would be (positive) - (neg) = score.\n\nBut some sites do calculate it that way. And everything already is in a common range -- numbers expressible by int/long/float -- the question is whether that is a useful range. But they have to be normalized.  And normalization always requires division.  So the article is correct, but I can't really believe anyone would be stupid enough to actually do it that way.",
    "url" : "http://evanmiller.org/how-not-to-sort-by-average-rating.html"
  }, {
    "id" : 5,
    "title" : "Call for Papers: 2nd Imperial College Computing Student Workshop (ICCSW 2012), September 27--28 2012",
    "snippet" : "Hi /r/compsci,\n\nIt's this time of year again, and the 2nd Imperial College Computing Student\nWorkshop is now looking for papers! This year's event follows the amazing\nsuccess of last year's event!\n\nThe workshop is designed to be very low-key and easy going -- it's important\nthat the event is fun, allows for a lot of discussion and most importantly\nallows students to find out about what other students and institutes are\nworking on.\n\nThere are two important things to highlight:\n\n  * The event is sponsored by Google Inc. and we're using the money to provide\n    travel and accommodation bursaries to attendees who would otherwise be\n    unable to attend -- all you have to do is write a very short paper! We\n    expect Google employees to both be in attendance and to give a\n    keynote-style talk.\n    \n  * This year's proceedings will be officially published in the OpenAccess\n    Series in Informatics by Schloss Dagstuhl. This means that this year's\n    publications will have a long-term, open-access archive, as well as being\n    listed in places such as DBLP.\n\n**Important Dates**\n\n*Abstract registration*:\n    Friday, June 22, 2012, 11:59 p.m. BST\n\n*Paper submission*:\n    Friday, June 29, 2012, 11:59 p.m. BST\n\n*Author notification*:\n    Friday, August 10, 2012, 11:59 p.m. BST\n\n*Camera-ready version*:\n    Friday, September 7, 2012, 11:59 p.m. BST\n\nI know this is short notice -- but the page limit is pretty short (5 pages,\nexcluding bibliography) and we're not after works of art, just interesting\nideas! Short, \"extended abstract\" style papers is what we're after!\n\nFor more information, see:\n    http://iccsw.doc.ic.ac.uk/\n\nHopefully some of you can make ICCSW 2012!\n\n------------------------------------------------------------------------------\n         Imperial College Computing Student Workshop (ICCSW) 2012\n\n\n         September 27--28, 2012\n         London, United Kingdom\n\n         http://iccsw.doc.ic.ac.uk/\n------------------------------------------------------------------------------\n\nThe 2012 Imperial College Computing Student Workshop aims to provide an\ninternational forum for doctoral students to discuss a range of topics that are\ncurrent in computer science research. The workshop welcomes all research\nstudents in the field of computer science.\n\nFollowing the success of the inaugural workshop, the second workshop provides\nan excellent opportunity to share provocative ideas, interesting preliminary\nwork, or a cool research directions that may change the world. The workshop\naims to:\n\n    * offer research students the opportunity to submit and present their\n      research at an internationally-renowned institution;\n\n    * provide PhD students with the experience of organising a workshop,\n      including reviewing papers written by other students;\n\n    * provide a forum for open discussion of topics including those outside the\n      current focus of research;\n\n    * allow students to network with peers from other institutions and learn\n      about their research.\n\nThe workshop provides an excellent ground for discussing similarities and\ndifferences in graduate studies at various institutions across the globe. All\ndoctoral students are thereby encouraged to submit papers covering their\nongoing research.\n\nThe second edition of the workshop will span two days and will be held\nSeptember 27--28, 2012, at the Department of Computing at Imperial College\nLondon, London, United Kingdom.\n\nImportant dates:\n    \n    * Abstract registration:\n        Friday, June 22, 2012, 11:59 p.m. BST\n    * Paper submission:\n        Friday, June 29, 2012, 11:59 p.m. BST\n    * Author notification:\n        Friday, August 10, 2012, 11:59 p.m. BST\n    * Camera-ready version:\n        Friday, September 7, 2012, 11:59 p.m. BST\n\nThis workshop is sponsored by Google. \n\nSubmission details are available on our website [http://iccsw.doc.ic.ac.uk/](http://iccsw.doc.ic.ac.uk/)\n\n------------------------------------------------------------------------------\n\nTo find out more details about the workshop, please visit our website\n         http://iccsw.doc.ic.ac.uk/\n\nTo contact workshop organisers with any questions, please email\n         iccsw@imperial.ac.uk ",
    "url" : "http://www.reddit.com/r/compsci/comments/u8mfy/call_for_papers_2nd_imperial_college_computing/"
  }, {
    "id" : 6,
    "title" : "What other anti-bot verification systems are there than the common obscured text captcha?",
    "snippet" : "Are there any faster captchas, so that it even could be used in chat? Are there any text based? Are there any captchas which are targeted to only work for the intended audience, so that the captcha can't be redirected to other sites?\n\nI'm just wondering out of curiosity.  If a spammer is not specifically motivated to attack your site, then a very simple Captcha ('Enter \"Orange\" into this field') will suffice.\n\nIf a spammer is motivated, then a text-based Captcha is unlikely to suffice, even if it requires knowledge that is specific to your site/community. \n\nI know of two alternative Captchas that are quick for a human to solve though:\n\n1. Display an image at a random angle. Ask the user to rotate it into the correct orientation. Described in a [paper](http://research.google.com/pubs/archive/35157.pdf) from Google Research.\n\n2. Match 3D models that are displayed from different points of view. An implementation can be seen [here](http://www.yuniti.com/register.php).\n  Basically, you can use any method which is a) difficult for a computer to do b) easy and fast for a person. If you want to generate a database for the problem, you might aswell use the users to build it, similar to how reCaptcha works. \n\nThe users will answer twice, once for a known object and once for an unknown object. The first one is used to verify it's a human and the second one is to provide new answers to the database.\n\nThe task chosen should be something that AI's cannot do (efficiently or reliably).\n\nThe first idea I got was to identify an object in a picture. Just get random images from the internet, and if it's an apple the user will write \"apple\" etc.\n\nAnother idea I have would be to have the users mark objects (like mark all persons) from the image.\n\nOne more I just came up with: provide the users with two sentences, one is real english with meaning, another is grammatically correct but meaningless. The user would click on the meaningful sentence. The meaningless sentences would be computed automatically.  You probably don't want a fast CAPCHA because then those might be able to be solved by bots easily.   You might as well haven't heard of but have seen a captcha called reCaptcha. Its used by Google, Facebook and many other sites. Its based on human computation which helps to digitize books. [Have a look at this!](http://www.youtube.com/watch?v=euRAfUGX8wY)     I've seen clock based (what time does the clock say)... Also have seen simple math problems used. ",
    "url" : "http://www.reddit.com/r/compsci/comments/u80tl/what_other_antibot_verification_systems_are_there/"
  }, {
    "id" : 7,
    "title" : "For Those Who Are Curious: The Reddit Algorithm",
    "snippet" : "   &gt; This could explain why kittens (and other non-controversial stories) rank so high :)\n\nSo why isn't this fixed yet? I'm not sure I see this as a problem...  The concepts here seem intuitive enough if you read Reddit often. Seeing the code summed up was fun, though.  Great read. Similar to the author, I am stunned Amazon uses such a naive algorithm for ranking products. It works terribly, and I imagine it has a serious bottom line impact. Anyone know why they don't upgrade their algorithm? In an older blog post about this algorithm (I think it's linked to in this one), people in the comments observe that Amazon doesn't actually use such a simple algorithm as the post claims. So this could be an example of propogating misinformation. I'm not sure if it's actually as simple as the post claims, but it is pretty horrible when you see something like this:\n\nhttp://evanmiller.org/rating-amazon.png\n\nWhich was taken from another great blog post called [How Not to Sort by Average Rating](http://evanmiller.org/how-not-to-sort-by-average-rating.html). I'm not sure if it's actually as simple as the post claims, but it is pretty horrible when you see something like this:\n\nhttp://evanmiller.org/rating-amazon.png\n\nWhich was taken from another great blog post called [How Not to Sort by Average Rating](http://evanmiller.org/how-not-to-sort-by-average-rating.html). In an older blog post about this algorithm (I think it's linked to in this one), people in the comments observe that Amazon doesn't actually use such a simple algorithm as the post claims. So this could be an example of propogating misinformation.   If you look at the formula, it seems that, as long as upvotes &gt; downvotes, the score of a post _increases_ with time.\n\nlog(z) + y * t / 45000,\n\nwhere t is time. That doesn't make sense, the score is supposed to go down as time goes on. What am I missing? &gt; The score won't decrease as time goes by, but newer stories will get a higher score than older. This is a different approach than the Hacker News's algorithm which decreases the score as time goes by\n\n\nI think the idea is that because newer posts will have a higher *t* value and thus will carry more weight than an older submission. Also, the *t* value of a submission does not change over time. *t* is the time of a posts submission - the start date.  &gt; The score won't decrease as time goes by, but newer stories will get a higher score than older. This is a different approach than the Hacker News's algorithm which decreases the score as time goes by\n\n\nI think the idea is that because newer posts will have a higher *t* value and thus will carry more weight than an older submission. Also, the *t* value of a submission does not change over time. *t* is the time of a posts submission - the start date.    I always got the impression there were automatic downvotes though as some kind of balance. Wouldn't stories hit by that system be less likely to make the frontpage simply because they have downvotes, even if they weren't made by people?\n\nPlease correct me if I'm wrong about there being an automatic system. It's just something I've seen other people commenting about. [From the Reddit FAQ page.](http://www.reddit.com/help/faq#Howisasubmissionsscoredetermined) Oo, thanks! That makes sense now. Well, except how does that stop spam bots? Oo, thanks! That makes sense now. Well, except how does that stop spam bots?  [deleted]  [deleted] [deleted] [deleted]",
    "url" : "http://amix.dk/blog/post/19588"
  }, {
    "id" : 8,
    "title" : "Illustrating Compile-Time Verification in Whiley",
    "snippet" : "  Looks like an interesting language, would play with it if it wasn't JVM. What's wrong with the JVM? I am more interested in native compiling languages, I don't need a vm in the way so to speak. Well, in the future, Whiley will compile down to machine code either via LLVM or via a C translation ...",
    "url" : "http://whiley.org/2012/05/28/illustrating-compile-time-verification-in-whiley/"
  }, {
    "id" : 9,
    "title" : "Question about Kolmogorov Complexity &amp; Godel's Incompleteness Theorem",
    "snippet" : "Kolmogorov Complexity cast in terms of Godel's First Incompleteness Theorem states that there is a number c depending on T (where T is a consistent formal system that incorporates \"sufficient\" amount of arithmetic) such that T does not prove any statements of the form \"the complexity of string s is greater than c\". Thus unless T is inconsistent then there are statements like \"the complexity of string s is greater than c\" that are undecidable in T. Investigating this proof does not give rise to any such statements. I was wondering what are examples such statements?    If you take a random string s of length n (where n is, say, at least twice the encoding length of your proof system), then the with probability 1-o(1), the statement \"the complexity of s is greater than n/2\" is true but not provable in the proof system.",
    "url" : "http://www.reddit.com/r/compsci/comments/u4z13/question_about_kolmogorov_complexity_godels/"
  }, {
    "id" : 10,
    "title" : "Recommendations for a good book on designing concurrent / parallel systems?",
    "snippet" : "Undergrad courses and personal experience have served me well this far, but now my research depends on the efficiency and correctness of my concurrent designs.  Any and all input is appreciated!  [The Art of Multiprocessor Programming](http://www.amazon.com/The-Multiprocessor-Programming-Maurice-Herlihy/dp/0123705916) by Maurice Herlihy and Nir Shavit, this is the book you should be looking into.  [The Art of Multiprocessor Programming](http://www.amazon.com/The-Multiprocessor-Programming-Maurice-Herlihy/dp/0123705916) by Maurice Herlihy and Nir Shavit, this is the book you should be looking into.   The stuff at www.1024cores.net is interesting reading.  If you want to assure correctness, you might want to use something like Spin (spinroot.com) to \"prove\" your model.  At the research paper level, any of the papers by Leslie Lamport ( http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html ) are very good reading. For designing parallel algorithms, there's a good introduction at https://www.cs.cmu.edu/~guyb/papers/BM04.pdf and also the kernel perfbook ( https://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html ).\n\nFinally, for a more theoretic introduction to concurrency primitives, http://plg.uwaterloo.ca/usystem/pub/uSystem/uC++book.pdf is a good undergrad-level textbook.\n\nEdit: Two more from my bookmarks:\nhttp://web.cs.swarthmore.edu/~newhall/cs85/references.html\nhttp://www.cs.uiuc.edu/class/sp06/cs598ig/sched.htm\n\nHTH  The best books I know are: \"Java Concurrency in Practice\" (it is useful regardless of whether you use Java) and \"Elements of distributed computing\" if you're into distributed. Another good one on concurrency is \"The art of multiprocessor programming\" by Herlihy.      I honestly have no idea, but I figure if I keep immutable states, the compiler might be able to do it for me. Unfortunately, I've never really had the benefit of writing for concurrency because none of the programming languages I get to use at work let you do that (except maybe SQL?).",
    "url" : "http://www.reddit.com/r/compsci/comments/u43k4/recommendations_for_a_good_book_on_designing/"
  }, {
    "id" : 11,
    "title" : "A set of top Computer Science blogs",
    "url" : "http://drtomcrick.wordpress.com/2012/05/07/a-set-of-top-computer-science-blogs/"
  }, {
    "id" : 12,
    "title" : "Here's a website for some software I've been working on. It is a genetic algorithm image compression format, which is kinda of fun to watch.",
    "snippet" : "  How does it compare to simpler compression methods like wavelet transforms?  You can get EXTREMELY good localization with wavelets, and they are very simple.  Can you beat JPG2000, for example?  What about JPG? &gt;How does it compare to simpler compression methods like wavelet transforms?\n\nTerrible. So whats the benefit?  Coolness? So whats the benefit?  Coolness? Science, it would have been hard to know if this would work or not without doing it, so he did it and found it didn't work.\n\nMaybe in the future this will be expanded upon and he is just starting out something that will be cool later on, who knows.\n\nDoing things for the hell of it when it's something nobody has done before is very worthwhile indeed. &gt;How does it compare to simpler compression methods like wavelet transforms?\n\nTerrible. How does it compare to simpler compression methods like wavelet transforms?  You can get EXTREMELY good localization with wavelets, and they are very simple.  Can you beat JPG2000, for example?  What about JPG? How does it compare to simpler compression methods like wavelet transforms?  You can get EXTREMELY good localization with wavelets, and they are very simple.  Can you beat JPG2000, for example?  What about JPG?    You should xpost this in [/r/genetic_algorithms](http://www.reddit.com/r/genetic_algorithms/) Submitted 3 months ago. Submitted 3 months ago.   What are you using for a fitness function? The fitness is the summation of the square of the difference of every pixel, compared against the source image.\n\nIf it's not squared (just using the absolute value), it works but is much slower and doesn't produce very good images.\n   There's a link at the top of the page for the download. It requires Java to run, and the source code is not being released at this time. This is a demonstration of the speed of evolution on a single core.\n\nI hope the interface makes sense to people, tonight I will create a short video explaining how to compress a file and then open that compressed file later. The java app really does not work well on OS X by the way. I can provide more details tomorrow if you're interested. There's a link at the top of the page for the download. It requires Java to run, and the source code is not being released at this time. This is a demonstration of the speed of evolution on a single core.\n\nI hope the interface makes sense to people, tonight I will create a short video explaining how to compress a file and then open that compressed file later. Please let me know if you release the code in the future! It'd be really cool to extend this to video.  I'm mostly amazed at how fast it is. Even though he's only using one core, that's what happens when you use a decent language. :) so sick of the language wars so sick of the language wars  I have to say, I was severely disappointed by the lack of [lenna](http://www.lenna.org/) in this video.  Genetic algorithms are the bomb. I hope it gets the attention it deserves :) Genetic algorithms get too much attention already. This is a neat demo of them, but they typically don't actually gain you much (which is why they aren't more widespread) Genetic algorithms get too much attention already. This is a neat demo of them, but they typically don't actually gain you much (which is why they aren't more widespread) Random mutation and selection is the dumbest possible algorithm. Kind of like bozo sort. It can have impressive results, if you have millions of years to spare. Have you actually played with genetic algorithms at all? The speed with which things adapt is almost spooky. The neat thing about genetic algorithms, to me, is that they seem to narrow on in a relatively \"good\" answer rather quickly, but will also result in progressively better answers. And, because of this, you can stop any time and have an answer. Meaning you can also decide before execution on a fixed amount of time you're willing to spend.\n\nAs opposed to traditional algorithms for image and video encoding, where you state parameters up front and then simply have to wait for however long it takes. Thanks for explaining the benefit. I was trying to understand what the concept was because this is the first time I have heard of a genetic algorithm.. and I am doing  masters program in computer science.\n\nKind of a good enough within parameters solution.. which could have a lot of merit. &gt; this is the first time I have heard of a genetic algorithm.. and I am doing masters program in computer science.\n\nWhat program are you in that you've never heard of GAs? That's hard to believe. Considering I started class 5 days ago not that far fetched brah The neat thing about genetic algorithms, to me, is that they seem to narrow on in a relatively \"good\" answer rather quickly, but will also result in progressively better answers. And, because of this, you can stop any time and have an answer. Meaning you can also decide before execution on a fixed amount of time you're willing to spend.\n\nAs opposed to traditional algorithms for image and video encoding, where you state parameters up front and then simply have to wait for however long it takes. Random mutation and selection is the dumbest possible algorithm. Kind of like bozo sort. It can have impressive results, if you have millions of years to spare.  Why this new post? You or someone else has posted this weeks or months ago.\n\nEdit: Never mind. It's the same guy, but now there's a website. Nice job OP. :)     ",
    "url" : "http://www.clamcompression.com"
  }, {
    "id" : 13,
    "title" : "Why is Dynamic RAM is preferred in desktop PCs but Static RAM is preferred in embedded devices?",
    "snippet" : "  Standby Power.\n\nThat's it. Embedded devices live and die by power consumption.\n\nAll this discussion about the complexity of a DRAM memory controllers has a modicum of truth, but not much more than that. Yes DRAM controllers are somewhat complicated, but it isn't that many transistors and transistors are cheap (you can tell they're cheap because they're using memory that requires six transistors per bit). It isn't transistor count of the controller that that makes DRAM unappealing.\n\nSRAM is fast, yes, but it doesn't matter. Even low end ARM chips rely on on-die cache for speed (and the cache is SRAM also, but it doesn't have to pay the IO penalty of driving signals off-die).\n\nRuntime power isn't it either. While it is almost always less than DRAM, it isn't much less than DRAM - especially if you figure watts per unit memory. DRAM has tremendous density.\n\nHowever when SRAM is not being accessed, its idle power draw can be under 1mW. You don't have to put it into self-refresh and it is ready to respond at full speed instantly so you don't have to spend time and power going through a re-training sequence. If you need your device to have the least possible average power draw, SRAM is your best choice.     By the way, this has nothing to do with computer science.  Maybe electrical engineering?  It's not true. Desktop PC's have as much static RAM as possible in their processors. Then, in addition, they add cheaper and slower DRAM to get a few more gigabytes. Then, in addition; they add even cheaper and slower disk to get a few more terabytes.  SRAM is faster but more expensive. DRAM is slower but much cheaper.\n\nAt least, that is what I was taught in my undergrad computer architecture class. I'll let someone else correct me if I'm wrong or elaborate if I'm right. these days DRAM is faster and cheaper and offers higher density. Drastically so. However, DRAM is much more complicated to interface with - a complexity cost that can be unacceptable in an embedded system. DRAM also draws more power, another embedded nono. DRAM also has unpredictable timing. It is not truly random access. Memory is organized into cells, where there is a fixed cost associated with accessing any given cell - but access is very fast once you pay the upfront cost. With SRAM every address can be read in exactly N (usually 1) cycles. \n\nIf your processor lacks hardware to support DRAM, you are going to have a very difficult time adding DRAM to your design, whereas SRAM is usually a trivial addition. \n\n\nEDIT: haha, yeah everyone here is totally right about embedded sram/bram/cache being substantially faster. I was thinking of external SRAMs, which I can't seem to find with clock speeds far above 50MHz. I definitely brain farted there. Obviously the on-chip SRAM can have single cycle access times as fast as the transistors will switch although sometimes with some pipeline latency.\n these days DRAM is faster and cheaper and offers higher density. Drastically so. However, DRAM is much more complicated to interface with - a complexity cost that can be unacceptable in an embedded system. DRAM also draws more power, another embedded nono. DRAM also has unpredictable timing. It is not truly random access. Memory is organized into cells, where there is a fixed cost associated with accessing any given cell - but access is very fast once you pay the upfront cost. With SRAM every address can be read in exactly N (usually 1) cycles. \n\nIf your processor lacks hardware to support DRAM, you are going to have a very difficult time adding DRAM to your design, whereas SRAM is usually a trivial addition. \n\n\nEDIT: haha, yeah everyone here is totally right about embedded sram/bram/cache being substantially faster. I was thinking of external SRAMs, which I can't seem to find with clock speeds far above 50MHz. I definitely brain farted there. Obviously the on-chip SRAM can have single cycle access times as fast as the transistors will switch although sometimes with some pipeline latency.\n these days DRAM is faster and cheaper and offers higher density. Drastically so. However, DRAM is much more complicated to interface with - a complexity cost that can be unacceptable in an embedded system. DRAM also draws more power, another embedded nono. DRAM also has unpredictable timing. It is not truly random access. Memory is organized into cells, where there is a fixed cost associated with accessing any given cell - but access is very fast once you pay the upfront cost. With SRAM every address can be read in exactly N (usually 1) cycles. \n\nIf your processor lacks hardware to support DRAM, you are going to have a very difficult time adding DRAM to your design, whereas SRAM is usually a trivial addition. \n\n\nEDIT: haha, yeah everyone here is totally right about embedded sram/bram/cache being substantially faster. I was thinking of external SRAMs, which I can't seem to find with clock speeds far above 50MHz. I definitely brain farted there. Obviously the on-chip SRAM can have single cycle access times as fast as the transistors will switch although sometimes with some pipeline latency.\n &gt; these days DRAM is faster and cheaper and offers higher density\n\nDRAM is faster than it was before, but you can't compare it to old SRAM or embedded SRAM, because those aren't fair comparisons.  CPU cache is SRAM, and is much much faster than DRAM.  \n\nSandy Bridge SRAM cache can hit 350 GB/s 250 GB/s and 110 GB/s for Level 1, 2 and 3 caches, respectively.  That is an order of magnitude faster than DRAM for even the \"slow\" L3 cache. these days DRAM is faster and cheaper and offers higher density. Drastically so. However, DRAM is much more complicated to interface with - a complexity cost that can be unacceptable in an embedded system. DRAM also draws more power, another embedded nono. DRAM also has unpredictable timing. It is not truly random access. Memory is organized into cells, where there is a fixed cost associated with accessing any given cell - but access is very fast once you pay the upfront cost. With SRAM every address can be read in exactly N (usually 1) cycles. \n\nIf your processor lacks hardware to support DRAM, you are going to have a very difficult time adding DRAM to your design, whereas SRAM is usually a trivial addition. \n\n\nEDIT: haha, yeah everyone here is totally right about embedded sram/bram/cache being substantially faster. I was thinking of external SRAMs, which I can't seem to find with clock speeds far above 50MHz. I definitely brain farted there. Obviously the on-chip SRAM can have single cycle access times as fast as the transistors will switch although sometimes with some pipeline latency.\n these days DRAM is faster and cheaper and offers higher density. Drastically so. However, DRAM is much more complicated to interface with - a complexity cost that can be unacceptable in an embedded system. DRAM also draws more power, another embedded nono. DRAM also has unpredictable timing. It is not truly random access. Memory is organized into cells, where there is a fixed cost associated with accessing any given cell - but access is very fast once you pay the upfront cost. With SRAM every address can be read in exactly N (usually 1) cycles. \n\nIf your processor lacks hardware to support DRAM, you are going to have a very difficult time adding DRAM to your design, whereas SRAM is usually a trivial addition. \n\n\nEDIT: haha, yeah everyone here is totally right about embedded sram/bram/cache being substantially faster. I was thinking of external SRAMs, which I can't seem to find with clock speeds far above 50MHz. I definitely brain farted there. Obviously the on-chip SRAM can have single cycle access times as fast as the transistors will switch although sometimes with some pipeline latency.\n these days DRAM is faster and cheaper and offers higher density. Drastically so. However, DRAM is much more complicated to interface with - a complexity cost that can be unacceptable in an embedded system. DRAM also draws more power, another embedded nono. DRAM also has unpredictable timing. It is not truly random access. Memory is organized into cells, where there is a fixed cost associated with accessing any given cell - but access is very fast once you pay the upfront cost. With SRAM every address can be read in exactly N (usually 1) cycles. \n\nIf your processor lacks hardware to support DRAM, you are going to have a very difficult time adding DRAM to your design, whereas SRAM is usually a trivial addition. \n\n\nEDIT: haha, yeah everyone here is totally right about embedded sram/bram/cache being substantially faster. I was thinking of external SRAMs, which I can't seem to find with clock speeds far above 50MHz. I definitely brain farted there. Obviously the on-chip SRAM can have single cycle access times as fast as the transistors will switch although sometimes with some pipeline latency.\n The big thing about DRAM's power consumption is that power must be applied constantly. SRAM only needs power on updates. Hence 'static' ram. :) Only if you're willing to ignore leakage current. The likelyhood of not being refreshed due to needing a read or write before that happens is unlikely, unless it is going into a powered down state, then  some type of non-volatile memory would likely be used to save the state. ... what? How does this follow from johntb86's comment at all? He was pointing out in essence, that SRAM is volatile. It will need power at *some* point to keep the data.  I'm still not sure I understand. Do you have SRAM and DRAM backwards? He was pointing out in essence, that SRAM is volatile. It will need power at *some* point to keep the data.  The big thing about DRAM's power consumption is that power must be applied constantly. SRAM only needs power on updates. Hence 'static' ram. :)  SRAM is faster (can be as fast as the CPU) but DRAM is cheaper. Given the amount of RAM required in desktop PCs, using DRAM as the main memory is not practical. \n\nOn the other hand, there is still considerable amount of SRAM exists in desktop PCs : [CPU cache](http://en.wikipedia.org/wiki/CPU_cache). \n\nThe cache memory (SRAM) is placed between the CPU and the ~~DRAM~~ SRAM and holds a small portion of the data that is in the main memory (DRAM). It keeps a partial copy of DRAM that is either recently used or expected to be used soon, hence improves performance.\n\nEdit:DRAM-&gt;SRAM. Thanks xshoppyx!  ",
    "url" : "http://www.reddit.com/r/compsci/comments/u25xz/why_is_dynamic_ram_is_preferred_in_desktop_pcs/"
  }, {
    "id" : 14,
    "title" : "UPDATE: Improve your mobile device's battery life, help a fellow redditor, and contribute to science!",
    "snippet" : "This is a follow-up to a post from a few months ago about [an iOS app called Carat](http://www.reddit.com/r/compsci/comments/q1xxg/improve_your_iphones_battery_life_help_a_fellow/) that provides personalized recommendations for how to increase the battery life of your mobile device. We've been trying to get on the App Store since February, and, in the mean time, wrote a version for Android!\n\n[**Download the Android Beta**](http://carat.cs.berkeley.edu/)\n\nThis is part of a research project at [UC Berkeley's AMP Lab](http://amplab.cs.berkeley.edu/) that aims to diagnose energy bugs on mobile devices. You can learn more about the project, including our paranoia-friendly privacy policy, on the Carat website. The download link above asks for your email, which is only used to send you links to new builds; it is not, in any way, associated with the data we collect.\n\nThank you!\n\n**TL;DR** We now have an Android version of Carat, an energy debugging app, in beta and you can [download it here for free](http://carat.cs.berkeley.edu/).  is this the correct subreddit to post this? seems like /r/android would be more approriate.  I put it here because (1) it's a computer science research project, (2) the original post, on which this is following up, was posted in this subreddit, and (3) there is also an iOS version. Would it be bad form to cross-post? is this the correct subreddit to post this? seems like /r/android would be more approriate.   This sounds awesome! Any chance for a Windows Phone 7 version anytime soon?  Does this require root access?  About your privacy policy:\n\n1. Why does it report what apps are running, and how do I turn that off? I don't want to have to be careful about what apps you could see that I ran.\n2. Why does it collect device ID? this in particular is not something I can see any way you could need - at most, you'd need a cryptographic hash of the device ID so you can uniquely identify users. But my device ID is not information you need. 1) Carat identifies which apps are using large amounts of energy across many devices or just under certain conditions (bugs). This is the primary function of the app and it requires that we record the running apps.\n\n2) We need a way to uniquely identify your device, and this is how the API says we should do it. It's a random number from our perspective and we didn't see the point in hashing it to another random number. Perhaps we'll do so in the next build, to assuage such concerns.\n\nSo, basically, we need the app info and the device ID doesn't identify you, personally. Does that answer your questions? hey let me make a suggestion: why don't you hash the app names before you send them?  then, you can identify which apps are using energy across different phones without actually knowing what apps people are running.  this allows you to continue your research and also have much less privacy concerns.  just store the hash of the app name in the database where you would normally store the app name.\n\nEdit: if you use something strong like sha256, the probability of a collision is infinitesimal.  I'll be happy to link you to some papers on why using sha256 t 1) Carat identifies which apps are using large amounts of energy across many devices or just under certain conditions (bugs). This is the primary function of the app and it requires that we record the running apps.\n\n2) We need a way to uniquely identify your device, and this is how the API says we should do it. It's a random number from our perspective and we didn't see the point in hashing it to another random number. Perhaps we'll do so in the next build, to assuage such concerns.\n\nSo, basically, we need the app info and the device ID doesn't identify you, personally. Does that answer your questions? To some extent. I guess if the device ID is just random, it doesn't matter much if you get it - I imagine that if you were looking for me and got a hold of my device id, you could just hash it again anyway to see if it was the same.\n\nAs for the apps, though, I don't want your tool reporting the names of apps that I might be working on or apps that I have early access to (not as likely as me writing my own).  How do you propose it determine which apps are running down your battery without knowing which apps you're running? How do you propose it determine which apps are running down your battery without knowing which apps you're running? provide a way to ask me before it sends the data, so I can delete apps from the list before they're sent. leave this option off by default, and hide it reasonably so that only people who are really looking will find it; then people like me who have privacy worries with app names can filter them. or they could just hash the names of the programs and use the hashes in their database.  this is what they should do, i think. This is a good idea as long as we only want to report app data on the clients. If we want to, for example, provide a list on our website of popular apps that are top offenders, we would need to know what the original name was. As someone mentioned, we could crawl a list of popular apps and compute the hashes, but we might miss less popular sever offenders.\n\nAnyway, we're faced with a difficult tradeoff between privacy and diagnostic power. Our philosophy so far has been (1) if the database is made public, it should personally identify no one and (2) report as much data as possible given #1. I hadn't considered that names of private development apps might be sensitive. We are reading your comments and thinking of ways to improve, so thanks for articulating your concerns! you can literally hash every name in the app store and then you won't miss any apps.  you will only miss custom apps that aren't listed in the app store yet.  you can easily obtain the app store data using a webcrawler and a simple parser, then just store the results in a DB.  you can probably get some poor undergrad to do it in exchange for a letter of recommendation and research experience.  i know i would do it if i was still an undergrad.\n\nP.S. please admit me to your school It's actually not that simple. As it turns out, on iOS the process name reported by the API does not have to match the visible name of the app. That is, the app \"Pandora\" might show up in the process list as \"Pandora Mobile\". We can't actually access the visible names programmatically on the device. At the same time, the process names are not listed anywhere in the App Store. So, you see, there is a mapping problem of process names to app names. We have been skirting that problem by reporting process names and not worrying about what real app it maps to.\n\nWhew.\n\nThis would also be complicated for Android because there are multiple stores, but probably less of a hassle than iOS.\n\nP.S. By the power vested in me as a postdoc... I can do exactly bupkis. or they could just hash the names of the programs and use the hashes in their database.  this is what they should do, i think. That... is a much better idea. I feel like a fool.\n\nThey'd be able to easily \"brute force\" hashes of popular apps, but apps with names they have no access to would be difficult to hash. The less popular an app, the harder to brute force its hash. And they'd have a prepared database of known app name hashes from the data they've already collected and would have then had to hash.\n\n don't feel like a fool!  i do deduplication research so i think about hashing everything in the world all day and night.  someday it'll actually come in handy!  hahaha\n\nand you're totally right, but no brute force needed.  they could easily figure out the popular apps and link their names in the database: just go through the app store and hash every name.  then, ones that are in development and aren't in the app store yet won't have a name tied to the hash.  \n\nalso, you're the smart one who figured out the privacy concerns with this.  i read about it and thought \"what a great idea!\" and didn't even think of how it could impact my privacy.  i guess it takes all kinds :)\n\nlet's just hope they heed our awesome suggestions! &gt; don't feel like a fool!\n\nI feel like a fool because I thought of hashing, and then dismissed it because I couldn't figure out a way to reconcile my want to have each device have a salted hash, and their need to correlate apps. It didn't even occur to me to consider no salt.\n\n&gt; just go through the app store and hash every name. \n\nthat's what I meant by \"brute force\", and it's also why I put quotes on it.\n\n&gt; also, you're the smart one who figured out the privacy concerns with this.\n\nI'm a little bit irrationally paranoid about privacy concerns right now. &gt; don't feel like a fool!\n\nI feel like a fool because I thought of hashing, and then dismissed it because I couldn't figure out a way to reconcile my want to have each device have a salted hash, and their need to correlate apps. It didn't even occur to me to consider no salt.\n\n&gt; just go through the app store and hash every name. \n\nthat's what I meant by \"brute force\", and it's also why I put quotes on it.\n\n&gt; also, you're the smart one who figured out the privacy concerns with this.\n\nI'm a little bit irrationally paranoid about privacy concerns right now.  I'm running on a low end phone how much memory does this use?  i've registered it on february and i'm waiting for the download link since then!  Carat gave me [this suggestion](http://imgur.com/f5FPR) tonight...  Is there a good place to provide feedback about the app?\n\nI find some of the \"right arrows\" on the \"My Device\" page are a little difficult to register a successful touch to view the detailed information.  This is particularly an issue for me on the \"OS version\" and \"Device mode\" lines.  This is also on a Galaxy Nexus, which already has a pretty large screen.\n\nAll things considered, this is a relatively minor thing, but for a while I wasn't sure if the arrows were supposed to indicate that I could access more information.   First time I launched on my Galaxy Nexus running CM9 nightly, I got a \"Unfortunately Carat has stopped\" message.  After that, it has been fine.  Looking forward to using this.\n\nAre there any restrictions to running the beta on multiple devices with one email address? First time I launched on my Galaxy Nexus running CM9 nightly, I got a \"Unfortunately Carat has stopped\" message.  After that, it has been fine.  Looking forward to using this.\n\nAre there any restrictions to running the beta on multiple devices with one email address? I realize this is off topic, but how are the CM9 nightlies? Any serious lack of functionality? And it's there a big list somewhere that lists all of the issues for the nightlies?  First time I launched on my Galaxy Nexus running CM9 nightly, I got a \"Unfortunately Carat has stopped\" message.  After that, it has been fine.  Looking forward to using this.\n\nAre there any restrictions to running the beta on multiple devices with one email address? Having multiple devices won't affect anything related to Carat; only the beta distribution service (Zubhium) uses the email. I say go for it! Let me know if it works, haha.\n\nWe should get a crash report from Carat. I'm glad it worked after that. Thanks for signing up.  Will you be releasing this to the Play Store soon? I wouldn't worry about the \"beta\" status of your software since it will probably still be far more stable than a lot of apps out there.  [deleted] It is usually asleep but awakens occasionally to take a sample of the device state. We did energy metering experiments with hardware and verified that Carat takes less energy than the standard weather app that comes with most phones. It's very lightweight.  I'll have a look at it!\n\nI'll second the suggestion to cross-post to r/android.  I had a research project that involved an android app and I got a very positive response there (and lots of new users).     Installed on my Droid X2.  Thanks. Installed on my Droid X2.  Thanks.   First time I ran it on my Captivate Glide it force closed. Then I tried it again and it ran. After I left it though there's no service listed in Running Services -- should there?  This is great. I am really looking forward to getting my first set of suggestions.  ",
    "url" : "http://www.reddit.com/r/compsci/comments/u1jk8/update_improve_your_mobile_devices_battery_life/"
  }, {
    "id" : 15,
    "title" : "Best graphics-oriented books?",
    "snippet" : "Could anyone offer recommendations for best reading materials for interest in rendering/graphics?  I'm trying to self-study and would like to find some materials that would explain to a newbie but also be a good investment.  For video game type graphics, [Real-Time Rendering](http://www.amazon.com/Real-Time-Rendering-Third-Edition-Akenine-Moller/dp/1568814240/ref=sr_1_1?ie=UTF8&amp;qid=1337845740&amp;sr=8-1) covers a little bit of everything, and doesn't get bogged down by too much code or math. For offline stuff like ray-tracing, I'd agree with admplaceholder that [Physically Based Rendering](http://www.amazon.com/Physically-Based-Rendering-Second-Edition/dp/0123750792/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1337846283&amp;sr=1-1) is rather awesome.      ",
    "url" : "http://www.reddit.com/r/compsci/comments/u23uo/best_graphicsoriented_books/"
  }, {
    "id" : 16,
    "title" : "Want to learn more about memory but don't know where to start.",
    "snippet" : "Hello CompSci! I have a degree in Computer Science and I'm in my second year of graduate school for offensive and defensive security.  I am looking for something to really get into and get a deep understanding of. Because programs need space to execute (duh) I figured memory (heaps, stacks, etc.) would be a good place to invest some time.\n\nI know the basics and have good programming skills, so what I'm really looking for are articles, tutorials, books, etc. on pretty much anything memory related: buffer overflows, how applications are assigned memory, how the OS manages memory, what a stack is, what a heap is, how to access your allotted heap or stack space manually, how it is possible for memory to be accessed by crackers, paging, cache, to just name a few. As I said already I have a CS degree so some of these listed items would be review, but it never hurts to brush up on them.\n\nI'm really just getting into my field so any resources, even if novice level, would be appreciated to get me up and going! Thanks for your help!  Drepper's \"What every programmer should know about memory\".  It's seven *long* parts starting here:\nhttp://lwn.net/Articles/250967/\n Drepper's \"What every programmer should know about memory\".  It's seven *long* parts starting here:\nhttp://lwn.net/Articles/250967/\n great thanks! Drepper's \"What every programmer should know about memory\".  It's seven *long* parts starting here:\nhttp://lwn.net/Articles/250967/\n  You should start at 0x00000000 You should start at 0x00000000 You should start at 0x00000000  &gt; Want to learn more about memory but don't know where to start.\n\nI know there's a pun in there somewhere...  I thought that this series of articles (more links in the side bar) was a great quick-start on many of the important topics:\nhttp://duartes.org/gustavo/blog/post/anatomy-of-a-program-in-memory\n   Are you more interested in Linux, Windows, both or other?  Learning the ins and outs of the executable format for your platform of choice will pay off.  I don't have any great links handy, just look up a tutorial for Portable Executable (Windows) or ELF (Linux).  \n\nFrom there, understanding the stages of linking and loading is a good next step.  \n\nThese topics will give you lots of great theoretical background for understanding points of entry for hackers and defense mechanisms against them. Well I'm INTERESTED in learning both. However I've learned from past mistakes one should focus on just one thing and get really good at it, while having at lease a decent understanding of the other. Having said that I'd like to focus on Windows, it will be more relevant to my future. \n\nBut how much similarity exists between the two, so that a person could really focus on that then branch off later?    [deleted]",
    "url" : "http://www.reddit.com/r/compsci/comments/u11dr/want_to_learn_more_about_memory_but_dont_know/"
  }, {
    "id" : 17,
    "title" : "Good online resources for quantum computing theory?",
    "snippet" : "I'm interested in learning more about the theory of quantum computing. Which is to say, construction of quantum logic ( quantum gates ), quantum complexity theory, quantum algorithm analysis, etc. but I haven't managed to find anything in the way of good online resources.\n\nWikipedia suffers from the same pathology it always does when it comes to advanced topics, which is there's either no information on the topic or the information available depends on so much other information it's hard to get a handle on where to start and my second-tier university didn't really prepare me for any of it (besides the requisite linear algebra).\n\nTo be clear, I'm not interested at all in actual physical quantum computers and their construction, but rather the theoretical underpinnings of the idea of quantum computers.  http://michaelnielsen.org/blog/quantum-computing-for-the-determined/        ",
    "url" : "http://www.reddit.com/r/compsci/comments/u10os/good_online_resources_for_quantum_computing_theory/"
  }, {
    "id" : 18,
    "title" : "Modern computers are supposed to be Von Neumann machines, and thus are supposed to be Turing equivalent... but has anyone ever shown that Turing machines are Von Neumann equivalent?",
    "snippet" : "I have been looking at Sipser's *Introduction to the Theory of Computation* and comparing it with Boolos, Burgess, and Jeffrey's *Computability and Logic*.\n\nIt looks like any proof of Turing-equivalence rapidly gets very long, so authors reasonably just want to prove the Turing-equivalence of very small systems.\n\nAnd of course, while lambda-calculus and Turing machines compute the same class of number-theoretic functions, i.e. the recursive functions, lambda calculus is not exactly equivalent to Turing machines because Turing machines can compute some things that lambda calculus cannot, e.g. the \"formal Church's thesis\" given at:\n\nhttp://cstheory.stackexchange.com/questions/625/relationship-between-turing-machine-and-lambda-calculus\n\n\nThe issue seems to be that the Church-Turing Thesis only applies to computations from the naturals to the naturals (N-&gt;N), and it is possible to consider other types of computations, as mentioned at:\n\nhttp://cstheory.stackexchange.com/questions/1117/realizability-theory-difference-in-power-between-lambda-calculus-and-turing-mac\n\nI had presumed that at some point around 1941 or so, Von Neumann had written a proof to show that every electronic computer would be able to do nothing more than could be done by a Turing machine.\n\n\nHowever, I can't find a proof that Von Neumann architecture (or modified Harvard architecture, or whatever computers use for Intel and AMD chips) is *limited* to Turing machines. I know von Neumann machines can *simulate* Turing machines, but it's not clear to me that everything a von Neumann machine can do must also be doable by a Turing machine.\n\nSo, it seems to me that perhaps Turing machines can only calculate one type of computations, from N to N.  Perhaps von Neumann machines can do something that Turing machines cannot do, e.g. computations ((N-&gt;N)-&gt;N).\n\nIf von Neumann machines are more powerful than Turing machines, then models such as hypercomputation ought to be not just possible, but quite easy to investigate on normal Intel and AMD chips:\n\nhttp://en.wikipedia.org/wiki/Hypercomputation\n\nIf on the other hand, Intel and AMD chips and all other von-Neumann-derived architectures are limited only to what a Turing machine could do, then hypercomputation ought to be disprovable.  This isn't what you're looking for, but remember that you can model a Von Neumann machine with a simple DFA, if you have the patience - a Von Neumann computer has a (large) finite number of states and deterministic transitions between them.\n\nThat said, it's pretty straightforward to model a Von Neumann machine with infinite memory using a Turing machine, isn't it? Turing machines can do all the control and arithmetic operations a normal CPU does, and the tape can serve as input, output, and memory trivially. Certainly one can model an electronic circuit with a DFA, but a DFA is generally designed to have at least one reachable halt state, and it's not clear that an electronic circuit is so designed.\n\nEven if we could guarantee that the DFA modeled the electronic circuit, the electronic circuit might be producing outputs that were always \"side effects\" to its halt state, so the output of the DFA would not necessarily model the output of the electronic circuit. I don't have Sipser in front of me, but I don't think there's anything in the definition of a DFA that requires it to have a reachable halt state. But that objection doesn't make much sense; your question is, essentially, whether there's some computation that can be carried out by a von Neumann computer but not a Turing machine. If you're modeling some particular computation done by a von Neumann machine with a DFA, the end of the computation will correspond to a set of halt states.\n\nAs to your second paragraph, the side effects are reflected by which halt state the DFA reached. That is to say, each state of the DFA would represent the von Neumann machine being in a particular memory state and processor state; you read the side effects from that. I should mention also that real electronic circuits tend to have inputs from the outside world, which looks more like a Turing oracle than a finite set of input.\n\nThe reachable halt states are important:  there must be a set of accept states.  The objection is that if the electronic circuit is always on and always processing input from the real world, it can't ever reach a halt state. The input from the outside world is the input to the DFA. It's probably better to think of it as an NFA with epsilon-transitions where necessary (which will be a lot of them).\n\nSipser, second edition, page 36: \"you can see that setting F to be the empty set yields 0 accept states, which is allowable.\" That's a DFA that only accepts the empty language.  Most Turing machine proofs depend on the TM accepting a nontrivial, non-empty language.\n\nEdit:\nPage 142 of the same text mentions that for the advanced stuff, they want TMs that both recognize and decide.  A DFA with no accept states is equivalent to a TM that cannot even recognize, much less decide.\n\n  Modern computers are not even as powerful as Turing machines. Turing machines have infinite state, which is simply not physically realizable. Modern computers are finite state machines with very large but finite tapes.\n\nFor instance, a 32 bit computer can address 2^32 bytes of memory, and combined with its register set, that's a fixed number of states. Even if you augment it with a device to extend the addressable memory, like a hard drive, the machine can at most compute a 2^32 bit address on that device, which is also finite (although certainly astronomically large).\n\nIn principle, if we had a true universal Turing machine, we could solve the Halting problem for any program written for an Intel/AMD CPU. We just need to simulate the computer using all possible inputs and outputs, and since this is a finite number of states, the simulation will eventually halt. We can't do this now simply because the number of states is far too large to fit in our addressable memories, but this limitation doesn't exist for Turing machines. If we know that real hardware computers are finite state machines, then the article linked above has some relevant portions:\n\nhttp://plato.stanford.edu/entries/church-turing/\n\n&gt;Turing did not show that his machines can solve any problem that can be solved \"by instructions, explicitly stated rules, or procedures\", nor did he prove that the universal Turing machine \"can compute any function that any computer, with any architecture, can compute\". He proved that his universal machine can compute any function that any Turing machine can compute; and he put forward, and advanced philosophical arguments in support of, the thesis here called Turing's thesis. But a thesis concerning the extent of effective methods -- which is to say, concerning the extent of procedures of a certain sort that a human being unaided by machinery is capable of carrying out -- carries no implication concerning the extent of the procedures that machines are capable of carrying out, even machines acting in accordance with ‘explicitly stated rules’. For among a machine's repertoire of atomic operations there may be those that no human being unaided by machinery can perform.\n\n&gt;The further proposition, very different from Turing's own thesis, that a Turing machine can compute whatever can be computed by any machine working on finite data in accordance with a finite program of instructions, is sometimes also referred to as (a version of) the Church-Turing thesis or Church's thesis. For example, Smolensky says:\n\n    \n&gt;connectionist models ... may possibly even challenge the strong construal of Church's Thesis as the claim that the class of well-defined computations is exhausted by those of Turing machines. (Smolensky 1988: 3.)\n\n\n&gt;This loosening of established terminology is unfortunate, for neither Church nor Turing endorsed, or even formulated, this further proposition.  &gt; Turing did not show that his machines can solve any problem that can be solved \"by instructions, explicitly stated rules, or procedures\", nor did he prove that the universal Turing machine \"can compute any function that any computer, with any architecture, can compute\".\n\nCorrect. If your machine supports instructions that can function as an Oracle, then you have a super-Turing machine. This was all in Turings thesis as well IIRC. Hypercomputers are a type of super-Turing machine.\n\nBut true Oracles seem to require infinities of various sorts. For instance, instructions for arithmetic on real numbers.\n\n&gt; For among a machine's repertoire of atomic operations there may be those that no human being unaided by machinery can perform.\n\nThis would require some physical law that violates classical physics in some way, ie. superluminal communication, time travel, basically an operation that computes some infinity in finite time. Even quantum mechanics yields at best a quadratic speedup on classical computation (via superposition), but it can't compute any more functions than a Turing machine can compute.\n\nI think a better way to look at it is that a Church-Turing computer has characteristics that can solve problems in various complexity classes. [Here's a great paper on physics and computational complexity by Scott Arronsen](http://www.reddit.com/r/compsci/comments/ppdj1/connections_between_computer_science_and/c3r8g3b).\n\nVery unlikely that any computer we can build will be able to compute *more* functions than a Turing machine though. In fact, I'd put money down that it is fundamentally impossible. Various laws of physics define upper bounds on the amount of energy in the universe, and thus the amount of information. Oracles all require solving infinite problems in finite time, which in principle means infinite time and/or infinite energy and/or infinite space. The physical limits of reality on computing are noteworthy.  Solving NP complete problems as Scott Arronsen describes it is interesting, but Goldin and Wegner have described the importance of breaking from the Turing paradigm just to describe everyday technology, such as Internet processes.\n\nhttp://www.cse.uconn.edu/~dqg/papers/cacm02.rtf\n\n Interaction is not magic. Every interaction point where the input is not known beforehand is simply a non-deterministic choice. These are modelled in lambda calculi all the time, even if they do often get their own calculi for convenience (Pi, join, etc.). In particular, this point from section 3 is completely false:\n\n&gt; The field of computing has greatly expanded since the 1960s, and it has been increasingly recognized that artificial intelligence, graphics and the Internet cannot be expressed by Turing machines.\n\nHaskell is a perfect counterpoint, since it provides the purity of abstract mathematics while allowing interaction with an environment in a way that doesn't violate referential transparency. Haskell has been applied to all of these domains.\n\nWegner further holds the Pi calculus as an example of an interactive calculus that ought to be more powerful than Turing machines and the lambda calculus, but there are embeddings of the Pi calculus in lambda, and lambda in Pi, so neither is more powerful than the other. At best, one formalism may merely be more *convenient* for modelling certain scenarios. &gt;Wegner further holds the Pi calculus as an example of an interactive calculus that ought to be more powerful than Turing machines and the lambda calculus, but there are embeddings of the Pi calculus in lambda, and lambda in Pi, so neither is more powerful than the other.\n\nClearly the lambda calculus can be done in the pi-calculus.  I don't know that the pi-calculus can be done in terms of the lambda calculus, but that might be true.  I have been looking for papers that prove that the pi-calculus (or the cost-calculus) can be done with nothing more than lambda calculus, but so far I haven't found papers that prove it.  (Cockshott and Michaelson say Milner proved it, but I've looked at the Milner paper and I think they're misquoting it.)\n\n\n\n\nJohn R. Longley claims that different formalisms can compute at different type-levels.\n\n\nhttp://www.citeulike.org/user/ds/author/Longley\n\n\nEberbach has a lot of proofs about his cost-calculus that seem correct to me, but not everyone agrees with Eberbach.  (Obviously, the referees who keep publishing his papers must think his proofs are correct.)  Ekhard has argued that Eberbach, Goldin, and Wegner are plain wrong.\n\nAlso, there's a paper called:\n\nAre There New Models of Computation? Reply to Wegner and Eberbach\n\n\nby Paul Cockshott and Greg Michaelson, but it seems to avoid the argument entirely.  Wegner and Eberbach lay out huge, detailed proofs, and Cockshoot and Michaelson just write a sentence or two saying, \"Nope, we don't believe it.\"  As far as I can tell, their arguments are just misguided. For instance:\n\n&gt;The π-calculus is not a model of computation in the same sense as the TM: there is a difference in level. ... Since it is possible to write a conventional computer program that will apply the formal term re- write rules of the π-calculus to strings of characters representing terms in the calculus then it would appear that the π-calculus can have no greater computational power than the von Neumann computer on which the program runs. The language Pict[40] is an example of this. Since it is also possible to implement the λ- calculus in the π-calculus[41] one can conclude that the π-calculus is just one more of the growing family of computational models that are Turing Machine equivalent.\n\nTheir argument depends on Milner:\n\nElements of interaction: Turing award lecture\n\nAuthor: \tRobin Milner \t\n\nPublished in:\n\nCommunications of the ACM CACM Homepage archive\n\nVolume 36 Issue 1, Jan. 1993\n\nPages 78-89 \n\nAnd I've looked at that article, and I don't think it says that the pi-calculus is just an alternate form of lambda-calculus.  I could be insufficiently familiar with the underlying issues. &gt; I don't know that the pi-calculus can be done in terms of the lambda calculus, but that might be true. \n\nConsider how a uniprocessor simulates multiprocessing via time-slicing. Take a lambda calculus program, convert it to CPS form with a \"time\" token in the environment indicating the number of ticks remaining until the next time slice, and a list of pi processes that is managed by the pi calculus api. The token is decremented on every lambda application.\n\nWhen that token reaches 0, append the currently executing pi process to the end of the pi process list, pop the next pi process off the list, reset the timeslice token, and resume the popped computation.\n\nThat's how you'd implement a simple pi calculus embedding in a straight single-threaded lambda fragment of Haskell. A more straightforward translation to simpler lambda terms might be possible, but the above suffices to demonstrate that it is possible in principle, as all of the above features have well known expressions in the lambda calculus. Wow.  That will take a bit of research and mental digestion before I can really understand it.\n\nI am not a Haskell programmer.  I'll have to look into that a bit.\n\n\nBy CPS I am assuming you mean this:\n\nhttp://en.wikipedia.org/wiki/Continuation-passing_style\n\nThanks for the thoughtful answer.  I must not be understanding your question correctly. Why is this not just an application of the Church-Turing thesis?  First, Von Neumann machines with finite memory are Finite State Machines.\n\nSecond, it is possible to simulate all finite state machines on a Universal Turing Machine. [See this article.](http://web.mit.edu/manoli/turing/www/turing.html)\n\nSo all computations that a Von Neumann architecture can perform, some Turing Machine can perform. &gt;Von Neumann machines with finite memory are Finite State Machines.\n\nI would appreciate links to textbooks, papers, or FAQs that spell out the details of this.  Possibly I need to be looking at architecture-of-circuits books rather than math books for this issue.\n\nAs I have mentioned before, there are a lot of papers talking about computations that are done on Von Neumann machines that are claimed to be super-Turing.  These papers have lengthy proofs.\n\nIf the statement \"Any computation done on a Von Neumann machine is necessarily Turing-equivlent\" had been accepted by the journals that printed those papers, those papers would never have been printed. So you're saying that Von Neumann machines have an infinite number of states or that they're nondeterministic?\n\nWhich is it? I'm saying that I don't have a citation of the relevant proofs.\n\nGoing from a proof about pure mathematics to a proof about a class of  physical objects is a pretty huge leap. If they have a finite number of states and are deterministic, they can be modeled by a finite state machine, because *that's what a finite state machine does.*\n\n",
    "url" : "http://www.reddit.com/r/compsci/comments/u1ptn/modern_computers_are_supposed_to_be_von_neumann/"
  }, {
    "id" : 19,
    "title" : "Is there any formal theory dealing with object oriented programming?",
    "snippet" : "A Grad student mentioned that there was something about reasoning in a formal manner but did not remember the name of it.  The book *Types and Programming Languages, Benjamin C. Pierce, MIT Press, 2002* covers the subject quite well.\n\nPierce is also one of the authors of Featherweight Java, which has been mentioned in other comments.        Formalisation of object-oriented programming exists, but seems to be retroactive. Does that mean delivering a cart then putting a horse to push it afterwards? Something like that. To be fair, a lot of things were invented first and understood later. I think only functional programming came after the theory.      Welcome to computer science. There will be no science.    I'm interested to see what people come up with here. My understanding has always been that it would be impossible to formalize OO since it is based on human psychology and behavior and not math. Where'd you get that idea?\n\nI honestly don't see the connection between psychology and OO. Not sure I can lead a discussion on this, but the way we classify concepts in OO is based on how we as humans best learn and interpret code.  It's intended to make code easier to understand for humans and can have cultural and other non-mathematical implications\n Formal aspects of programming languages are rarely concerned about how a programmer might come up with a certain program, and most often deal with reasoning about a program that has already been written. If your programming language is not amenable to any kind of automated reasoning, how do you compile it? [deleted] Not sure I can lead a discussion on this, but the way we classify concepts in OO is based on how we as humans best learn and interpret code.  It's intended to make code easier to understand for humans and can have cultural and other non-mathematical implications\n I'm interested to see what people come up with here. My understanding has always been that it would be impossible to formalize OO since it is based on human psychology and behavior and not math.",
    "url" : "http://www.reddit.com/r/compsci/comments/u0p1v/is_there_any_formal_theory_dealing_with_object/"
  }, {
    "id" : 20,
    "title" : "Microsoft's FDS data-sorter crushes Hadoop",
    "snippet" : " ",
    "url" : "http://www.theregister.co.uk/2012/05/22/microsoft_research_flat_datacenter_storage/"
  }, {
    "id" : 21,
    "title" : "Computer Algorithms: Karatsuba Fast Multiplication",
    "url" : "http://architects.dzone.com/articles/computer-algorithms-karatsuba"
  }, {
    "id" : 22,
    "title" : "How does DDR3 RAM Keep up with processors that are 3* as fast ?",
    "snippet" : "Title, thanks  Because every processor operation doesn't require loading/unloading data from RAM. The processor has cache and registers on it that are used as intermediary holding places for data that needs to be accessed very, very quickly, and it swaps information between these and the RAM to perform work on it. Efficient caching limits the amount of time needed to retrieve data from memory. Exactly.  In other words, RAM doesn't keep up.  Systems like cache just help mitigate the difference.\n\nIf it seems silly to anyone, remember: cost, cost, cost. Cost? Blasphemy. All my machines have at least 16GB SRAM wired directly to the CPU! Even apart from cost I think this would be problematic? How? For SRAM, the time it takes to get a signal to the memory and the time it takes to select a line dominates the time it takes to do a read (or write). That 16GB SRAM bank would be pretty slow compared to a cache. Cache *is* SRAM. The only distinction is that it's on the same die as the CPU so it's much faster to access. You are right though, die-external SRAM would be much slower than cache. Still a hell of a lot faster than DRAM, though.\n\nEDIT: I just remembered that Slot 1 Pentium IIs had die-external cache on the daughterboard. So this is not a hard-and-fast rule. Well, that was what I said. Note that even on the same die, increasing the size makes it slower. That's why there are multi-level caches -- to get the kind of access times you want for high-speed cpus, you really cannot access a bank bigger than roughly 64kB, no matter what. Cache *is* SRAM. The only distinction is that it's on the same die as the CPU so it's much faster to access. You are right though, die-external SRAM would be much slower than cache. Still a hell of a lot faster than DRAM, though.\n\nEDIT: I just remembered that Slot 1 Pentium IIs had die-external cache on the daughterboard. So this is not a hard-and-fast rule. &gt;The only distinction is that it's on the same die as the CPU\n\nIBM engineers would like to talk to you: http://en.wikipedia.org/wiki/File:Power5.jpg Cache *is* SRAM. The only distinction is that it's on the same die as the CPU so it's much faster to access. You are right though, die-external SRAM would be much slower than cache. Still a hell of a lot faster than DRAM, though.\n\nEDIT: I just remembered that Slot 1 Pentium IIs had die-external cache on the daughterboard. So this is not a hard-and-fast rule. Not 20 seconds ago I was wondering what happened to COAST.  I don't remember what era they were, but the old side-card processors used sram on the board next to the die didn't they? Exactly.  In other words, RAM doesn't keep up.  Systems like cache just help mitigate the difference.\n\nIf it seems silly to anyone, remember: cost, cost, cost. Exactly.  In other words, RAM doesn't keep up.  Systems like cache just help mitigate the difference.\n\nIf it seems silly to anyone, remember: cost, cost, cost. Because every processor operation doesn't require loading/unloading data from RAM. The processor has cache and registers on it that are used as intermediary holding places for data that needs to be accessed very, very quickly, and it swaps information between these and the RAM to perform work on it. Efficient caching limits the amount of time needed to retrieve data from memory. What about fetching the instructions in the first place ?  There is an instruction cache as well. To clarify, sometimes there's a separate instruction cache, and sometimes the cache is shared between instructions and data. Is that the difference between Von Neumann architectures and Harvard architectures? No; Harvard and von Neumann architectures are the two extremes of code/data separation. The \"modified Harvard architecture\" is the hybrid between the two that you're thinking of.\n\nIn a pure Harvard architecture, you have completely separate memory for code and data - there is no overlap. Pure Harvard machines are usually DSPs or microcontrollers (e.g. the AVR beloved of the Arduino), as there's extra complexity in handling separate code memory, because there's a difference between the code address 0x1000 and the data address 0x1000, and you end up having to carefully code around the limits of your code memory and data memory (e.g. recalculating values, rather than storing them, because you're out of data memory, but have spare code memory).\n\nIn a pure von Neumann architecture, code and data all come from the same memory space; this is your familiar cacheless 8086, 68000, ARM, 6502 etc. They're simpler to program due to the fact that you can interleave code and data freely, but are more complex for a given performance level.\n\nFinally, we have the modified Harvard architecture. On their external buses, this behaves just like von Neumann architecture (one address space for code and data), but internally, it has separate memory spaces like a Harvard architecture, with some sort of multiplexer converting instruction space and data space accesses to memory spaces accesses on the external bus. This is the current common model for x86 processors; you typically have L1 I$ and L1 D$, then the multiplexing into general memory cache is done, then you have L2$ and beyond to memory. Such a design is a Harvard architecture at the execution units and in L1 cache, and a von Neumann architecture from L2 cache and below. The downside here is a little extra complexity in cache management (for modern architectures, it's not so bad as you can insist that the code manages I$/D$ coherency itself, but x86 needs hardware that ensures that I$ and D$ are coherent), but the upside is that you have the programming simplicity of von Neumann, with the performance of Harvard. Is that the difference between Von Neumann architectures and Harvard architectures? Is that the difference between Von Neumann architectures and Harvard architectures? To clarify, sometimes there's a separate instruction cache, and sometimes the cache is shared between instructions and data. What about fetching the instructions in the first place ?  What about fetching the instructions in the first place ?  They are loaded from memory, slowly, into cache; then they run fast until other instructions are needed.  In most applications, the hot blocks of code are very small, and will fit in cache (some whole programs can fit in cache, but it doesn't work that way), while that program is running.\n\nAside: Wtf is up with Reddit?  Why can't make more than one comment in 7 minutes? Automated spam detection is done per subreddit; rate-limiting is done as part of that. (In other words, if you haven't posted a lot of upvoted stuff to /r/compsci before, you'll get rate-limited.) That's interesting; I would have assumed that the site-wide spam detection results would be collectively used to do rate-limiting, because it seems weird that somebody would spam one sub and be a useful contributor in others, but I suppose it prevents mods of a small sub from effectively banning someone from the site by marking all their posts as spam. They are loaded from memory, slowly, into cache; then they run fast until other instructions are needed.  In most applications, the hot blocks of code are very small, and will fit in cache (some whole programs can fit in cache, but it doesn't work that way), while that program is running.\n\nAside: Wtf is up with Reddit?  Why can't make more than one comment in 7 minutes? They are loaded from memory, slowly, into cache; then they run fast until other instructions are needed.  In most applications, the hot blocks of code are very small, and will fit in cache (some whole programs can fit in cache, but it doesn't work that way), while that program is running.\n\nAside: Wtf is up with Reddit?  Why can't make more than one comment in 7 minutes? &gt;Wtf is up with Reddit? Why can't make more than one comment in 7 minutes?\n\nYou normally can, there might be some sort of issue with your connection or your browser (or the site!) I know; this is the first time I've hit such a limit. Of course it is only after the fact that I hover over your name to see your account age.  :blush:\n\nTo make things more interesting, your message reply triggered 7 other orangereds (ones that were already marked as read a day ago through over a year ago). Not a problem.\n\nSame thing happened to me yesterday; I think all is not well in reddit land. Not a problem.\n\nSame thing happened to me yesterday; I think all is not well in reddit land.  if you're the processor, stuff in your house is in your cache. Ram though is stuff in your storage space across town. When ram speed increases you just basically buy a faster car. It'll get you there faster than your old car, but it's still way across town.\n\nSo when the weather changes and you need to put away your bicycle and pick up your ski equipment, that takes a while.\n\nA disk is stuff in China that needs to take a boat. It'll be months. Don't hold your breath. And an SSD is simply a faster boat than a hard disk.  It'll get you there twice as fast, but it's still going to be a couple months. And the internet is a planet in another solar system. But still not as far away as your monitor, according to John Carmack.   Yeah, like others said, the RAM can't keep up. However, also note that a single DDR module transfers 64 bits at a time, and dual and triple channel setups exist, resulting in transfers of up to 192 bits at a time.  If it's only a third the speed, that's actually ridiculously good compared to how bad it has been.\n\nBut yeah, caching, especially cache coherency. And the reality is, it doesn't, especially if you have poor cache coherency -- it's often possible that you can get better performance by using an algorithm that takes more clock cycles, but ends up loading fewer blocks from RAM, or loading them closer together in space or time.\n\nAnother way to look at it: Disks are *absurdly* slower than RAM. How do disks keep up with RAM? They don't. Massive caching and prefetching can help, but unless you have an *insane* solid-state drive, things like opening a program are going to be bound by how slow your disk is and whether you have enough RAM, not how fast it or your CPU is. i think the bandwidth might be 1/3 but the latency is much higher than that. (15 cycles rings a bell from memory timings) If it's only a third the speed, that's actually ridiculously good compared to how bad it has been.\n\nBut yeah, caching, especially cache coherency. And the reality is, it doesn't, especially if you have poor cache coherency -- it's often possible that you can get better performance by using an algorithm that takes more clock cycles, but ends up loading fewer blocks from RAM, or loading them closer together in space or time.\n\nAnother way to look at it: Disks are *absurdly* slower than RAM. How do disks keep up with RAM? They don't. Massive caching and prefetching can help, but unless you have an *insane* solid-state drive, things like opening a program are going to be bound by how slow your disk is and whether you have enough RAM, not how fast it or your CPU is. If it's only a third the speed, that's actually ridiculously good compared to how bad it has been.\n\nBut yeah, caching, especially cache coherency. And the reality is, it doesn't, especially if you have poor cache coherency -- it's often possible that you can get better performance by using an algorithm that takes more clock cycles, but ends up loading fewer blocks from RAM, or loading them closer together in space or time.\n\nAnother way to look at it: Disks are *absurdly* slower than RAM. How do disks keep up with RAM? They don't. Massive caching and prefetching can help, but unless you have an *insane* solid-state drive, things like opening a program are going to be bound by how slow your disk is and whether you have enough RAM, not how fast it or your CPU is.   Fetching from RAM is an asynchronous operation. If you need something that isn't in the CPU's cache, the CPU spins (or executes non-memory-dependent instructions in the pipeline) until the memory controller alerts it that the value has been fetched. Some of the time, cache misses still dominate performance quite a bit also how is the CPU supposed to execute non-memory-dependent instructions when in a syscall? Pipelining. Why would the CPU care about the concept of a \"syscall\"?   Everyone else has already explained the answer, but I'd recommend taking a computer architecture course (or CPU architecture) in order to really find out what's going on.  Its a ton of information, but it will definitely clear up most questions related to how all the pieces work together.\n can yourecommed any tutorials/free information for someone to start with?  This stuff really interests me. I don't know of any free tutorials, but [this](http://www.amazon.com/Computer-Architecture-Quantitative-Approach-Edition/dp/0123704901) is *the* computer architecture texbook. That said, at my college I used an updated version of [this](http://www.amazon.com/Computer-System-Architecture-3rd-Edition/dp/0131755633/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1337754815&amp;sr=1-1) book.\n\nYou could also look up tutorials on how to program simpler microprocessors such as the 8051 or one of those AVR chips used by the Arduino. They're not as complex as desktop-class processors, but many of the concepts are the same. (Also, writing games on a slow as shit microprocessor is badass. When you optimize your game loop, you can actually see the game go from 10 FPS to 30FPS. Fun!) can yourecommed any tutorials/free information for someone to start with?  This stuff really interests me. Anything specifically interesting to you?  CPU design?  Memory?  How hard drives work?  There's a metric ton of information on the internet - a lot of it is hard to understand.  \n\nOn top of what GeneralMaximus said, another thing you could do is look at [the Wikipedia article for computer architecture](http://en.wikipedia.org/wiki/Computer_architecture).  Click on pretty much all of the links on that page.  Its not the best, but its somewhere to start.      It doesn't. Just like your hard drive doesn't keep up with your RAM. The traditional computer design has 3 levels of volatile memory.\n\n1: On CPU. These are your L1 and L2 caches. Really fast, really expensive.\n\n2: RAM: This is your workhorse for running applications, not as fast, not as expensive. If it can't be stored in the CPU cache, this is where it goes.\n\n3: Hard Disk: What happens when you run out of RAM? The memory segments get \"paged\" and sent to your pagefile/swap space. This memory is really slow and really cheap.\n\n Let me clarify a bit. The reason why we have so small L1 and L2 caches is not because it is extremely expensive, but because they should work on the same frequency as a processor and there are physical limitations on how big single chip can be.  \nModern consumer processors have synchronous design, which means that signal should propagate through all pathways at every cycle. With 3Ghz you have 3 * 10^9 cycles per second and speed of light is roughly 0.3 * 10^9 m/s, then on each cycle signal can cover 0.1 m or 4 inches. If you think that modern processor consists of more than 20 layers you can see that we are already hit this limitation.         ",
    "url" : "http://www.reddit.com/r/compsci/comments/tz3o6/how_does_ddr3_ram_keep_up_with_processors_that/"
  }, {
    "id" : 23,
    "title" : "An interactive tutorial on the sequent calculus for first-order logic",
    "url" : "http://logitext.ezyang.scripts.mit.edu/logitext.fcgi/tutorial"
  }, {
    "id" : 24,
    "title" : "Finding Real-World Concurrency Bugs Using Static Analysis",
    "url" : "http://research.microsoft.com/en-us/projects/poirot/default.aspx"
  }, {
    "id" : 25,
    "title" : "Who wants to help me crack an 18th century masonic cypher?",
    "snippet" : "Best Photos: http://imgur.com/a/isCoU#0  Edit 6: Tired. Have to wake up in five hours. Last Edit:http://imgur.com/a/rfLtk  Edit 5: Here's a higher quality close up of one line.  This is about as good as I can do: http://imgur.com/YIlbk   Edit 4: Fuck you reddit, you incredulous bastards.  If I had wanted to fake it, why would I have made it unreadable?  Unless this is my first scam... http://imgur.com/xC5MR\n\nMy name is Matthew Kaminski, currently a Masters Student in the History Graduate Program at UMass Boston.  I'm an expert in pre-Revolutionary Boston history, and my Masters Thesis is a biography of the Grand Master Freemason John Rowe and his connection to James Otis Jr, William Molineux, and Solomon Davies.  While doing some standard archive work, I came across a letter written by Bostonian, Preacher and Freemason William Gordon.  Hidden in the bottom of the letter and the lining of the envelope was a complex cypher, easily the most complex I've ever seen the Masons use.  I sent a copy to Tufts History Professor Benjamin Carp, who expressed interest but was unable to help me crack it.  This code is interesting because, according to the Mass. Historical Society, in later years, Gordon claimed to be connected to some kind of intelligence network that involved both General George Washington and British General Gage.  Again, I am not a random conspiracy theorist.  I've given lectures to groups of masons from the New England lodges, and although I'm not yet a world renowned expert on Masonic history, I know more about Boston masonry than most Post-graduate historians.  William Gordon was a loudmouth, a liar, and a braggart, but this letter could contain valuable information.  The original is located in the Mass. Historical Society's Bound folio archive for the year 1772.  I figured the computer group might be able to point me toward a program that can isolate and compare the symbols to create a single set I can analyze and decipher.  Here's the link, in case the first one didn't work:\nhttp://imgur.com/a/rfLtk\n\nTLDR: Help me decipher a Masonic Cipher that could hold the key to proving Dan Brown is full of shit.\n\nEdit 1: Here's a better image. http://imgur.com/Wmw0t\nEdit 2: And another: http://imgur.com/ftZoz\nEdit 3: Here's a hopefully higher quality.  I'll add the other two sections if it's better. http://imgur.com/Wrhol  It would probably be best if you transcribe the symbols into some standard alphabet and post the text instead. It doesn't matter if it's a,b,c,d,e... or some weird symbols to a computer, or to us for that matter. And then try /r/crypto that's the cryptography subredit. It would probably be best if you transcribe the symbols into some standard alphabet and post the text instead. It doesn't matter if it's a,b,c,d,e... or some weird symbols to a computer, or to us for that matter. And then try /r/crypto that's the cryptography subredit.   Wow, can we get some lower quality images? Can't do anything without higher res images. I could not tell the difference between noise and actual characters. Wow, can we get some lower quality images? 216 x 119 px photo, I can't even imagine the device that produced that :D The camera on [this phone](http://i.imgur.com/umtQI.jpg) takes better pictures. [I took a better picture with this.](http://naturalskintreatmentguide.com/images/potato.jpg)  216 x 119 px photo, I can't even imagine the device that produced that :D So it's a PDF copy of an original photocopy.  I'll try to get into the archives ASAP and take a photo of the original letter. Hopefully that will please the critics.  My buddies were over drinking and they told me to finally put it up and then started demanding we go to a bar, so my time was limited to say the least. Wow, can we get some lower quality images?  I feel like this is /r/iama, but could we have some proof of who you are? Your account is only an hour old and there was that recent string of students creating fake stories so you never know. I second this. A quick google search turned up nothing except this reddit post and a medical doctor. Plus, if this were real, there would be much better pictures. This seems suspiciously like that post about the professor who had his students create fake stories as a social experiment like ciferkey mentioned. Well, there is [this](http://www.firstmasonicdistrictofnh.com/StarInTheEast59/StarInTheEast59_2012-03.pdf) (left column, second paragraph).  I feel like this is /r/iama, but could we have some proof of who you are? Your account is only an hour old and there was that recent string of students creating fake stories so you never know. I added a pic at the top.  It's a bad photo, I look like old preppy self from the St. John's Prep days, but I could do more if you want.  Maybe you can work on getting better images. Try using a camera from the last decade Maybe you can work on getting better images. Try using a camera from the last decade  Hey there, I haven't seen anybody mention this yet, but the text in that picture looks strikingly similar to [pitman shorthand](http://www.christusrex.org/www1/pater/kuhl/pitmans-a.jpg), may be worth investigation. I'm going to see if they are similar now and I'll get back to you, but I don't know much at all about cryptography, so if it is code I'll be lost in the water. It may also be worth looking at other shorthand styles from the time period, considering that this looks to me to be a lot more like shorthand than it does a code. Hey there, I haven't seen anybody mention this yet, but the text in that picture looks strikingly similar to [pitman shorthand](http://www.christusrex.org/www1/pater/kuhl/pitmans-a.jpg), may be worth investigation. I'm going to see if they are similar now and I'll get back to you, but I don't know much at all about cryptography, so if it is code I'll be lost in the water. It's not quite it, but you're closer to the symbols than anything else that has been suggested.  I'm a Freemason and I'll help where I can but first we'll need a legible copy. Some of what I can make out looks like the Pigpen cipher but  other parts are definitely not Pigpen. A lot of older Masonic cryptograms are simple substitution ciphers.  I'm a Freemason and I'll help where I can but first we'll need a legible copy. Some of what I can make out looks like the Pigpen cipher but  other parts are definitely not Pigpen. A lot of older Masonic cryptograms are simple substitution ciphers.       I increased the size of the images, hopefully that helps a bit.  Other than that all I can do is go into the archive and take some digital photos when no one is looking. I increased the size of the images, hopefully that helps a bit.  Other than that all I can do is go into the archive and take some digital photos when no one is looking. Increasing the size doesn't help, it looks like the text came from a 1980s-era photocopier that tried to \"help\" by adjusting the contrast to 10,000% resulting in black-and-white polkadots instead of characters.\n\nIf you have access to a modern scanner that would be perfect, use the highest resolution possible. Otherwise use a high-end DSLR, and you may need to experiment with different lighting angles.\n\nAnyway, I am **dying** to be able to read and solve this cipher. Breaking historical ciphers is my hobby. Just this weekend I tried, and failed, to get my hands on a book from the 1500s with an unsolved cipher in it. Stupid graduation ceremony closing the reading room... That's funny.  The original letter was photocopied and put in a bound folio during the 1980s.  If you're in Boston you can go to the Mass. Historical Archive and read it for yourself...  I'll try to get a better copy ASAP but I teach and am a full-time activist.   Why don't you convert the image to a text file, **then** ask for help. I may be able to help with crypto, but I'd need to understand wtf I'm looking at first... it's not text I believe he is asking OP to do what Fuco1337 had also requested:\n\n&gt;*\"transcribe the symbols into some standard alphabet and post the text instead. It doesn't matter if it's a,b,c,d,e... or some weird symbols to a computer, or to us for that matter\"*  OK, I figured it out:\n\nOnce upon a time (75 million years ago to be more precise) there was an alien galactic ruler named Xenu. Xenu was in charge of all the planets in this part of the galaxy including our own planet Earth, except in those days it was called Teegeeack.\n\nXenu the alien ruler Now Xenu had a problem. All of the 76 planets he controlled were overpopulated. Each planet had on average 178 billion people. He wanted to get rid of all the overpopulation so he had a plan.\n\nXenu took over complete control with the help of renegades to defeat the good people and the Loyal Officers. Then with the help of psychiatrists he called in billions of people for income tax inspections where they were instead given injections of alcohol and glycol mixed to paralyse them. Then they were put into space planes that looked exactly like DC8s (except they had rocket motors instead of propellers).\n\nThese DC8 space planes then flew to planet Earth where the paralysed people were stacked around the bases of volcanoes in their hundreds of billions. When they had finished stacking them around then H-bombs were lowered into the volcanoes. Xenu then detonated all the H-bombs at the same time and everyone was killed.\n  &gt;TLDR: Help me decipher a Masonic Cipher that could hold the key to proving Dan Brown is full of shit.\n\n\n&gt;implying we need to do that to prove Dan Brown is full of shit\n\n\nDan Brown proves Dan Brown is full of shit, since the list of historically accurate art and architecture works on the inside cover of his book is not historically accurate. [deleted]    Can you assign each symbol to a unique number and post that for me?  I'm a software engineer with a Passion for cryptography, I want to run some tests on the data.  If it is an older cypher I want to run it through some tests, my gut tells me its either a vigineere (wrong spelling, typing on phone), or a Playfaire cypher.   Vigenère cipher, my lord**  hi\ni found this post, since we are working on similar ciphers in mission/ARG\nUMBRA  http://siftumbra.wikia.com/wiki/UMBRA_Wiki  http://webchat.freenode.net/?channels=sift\n\nfor breaking simple substitution ciphers (like PIgpen) this tool is very successful http://www.secretcodebreaker.com/scbsolvr.html (for english)\n\nPlayfair is also not so hard to solve just read how is it ciphered and it is easy to guess, we are using this tool to crack it http://home.comcast.net/~acabion/playfair.html. but you bust know at least few words from cipher text to start with.\n\nFor more information on Playfair (and other ciphers) check this page. It offers good explanations on cipher methods and good tools to cipher and decipher \nhttp://rumkin.com/tools/cipher/index.php\nhttp://rumkin.com/tools/cipher/playfair.php\n\nwe also have alot of experience with vigenere\n\nIf u have problems hop at IRC above and we will help you, we have some pretty good deciphers there.   Ok, I finally got back into the archives and took a bunch of photos of the original letters.  Hopefully, this is the quality everyone wanted. http://imgur.com/a/isCoU#2      You've given lectures to \"Modern Day Grand Masters from New England\"? How many New England Grand Masters were you lecturing at once? I call bullshit on you being anything remotely close to an expert or actual masonic scholar.  There were 30 grandmasters at the Exeter Lodge listening to me for two hours and then I got a standing ovation. They offered to start paying me to come back every month but I don't have time. My thesis topic is John Rowe, grandmaster of all Masons in North America.  And I'm not an expert on Masonry in general, but I can tell you more than anyone else on earth about how John Rowe used networks of Boston freemasons to foment the 1765 Stamp Act riots and how Rowe personally wrote the 1768 and 1770 non-importation agreements. I'll begin by saying I apologize if you've been mislabeled by me as an imposter, the issue I have with what you've written is here:\n\n&gt;There were 30 grandmasters\n\n\nIf you check out [this wiki link](https://en.wikipedia.org/wiki/Grand_Master_\\(Masonic\\)), it supports my understanding that a Grand Master only exists for specific Jurisdictions. Jurisdictions cover large areas, like states, for example. So from my experience it would be impossible to speak to 30 Grand Masters from a single state, as there would be typically just one Grand Lodge in the state, not counting appendant bodies. \n\nSo, what you wrote seems innacurate and impossible. Further, misquoting significant titles in Freemasonry suggests to me a significant lack of understanding regarding your expressed subject matter and with this your qualifications come into question in my mind.\n\n**edit:** \n&gt;And I'm not an expert on Masonry in general, but I can tell you more than anyone else on earth about how John Rowe used networks of Boston freemasons to foment the 1765 Stamp Act riots and how Rowe personally wrote the 1768 and 1770 non-importation agreements.\n\nThis clarifies the confusion thought I think.. as it seemed to me from your first post you aimed to be an expert on Freemasonry as well. I said I was an expert in Pre-Revolutionary Boston history, which is true. I said I wanted to be an expert in Masonic History. Perhaps I should have added someday to the end of that sentence.  All I know is 20 or 30 people dressed like Grandmasters heard me speak.  Only two people were wearing standard garb.  I might be misunderstanding the nature of the title of grandmasters, so just contact the Star of the East Lodge in Exeter, NH, and ask them who attended. http://www.starintheeast59.com/www.starintheeast59.com/Welcome.html  I know someone is gonna call BS on me, but here it goes. \n\nI'm a member of the not-as-secret-but-simply-not-well-known group, Demolay International. Think of us as the youth version (12-21, I'm closer to the latter number) of the Free Masons. \n\nI would like to say that, though we do not necessarily have the same codes and histories, we are most certainly affiliated with them, and would even have members visit us during our chapter meetings. \n\nThis being said, I do not recognize these symbols at all. Members of both their organization and our own have a deep appreciation for symbols, of course, as well as secrecy. I'd like to ask that you do not post such things to the internet. The internet, which is an amazing tool for sharing knowledge, lacks any capacity to hold such secrets with high regard. Internet and privacy are simply conflicting concepts. \n\nIf you're serious about this, I would recommend you see a mason about it. Their chapters aren't exactly secret in their existence, and neither is their members. They will help you, especially if it restores some knowledge that may even be lost to them over the years. And if they don't, it's not something that the public should know then. \n\nEDIT: Told ya. Look at those downvotes. Yea, except that no one in the Historical Community is willing to help me, and no one who studies Masonic history has ever seen it.  I'm not a conspiracy theorist.  But if there's valuable historical information in this letter I don't care what happens.  Believe me, this is nothing.  I'm an economic activist on the side.  JP Morgan and Monsanto are already pissed at me and nothing has happened, so what are secret societies going to do? Also, when I gave the lecture on Rowe to the Masons, I handed out copies and no one has had any luck. &gt; I'm not a conspiracy theorist. But if there's valuable historical information in this letter I don't care what happens.\n\nUh, I didn't mean to sound threatening. I just wanted to ask.  Haha, sorry, I just get a lot of conspiracy theorists in my area of research and I'm starting to get sick of it. Every time I do a radio interview or whatever someone calls in and asks me about either the illuminati or the new world order.  It's like, dude, the New World Order is a book written by a science fiction writer in the 1930s.  A really good one, but still. I was actually a little bit concerned about posting this stuff, to be honest.  In my areas of historical research, and especially in my anti-war and anti-finance activism, you're really not allowed to be a conspiracy theorist AND a legitimate historian.  I figured it would attract a little more attention if I made it seem like some kind of conspiracy, but for all I know this Gordon guy was writing letters to his lover in Philadelphia or something. Haha, sorry, I just get a lot of conspiracy theorists in my area of research and I'm starting to get sick of it. Every time I do a radio interview or whatever someone calls in and asks me about either the illuminati or the new world order.  It's like, dude, the New World Order is a book written by a science fiction writer in the 1930s.  A really good one, but still. I know someone is gonna call BS on me, but here it goes. \n\nI'm a member of the not-as-secret-but-simply-not-well-known group, Demolay International. Think of us as the youth version (12-21, I'm closer to the latter number) of the Free Masons. \n\nI would like to say that, though we do not necessarily have the same codes and histories, we are most certainly affiliated with them, and would even have members visit us during our chapter meetings. \n\nThis being said, I do not recognize these symbols at all. Members of both their organization and our own have a deep appreciation for symbols, of course, as well as secrecy. I'd like to ask that you do not post such things to the internet. The internet, which is an amazing tool for sharing knowledge, lacks any capacity to hold such secrets with high regard. Internet and privacy are simply conflicting concepts. \n\nIf you're serious about this, I would recommend you see a mason about it. Their chapters aren't exactly secret in their existence, and neither is their members. They will help you, especially if it restores some knowledge that may even be lost to them over the years. And if they don't, it's not something that the public should know then. \n\nEDIT: Told ya. Look at those downvotes.",
    "url" : "http://www.reddit.com/r/compsci/comments/twl6j/who_wants_to_help_me_crack_an_18th_century/"
  }, {
    "id" : 26,
    "title" : "Analogy computer programs &lt;--&gt; mathematical proofs",
    "snippet" : "  ",
    "url" : "http://dijkstrascry.com/node/80"
  }, {
    "id" : 27,
    "title" : "Help creating an algorithm for building cables.",
    "snippet" : "I've been mulling over the problem of building cables as it relates to my job, and it seems like there might be a good way to approach the problem automatically, but I haven't been able to see things clearly enough and was hoping this forum might have some insight on the right direction to look.\n\nThe set up:\n\nWe need to connect from our hardware (Side A)  to a device under test (Side B).  Due to limitations of what we'll be connecting through, on side A we are limited to 62-pin connectors and a light gauge wire.  Side B has a range of different connections including some bulky power-connection pins.  Obviously each pin has an assignment for a signal\n\nSome example connections:\n\n* Side B has a 78-pin data connector.  From side A we have to pull from 2 different 62-pin connectors forming a Y type connection.\n\n* Side B has an 8-pin power connector.  Because of the power draw we need 10 light-gauge wires to connect to each power pin. So, once again we build a Y type connector with 80-pins (A) going to 8 pins (B).\n\nWe currently solve the problem by hand, and with more complicated connector setups this can get out of hand (imagine cables that look like a WWW or worse.) \nThis presents a big mechanical problem that we'd like to avoid.  Essentially, we're trying to minimize the number of divergences\n\nThe problem:\n\nInputs:\n\n* Number of 62-pin connectors on side A\n\n* Number (and type) of connectors on side B\n\n* How many wires per side B pin.\n\nOutput:\n\nA connection list (I'm thinking something like J1-1 -&gt; J12-1 for connecting side A connector 1, pin 1 to side B connector 12 pin 1) that has minimized the number of cable divergences.\n\nI've been thinking about setting this up as a graph problem, and I got to the point of minimum spanning tree, but that doesn't quite seem to fit the bill.  Does anyone have some insight on how to set up the problem appropriately and what algorithm or sorts of algorithms may be good at solving it?\n\n  &gt;Essentially, we're trying to minimize the number of divergences\n\nWhat is a divergence? Please elaborate. A divergence is any time the cable coming from a connector splits, which can be broken down into two cases:\n1) When 2 or more 62 pin connectors on side A connect to 1 connector on side B\n2) When 1 connector on side A connects to 2 or more connectors on side B\n\n(This is a problem because of mechanical stresses that we place the cables under)  So, I'm not sure if I have this right, but here's how I'm interpreting your problem.\n\nYou have connectors on B which each have a set number of pins on A that they must be connected to. You have at least a sufficient number of pins on A to accommodate this, divided into chunks of 62. For each connector on A, A_i, let a_i denote how many connectors on B that A_i is connected to. For each connector on B, B_i, let b_i denote how many connectors on A that B_i is connected to. You want to minimize the sum over all i of a_i + b_i.\n\nCall the number of pins B_i requires P_i. Call the number of pins A_i contains R_i. Let n be the number of connectors on B. Let m be the number of connectors on A. Consider all lists &lt;Q_1, Q_2,...Q_k&gt; such that two properties hold:\n\n1. There exists a partition of Q, P_B = &lt;S_1, S_2, ... S_n&gt; such that the sum over every element in any S_i = P_i.\n2. There exists a partition of Q, P_A = &lt;T_1, T_2, ... T_m&gt; such that the sum over every element in any T_i &lt;= R_i.\n\nThe problem is to find one such list such that k is minimized.   Is it possible to do brute force, i.e., try out all possible combinations?  Have you considered using a Genetic Algorithm? It wouldnt be difficult to design a schema and fitness function for this problem. That was my fallback position.  It's a good idea, but there are no guarantees of being optimal.  Something about the setup makes me think there might be such a solution and I just don't see how to get there. \n\nEdit: It's also possible I spent too many hours pondering these sorts of questions in algorithms classes so they all trigger that sort of reflex (because there *was* a good solution), even if it may not be warranted in this case. is it absolutely necessary for the project that the solution be optimal, eg its very sensitive to signal loss and it must be avoided at all costs, or is it more for ease of building the connections? If its for ease of building you should be fine with an approximate solution that is similar to the optimal one. If its off by ~5% thats probably only going to be a few extra wires you have to set up and setting up those extra wires might be easier than finding an optimal solution (both time invested trying to think of a way to find it and actually finding it).    Any chance you can define the connector locations in Cartesian space?  If so, you could find the shortest path tree for each connector pin using [Dijkstra's algorithm](http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm), which should minimize divergences. I breezed past Dijkstra's algorithm earlier.  I can define the locations, but the edge weights would all be all either infinite or 1 (or some other reasonable weight.) \n\n  so, your problem looks like this:   \n\n    side A:     (....62....) (....62....) (....62....)  \n    connectors:    |            |---------/\n                   v            v\n    side B:     (..12..) (.....72......)  \n\nand you want an algorithm to find how to group connectors together to have the least possible amount of divergences (the spot wheку cables merge together), right? Right.  For any arbitrary set of side B connectors and some set number of side A connectors (user input.)  I think there should be some physical limitation you didn't said us about. For example connecting A1 with B999 might not be such a great idea. ",
    "url" : "http://www.reddit.com/r/compsci/comments/twzp3/help_creating_an_algorithm_for_building_cables/"
  }, {
    "id" : 28,
    "title" : "Listening to audiobooks while programming",
    "snippet" : "I've tried watching movies, but they were too distracting. I do enjoy listening to music while programming too, but have found something better. I love listening to audiobooks as I'm programming. I found that I'm able to focus on both perfectly with the exception of writing/reading more that a few sentences of text. If I need to read or write something I will just pause and then play after I'm done. I think that the left/logical side of my brain can focus on the code while the right/creative can focus on the audiobook without any interference. I'm really curious as to if anyone else has tried this.   i find that anything other than monotonous sound distracts me. movies distract me, new songs distract me, and sometimes songs i like distract me. silence can be defeaning too. \n\nif i were listening to just the sound of the voice of the speaker, it might be alright for me, but if i tried to listen to what they were saying, it would make me worse.\n\ni listen to dubstep while programming. its enough to not distract, and its always the same. also it blocks out the sound of other people around me pretty well. i can get into the zone pretty well with it. occasionally i have to stop it when i get to debugging a specific problem that is intese, but mostly, its been effective for me I find that if I have to do some intense thinking that I need to pause for a while too. And I do like dubstep, but I think something like Dark Side of the Moon better suits my mood when I'm programming. i find that anything other than monotonous sound distracts me. movies distract me, new songs distract me, and sometimes songs i like distract me. silence can be defeaning too. \n\nif i were listening to just the sound of the voice of the speaker, it might be alright for me, but if i tried to listen to what they were saying, it would make me worse.\n\ni listen to dubstep while programming. its enough to not distract, and its always the same. also it blocks out the sound of other people around me pretty well. i can get into the zone pretty well with it. occasionally i have to stop it when i get to debugging a specific problem that is intese, but mostly, its been effective for me i find that anything other than monotonous sound distracts me. movies distract me, new songs distract me, and sometimes songs i like distract me. silence can be defeaning too. \n\nif i were listening to just the sound of the voice of the speaker, it might be alright for me, but if i tried to listen to what they were saying, it would make me worse.\n\ni listen to dubstep while programming. its enough to not distract, and its always the same. also it blocks out the sound of other people around me pretty well. i can get into the zone pretty well with it. occasionally i have to stop it when i get to debugging a specific problem that is intese, but mostly, its been effective for me   [deleted]  I certainly get a lot of benefit from listening to music while I work, so I might need to try podcasts/audiobooks.\n\nHow do you avoid running out? Do you just recycle the same ones?    ",
    "url" : "http://www.reddit.com/r/compsci/comments/twx7q/listening_to_audiobooks_while_programming/"
  }, {
    "id" : 29,
    "title" : "What are some of the best textbooks to introduce some one (interested) to computer science?",
    "snippet" : "Of course, in the modern world. Books that  go into LISP more extensively than they do in C, or books that talk more about MIPS-based CPUs than they do x86-based would be...eh. \n\nOf course I'm not looking for a textbook that introduces someone to a specific programming languages. I'm talking about an introduction to algorithms and the study of computers in general.   I would highly recommend the following for algorithms:\n\n* Introduction to Algorithms (CLRS)\n* [Structure and Interpretation of Computer Programs](http://mitpress.mit.edu/sicp/full-text/book/book.html)\n\nFor hardware:\n\n* Computer Organization and Design, Third Edition: The Hardware/Software Interface\n* Computer Architecture: A Quantitative Approach\n\nEdit: yes, I know that you said you didn't want Lisp or MIPS, but IMO using C to learn algorithms is a very bad idea. OTOH, C is a great way to learn about low-level systems programming and hardware. Using x86 to learn about hardware (or anything for that matter) is an even worse idea.  Note that recent x86 processors use micro-operations that look more RISC than CISC [[wikipedia](http://en.wikipedia.org/wiki/X86#Current_implementations)] I would highly recommend the following for algorithms:\n\n* Introduction to Algorithms (CLRS)\n* [Structure and Interpretation of Computer Programs](http://mitpress.mit.edu/sicp/full-text/book/book.html)\n\nFor hardware:\n\n* Computer Organization and Design, Third Edition: The Hardware/Software Interface\n* Computer Architecture: A Quantitative Approach\n\nEdit: yes, I know that you said you didn't want Lisp or MIPS, but IMO using C to learn algorithms is a very bad idea. OTOH, C is a great way to learn about low-level systems programming and hardware. Using x86 to learn about hardware (or anything for that matter) is an even worse idea.  Note that recent x86 processors use micro-operations that look more RISC than CISC [[wikipedia](http://en.wikipedia.org/wiki/X86#Current_implementations)] I would highly recommend the following for algorithms:\n\n* Introduction to Algorithms (CLRS)\n* [Structure and Interpretation of Computer Programs](http://mitpress.mit.edu/sicp/full-text/book/book.html)\n\nFor hardware:\n\n* Computer Organization and Design, Third Edition: The Hardware/Software Interface\n* Computer Architecture: A Quantitative Approach\n\nEdit: yes, I know that you said you didn't want Lisp or MIPS, but IMO using C to learn algorithms is a very bad idea. OTOH, C is a great way to learn about low-level systems programming and hardware. Using x86 to learn about hardware (or anything for that matter) is an even worse idea.  Note that recent x86 processors use micro-operations that look more RISC than CISC [[wikipedia](http://en.wikipedia.org/wiki/X86#Current_implementations)] I would highly recommend the following for algorithms:\n\n* Introduction to Algorithms (CLRS)\n* [Structure and Interpretation of Computer Programs](http://mitpress.mit.edu/sicp/full-text/book/book.html)\n\nFor hardware:\n\n* Computer Organization and Design, Third Edition: The Hardware/Software Interface\n* Computer Architecture: A Quantitative Approach\n\nEdit: yes, I know that you said you didn't want Lisp or MIPS, but IMO using C to learn algorithms is a very bad idea. OTOH, C is a great way to learn about low-level systems programming and hardware. Using x86 to learn about hardware (or anything for that matter) is an even worse idea.  Note that recent x86 processors use micro-operations that look more RISC than CISC [[wikipedia](http://en.wikipedia.org/wiki/X86#Current_implementations)] i agree on the mips point.  the purpose of p&amp;h is to explain how a computer works at a very high level, so it uses a relatively simple processor (mips).\n\ni disagree on the lisp point.  what on earth does a new programmer need to know lisp for?  yes, it is interesting, but in due time he will learn it anyways.  conversely, c or c++ would teach him a language that is widely used, so that he would have a larger menu of projects to contribute to (and be more valuable to an employer).\n\nalso, can you imagine telling an interviewer that you only know lisp? Who said anything about programming? Who said anything about getting a job? The question was about computer science :-)\n\nNote that I am not advocating learning only LISP, I'm saying that SICP is a good book to learn about computer science and lisp (scheme if I remember correctly) is a good way to illustrate the ideas presented in the book.\n\nNow if we are talking about programming, I wouldn't generally advise people to learn programming using C (unless they are interested in learning about low-level details/hardware/etc), and especially not C++. I think the best way is to start with a language like python for many reasons which I can enumerate if somebody cares enough to ask :-) Please do enumerate... I ask this as a guy whose little brother wants to be a CS major. He's learned Java in school, and I've started teaching him C. Teaching him Python never occurred to me... In fact, I didn't learn Python myself until well after learning C/C++/Perl/Java. OK, here's my list:\n\n* (Good) Python code (in my experience) tends to resemble pseudocode more than any other programming language. I guess this has a lot to do with iterators and its simplicity.\n* Python enforces indentation. New programmers **need** to learn indentation and it's one of the most difficult things to do if the language does not enforce it.\n* Python has built-in primitive types like lists,dicts and sets, which allows you to focus on your program and not on building support for it. Using these data-structures is very easy (again iterators!). I used to program in perl, and every time I had to check how to iterate the keys and values of a hash.\n* Python is a very practical language and comes with \"batteries included\" -- i.e., a large number of libraries you can use to make interesting projects.\n* Python's [Read-eval-print loop](http://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop) makes it very easy to experiment and understand the language constructs. I would highly recommend the following for algorithms:\n\n* Introduction to Algorithms (CLRS)\n* [Structure and Interpretation of Computer Programs](http://mitpress.mit.edu/sicp/full-text/book/book.html)\n\nFor hardware:\n\n* Computer Organization and Design, Third Edition: The Hardware/Software Interface\n* Computer Architecture: A Quantitative Approach\n\nEdit: yes, I know that you said you didn't want Lisp or MIPS, but IMO using C to learn algorithms is a very bad idea. OTOH, C is a great way to learn about low-level systems programming and hardware. Using x86 to learn about hardware (or anything for that matter) is an even worse idea.  Note that recent x86 processors use micro-operations that look more RISC than CISC [[wikipedia](http://en.wikipedia.org/wiki/X86#Current_implementations)] I understand the value of LISP, MIPS, etc. for education purposes; all I'm saying is I don't want a book from 1965 that exclaims \"Go out and buy a LISP machine!\" SICP never claimed that. In fact a lot of Wall Street firms are now using a bunch of different kinds of functional programming languages, such as Clojure which is a Lisp dialect. Trust me there is nothing in SICP that is obsolete. And it is such a great book that you will have no way to not to love. Reading SICP as an introduction to Computer Science is absolutely risk-free.\n\nI would like to know what do you want to study? Computer Architecture, which is the study of the construction of a physical computer, or Computer Science, which is the study of, roughly, programming computers?\n\nYou can read SICP if you want Computer Science. I don't quite know how to get started on Computer Architecture though. I wasn't insulting SICP in my original post, which seems to be what you inferred. I will purchase it.   I'm of the opinion that learning on MIPS is still a decent way to go. There is a good simulator (SPIM), and it's much simpler than x86. Any skills you pick you for MIPS will transfer over to x86 fairly well.     ",
    "url" : "http://www.reddit.com/r/compsci/comments/tw73p/what_are_some_of_the_best_textbooks_to_introduce/"
  }, {
    "id" : 30,
    "title" : "Has any work on (post-compilation) adaptive optimization been done?",
    "snippet" : "This is a question I've been thinking about for a while, just out of curiosity. \n\nI was thinking about how compilers do all their optimizations during compile-time and it's all baked into an executable at the end. But the only heuristics that can be safely used for optimizations are those that the compiler (perhaps with the help of command-line flags) can prove to work on all inputs, etc. \n\nBut what actually matters is how well the program can perform on real-world inputs. So what if there was a way to keep track of what kinds of inputs are being passed into the program and then perhaps optimize certain sections of it and do a 'partial' re-compilation and repeat, etc. \n\nSuppose for example there was an efficient algorithm to do matrix multiplication if the values in the matrix were only 0s and 1s, but did poorly on arbitrary values. Then suppose that after observing inputs fed into this program for some amount of time, it was observed that 99.9% of inputs are matrices with only 0s and 1s. Swapping whatever fast general matrix multiplication algorithm with the 0-1 optimized one would yield faster expected real-world performance. But the key is that this optimization happens completely automatically, perhaps allowing it to pick up some patterns a human may miss.\n\nI'm sure this has been thought of by many people already, but I can't seem to find anything relating to it. Does anyone know if this idea is even feasible and/or if there is any work already done on it? I imagine it's a much easier problem for VM-based languages, but I'm talking specifically about languages that compile to machine code and adding a lightweight 'listener'.  Profile-guided optimization has been in wide use for quite a while. It isn't quite as far-reaching as what you describe, but that mainly because compilers are generally not smart enough to make large-scale algorithmic changes. It works by first compiling the code, then thoroughly exercising it with realistic usage while a profiler is attached. The frequency information from the profiling is then used on the second compilation to do better static branch prediction and things like that.\n\nOne of the most notable uses of this is the official Mozilla builds of Firefox - after they started using PGO, gentoo users found that it was faster to run the mozilla.org build under wine than to run their native build, despite all the optimization settings they used locally. Profile-guided optimization has been in wide use for quite a while. It isn't quite as far-reaching as what you describe, but that mainly because compilers are generally not smart enough to make large-scale algorithmic changes. It works by first compiling the code, then thoroughly exercising it with realistic usage while a profiler is attached. The frequency information from the profiling is then used on the second compilation to do better static branch prediction and things like that.\n\nOne of the most notable uses of this is the official Mozilla builds of Firefox - after they started using PGO, gentoo users found that it was faster to run the mozilla.org build under wine than to run their native build, despite all the optimization settings they used locally. Profile-guided optimization has been in wide use for quite a while. It isn't quite as far-reaching as what you describe, but that mainly because compilers are generally not smart enough to make large-scale algorithmic changes. It works by first compiling the code, then thoroughly exercising it with realistic usage while a profiler is attached. The frequency information from the profiling is then used on the second compilation to do better static branch prediction and things like that.\n\nOne of the most notable uses of this is the official Mozilla builds of Firefox - after they started using PGO, gentoo users found that it was faster to run the mozilla.org build under wine than to run their native build, despite all the optimization settings they used locally.     Have a look at HP's Dynamo:\n\nhttp://www.hpl.hp.com/techreports/1999/HPL-1999-77.html\n Have a look at HP's Dynamo:\n\nhttp://www.hpl.hp.com/techreports/1999/HPL-1999-77.html\n  isn't this the basic concept that JIT is about? The jvm does this too.   So what happens when a program designed to optimize other programs is run on itself? It gets faster?  ",
    "url" : "http://www.reddit.com/r/compsci/comments/tvo9k/has_any_work_on_postcompilation_adaptive/"
  }, {
    "id" : 31,
    "title" : "Anyone else from this subreddit at STOC this weekend?",
    "snippet" : "Which sessions are you going to? Impressions so far?  ",
    "url" : "http://www.reddit.com/r/compsci/comments/tvfal/anyone_else_from_this_subreddit_at_stoc_this/"
  }, {
    "id" : 32,
    "title" : "Alternative machine models for asymptotic algorithmic complexity",
    "snippet" : "If you are reading this subreddit, you are probably familiar with asymptotic algorithmic complexity (the \"big-O notation\"). In my experience with this topic, an algorithm's time complexity is usually derived assuming a simple machine model:\n\n* any \"elementary\" operation on any data takes one unit of time\n* at most one operation can be done in a unit of time\n\nI have read a little about algorithmic complexity of parallel algorithms where at most P operations per unit time are allowed (P = number of processors) and this seems to be a straightforward extension of the machine model above.\n\nThese machine models, however, ignore the latency of transferring the data to the processor. For example, a dot product of two arrays and linear search of a linked list are both O(N) when using the usual machine model. When data transfer latency is taken into account, I would say the dot product is still O(N), since the \"names\" (addresses) of the data are known well ahead of time they are used and thus any data transfer delays can be overlapped (i.e. pipelined). In a linked list traversal, however, the name of the datum for the next operation is unknown until after the previous operation completes; hence I would say the algorithmic complexity would be O(N \\times T(N)), where T(*n*) is the transfer latency of an arbitrary element from a set of *n* data elements.\n\nI think this (or similar) machine model can be useful, since data transfer latency is an important consideration for many problems with large input sets. \n\nI realize that these machine models have probably been already proposed and studied and I would greatly appreciate any pointers in this direction.\n\n**EDIT:** It turns out I was right: this idea has been proposed as early as [1987](http://wiki.epfl.ch/edicpublic/documents/Candidacy%20exam/aggarwal.pdf), and, in fact, following the [citations](http://scholar.google.com/scholar?hl=en&amp;lr=&amp;cites=13228865515967619757), it seems like there's a lot of follow up work.  It's still O(N), no matter the latency -- even if it took a week to access each byte, it would still be O(N).\n\nIt's a mistake to think that complexity analysis can tell you anything about the runtime of a program.  It's useful as a first approximation, but in reality you always have to measure.  For example, there are matrix multiplication algorithms known to be O( n^2.37 ), but their break-even point exceeds the practical size limit of any physical computer, so in real code an O( n^2.81 ) algorithm is used as it's much faster, despite having a worse Big-O on paper.\n\nTrying to incorporate those constant scale factors into the analysis would completely invalidate everything that makes them useful, namely their simplicity.  And besides, the performance of modern hardware is far too complex to model even if you wanted to -- the reference materials of modern processors can't even tell you how long a single instruction takes to execute, because it depends on so many factors.  It's simply hopeless to try to model performance, you really only can measure.\n &gt; It's still O(N), no matter the latency -- even if it took a week to access each byte, it would still be O(N).\n\nNot necessarily so. Each time you specify which element to access next, you need to send log(N) bits of information. Should N log(N) then, right?\n(Which makes me wonder why we even assume element access is constant time in the first place....) &gt; It's still O(N), no matter the latency -- even if it took a week to access each byte, it would still be O(N).\n\nNot necessarily so. Each time you specify which element to access next, you need to send log(N) bits of information. Should N log(N) then, right?\n(Which makes me wonder why we even assume element access is constant time in the first place....) It's still O(N), no matter the latency -- even if it took a week to access each byte, it would still be O(N).\n\nIt's a mistake to think that complexity analysis can tell you anything about the runtime of a program.  It's useful as a first approximation, but in reality you always have to measure.  For example, there are matrix multiplication algorithms known to be O( n^2.37 ), but their break-even point exceeds the practical size limit of any physical computer, so in real code an O( n^2.81 ) algorithm is used as it's much faster, despite having a worse Big-O on paper.\n\nTrying to incorporate those constant scale factors into the analysis would completely invalidate everything that makes them useful, namely their simplicity.  And besides, the performance of modern hardware is far too complex to model even if you wanted to -- the reference materials of modern processors can't even tell you how long a single instruction takes to execute, because it depends on so many factors.  It's simply hopeless to try to model performance, you really only can measure.\n As I understand you, you are implying that data transfer latency is a constant factor to the runtime of an algorithm. My point is that **data transfer latency is not a constant factor**: it grows (**edit:** without bound) with the size of the problem. Therefore I think it may need to be taken into account when analyzing asymptotic complexity of an algorithm. Can you give an example of data transfer growing without bound while transferring the same amount of information?  In your linked list traversal example, while the address of the next node may be unknown until after the current traversal it'll always be the same size.  And since we have random-access memory, all addresses have the same upper bound on access time. Sure. I have two ways of looking at it: real hardware perspective and a physics perspective.\n\nIn real hardware, there are multiple levels of memory hierarchy. With each level, both the capacity and the data transfer latency increase:\n\nLevel | Capacity (in bytes) | Latency (in processor cycles)\n:-- | --: | --:\nRegisters | ~100B | 1\nL1 Cache | ~32KB | ~3\nL2 Cache | ~512KB | ~10\nL3 Cache | ~4MB | ~20\nDRAM | ~4GB | ~300\nDisk | ~4TB | a lot\n\nYou can see that the latency of random access memory grows with its capacity (and the size of the data set it can support).\n\nThe physics perspective is even simpler: assuming each data element is stored in a bit of space, the space needed to store *n* data elements grows linearly with *n*. How does that affect the latency of transferring an arbitrary element from that set to the processor? Assuming a constant \"speed\" of transfer, it must grow at least as a cubic root of *n* (if the data is laid out in a \"cube,\" for example).\n\nThe reason I am interested in deriving the effect of this data transfer latency on the asymptotic complexity of an algorithm is that people actually already care about it. For example, merge sort is often described as being preferable for large data sets, despite having the same algorithmic complexity as other Nlog(N) sorts. Perhaps this benefit would be easy to see if we considered data transfer latency T(N) when expressing these algorithms' asymptotic complexity. &gt; You can see that the latency of random access memory grows with its capacity\n\nNO - The latency of random access does NOT grow due to capacity. 4MB of DRAM memory is going to be exactly as fast as 4GB or even 4TB of DRAM memory (There may be minor degradation of performance - but we can have an upper bound that is independent of N or at least so slow growing that it can be ignored). The reason we have only ~4GB of DRAM is that  DRAM is costly and 4TB of DRAM would cost prohibitively high. However, when we are dealing with abstract models - the type and cost of memory doesn't matter, and we just assume that enough memory is available for all the data to be accessed with uniform latency. Whether it is register memory or cache memory or dram or disk, there is just a single type and we have an upper bound for the latency which can be used to substitute T(n). &gt; &gt; You can see that the latency of random access memory grows with its capacity\n\n&gt; NO - The latency of random access does NOT grow due to capacity. 4MB of DRAM memory is going to be exactly as fast as 4GB or even 4TB of DRAM memory\n\nI cannot agree with you on this. From my experience in both circuit design and computer architecture, SRAM (caches) and DRAM random memory access latency grows at least logarithmically with capacity (due to address decoding and data muxing logic). My physics argument indicates an even greater asymptotic factor. Can you justify your claim or at least refute my arguments?\n\n&gt; However, when we are dealing with abstract models - the type and cost of memory doesn't matter, and we just assume that enough memory is available for all the data to be accessed with uniform latency.\n\nYou are telling me that people make this assumption when looking at asymptotic complexity and I am not disputing that. You are not, however, telling me *why* this assumption is OK.\n\nI would argue that we construct the abstract models to be as simple as possible yet still capture the important details of what we are modeling. In that sense, the traditional abstract model seems inadequate. It leads to a situation where the asymptotic complexities of two algorithms are the same yet the ratio of their execution time grows with the problem size. I think I see your point: as the actual memory grows, the access time grows logarithmically. This is true because to access an address in memory you need to have n bits where the size of memory is 2^n . The problem here is \nThat we arent usually measuring a problem's complexity in terms of memory capacity, but in terms of how many operations we have to do. It is true that larger problems may need more memory, but you can assume you have enough RAM to complete your problem, and that RAM will have a constant access time bounded by a log of its size.\n\nTLDR: Memory access and other real computer operations do not happen in constant time as their size grows, but we normally arent interested in switching out hardware as the size of our problem grows. &gt; The problem here is That we arent usually measuring a problem's complexity in terms of memory capacity, but in terms of how many operations we have to do.\n\nNumber of operations is a proxy for execution time, generally assumed to be accurate within a constant factor. If that assumption is not true, it may be a bad proxy.\n\n&gt; TLDR: Memory access and other real computer operations do not happen in constant time as their size grows, but we normally arent interested in switching out hardware as the size of our problem grows.\n\nYou don't need to switch out memory to expose the growth of memory access latency. I've already shown this happens in existing systems: as the data set grows it has to occupy the slower, bigger level of the memory hierarchy.\n\nIt turns I was right at least about prior work: this exact idea of extending asymptotic complexity to take memory access latency into account has been studied as early as [1987](http://wiki.epfl.ch/edicpublic/documents/Candidacy%20exam/aggarwal.pdf). Right, but the memory access time is bounded by how long it takes to access the slowest memory type!  In your argument, T(n) can be replaced by a constant value - the upper bound of the time required to transfer any given element of the set of n data elements. That would make the complexity to be O(N times a constant) which is still O(N).\nThe number of data transfers required are dependent on the (upper bound of the) number of operations to the performed - while the time for each operation can be replaced by a constant (because it is not dependent on N - the size of the problem), hence it will be not affect the upper bound of the computational complexity. What makes you think there's an upper bound to T(*n*)? I would argue that both from a fundamental physics perspective and real hardware perspective, the latency of accessing an arbitrary element from a set of *n* elements cannot be bound by a finite number. That is, the limit of T(*n*) as *n* goes to infinity is infinity. Hence T(*n*) is not a constant factor. Why do you say there is no upper bound on T(n)? Wouldn't a memory operation have a fixed number of steps and terminate at some point?",
    "url" : "http://www.reddit.com/r/compsci/comments/tulhn/alternative_machine_models_for_asymptotic/"
  }, {
    "id" : 33,
    "title" : "How not to solve P = NP?",
    "snippet" : "  DO NOT attempt to prove that N = 1. DO NOT attempt to prove that N = 1. N = 0/0 is that the same as zero multiplied by infinity? By definition, *undefined*.    ",
    "url" : "http://cs.stackexchange.com/q/1877/1238"
  }, {
    "id" : 34,
    "title" : "Non-textbooks worth reading that will make you a better computer scientist",
    "snippet" : "I'm entering a PhD program in the fall (scientific computing/bioinformatics) and am taking the summer off to travel. As such, I feel like I'm going to have a lot of free time for reading. I'm looking for suggestions for books that I should read that will make me a better computer scientist. I'm not interested in textbooks, since I'll be reading enough of those in the Fall and would prefer topics that I likely wouldn't get exposed to in a class. Also, everything I plan on reading I'm going to have to carry with me for the whole summer, so lighter and smaller is better. \n\nSo far I've compiled the following list based off of previous similar discussions:\n\n* The Soul of A New Machine - Tracy Kidder\n* COMPLEXITY: THE EMERGING SCIENCE AT THE EDGE OF ORDER AND CHAOS - M. Mitchell Waldrop\n* The Society of Mind - Marvin Minsky\n* Gödel, Escher, Bach: An Eternal Golden Braid - Douglas R. Hofstadter\n* Computer Power and Human Reason - Joseph Weizenbaum\n\nWhat else is there anything else that I definitely should add?\n\n\nEDIT: Thank you all for your suggestions. I'm definitely going to have a lot of good choices this summer.  I've heard good things about \"Code: The Hidden Language of Computer Hardware and Software\" by Charles Petzold (though I haven't read it yet)\n\nAlso, if you're traveling and bringing lots of book, you should pick up an e-reader.  You'll be really happy you did.  Think of it as the book-weight equivalent of a constant time algorithm.   &gt; Think of it as the book-weight equivalent of a constant time algorithm.\n\nHa. Love it.\n\nAlso, definitely seconding the e-reader idea. I was staunchly opposed to them, being very much a I-want-to-feel-the-pages-of-my-book person, but I received one as a gift at Christmas and absolutely love it. I still love my books, but for portability the Kindle can't be beat. I wouldn't ask Charles Petzold to sign my Kindle however and place it back on my bookshelf with pride :) I've heard good things about \"Code: The Hidden Language of Computer Hardware and Software\" by Charles Petzold (though I haven't read it yet)\n\nAlso, if you're traveling and bringing lots of book, you should pick up an e-reader.  You'll be really happy you did.  Think of it as the book-weight equivalent of a constant time algorithm.   In my opinion, *Code* is too basic for anyone who has a degree in CS and who already understands digital logic design and computer architecture. I enjoyed reading it still, as I think the book's approach is a great way to introduce these concepts. I recommend *Code* more as a way to give \"non-technical people\" (e.g. my mom) an understanding of what computers are actually doing on the inside, since it builds the system up from the very bottom and uses a minimal amount of technical terminology. I've heard good things about \"Code: The Hidden Language of Computer Hardware and Software\" by Charles Petzold (though I haven't read it yet)\n\nAlso, if you're traveling and bringing lots of book, you should pick up an e-reader.  You'll be really happy you did.  Think of it as the book-weight equivalent of a constant time algorithm.   I've heard good things about \"Code: The Hidden Language of Computer Hardware and Software\" by Charles Petzold (though I haven't read it yet)\n\nAlso, if you're traveling and bringing lots of book, you should pick up an e-reader.  You'll be really happy you did.  Think of it as the book-weight equivalent of a constant time algorithm.   I've heard good things about \"Code: The Hidden Language of Computer Hardware and Software\" by Charles Petzold (though I haven't read it yet)\n\nAlso, if you're traveling and bringing lots of book, you should pick up an e-reader.  You'll be really happy you did.  Think of it as the book-weight equivalent of a constant time algorithm.   Yeah I've been looking into an e-reader. I'm still mentally debating but am now leaning towards getting one. e-readers are perfect for traveling. I still choose a physical book over my e-reader when I can, but nothing beats the convenience of an entire library under half a pound. I've heard good things about \"Code: The Hidden Language of Computer Hardware and Software\" by Charles Petzold (though I haven't read it yet)\n\nAlso, if you're traveling and bringing lots of book, you should pick up an e-reader.  You'll be really happy you did.  Think of it as the book-weight equivalent of a constant time algorithm.    [The Pragmatic Programmer](http://www.amazon.com/The-Pragmatic-Programmer-Journeyman-Master/dp/020161622X)\n\nWhen that book came out I bought copies for everyone on my team. Outstanding.\n [The Pragmatic Programmer](http://www.amazon.com/The-Pragmatic-Programmer-Journeyman-Master/dp/020161622X)\n\nWhen that book came out I bought copies for everyone on my team. Outstanding.\n [The Pragmatic Programmer](http://www.amazon.com/The-Pragmatic-Programmer-Journeyman-Master/dp/020161622X)\n\nWhen that book came out I bought copies for everyone on my team. Outstanding.\n I'm always amazed when no one has heard of this. This is one of 3 books that I own, and the other 2 are good textbooks.  Being a researcher is probably not just going to be just going to be about knowing a lot about computer science, unfortunately.  You should definitely check out some books about giving scientific talks and how to best describe your research to people that know little to nothing about your field (yay, grant applications).  Some books of note:\n\n* [The Craft of Scientiﬁc Presentations: Critical\nSteps to Succeed and Critical Errors to Avoid](http://www.amazon.com/The-Craft-Scientific-Presentations-Critical/dp/0387955550) - Michael Alley\n\nIt has stories about Feynmann, Einstein, and other scientists and how they give presentations, so it's very readable.\n\n* [Writing for Computer Science](http://www.amazon.com/Writing-Computer-Science-Justin-Zobel/dp/1852338024) - Justin Zobel\n\nMaybe not a particularly good summer read, but a really good reference and it might be helpful to skim it just before starting the school year.\n\n* [Maps of the Imagination: The Writer as Cartographer](http://www.amazon.com/Maps-Imagination-The-Writer-Cartographer/dp/159534005X) - Peter Turchi\n\nIt's not really computer science related, per se.  But it helps to show the parallels between a more technical discipline (cartography) and writing.  Highly recommended and really interesting read.\n\n* [In Pursuit of the Traveling Salesman:\nMathematics at the Limits of Computation](http://www.amazon.com/Pursuit-Traveling-Salesman-Mathematics-Computation/dp/0691152705) - William J. Cook\n\nThis is probably closer to what you were originally looking for with your original post.  I saw the actual presentation (for a general audience) that the author of this book gave, and I thought it was really fantastic in introducing a non-technical audience to NP-completeness and the like though, so check it out.\n\nHope this helps, and good luck with your PhD! Being a researcher is probably not just going to be just going to be about knowing a lot about computer science, unfortunately.  You should definitely check out some books about giving scientific talks and how to best describe your research to people that know little to nothing about your field (yay, grant applications).  Some books of note:\n\n* [The Craft of Scientiﬁc Presentations: Critical\nSteps to Succeed and Critical Errors to Avoid](http://www.amazon.com/The-Craft-Scientific-Presentations-Critical/dp/0387955550) - Michael Alley\n\nIt has stories about Feynmann, Einstein, and other scientists and how they give presentations, so it's very readable.\n\n* [Writing for Computer Science](http://www.amazon.com/Writing-Computer-Science-Justin-Zobel/dp/1852338024) - Justin Zobel\n\nMaybe not a particularly good summer read, but a really good reference and it might be helpful to skim it just before starting the school year.\n\n* [Maps of the Imagination: The Writer as Cartographer](http://www.amazon.com/Maps-Imagination-The-Writer-Cartographer/dp/159534005X) - Peter Turchi\n\nIt's not really computer science related, per se.  But it helps to show the parallels between a more technical discipline (cartography) and writing.  Highly recommended and really interesting read.\n\n* [In Pursuit of the Traveling Salesman:\nMathematics at the Limits of Computation](http://www.amazon.com/Pursuit-Traveling-Salesman-Mathematics-Computation/dp/0691152705) - William J. Cook\n\nThis is probably closer to what you were originally looking for with your original post.  I saw the actual presentation (for a general audience) that the author of this book gave, and I thought it was really fantastic in introducing a non-technical audience to NP-completeness and the like though, so check it out.\n\nHope this helps, and good luck with your PhD! Being a researcher is probably not just going to be just going to be about knowing a lot about computer science, unfortunately.  You should definitely check out some books about giving scientific talks and how to best describe your research to people that know little to nothing about your field (yay, grant applications).  Some books of note:\n\n* [The Craft of Scientiﬁc Presentations: Critical\nSteps to Succeed and Critical Errors to Avoid](http://www.amazon.com/The-Craft-Scientific-Presentations-Critical/dp/0387955550) - Michael Alley\n\nIt has stories about Feynmann, Einstein, and other scientists and how they give presentations, so it's very readable.\n\n* [Writing for Computer Science](http://www.amazon.com/Writing-Computer-Science-Justin-Zobel/dp/1852338024) - Justin Zobel\n\nMaybe not a particularly good summer read, but a really good reference and it might be helpful to skim it just before starting the school year.\n\n* [Maps of the Imagination: The Writer as Cartographer](http://www.amazon.com/Maps-Imagination-The-Writer-Cartographer/dp/159534005X) - Peter Turchi\n\nIt's not really computer science related, per se.  But it helps to show the parallels between a more technical discipline (cartography) and writing.  Highly recommended and really interesting read.\n\n* [In Pursuit of the Traveling Salesman:\nMathematics at the Limits of Computation](http://www.amazon.com/Pursuit-Traveling-Salesman-Mathematics-Computation/dp/0691152705) - William J. Cook\n\nThis is probably closer to what you were originally looking for with your original post.  I saw the actual presentation (for a general audience) that the author of this book gave, and I thought it was really fantastic in introducing a non-technical audience to NP-completeness and the like though, so check it out.\n\nHope this helps, and good luck with your PhD!  The book that comes to my mind is closer to Software Engineering than it is really to Computer Science.\n\n[The Design of Everyday Things](http://books.google.com/books/about/The_Design_of_Everyday_Things.html?id=w8pM72p_dpoC) Really a must if you plan on building anything people are going to interact with. Not so much for Algorithm design.   The Computational Beauty of Nature - Gary William Flake. It goes over the topics of computation, fractals, chaos, complex systems and adaptation. It is simple to understand, yet doesn't actually skip the important math and concepts.\n\nedit: (author)  These can help you to think about how to be a better researcher:\n\n *  [Drive: The Surprising Truth About What Motivates Us](http://www.amazon.com/Drive-Surprising-Truth-About-Motivates/dp/0143145088)\n * [Learned Optimism: How to change your mind and your life](http://www.amazon.com/Learned-Optimism-Change-Your-Mind/dp/0671019112)\n * [Pasteur's Quadrant: Basic Science and Technological Innovation](http://www.amazon.com/Pasteurs-Quadrant-Science-Technological-Innovation/dp/0815781776)\n * [Even a Geek can Speak.](http://www.amazon.com/Even-Geek-Can-Speak-Presentation/dp/156352628X)\n\nI also think that Neal Stephenson's fiction gives a fun perspective on the potential of CS.  In particular you might consider reading Cryptonomicon, Snow Crash, and Diamond Age.  The [Annotated Turing by Charles Petzold](http://www.theannotatedturing.com/) is also an interesting read. It's a nice guide to Turing's famous paper on computability.   To add to the list:\n\nThe Information: A History, A Theory, A Flood\n\nand\n\nChaos: Making a New Science\n\nBoth by James Gleick.  [Artificial Life](http://www.amazon.com/gp/product/0679743898/) by Steven Levy. Completely changed my life. [Artificial Life](http://www.amazon.com/gp/product/0679743898/) by Steven Levy. Completely changed my life.   * Anything Richard Feynman, Carl Sagan or Stephen Jay Gould  ever wrote\n* The Ascent of Man - Jacob Bronowski\n* The Age of Spiritual Machines - Ray Kurzweil\n* The Hitchhiker's Guide to the Galaxy et.al. - Douglas Adams\n\n(Why that last one?  Because every good researcher needs a sense of the absurd.) Strongly agreed on Richard Feynmann; \"Surely You're Joking, Mr. Feynmann\" is an excellent place to start on that front.   [Hackers: Heroes of the Computer Revolution] (http://www.amazon.com/Hackers-Computer-Revolution-Anniversary-Edition/dp/1449388396/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1337114645&amp;sr=1-1)     Ray Kurzweil - The Age of Spiritual Machines    GDB\nhttp://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567/ref=sr_1_1?ie=UTF8&amp;qid=1337117213&amp;sr=8-1 GDB is a debugger :)      [Programming Pearls](http://www.amazon.com/Programming-Pearls-2nd-Edition-Bentley/dp/0201657880/ref=sr_1_1?ie=UTF8&amp;qid=1337117669&amp;sr=8-1) (not to be confused with Programming Perl or something of that ilk). My dad got it for me a couple of Christmases ago and it gives a lot of insight on really, really important topics.  1. The Soul of A New Machine is a decent read, but won't make you a better computer scientist any more than reading about the American Civil War will make you a better fighter pilot.  \n2. GEB isn't worth reading unless you are still in middle school or are a fan of woo-woo pseudoscience disguised as academic literature.  \n3. Flatland. It's short. It's brilliant. Read it.  \n4. On the Origin of Species. One of the most influential books on modern scientific thought. Read from the eyes of an explorer. It'll help you regardless of your field.  \n5. Logicomix. Admittedly, I haven't read this myself, but it has great reviews and pretty much anything by Papadimitriou is a worthwhile read anyway. 1. The Soul of A New Machine is a decent read, but won't make you a better computer scientist any more than reading about the American Civil War will make you a better fighter pilot.  \n2. GEB isn't worth reading unless you are still in middle school or are a fan of woo-woo pseudoscience disguised as academic literature.  \n3. Flatland. It's short. It's brilliant. Read it.  \n4. On the Origin of Species. One of the most influential books on modern scientific thought. Read from the eyes of an explorer. It'll help you regardless of your field.  \n5. Logicomix. Admittedly, I haven't read this myself, but it has great reviews and pretty much anything by Papadimitriou is a worthwhile read anyway.  - Pragmatic Thinking and Learning\n\n- The Art of War\n\n- Zen and the Art of Motorcycle Maintenance\n\n- Tao of Jeet Kune Do     A good one to learn about the history of hackers (not the bad kind...) is \n\"Linus Walleij - Copyright existiert nicht\" or \"Copyright finns inte\".\n\nI only found the german and the original swedish title, I don't know how the english version is called. [\"Copyright Does Not Exist\", by Linus Walleij](http://home.c2i.net/nirgendwo/cdne/)          Since we're talking about textbooks.., whats a great beginner's Java book?  Code Complete by Steven C. McConnell http://www.amazon.com/Code-Complete-Practical-Handbook-Construction/dp/0735619670 I cannot get into this book. I've had Code Complete 2 on my shelf for over a year after reading the first couple of chapters.  It seemed incredibly dry.  What am I missing by skipping it? I cannot get into this book. I've had Code Complete 2 on my shelf for over a year after reading the first couple of chapters.  It seemed incredibly dry.  What am I missing by skipping it?    ",
    "url" : "http://www.reddit.com/r/compsci/comments/toe8f/nontextbooks_worth_reading_that_will_make_you_a/"
  }, {
    "id" : 35,
    "title" : "I want to learn computer science but our school cut the program.  Any ideas where to start on my own?",
    "snippet" : "So, as I entered as a freshman, I didn't really have any idea what I wanted to do.  I was a math / physics double major and thus had to take a mandatory class in C programming.  I 100% loved it, it was by far my favorite class I've taken (outside of one or two math classes) and I want to continue doing it.  However.... the entire program got the axe at our school as I was in the class.  I feel like knowing computer science, even if it's self taught, would be obscenely beneficial to my future in mathematics (especially if PhD math falls through).  Anyone have any kinds of tips as to where I should start?\nThanks!  \n\n**tl;dr My school doesn't offer it anymore, and I still want to learn computer science. Any help?**  Is it a possibility to switch schools?  Get into algorithms and data structures first.\n\nStart with brute force searching. Solve problems using this technique (Usaco mentioned later has a lot of problems that can be solved using brute force search). Go on learning about recursion, divide and conquer, dynamic programming, dfs and bfs. These are fundamental, and a great place to start.\n\nWikipedia has decent articles on most algorithms, otherwise, find a book Introduction to Algorithms is heavy, but good, and since you're studying math, you should be able to pick it up. Just read on one thing at a time. Draw and perform the algorithm on a problem on paper to understand it properly. Then implement it to solve an obvious problem. Once you've got it, find a problem on http://codeforces.com/ using this kind of algorithm, it's neatly divided into \"categories\". You could also go through the Usaco training program, which has small instructions to the most common algorithms: http://ace.delos.com/usacogate\n\nYou're studying in mathematics, so you'll love solving these kinds of problems. Along the way, you'll learn C++/C by doing. I'd personally recommend C++ for implementing algorithms, as the trivial algorithms and data structures are in the standard library (queue, stack, priority queues, sets, lists, quicksort, binary search etc., read up on those you don't understand). What sort of math would you recommend someone look into before/while learning algorithms/data structures? discrete math (especially graphs) What sort of math would you recommend someone look into before/while learning algorithms/data structures?   [khaaaaaaaaaaaaaaaaaaan](http://www.khanacademy.org/science/computer-science) is a good place for the very basics; the lessons study with Python, which if you've worked in C should not be that large of a jump. if you feel like all of that has been covered before for you, try... [udacity?](http://udacity.com) I haven't really seen much of their syllabi.\n\nBoth of those are sites geared to online self-paced learning for free.  What school do you currently attend? I would wager University of Florida, which just cut their CS program due to cuts in funding from the state (and ended up giving more money to athletics anyways). Nah, SUNY Geneseo.  Similar situation, they cut the program and then we ended up paying $100000 to have Kesha preform later that year. Awesome. When did they cut the program? I was actually thinking about going there for cs but ended up in another school.        Start with java. It's simple and useful. In high school my first program was in VisualBASIC and it did my statistics homework for me. Since you like physics you can do programs for trajectory, velocity, and stuff like that. Android phones run off Java and the development tools are all free. After you learn java you could dabble in that for graphics. Also, you could just stick with C, I can give you the problems our teachers made us do if the sites are still up. You think Java is simple? I guess its robust and very structured. But I think python or php are much more simple. Then again i don't want o get into this argument im sure everyone has their opinion :P. \n\nJust wondering what do you consider to be a harder language? LISP is the hardest I have encountered. My current job is BASIC with PICK (Legacy systems) the reason I would recommend java to you is because it is simple enough, can be used with drivers, and I have helped physics majors with robots that were written in Java or C/C++. Look up Erlang, LISP, Haskall. Welcome to the world of functional languages. They are weird at first. Awesome once you know them. Start with java. It's simple and useful. In high school my first program was in VisualBASIC and it did my statistics homework for me. Since you like physics you can do programs for trajectory, velocity, and stuff like that. Android phones run off Java and the development tools are all free. After you learn java you could dabble in that for graphics. Also, you could just stick with C, I can give you the problems our teachers made us do if the sites are still up. Start with java. It's simple and useful. In high school my first program was in VisualBASIC and it did my statistics homework for me. Since you like physics you can do programs for trajectory, velocity, and stuff like that. Android phones run off Java and the development tools are all free. After you learn java you could dabble in that for graphics. Also, you could just stick with C, I can give you the problems our teachers made us do if the sites are still up. Ugh, screw Java.  Start with C/C++.  Java is great for hand-holding and not-so-great for producing good programmers. Java is good for learning the basics. Like recursion, mutual recursion, and how sorting algorithms help. Once you know the basics you never go back. But trying to program a sort in C++ and getting knowing but seg-faults and memory dumps is not so useful. At least java has some decent errors for the newbies. Start with java. It's simple and useful. In high school my first program was in VisualBASIC and it did my statistics homework for me. Since you like physics you can do programs for trajectory, velocity, and stuff like that. Android phones run off Java and the development tools are all free. After you learn java you could dabble in that for graphics. Also, you could just stick with C, I can give you the problems our teachers made us do if the sites are still up. One does not \"program in java\", one writes a bunch of XML in order to tell Java what to do.  Since you've taken C before, I would recommend learning C++. Yes, it's an incredibly bloated language. However, if you fully understand C++, you know most concepts used in syntax. (functional languages will still be pretty mysterious though.) It's used nearly everywhere for application programming, too. Try [here](http://www.learncpp.com/), for a good syntax tutorial. \n\nKeep in mind though, CS isn't just about programming. There's a lot of theory behind it, given that it is a branch of math. I don't know a good theory site off the top of my head, but Wikipedia probably has some good articles, once you know what you're looking for. First thing I'd start with (after you do syntax) is sorting algorithms, if you haven't done it already. Great introduction to complexity. Yeah, thats the thing.  I really like programming, and I know I can find things online to figured that out, but I want to learn the other stuff behind it, like the algorithms, etc.  Thanks! You might enjoy reading Skiena's [*Algorithm Design Manual*](http://www.algorist.com/) (your school library may have either a physical or digital copy of it). A few days ago, someone posted the [*Open Data Structures*](http://opendatastructures.org/) book here (haven't read it though).\n\nIf you're interested in programming and programming languages, there are a lot of good (free) resources. Starting from [*How to Design Programs*](http://www.htdp.org/) and then [*Programming Languages: Application and Interpretation*](http://www.cs.brown.edu/~sk/Publications/Books/ProgLangs/) (PLAI). There is also [SICP](http://mitpress.mit.edu/sicp/). I recommend PLAI over SICP for languages, but SICP has some other useful content.\n\nTry to find a copy of Russell and Norvig's *AI: A Modern Approach* if you want to learn AI. Try Sipser's [book](http://www-math.mit.edu/~sipser/book.html) for theory of computation. Yeah, thats the thing.  I really like programming, and I know I can find things online to figured that out, but I want to learn the other stuff behind it, like the algorithms, etc.  Thanks! Yeah, thats the thing.  I really like programming, and I know I can find things online to figured that out, but I want to learn the other stuff behind it, like the algorithms, etc.  Thanks!  ",
    "url" : "http://www.reddit.com/r/compsci/comments/tp7mx/i_want_to_learn_computer_science_but_our_school/"
  }, {
    "id" : 36,
    "title" : "Memoization memory optimization",
    "snippet" : "Is there a way to memoize such that results that are subsets of larger results, small areas in tiling for example, which I think are no longer reachable (is this plausible?) can be deleted to save memory?  Yes. This is possible through [dynamic programming](http://en.wikipedia.org/wiki/Dynamic_programming). The key is to determine the proper order of computations needed for the recursion relation (by linearizing the DAG of dependencies), then performing those computations in that order. Once you've reformulated your algorithm in this way, you may be able to determine which results are no longer necessary at a certain step of the algorithm, and incorporate any deletion of unnecessary results into your algorithm. If this is a practical concern about memory, you could consider using a least-recently-used cache if you want to avoid actually computing the DAG. This would have some of the same effects with substantially less effort (depending on the complexity of the the DAG). Yes. This is possible through [dynamic programming](http://en.wikipedia.org/wiki/Dynamic_programming). The key is to determine the proper order of computations needed for the recursion relation (by linearizing the DAG of dependencies), then performing those computations in that order. Once you've reformulated your algorithm in this way, you may be able to determine which results are no longer necessary at a certain step of the algorithm, and incorporate any deletion of unnecessary results into your algorithm. ",
    "url" : "http://www.reddit.com/r/compsci/comments/tpc15/memoization_memory_optimization/"
  }, {
    "id" : 37,
    "title" : "Top rankings of computer science researchers.",
    "snippet" : "  How do I filter by beard? I don't care about the computer science research of anyone lacking a beard.  Scott Shenker, in addition to being the highest-rated computer scientist on that list, is also an [amazingly cool guy.](http://www.reddit.com/r/berkeley/comments/nept7/would_your_professor_do_this_a_couple_hours/) Scott Shenker, in addition to being the highest-rated computer scientist on that list, is also an [amazingly cool guy.](http://www.reddit.com/r/berkeley/comments/nept7/would_your_professor_do_this_a_couple_hours/)  ..and that, folks, demonstrates the imperfections of citation-based rankings except these are not based on citation numbers, but on h-index, which requires your papers to also be well cited by other researchers. also the list contains people that every researcher will know of in that field (shenkar, culler, estrin for systems and networks, mitichili, vinticilli for eda, stonebreaker for db, on and on) that the fact that people here have no idea who these guys are tells me that this subreddit surprisingly has few graduate students / phds. \n\n\n ..and that, folks, demonstrates the imperfections of citation-based rankings ..and that, folks, demonstrates the imperfections of citation-based rankings How is this a demonstration of the imperfection of citation-based rankings? Well, for one thing, it's biased toward people who publish large amount of papers in huge groups (based on examining the publications of the top few).  \n  \nAlice and Bob, both individual workers, can double their citation count by just putting each other's names on their papers as well.  \n  \nFurther, the groundbreaking results oftentimes \"lose\" citations to papers providing only marginal improvements. I certainly hope universities, companies, and grant-providing agencies don't solely rely on these naive metrics when making decisions on any given researcher.\n\nThere are additional metrics, such as the [H-index](http://en.wikipedia.org/wiki/H-index) and the [G-index](http://en.wikipedia.org/wiki/G-index), that can be utilized. However, I do not know what the metric values mean.\n\n*Off to Google* They do not :) It's usually done on a basis split between proficiency and being part of a nice, closed social network. Shit, my mother was right when she said networking was (mostly) everything D:    No Donald Knuth.  This is wrong.      Pretty cool.  I've seen met and seen presentations by several in the 1st 3 pages.  Re the \"imperfections of citation-based rankings\" comment.  You statement is akin to arguing about the imperfection of alphabetic indexing.  In databases we refer to this mechanism as an \"order by \" clause.  You might feel better if they were ranked by birthdate? So your saying the results differ if you choose different criteria to sort by such as H-index or g-index, or birthdate? Sounds legit! Pretty cool.  I've seen met and seen presentations by several in the 1st 3 pages.  Re the \"imperfections of citation-based rankings\" comment.  You statement is akin to arguing about the imperfection of alphabetic indexing.  In databases we refer to this mechanism as an \"order by \" clause.  You might feel better if they were ranked by birthdate?",
    "url" : "http://academic.research.microsoft.com/RankList?entitytype=2&amp;topdomainid=2&amp;subdomainid=0"
  }, {
    "id" : 38,
    "title" : "[question] What is the fastest way to 'transpose' a binary file?",
    "snippet" : "I have a binary file containing n*m object, all of equal size. They are ordered\n\n{ O[1,1], O[1,2], ..., O[1,m], O[2,1], O[2, 2], ..., O[2, m], ..., O[n, 1], O[n, 2], ..., O[n, m] }.\n\nI want to transpose this file, meaning I need a file ordered\n\n{ O[1,1], O[2,1], ..., O[n,1], O[1,2], O[2,2], ..., O[n,2], ..., O[1,m], O[2,m], ..., O[n,m] }\n\nI can't do this in memory, since the size of the file is massive and the data can't be generated in a different order. So I have to do this on the harddrive, but instead of skipping through the file like a moron, I would like to use the memory that I do have in an optimum way. Is there any literature on this? Does anybody have any brilliant ideas?\n\nEDIT: for clarity\n\n    a b c d\n    e f g h\n    i j k l\n\nbecomes\n\n    a e i\n    b f j\n    c g k\n    d h l\n\nwhere a, b, ..., l are objects of equal size and there is no data in the file besides those objects  Look into algorithms for matrix transposition based on optimizing cache locality. \n\nThis is basically the same situation as that, pushed one level down the memory hierarchy, and the same principles should apply.  Thanks, your (short) answer helped me out a lot. They keyword that did it was 'cache oblivious'.\n\nI am transposing a 15 GB file at a throughput of 20 MB/s right now. I think that will do :) Look into algorithms for matrix transposition based on optimizing cache locality. \n\nThis is basically the same situation as that, pushed one level down the memory hierarchy, and the same principles should apply.      How about a first preprocessing step that slices the input file into 'n' separate temporary files, one for each row in the input matrix.  Then creating the final output file means opening all of them simultaneously and reading one record at a time from each input file in sequence, like the `zip` function in common programming languages.  Most importantly, this means no seeking -- it's all streaming, which means you can just sit back and let your OS' readahead disk cache do what it was designed to do.  Your program only ever needs to have one object in memory at a time, so the bulk of system memory will be used by disk cache.\n\n(Edit: \"no seeking\" from a conceptual level -- of course the disk will still have to seek to access each file, but hopefully this won't be too ruinous.)\n  How big is the file? ",
    "url" : "http://www.reddit.com/r/compsci/comments/todb0/question_what_is_the_fastest_way_to_transpose_a/"
  }, {
    "id" : 39,
    "title" : "What do you think of my soon to be summer project.",
    "snippet" : "\nWell the summer holidays are coming up really fast. I'm at uni doing a physics degree but have recently fallen in love with computing. So I decided that during my summer holiday I will be trying to make a 4-bit ALU out of marble mechanics!\n\nI currently have a youtube channel where I am making an ALU with mine carts in minecraft and that's what inspired me to use marbles IRL. Anyway the idea is to have the following functions:\n\nAND\nOR\nNOT\nXOR\nA+B\nA-B\nShift right (not shift left seen as you can just to A+A for shift left)\n\nAnyway what do you think?\n  I think it sounds really awesome! If you do it make sure to update us with the results! :) Yes indeed =D I will do exactly that =) I think I should be doing compsci at uni instead of physics now I think about it... but oh well =P possibly more ee than compsci ! depends on the school, but digital logic often falls under the auspices of Computer Engineering, which is sort of a hybrid of EE and CS. Well I'm actually thinking about seeing if I can take some modules in logic next year. I think I'd probably enjoy it more than physics too which is the strange thing =P. So you know, logic taught by a Math professor and logic taught by a EE/CE/CS professor will probably have a very different feel. If you are most interested in practical applications of logic, I would recommend the latter course, and perhaps even a course in FPGA or ASIC design if you like it.\n\nAt my alma mater, EE and CE are mashed into the same major, with multiple subfields focusing on chip fabrication/materials science/physics, EE, ASIC design, embedded software design, etc. At this point the field is so huge that it is quite unusual to see people with cross-cutting knowledge across the spectrum of subfields. That said, a lot of the best work being done today is at the intersection of fields, so a physicist with  deep CS knowledge is likely to be in high demand. See I was thinking it may be useful especially if I wanted to go into quantum computing =D. I'm not really sure what I am interested in. I'm interested in boolean algebra but I have never been able to figure out what the heck is going on in textbooks xD. I am also interested in the uses of logic and just making stuff with logic =P\n\nNot sure if there is a course on that but there you go =P. Also some of the uses like how logic can be used to make things add. It took me many months to work out how on earth a full adder worked... xD But I finally did it and I was like 'wow... who the heck came up with that?!' xD I think the whole field is just fascinating, though material science doesn't really interest me much... Sure, I took a few courses on exactly that subject, IIRC the main one was called \"Digital Logic Design\" (and I think some of it was also covered in another course called \"Discrete Algorithmic Mathematics\"). We designed a complete MIPS-compatible processor by the end of it. See if your school offers a course where you use a hardware descriptor language (probably Verilog or VHDL). Boolean algebra is pretty easy once you get the hang of it, you mostly just need to know understand the symbols and nomenclature, and know a few key rules. I will look out for that thanks =D what does IIRC stand for though? =P\n\nYeah, I got a book on boolean algebra and it started off with a tonne of proofs and theorems and I think the problem was it was meant to be a book on logic and I was thinking 'how does this maths have any relation to logic?' xD I understand now of course that boolean algebra pretty much is logic though  It sounds great! Do you have a link to your youtube channel? Also will you be making youtube videos for your marble ALU?\n\n  What's an ALU? Serious noob here. What's an ALU? Serious noob here. arithmetic logic unit  :) And for those who don't understand what one of those is the thing inside a computer which does the extremely basic maths (like adding and subtracting (possible multiplying and dividing on high end ALUs)) and also basic logic functions bit wise.\nSo the logic functions such as AND, OR and XOR are often found in an ALU. Shifting is also another function of the ALU and this can take a number like 010010 and shift it to 100100 or 001001 =) People are so kind here! Thanks! What's an ALU? Serious noob here. arithmetic logic unit\n\ngoogle will do the rest I tried it but there really isn't anything on what it is. Its more of just people asking how to ALU. [bull](http://imgur.com/mVYmi) [shit](http://lmgtfy.com/?q=arithmetic+logic+unit&amp;l=1) Wow mine totally came up with no wikis and never even occurred to me to look on wikipedia. Thanks for that I needed it.  Why stop at the basic logic gates? Why not build a whole little CPU with registers and and instruction set and the whole bit? you can definitely do it in a summer, we did it in a semester in undergrad (albeit using TTL not marbles)  Sounds fun. Reminds me of [billiard-ball computers](http://en.wikipedia.org/wiki/Billiard_ball_computer), which I read about in Richard Feynman's book on computation.  You could do an FPGA instead, and make an ALU by programming the FPGA. :) Excuse me for not knowing what on earth that is =P From a very brief look on wikipedia ... that looks insanely more complex than an ALU... Is it? =P Well in your marble system I wouldn't do a full FPGA, but rather FPGA inspired. Make it so that what you use to build the ALU is modular and you switch it around easily and a lot. The programming part of it would be by hand. So no, not really FPGA. What are you even...\n\nAre you trying to say FPGA?  Fuck. I even opened the wikipedia page by typing 'FPGA' into Google on my second post. Thank you for this correction.",
    "url" : "http://www.reddit.com/r/compsci/comments/tnvwd/what_do_you_think_of_my_soon_to_be_summer_project/"
  }, {
    "id" : 40,
    "title" : "Natural Language Processing",
    "snippet" : "Hey everyone, I'm currently studying computer science in my second year and am hoping to get into some NLP this summer. I've Googled a bit for resources but was hoping to get some more feedback from the people here!\n\nSo, what are some people's favorite NLP focused books? Preferably for beginners in the NLP/ML area.\n\nThanks!\n  https://www.coursera.org/course/nlp\n\nhttp://www.nltk.org/book https://www.coursera.org/course/nlp\n\nhttp://www.nltk.org/book https://www.coursera.org/course/nlp\n\nhttp://www.nltk.org/book https://www.coursera.org/course/nlp\n\nhttp://www.nltk.org/book I am signing up for this.  It looks awesome.   https://www.coursera.org/course/nlp\n\nhttp://www.nltk.org/book  Manning and Schütze's *Foundations of Statistical Natural Language Processing* is outstanding: http://nlp.stanford.edu/fsnlp/\n\n ugh, this reminds me I lent this book out like 6 years ago and never got it back.  Manning and Schütze's *Foundations of Statistical Natural Language Processing* is outstanding: http://nlp.stanford.edu/fsnlp/\n\n Is this the dice one or is that the other? Dice. Jurafsky's book is the \"other\", I suspect.\n\n   [here is a .zip folder](http://www.mediafire.com/?y164e09283v23a9) with the following books on NPL.\n\n* Foundations of Statistical Natural Language Processing - Christopher D. Manning\n\n* Natural Language Processing with Python (2009)\n* Prolog and Natural Language Analysis - Fernando C. N. Pereira , Stuart M. Shieber\n* Speech and Language Processing  An Introduction to Natural Language Processing, Computational Linguistics  and Speech Recognition \n\nNote: some of these books are old , But they are quite good :) .\nHope this helps !   While not focused on NLP, Machine Learning by Thomas Mitchell is a great book if you are new to the subject.        I suggest reading Jurafsky until chapter 17. Skip the phonetics stuff as well. Then read Manning's book. Skip the ML stuff. The two books really complement each other. Those should give you a solid foundation.\n\nThe stuff beyond chapter 16 in Jurafsky is really abstract and would require some serious coding on the side to really understand what is going on.  Skip ML?  Isn't it a big part of NLP though? It is. I was assuming a prior knowledge of ML before diving into NLP. I don't think learning ML from an NLP book is a good idea.   Natural Language Processing in Python - all you ever need to get your passion started. (it could be \"with Python\", idk)\n\nTrust me, I'm an NLP. &gt;Trust me, I'm an NLP.\n\nI think you need to tweak your sentence generation rules. I think there could be a problem with the fuzziness metric of your computational humour model.\nTry sudo pip install pyBritish &gt;Trust me, I'm an [Natural Language Programming|Processing].\n\nIsn't very good English, British or not. ;-) ",
    "url" : "http://www.reddit.com/r/compsci/comments/tmcri/natural_language_processing/"
  }, {
    "id" : 41,
    "title" : "When is using a specialized graph database appropriate? Should I use one in this case?",
    "snippet" : "Hi, I'm just starting with trying to build web applications and I'm not really familiar with how to store data to make sure something like this scales well. \n\nConsider this pseudo-data, in JSON:\n    { unique id,\n      [array of child ids],\n      [auxiliary data]\n    }\n\nRight now, every request I get requires me to get auxiliary info given an ID. About 3/4 of the time I need to update some of the auxiliary data. About half of the time, I need to query again for the auxiliary data of one of the \"child\" id's from the array of ids. This makes me want to just use a relational database and generate another query based on the child IDs if I need to traverse the natural graph structure of the data (do very \"shallow\" searches).\n\nI'm wondering how well this would continue to work if I suddenly decided to do a lot more \"depth-first\" query patterns (that is, every query would likely be followed by a query to it's child, which has an unpredictable ID), and whether specialized graph databases (not SQL) would give me more scalability in this case. I don't actually know much about how they work but I imagine if there's any reason they exist it's for stuff like this.\n\nCan anyone point me in the right direction? If a single request generates a chain of sequential SELECTs to traverse a graph am I doing it wrong?   ",
    "url" : "http://www.reddit.com/r/compsci/comments/tnboc/when_is_using_a_specialized_graph_database/"
  }, {
    "id" : 42,
    "title" : "how does file compression work?",
    "snippet" : "how does compression work? does it work the same every time? if I had two identical jpeg files and saved them equal times, would they both get compressed exactly the same way? what about different computers? OSs? can compression show history when cross referenced with the original file?  \n\nthanks!  Simple example:\n\nSay you have the following string (text):\n\n    aaaabbccccccddeeeeaaaabbbb\n\nThere are various ways to compress the above string. The most simple way is to group repeated letters:\n\n    a4b2c6d2e4a4b4\n\nA more complex way is to re-encode all the data by a new lexicon (using a new way of representing the data). For example you can encode:\n\n    aaaa =&gt; x\n    bb =&gt; y\n    ccc =&gt; z\n    dd =&gt; m\n    eeee =&gt; n\n\nIn this case, you can encode the original string to:\n\n    xyzzmnxyy\n\nTo decompress the string, you simply substitute the other way. Here we are assuming that my alphabet is only a,b,c,d and e, and that the lexicon is known beforehand by both the compression and decompression algorithms.\n\n\nNow, for your other questions:\n\nQ: does it work the same every time?\n\nA: If you use the same compression algorithm with the same settings, yes. (You can check this yourself by hashing your files)\n\n\nQ: if I had two identical jpeg files and saved them equal times, would they both get compressed exactly the same way?\n\nA: Again, if you use the same compression algorithm with the same settings yes. Change the settings slightly and you get a completely different file (the decompressed files are still identical). Also, JPEG is already compressed, so you will not likely see any space reduction when compressing (there are some rare cases where you can get a bigger file when compressing).\n\n\nQ: what about different computers? OSs?\nA: Same answer as above.\n [deleted] Simple example:\n\nSay you have the following string (text):\n\n    aaaabbccccccddeeeeaaaabbbb\n\nThere are various ways to compress the above string. The most simple way is to group repeated letters:\n\n    a4b2c6d2e4a4b4\n\nA more complex way is to re-encode all the data by a new lexicon (using a new way of representing the data). For example you can encode:\n\n    aaaa =&gt; x\n    bb =&gt; y\n    ccc =&gt; z\n    dd =&gt; m\n    eeee =&gt; n\n\nIn this case, you can encode the original string to:\n\n    xyzzmnxyy\n\nTo decompress the string, you simply substitute the other way. Here we are assuming that my alphabet is only a,b,c,d and e, and that the lexicon is known beforehand by both the compression and decompression algorithms.\n\n\nNow, for your other questions:\n\nQ: does it work the same every time?\n\nA: If you use the same compression algorithm with the same settings, yes. (You can check this yourself by hashing your files)\n\n\nQ: if I had two identical jpeg files and saved them equal times, would they both get compressed exactly the same way?\n\nA: Again, if you use the same compression algorithm with the same settings yes. Change the settings slightly and you get a completely different file (the decompressed files are still identical). Also, JPEG is already compressed, so you will not likely see any space reduction when compressing (there are some rare cases where you can get a bigger file when compressing).\n\n\nQ: what about different computers? OSs?\nA: Same answer as above.\n One common algorithm to do this is what's known as [Huffman Code](http://en.wikipedia.org/wiki/Huffman_coding). Basically, it's a way of encoding data into binary where the most common data is represented by the least amount of bits. That creates a very memory efficient data storage.\n Simple example:\n\nSay you have the following string (text):\n\n    aaaabbccccccddeeeeaaaabbbb\n\nThere are various ways to compress the above string. The most simple way is to group repeated letters:\n\n    a4b2c6d2e4a4b4\n\nA more complex way is to re-encode all the data by a new lexicon (using a new way of representing the data). For example you can encode:\n\n    aaaa =&gt; x\n    bb =&gt; y\n    ccc =&gt; z\n    dd =&gt; m\n    eeee =&gt; n\n\nIn this case, you can encode the original string to:\n\n    xyzzmnxyy\n\nTo decompress the string, you simply substitute the other way. Here we are assuming that my alphabet is only a,b,c,d and e, and that the lexicon is known beforehand by both the compression and decompression algorithms.\n\n\nNow, for your other questions:\n\nQ: does it work the same every time?\n\nA: If you use the same compression algorithm with the same settings, yes. (You can check this yourself by hashing your files)\n\n\nQ: if I had two identical jpeg files and saved them equal times, would they both get compressed exactly the same way?\n\nA: Again, if you use the same compression algorithm with the same settings yes. Change the settings slightly and you get a completely different file (the decompressed files are still identical). Also, JPEG is already compressed, so you will not likely see any space reduction when compressing (there are some rare cases where you can get a bigger file when compressing).\n\n\nQ: what about different computers? OSs?\nA: Same answer as above.\n &gt; Q: what about different computers? OSs? A: Same answer as above.\n\nByte order may be important here, depending on whether the algorithm was designed with portability is mind. Can you name a compression algorithm that relies on byte order?\n\n(Common algorithms really don't, since they usually work on the byte level and pack bits within each byte.) Or, being pedantic, they work on arbitrary sized integers, or on arbitrary rational numbers. Then it's a question of how you want to persist said numbers. Files? That's just one possible persistence mechanism.  Can you name a compression algorithm that relies on byte order?\n\n(Common algorithms really don't, since they usually work on the byte level and pack bits within each byte.) lossless encoding that works in the frequency domain, or any compression that takes advantage of nontrivial structural properties of the data (for example if a sound file is 16 bit interleaved, you will find strong correlations in the frequency domain for natural sounds (when analyzed via lossless fft) between alternate 16 bit chunks.\n\n That means, given an audio recording stored in a buffer of short ints (CD format), with the following declaration\n\nint16 data[n]\n\nthere will be correlated in the frequency domain when you analyze data[0] data[2] data[4] data[6] ...\n\nthere will also be a correlation in the frequency domain when you analyze data[1] data[3] data[5] ...\n\nIf you just analyzed the data sequentially, and ignore the data size and frame size and data format, you would have to work much harder to find the patterns to encode.\n\nSimilarly with lossless image compression, the column width and data size help you know which pixels are adjacent, and for a natural image (most images meant for a human to look at) there will be higher correlation between adjacent pixels. Analyzing data points x=0,2,4... and x=1,3,5... is not how you transform something into the frequency domain.  You need something like a DCT or a wavelet transform, which is completely different.  The closest thing to what you describe is the two most extreme samples of a DCT, which will have the DC coefficient (odds PLUS evens) and the highest frequency coefficient (odds MINUS evens).  And yes, I know that wavelet isn't really frequency domain.\n\nWhat I meant is that compression algorithms don't depend on byte order *of the machine they run on.*\n\nAudio, video, and image compression algorithms typically treat uncompressed data as an array of integers of some fixed width, and compressed data as a byte stream.  You'll get the same result no matter what byte order the images are encoded in.\n\nNow, PNG does a weird thing where for 16-bit sample depths it filters the low bytes and the high bytes separately, to simplify the encoder/decoder implementation.  However, this doesn't depend on machine byte order since PNG always uses network byte order. The point is, you have two independent streams, the streams are correlated individually, but their data is interlaced, so before you do a dct, you need to get your separate streams, or the quality of your analysis suffers. And when the data are each 16 bits (cd audio), scrambling those two bytes, or treating the interleaved channels as if they were one sequence, will definitely obscure patterns. Audio encoders, lossy or lossless, need to know the endianness and the frame format, I promise.\n\nTo get more concrete: imagine you have two channels on a standard red book CD, a recording of a vocalist hitting the note a 440hz. The left microphone was about 3.855 feet from the right microphone with the vocalist in the center. The left and right mics will be 180 degrees out of phase for a 440 hz signal at that spacing. If you read the interleaved channels of the digital recording of that event, and you do your dct based on the individual bytes, you are making 2 huge mistakes: each sample is 16 bits. By reading byte by byte you are giving way too much weight to the lower bytes, and what was previously a strong fundamental with a simple harmonically related set frequencies making up its spectrum is now an extremely noisy signal with a strong frequency component of sr/2 (because every other sample is strongly correlated (the upper bytes) and the others they are mixed with are nearly random (the lower bytes, changing much faster). Then, the second huge mistake is that the channels are interleaved, that means you have 16 bits of left channel immediately followed by 16 bits of right channel, back and forth for the whole file. Which means you have added yet another strong frequency component of sr/4 (the previous frequency divided by two, since now the data is spaced further apart), while reintroducing yet another set of relatively uncorrelated data at sr/2 again. Because of the phase shift of the two channels, you cannot predict the values of one frame based on the previous without taking the interleaving and data size into account. A dumb encoder that went byte by byte may still find patterns, but it will not be able to compress a normal musical sound nearly as efficiently (both in cpu and size compression) as an algorithm that knows that the samples are two bytes wide, little endian, two channels, interleaved.\n\nNot to mention, if you tried to perceptually encode data on a byte by byte basis (and the data was not 8 bit mono) the result would be garbage. Not to mention the consequences of ignoring sample rate when perceptual coding has to be weighted for frequency (human hearing is very nonlinear). You keep going on, but the original question was whether the endian **of the machine** affects the result of the compression.  And the answer, just like you say, is that the endian **of the machine** does not affect the results of the compression.\n\n&gt; compression algorithms typically treat uncompressed data as an array of integers of some fixed width\n\nSo I didn't go into exacting detail here, and I'm sorry if you find that unforgivable.\n\n1. Audio starts as a stream of bytes in some format.\n\n2. It's then decoded into separate channels, each channel a stream of samples.  I expect that most compressors work on samples of a certain fixed size, probably 32 bits.  This is the data that is fed to the compressor.\n\n3. The compressor then spits out a stream of bytes.\n\nSince an audio compressor is fed a stream of decoded samples, it has no knowledge of the original byte order.\n\nCorrelation between channels is handled in a much simpler way than you might expect.  Two common techniques are encoding the L and R channels separately, or encoding L+R and L-R separately.  Some algorithms choose between those two alternatives on a frame-by-frame basis.\n\nYou say that handling the high and low bytes separately is a \"mistake\", but I am merely trying to describe how actual compression algorithms work.  The PNG format **does** handle high and low bytes separately, so it's worth mentioning.  Even if it's not optimal from a compression ratio standpoint, it's simpler from an implementation standpoint since you only need to write one set of predictors instead of rewriting them for each bit depth. And you asked if someone could name an algorithm that depended on byte order, and you talked down to me as if I was an idiot who thought unwrapping interleaved samples was  the same as a DCT. My point was that there are algorithms for which byte order is extremely important.\n\nI listed CD audio as an example, but for platform specific sound file formats, they use the platform's native byte order. In fact you could see little endian as the \"native byte order\" of a standard CD player. So the machine byte order is a concern in this kind of compression. You can take any of those PCM formats, and convert them to any other format, and the results won't depend on the byte order **of the machine** (or even the original format) which is what I keep saying, and what I thought the original question was about, which is why I didn't think you were talking about interleaved samples in the first place.\n\nCan you name an algorithm for which the byte order **of the machine** is important?  This was in response to a parent comment\n\n&gt; Q: does it work the same every time?\n&gt; A: If you use the same compression algorithm with the same settings, yes. (You can check this yourself by hashing your files)\n\n...\n\n&gt;&gt; Q: what about different computers? OSs? A: Same answer as above.\n\n&gt; Byte order may be important here, depending on whether the algorithm was designed with portability is mind.\n\nSo the answer is no, the results of compression do not generally depend on the byte order **of the machine.**  CD audio always has the same encoding no matter whether it was produced by an ARM or PowerPC or IA32 processor.\n\nI'm not trying to talk down, but I was honestly confused by this sentence:\n\n&gt; there will be correlated in the frequency domain when you analyze data[0] data[2] data[4] data[6] ... Simple example:\n\nSay you have the following string (text):\n\n    aaaabbccccccddeeeeaaaabbbb\n\nThere are various ways to compress the above string. The most simple way is to group repeated letters:\n\n    a4b2c6d2e4a4b4\n\nA more complex way is to re-encode all the data by a new lexicon (using a new way of representing the data). For example you can encode:\n\n    aaaa =&gt; x\n    bb =&gt; y\n    ccc =&gt; z\n    dd =&gt; m\n    eeee =&gt; n\n\nIn this case, you can encode the original string to:\n\n    xyzzmnxyy\n\nTo decompress the string, you simply substitute the other way. Here we are assuming that my alphabet is only a,b,c,d and e, and that the lexicon is known beforehand by both the compression and decompression algorithms.\n\n\nNow, for your other questions:\n\nQ: does it work the same every time?\n\nA: If you use the same compression algorithm with the same settings, yes. (You can check this yourself by hashing your files)\n\n\nQ: if I had two identical jpeg files and saved them equal times, would they both get compressed exactly the same way?\n\nA: Again, if you use the same compression algorithm with the same settings yes. Change the settings slightly and you get a completely different file (the decompressed files are still identical). Also, JPEG is already compressed, so you will not likely see any space reduction when compressing (there are some rare cases where you can get a bigger file when compressing).\n\n\nQ: what about different computers? OSs?\nA: Same answer as above.\n  That is a huge question, with many different answers. First of all, compression is not one single method which takes in data and spits out smaller data. It is a giant umbrella term for many different methods, or algorithms, which, generally speaking, do just that.\n\nFor example, the algorithm used by gzip to compress tarballs full of text files is *drastically* different from the algorithm used by LAME to compress audio files. These are *both* in turn drastically different from the JPEG algorithm often used for image files. Generally speaking, algorithms designed for one type of data will not work with other types of data, or at the very least will not provide optimal compression. For example, many music \"compression\" algorithms actually work by shaving off sounds that the human ear cannot perceive or are difficult for the human ear to perceive, which obviously would make no sense for image or textual data.\n\nFurthermore, even dealing with just one type of data (let's say images for example purposes) two different algorithms can behave *very* differently. For example, (in a gross simplification of both JPEG and PNG), JPEG compression essentially works by describing how to recreate an image, without storing any actual pixel data. PNG works by giving each individual color a shorter name (if an image only uses 20 distinct colors, then 32 bits per color is not necessary. 5 bits per color will do the trick, along with a table linking each 5 bit value to an actual 32 bit color) and assigning each pixel one of those new shorter names.\n\nTo answer your question directly, JPEG (as I touched on before) is just a compression algorithm. I have no idea whether it is deterministic or not, however (edit: see below), and it is quite possible that two different implementations of the algorithm will have slight variations between the two, so I cannot personally say for sure whether you will get *identical* data if you save the same image as a jpeg with two different programs, but assuming you use an identical compression level and other such settings then the data should be *very nearly* identical, with only some very slight differences.\n\nEdit: actually, Google says the JPEG algorithms are non-deterministic, although a handfull of implementations of them are. In other words, no. It is not guaranteed that you'll get perfectly identical data everytime you compress something via JPEG.  You say MP3 is drastically different from JPEG, but they're both based on the discrete cosine transform and Huffman coding the results.  They **are** similar.  Much more similar than, say, JPEG and PNG are similar (JPEG and PNG are very, very different).\n\nIn general, image compression and audio compression techniques are very similar, because they can exploit redundancy / masking effects in the time/space domain or the frequency domain, starting with simple 1- or 2-dimensional PCM data sampled at some frquency.  The first step in such a process is transforming time/space into frequency data.  JPEG and MP3 both use the DCT or some variant, the DCT is also extremely common in video compression.  Wavelet transforms are newer, used in JPEG2000 and AAC for example.\n\nOnce you've gotten the transformed data, you use some kind of simple quantization / masking followed by a simple coding scheme such as Huffman or arithmetic coding.  Note that Huffman coding is basically a quantized version of arithmetic encoding.\n\n&gt; JPEG compression essentially works by describing how to recreate an image\n\nI don't know where you heard that, but that's wrong.  It's just quantized and coded DCT data, instead of (non-quantized) coded spatial data mixed with backreferences.\n\nIf you want to look at algorithms that really *are* quite different from each other, you can compare JBIG to JPEG, or you can compare something like AAC/MP3 to something like Speex.  There are also some novel text compression algorithms out there, using arithmetic coding combined with sophisticated models such as Markov chains or neural networks. &gt; using arithmetic coding combined with sophisticated models such as Markov chains or neural networks.\n\n... wait ... compression with neural networks?! makes sense, neural networks are good at learning and finding patterns, and compression is all about patterns I suppose so. still, it'd take some convincing before I'd be content that the use of a neural network wouldn't put the data at risk. A neural network is just a type of algorithm for function approximation, it's completely deterministic unless you specifically program it *not* to be deterministic. the key word there is approximation. determinism just means it will fail to compress properly the same way each time, if it doesn't work. I'm not sure what you mean.\n\nNeural networks approximate functions in the same way that polynomials approximate functions.  They're completely deterministic and exact in the same way that polynomials are deterministic and exact, unless you add some nondeterministic element to the neural network.\n\nBut then again, you could always add a nondeterministic element to a polynomial, too. I mean, if you compress something by approximating it, then you are losing data. if you do this with text, it will result in scrambled text which is hard to read. However, from what you said in your explanation, it sounds like it doesn't work solely by approximation. If I understood correctly, you're saying that it checks the approximation first, and then if the approximation is accurate then it stores some indicator to use the approximation rather than storing the actual data? &gt; using arithmetic coding combined with sophisticated models such as Markov chains or neural networks.\n\n... wait ... compression with neural networks?! Here's how it works.  Open a book and pick a random point in the text and cover it with your thumb.  Now, read the part before what your thumb is covering, and try to guess the letter underneath your thumb.  How good was your guess?\n\nStudies show that people guess correctly about 35%-65% of the time, depending on the text and the study.  If you can get a computer program to guess right often enough, then you can compress a text file by recording which guesses were right, and which guesses were wrong.  If half of the guesses are right, then you only need one bit to record a correct guess, which is an 8:1 compression factor.  (The incorrect guesses will require more space, bringing the overall ratio down.)\n\nhttp://mattmahoney.net/dc/mmahoney00.pdf  These guys are spot on, but I'd like to add that the efficiency of it is a tricky process. For example, if you have x = your entire string, you haven't compressed anything, and if you have a variable for every character, you've effectively just encrypted it. It's a balancing act. For a compression algorithm to work on an arbitrary file, there have to be files for which it makes them bigger. yes - the smaller the file the more likely this is to occur.  Most people do not compress small files however and this makes in unusual but I have seen it a few times.  Mainly from people who try to use winrar on a zip file to make it even smaller - somtimes multiple times. For a compression algorithm to work on an arbitrary file, there have to be files for which it makes them bigger. Interesting result, is there a general proof of this?  These guys are spot on, but I'd like to add that the efficiency of it is a tricky process. For example, if you have x = your entire string, you haven't compressed anything, and if you have a variable for every character, you've effectively just encrypted it. It's a balancing act. An illustration: LenPEG 2 can compress an image to 0 bits and still recover it perfectly.\n\nhttp://www.dangermouse.net/esoteric/lenpeg.html        &gt; if I had two identical jpeg files and saved them equal times, would they both get compressed exactly the same way?\n\nShould be, if compression library doesn't use random generator! Actually, the result depends on the parameters, which are usually chosen by the program for you.\n\nSo, no, it doesn't have to result in the same files (excluding meta-info, just the image). Again, the output should be the same unless the program doing the compression is not stable (in the algorithmic sense).\n\nYou have a.jpg and b.jpg, where at iteration 0 a.jpg === b.jpg. Using the same library, you re-save both images using the same parameters 256 times. a.jpg will still be identical to b.jpg. There you said it: *using the same parameters*.    [deleted]  &gt; how does compression work\n\nResearchers have spent careers exploring this subject. That's not even remotely true. Some sophomores in college have spent a couple weeks exploring that subject. Researchers have spent careers exploring news ways to compress data. It's very different.",
    "url" : "http://www.reddit.com/r/compsci/comments/tk8u8/how_does_file_compression_work/"
  }, {
    "id" : 43,
    "title" : "Ideas for a data set",
    "snippet" : "Hi everyone, I'm trying to create a device that stores accelerometer information so in the most general sense x,y,z axis at say 60 times a second. I'm trying to brainstorm what I could do with that information for programming projects and would like to know what you guys think. Thanks everyone. \n\nEdit one:\nThanks everyone for their feedback! For more information, what I'm thinking of creating is a racket based accelerometer. So basically, I'm shoving an accelerometer in say a baseball or tennis racquet and collecting data at a rate of say 60Hz. I'm going to import the data onto a computer where I am going to do some number crunching and so far it seems like that would be a hard task.     Some kind of inertial navigation system? accelerometer on a racket     [This is too hard for you probably, but I've always wanted to do it.](http://boingboing.net/2011/10/20/accelerometer-based-keylogger-in-your-phone-guesses-your-pc-keyboard-typing-from-your-bodys-motions.html) Sounds a lot like data collection and probability mapping to get accurate result. [This is too hard for you probably, but I've always wanted to do it.](http://boingboing.net/2011/10/20/accelerometer-based-keylogger-in-your-phone-guesses-your-pc-keyboard-typing-from-your-bodys-motions.html) Do you have any idea where the phone goes to measure your movements?  I think it's probably the case that the movements you make while typing are so small that if the phone were in your pocket it would be almost impossible to get beyond latent noise.  \n\nThis seems like a bad idea; people have had good results with modeling audio recordings of key strokes, however.   Well, as you should know, apple tries to have good hardware so there can be applications to suit the phone. IE, games. Like the maze and ball games on the iphone. The accelerometer on the iphone can pick up the noise through an acoustic amplifier, so essentially, on the desk. Possible, but you'd need a large amount of sampling of keystrokes. if your interested in gathering samples, i've seen this video that might be of some interest to you. [Machine Learning](http://hackaday.com/2012/05/03/machine-learning-lets-micro-decode-your-handwriting/)",
    "url" : "http://www.reddit.com/r/compsci/comments/tjyx6/ideas_for_a_data_set/"
  }, {
    "id" : 44,
    "title" : "Find the set of all integer points within a subregion of an annulus. (xpos from /r/math)",
    "snippet" : "  The angle of the point is atan(y/x), and the distance is sqrt(x^2 + y^2 ).  So you need to find the set of all lattice points such that:\n\n    alpha &lt;= atan(y/x) &lt;= beta  \n    r1 &lt;= sqrt(x^2 + y^2) &lt;= r2\n\nor\n\n    tan(alpha) &lt;= y/x &lt;= tan(beta)  \n    r1^2 &lt;= x^2 + y^2 &lt;= r2^2\n\nTo make it work with an arbitrary center, just shift the x and y coordinates before you put them in the formulas. This solution has a lot of holes though.  What if, for example, beta is in quadrant 2 of the graph?  Then tan(beta) is negative and the system of inequalities fails.  Or what if x is 0?  Then y/x is undefined.\n\nNaively, you could deal with each quadrant specifically (i.e. if x &gt; 0 &amp;&amp; y &gt;= 0).  But, my gut tells me there's a less complicated way to do it. The angle of the point is atan(y/x), and the distance is sqrt(x^2 + y^2 ).  So you need to find the set of all lattice points such that:\n\n    alpha &lt;= atan(y/x) &lt;= beta  \n    r1 &lt;= sqrt(x^2 + y^2) &lt;= r2\n\nor\n\n    tan(alpha) &lt;= y/x &lt;= tan(beta)  \n    r1^2 &lt;= x^2 + y^2 &lt;= r2^2\n\nTo make it work with an arbitrary center, just shift the x and y coordinates before you put them in the formulas. Yup! That's a great way to state the problem. I'm not sure how to use that to write an algorithm to solve it. I'll have to go look back at constraint based programming stuff. The quickest way I can think of is to find a bounding box and then test every point in the box explicitly.  You won't have to use the relatively expensive atan() or sqrt() operations for each point if you use the 2nd set of inequalities I gave, so it shouldn't take too long to run through them all. That's currently the way I'm doing it. It's definitely not bad, but not completely satisfying. I considered if I establish a bounds before hand, I can create a hash table of all of the locations surrounding (0,0) with their associated distances and angles. Then I can just pull out the parts of the table that are associated with the bounds. The only issues are the setup cost, the algorithm is difficult to read, you can't dynamically change the radii beyond the size of the table, and I'm bad at hash functions so it's going to take my a long time to write compared to my working solution. I posted a relatively efficient Python (yes, that's an oxymoron) solution [here](http://www.reddit.com/r/compsci/comments/tim0d/find_the_set_of_all_integer_points_within_a/c4mystz).  Your picture appears to be messed up, though, since the vertical axis doesn't have integer scaling. You're right, I noticed that after posting it. I manually drew on the dots thinking it would illustrate the point better. Obviously those are not integer points!\n\nYour solution is almost the exact same as mine. Unfortunately when I'm trying them, but of ours seem to produce good answers until the angle leaves a certain bounds. I'm sure this has to do with the trig calculations, but I'm not sure where to fix it yet. That's currently the way I'm doing it. It's definitely not bad, but not completely satisfying. I considered if I establish a bounds before hand, I can create a hash table of all of the locations surrounding (0,0) with their associated distances and angles. Then I can just pull out the parts of the table that are associated with the bounds. The only issues are the setup cost, the algorithm is difficult to read, you can't dynamically change the radii beyond the size of the table, and I'm bad at hash functions so it's going to take my a long time to write compared to my working solution. Whenever you attempt iterating over a sorted array, if you can assert that [f(x) &amp;&amp; f(z) -&gt; f(y)] where x &lt; y &lt; z and '&lt;' is the ordering operator, consider binary-search or range-search-like optimizations.  \n\nFor example, with your second inequality, when you're in a nested loop for x and y, you don't need to iterate over every value. For a certain x, you'll need the smallest and the largest y values that will work: and you can assume that the range of numbers between them will.      int r1sq = r1 * r1;\n    int r2sq = r2 * r2;\n    for (int x = -r2; x &lt;= r2; x++)\n    {\n        for (int y = -r2; y &lt;= r2; y++)\n        {\n            int distSq = x * x + y * y;\n            double angle = atan2(y,x);\n            if (distSq &gt; r1sq &amp;&amp; distSq &lt; r2sq &amp;&amp; angle &gt; alpha &amp;&amp; angle &lt; beta)\n            {\n                printf(\"(%d, %d) \",x,y);\n            }\n        }\n    } Did you mean:\n    \n    if (distSq &gt; r1sq &amp;&amp; distSq &lt; r2sq &amp;&amp; ... \n\n?  This subreddit makes me feel like such an ignoramus.  Does this stuff just come naturally to you guys? No, they've taken a lot of math classes.  I'm just happy I can read it and understand what's going on! You know, math is something I've always been terrible at, and yet it looks so fun.  One day I will understand those terrifying formulas on Wikipedia. You know, math is something I've always been terrible at, and yet it looks so fun.  One day I will understand those terrifying formulas on Wikipedia. This subreddit makes me feel like such an ignoramus.  Does this stuff just come naturally to you guys? Some problems come naturally, some I struggle with incredibly just because the underlying concepts were never that intuitive to me. Trig has always been something I've had poor intuition for, but sometimes knowing that, you can end up finding really cool solutions. I feel like I should know advanced math for programming, but it honestly doesn't come up that often.  Though I don't work in advanced algorithms or anything like that. I think it depends on the sort of programming you get yourself involved with. I do a lot of image processing and end up getting into some fairly advanced math, but I rarely use diff-eq so I get weaker and weaker in that all the time. I don't even really use trig much because the only functions that require it are simple rotations. If you want to get involved in more advanced math, you'll have to take on projects which aren't your typical programming project (which in my experience are data management, GUI usability and maybe systems integration). Funny, I never thought of data management as being math-driven.  I would have guessed 3D game programming or similar instead.  there s probably an efficient algorithm that makes use of Pick s theorem:\n\nhttp://en.wikipedia.org/wiki/Pick%27s_theorem\n\nYou can triangulate this region, or use your favorite convex polygon, to use the formula given in the article. I ll probably provide a solution involving pick's theorem at some point. this is a good prob.  Python solution:\n\n    def func(alpha, beta, x, y, r1, r2):\n\timport math\n\t'''Constraints:\n\t   0 &lt;= alpha &lt; beta &lt; pi/2\n\t   r2 &gt;= r1 &gt; 0'''\n\tpoints = []\n\tr12 = r1**2\n\tr22 = r2**2\n\ttana = math.tan(alpha)\n\ttanb = math.tan(beta)\n\tfor xi in range(0, r2+1):\n\t\tfor yi in range(0, r2+1):\n\t\t\tif xi != 0 and tana &lt;= float(yi)/xi and float(yi)/xi &lt;= tanb and\\\n\t\t\t   r12 &lt;= xi**2 + yi**2 and xi**2 + yi**2 &lt;= r22:\n\t\t\t\tpoints.append((xi+x, yi+y))\n\treturn points\n\nThe problem in the picture looks like:\n\n    func(math.atan(1.07775/3.5), math.atan(5.75/.5), 0, 0, 4, 10)\n\nwhich gives the following set of points:\n\n    [(1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (5, 7), (5, 8), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6), (6, 7), (6, 8), (7, 3), (7, 4), (7, 5), (7, 6), (7, 7), (8, 3), (8, 4), (8, 5), (8, 6), (9, 3), (9, 4)]\n\nI'm not quite sure what's going on in that picture, since it appears that the axes aren't scaled the same.  You can see that it's centered at (0, 0) by the lines, but the inner circle goes 4 grid points over on the x and about 5.6 on the y axis.  It looks like the vertical axis is missing its integer scaling. One-liner:\n\n\t[(xi+x, yi+y) for yi in xrange(0, r2+1) for xi in xrange(0, r2+1) if xi != 0 and math.tan(alpha) &lt;= float(yi)/xi and float(yi)/xi &lt;= math.tan(beta) and r1**2 &lt;= xi**2 + yi**2 and xi**2 + yi**2 &lt;= r2**2] First, you're calculating tan(alpha) and tan(beta) every time, which is a relatively expensive operation.  Second, if I had to maintain code like that, I would hunt down the person who wrote it and beat him to death with his own optic nerve.   That image has a nice optical illusion, being slanted to the right a little.  Okay, here's the most intuitive and clear way I could think of. The solution is not necessarily elegant, and I'm not sure of the efficiency, but if I were writing it in software and I didn't need to squeeze every last drop of performance out, this would be how I do it. The code I write out is aiming for reasonable clarity, not minute fine-tuned efficiency, and I leave the trig details and so on up to you.\n\nTo simplify, let's say the center is always (0, 0), and we're only working in the first quadrant. It's not too hard to extend it with an arbitrary center, and you'll have to iterate slightly differently for the different quadrants.\n\nAssuming an angle of 0 is positive x-axis, and 90 (or pi/2) is the positive y-axis. So in that diagram, alpha represents the right edge and beta represents the left edge.\n\nThe way I'm going to approach it is to actually sweep out the integers, moving top to bottom, left to right. Compute the hard corner points, which are the combinations of r1 and r2 with alpha and beta in polar coordinates (but convert them to cartesian coordinates).\n\nNow imagine that the y coordinates of those corner points form horizontal lines which slice the sliced annular subregion into 3 sections (with 4 bounds):\n\n    yTop = PolarToCartesian (r2, beta).y\n    yBottom = PolarToCartesian (r1, alpha).y\n    yUpperMid = PolarToCartesian (r1, beta).y\n    yLowerMid = PolarToCartesian (r2, alpha).y\n    \n    if (yLowerMid &gt; yUpperMid)\n        Swap (yLowerMid, yUpperMid)\n        swap = 1\n\nSo, we now have three slices.\n\nFor the top slice, we loop over each row from the floor of yTop to the ceiling of yUpperMid. For each row, compute the left bound and the right bound, and loop for every column between the ceiling of the left bound and floor of the right bound.\n\nIn the range from yTop to yUpperMid, the left bound is going to be the beta radial line, and the right bound is going to be the outer circle.\n\n    for (y = floor (yTop); y &gt;= ceiling (yUpperMid); y--) {\n    \tleftBorder = Intersection (y, betaLine).x\n    \trightBorder = Intersection (y, outerCircle).x\n    \tfor (x = ceiling (leftBorder); x &lt;= floor (rightBorder); x++)\n    \t\tsolution.add (x, y)\n    }\n\n\nThen you can apply the same method to the sections from yUpperMid to yLowerMid, and yLowerMid to yBottom. For the former section, if we did not swap, then the left border will be the inner circle and the right border the outer circle. If we did swap, the left border will be the beta line and the right border will be the alpha line. For the latter section, the left border will be the inner circle and the right border will be the alpha line.\n\nAs I said, there are much more elegant solutions, but this one is the most obvious to me. It might be the same as BurritoTime's solution, but longer ;).   ",
    "url" : "http://i.imgur.com/LuBzW.png"
  }, {
    "id" : 45,
    "title" : "Really cool concept - recursive drawing",
    "snippet" : "  This is really cool. I could see myself wasting hours with this. You can actually test it online you know:\n\nhttp://recursivedrawing.com/draw.html Umm... yea. Thanks. I posted this comment right as I was loading up the page, then I played around with it for hours. Did you think I was some kind of MORAN? I don't think that is the word you're looking for, [moron](http://dictionary.reference.com/browse/moron). You just can't take a joke, can you?  ",
    "url" : "http://recursivedrawing.com/"
  }, {
    "id" : 46,
    "title" : "Editing text is the opposite of handling exceptions",
    "snippet" : "  As a response to the author's final question, the work by Benjamin Pierce's group on Lenses similarly applies category theory to edits. They generalize the approach to not only multiple concurrent editors on the same document, but also multiple concurrent editors on different documents that have some mapping between them.\n\nIt's great stuff and accessible to anyone who was capable of reading through the linked post. Very interesting, thanks.\n\nThere seem to be quite a few papers with “Lenses” in the title in [Benjamin Pierce’s publication list](http://www.cis.upenn.edu/~bcpierce/papers/index.shtml). Any suggestions for a good place to start? The journal paper (http://www.cis.upenn.edu/~bcpierce/papers/lenses-toplas-final.pdf ) is probably the best place to start for an overview of their work and more comprehensive pointers into the rest of the literature in this space. I have to admit I've only quickly read the papers as it's outside of my sub-sub-area (compiler implementation &amp; static analysis), but I've been to several of the lens talks and chatted with Benjamin about it before, which is what triggered my connection to this link.   Wow, this is not my usual corner of compsci. I think I'll back out slowly now.   The classic merge diagram doesn't fulfill the universality requirements of pushouts. What do you mean by the classic merge diagram? The merges I’m talking about certainly do have the universal property of a pushout, because they’re defined as pushouts.\n\nThe universal property means, as you obviously know, that any resolution of the two conflicting edits can be obtained by a unique edit from the pushout; so in a sense the pushout is the simplest or least destructive possible resolution of the conflict.\n\nI’d love to know what you mean, if you’re able to elaborate. OK, it looks like I've misunderstood how you've defined merge in the post, and your proposed representation does fulfill universality.\n\nThe representation is reminiscent of the \"weave\" (which SCCS and BitKeeper used), which stored all revisions of a document in a single file. They are not the same, but they both seem to have the property where many operations take time order on the size of the history, rather than the size of the document.  I was just reading about operational transform the other day and made the connection between git's approach, mercurial's approach, and what OT has to accomplish.\n\nI really enjoyed this, but I definitely got lost at the end.  It looks like I'll finally have to learn Haskell. If you want to learn category theory, I recommend just learning category theory rather than learning Haskell to learn category theory. You'll learn more category theory my way :-) Does it make sense to learn category theory without at least knowing enough fields it could apply to? Or putting it the other way around, what are the minimum requirements?\n\nI've been doing some very casual reading on category theory for years. It's quite easy to follow the definitions and some of the results, but it seems hard to see the magnitude of how these results apply.\n(I keep reading about the importance of Yoneda's lemma and how some people take years to truly understand it...) If you want to learn category theory, I recommend just learning category theory rather than learning Haskell to learn category theory. You'll learn more category theory my way :-) Happen to know of a good starting point? [Yep](http://www.youtube.com/user/TheCatsters) :-) That's awesome, but I'm not sure where to start. The videos don't appear to have an order other than within a group. I started watching Monads 1 and was immediately lost. So I tried watching Natural Transformations 1 and still got lost by the word \"morphism\" and had to pause the video to look it up on wikipedia. Any idea where I might start that's slightly more basic? My experience is in: computer software, discrete math proofs, calculus, so I do know a little bit of the annotation. Bah! How annoying. It looks like most of the subjects can be tackled in any order, but you'll want something more basic to get started. The [notes from Eugenia Cheng's Sheffield lecture course](http://cheng.staff.shef.ac.uk/autumn07/) look pretty good - I learned much of what I know from the terser Cambridge notes also linked off that page.",
    "url" : "http://bosker.wordpress.com/2012/05/10/on-editing-text/"
  }, {
    "id" : 47,
    "title" : "Simplest explanation of P vs NP I've come across",
    "snippet" : "   Sucks that it's wrong, then. By his definition, UNSATISFIABLE is NP-Hard, though that's conjectured to be false (otherwise NP=CoNP and PH collapses). Would you be willing to explain this a little better or have a link/book that would help explain? I just think it is funny the \"and- comment\" is the top comment, and yet there is no explanation after 11 hours.  Sucks that it's wrong, then. By his definition, UNSATISFIABLE is NP-Hard, though that's conjectured to be false (otherwise NP=CoNP and PH collapses). I'm intrigued. From my complexity classes I remember the definition of HP-Hard being problems that all NP problems reduce to. Are you saying this is wrong? Also, a conjecture is just that, a conjecture - no proof exists. No, that's totally correct. But saying A reduces (in polynomial time) to B doesn't mean \"given an oracle for B, I can find a polynomial time solution for A\", but rather \"I can, in polynomial time, transform any instance of A into an instance of B with the exact same solution\".  \n  \nThe subtle difference here lies that, effectively, you're *only* able to use the oracle for B at the last step, meaning you can't negate its value. One reason that this distinction is important (i.e. that nondeterministic machines can't flip their answer) is because it allows you to claim \"NP is the class of problems with fast verification procedures\". While it's clear that an oracle for UNSAT can be used to generate a polytime solution for SAT (and thus any NP problem), it's *not* clear that there's a quick way to verify that a formula is unsatisfiable (and, in fact, it's conjectured that there isn't one). So, what you're calling wrong is the fact that he doesn't mention that the oracle must run in polytime? I'm calling wrong the fact that he's saying something is NP-Hard if it can be used to solve any NP instance, as opposed to the (correct) definition in which something is NP hard if all other NP problems can be polytime Karp-reduced to it.  \n  \nThe difference is in what you're allowed to do with the oracle: in the first case, usage is completely arbitrary. In the second, you can only use the oracle as the *absolute last step* of your computation. It's thought that UNSAT is in the first class but not the second exactly because using it to solve SAT the naive way would require us to flip the answer an UNSAT oracle would give, which we aren't allowed to do in Karp reductions. No, that's totally correct. But saying A reduces (in polynomial time) to B doesn't mean \"given an oracle for B, I can find a polynomial time solution for A\", but rather \"I can, in polynomial time, transform any instance of A into an instance of B with the exact same solution\".  \n  \nThe subtle difference here lies that, effectively, you're *only* able to use the oracle for B at the last step, meaning you can't negate its value. One reason that this distinction is important (i.e. that nondeterministic machines can't flip their answer) is because it allows you to claim \"NP is the class of problems with fast verification procedures\". While it's clear that an oracle for UNSAT can be used to generate a polytime solution for SAT (and thus any NP problem), it's *not* clear that there's a quick way to verify that a formula is unsatisfiable (and, in fact, it's conjectured that there isn't one). I'm intrigued. From my complexity classes I remember the definition of HP-Hard being problems that all NP problems reduce to. Are you saying this is wrong? Also, a conjecture is just that, a conjecture - no proof exists. See the Euler diagram on [the Wikipedia page for NP-hard](http://en.wikipedia.org/wiki/Np-hard).\n\nQuoting the same page: \"NP-hard [...] is a class of problems that are, informally, \"at least as hard as the hardest problems in NP\".\"\n That isn't a Venn diagram. I'm intrigued. From my complexity classes I remember the definition of HP-Hard being problems that all NP problems reduce to. Are you saying this is wrong? Also, a conjecture is just that, a conjecture - no proof exists. I'm intrigued. From my complexity classes I remember the definition of HP-Hard being problems that all NP problems reduce to. Are you saying this is wrong? Also, a conjecture is just that, a conjecture - no proof exists. I could be wrong, but I think you're confusing NP-hard and NP-complete.\n\nThere is a polynomial-time reduction from all NP to any problem in NP-complete.\n\nNP-Hard is a problem that's at least as hard as any problem in NP - at least as hard as NP-complete.\n\nEDIT: Never mind - NP-complete problems can also be reduced to NP-hard. We're both correct (I think). I'm not. NP-Complete problems are NP-Hard by definition.  I still don't get it :( I'll try to explain it, since I clicked on the \"simplest explanation\", saw the wall of text, and my eyes glazed over.\n\nP is the set of all problems that can be solved in polynomial time, meaning that the time required to solve an instance of the problem is *O(n^c )*, where *n* is the size of the input and *c* is some constant.\n\nNP is the set of all problems where a given solution can be validated in polynomial time.  So for, say, the Traveling Salesman Problem, it is easy to verify that a given Hamiltonian cycle exists for a given graph with length *k*.  Clearly P is a subset of NP.\n\nAnother subset of NP, called NP-complete, consists of problems that can be reduced to each other in polynomial time.  That is, if *one* of the problems can be solved in polynomial time, then they *all* can.  Because no polynomial time algorithm exists for any of the thousands of known NP-complete problems, it is assumed that none exists.\n\nIf P=NP, then all those NP-complete problems can actually be solved in polynomial time.  Besides being an important theoretical result, there would be many real-world implications as well.  For instance, most common cryptography is based on the premise that integer factorization is not in P.  If P=NP, then that cryptography is proven to be insecure.\n\nIf you have any questions I'll be happy to answer.  That was the best I could do informally after a night at the bar :) Thanks I appreciate it :)\nWhat is your opinion? Can quantum Computers be able to  create that algorithm? Thanks I appreciate it :)\nWhat is your opinion? Can quantum Computers be able to  create that algorithm?  Does this mean that the \"P = NP\" question is in NP? If we can answer the \"P = NP\" question, then P = NP. Otherwise, P != NP.\n\nTrollmath. Does this mean that the \"P = NP\" question is in NP? If we can answer the \"P = NP\" question, then P = NP. Otherwise, P != NP.\n\nTrollmath. Does this mean that the \"P = NP\" question is in NP? If we can answer the \"P = NP\" question, then P = NP. Otherwise, P != NP.\n\nTrollmath.      You are the third person to post this in /r/compsci in three days. You are the third person to post this in /r/compsci in three days. Have any links to the other posts? I'm not seeing them. http://www.reddit.com/submit?url=http%3A%2F%2Falecbenzer.com%2Fblog%2Fp-vs-np%2F The other two are in /r/programming and /r/pfi, try again. Hmm...I checked the box specifying to restrict my search to /r/compsci. Odd. I think that only works for regular searches, not links. Link searches automatically show you all the postings of that link across subreddits. So it seems. Sorry.  ",
    "url" : "http://alecbenzer.com/blog/p-vs-np/"
  }, {
    "id" : 48,
    "title" : "A professor of mine, sick of students paying way too much for textbooks, has made an open source data structures book. Feel free to find errors, and suggest changes!",
    "snippet" : "  Wow, thanks for this! I'm excited to get started with it. Except you won't.  \n  \nAnd find yourself a real book. Not any of this java/c++ nonsense. What?     class MyReply {\n        public static void make_space() {\n            System.out.print(\"  \\n  \\n\"); /* Empty line for spacing that works well with all versions of markdown. */\n        }\n        public static void main(String[] args) {\n            System.out.print(\"If you're using code for anything other than programming you're doing it wrong.\"); \n            make_space();\n            System.out.print(\"Learning should be done without all of the cryptic syntax required by programming languages.\");\n            make_space();\n            System.out.print(\"English &gt; pseudocode &gt;&gt; code.\");\n        }\n    }     class MyReply {\n        public static void make_space() {\n            System.out.print(\"  \\n  \\n\"); /* Empty line for spacing that works well with all versions of markdown. */\n        }\n        public static void main(String[] args) {\n            System.out.print(\"If you're using code for anything other than programming you're doing it wrong.\"); \n            make_space();\n            System.out.print(\"Learning should be done without all of the cryptic syntax required by programming languages.\");\n            make_space();\n            System.out.print(\"English &gt; pseudocode &gt;&gt; code.\");\n        }\n    } \"cryptic syntax\"?\n\nIf your programming language is more verbose than plain English at expressing algorithms, you're probably doing something wrong. You want succinctness? Have fun reading garbage like  \n  \n     &gt;&gt;&gt; p = filter((lambda x: len(filter(lambda i: x % i == 0, range(2,1+int(x**.5)))) == 0), range(2,400))  \n  \nrather than\n  \n    Let p be the set of primes less than 400. Your statements are not equivalent.  In the first, you spend most of the line explaining how to generate primes, and with a fairly verbose lambda syntax.\n\nIn the second, you use a very terse syntax, and don't explain anything about generating primes.\n\nA more appropriate code translation would be:\n\n    p = Primes.TakeWhile(i =&gt; i &lt; 400); Which is still less pleasant than the former.  \n  \nThere's a reason almost no serious papers (eg. STOC/FOCS quality) have almost no pseudocode, and definitely no Java/C bullshit. Except you won't.  \n  \nAnd find yourself a real book. Not any of this java/c++ nonsense.  He should look into [OpenStax](http://openstaxcollege.org/).   I had a Game Theory class in undergrad where the professor brought in the book chapter by chapter, printed by himself. He also wore a different Hawaiian shirt everyday. Crazy math professors.   Out of curiosity, why isn't it just written in pseudocode instead of having different versions for c++/java?  Very, very cool. Thanks for showing this.    This is great.  Who is your professor?  I don't see any author's name on the website.    Great start. Free online lectures and text book. Looks good for students from developing countries         He is going to get sued...     \n\nNobody goes after big publishing's profits and gets away with it. They'll make up some nonsense lawsuit and then have a judge keep the book out of the public eye for tens of years until they win by default due to the other guy running out of money.   \n\n[Wouldn't be the first time either.](http://www.techdirt.com/articles/20120410/07284618438/open-textbook-startup-sued-allegedly-copying-distinctive-selection-arrangement-presentation-facts-existing-titles.shtml)  He is going to get sued...     \n\nNobody goes after big publishing's profits and gets away with it. They'll make up some nonsense lawsuit and then have a judge keep the book out of the public eye for tens of years until they win by default due to the other guy running out of money.   \n\n[Wouldn't be the first time either.](http://www.techdirt.com/articles/20120410/07284618438/open-textbook-startup-sued-allegedly-copying-distinctive-selection-arrangement-presentation-facts-existing-titles.shtml)  He is going to get sued...     \n\nNobody goes after big publishing's profits and gets away with it. They'll make up some nonsense lawsuit and then have a judge keep the book out of the public eye for tens of years until they win by default due to the other guy running out of money.   \n\n[Wouldn't be the first time either.](http://www.techdirt.com/articles/20120410/07284618438/open-textbook-startup-sued-allegedly-copying-distinctive-selection-arrangement-presentation-facts-existing-titles.shtml)  He is going to get sued...     \n\nNobody goes after big publishing's profits and gets away with it. They'll make up some nonsense lawsuit and then have a judge keep the book out of the public eye for tens of years until they win by default due to the other guy running out of money.   \n\n[Wouldn't be the first time either.](http://www.techdirt.com/articles/20120410/07284618438/open-textbook-startup-sued-allegedly-copying-distinctive-selection-arrangement-presentation-facts-existing-titles.shtml)   I think he's been rejected by publishers too many times and now he's taking matter in his own hands.\n\nIf he were publishing a lot of books, he wouldn't do this. &lt;/cynicism&gt; I think he's been rejected by publishers too many times and now he's taking matter in his own hands.\n\nIf he were publishing a lot of books, he wouldn't do this. &lt;/cynicism&gt;  I think the initiative is good, but \n\n&gt; sick of students paying way too much for textbooks\n\nReally? After a quick search, I found that the undergraduates [tuition fees](http://www1.carleton.ca/about/facts/) in the (Canadian) university of this professor are of $6000-$9000 a year, and the residence seems to cost approximately as much.\n\nIf your professor is \"sick\" of seeing people pay $60-$100 for a data structure textbook, what does he thinks of what they pay to attend the university? Mega-sick? Instant death? That's like saying, \"You paid $300K for your house, so you must *certainly* be able to pay $20K for an electrician to fix this wiring. After all, $20K is so little compared to $300K!\"\n\nThe comparison is just bad.\n\nThe point is that even though many (if not most) students can indeed afford the books in principle, they're simply overpriced for what they are.\n\nAlso, there are many students who really can't afford such textbooks. That's because they have merit and/or need-based scholarships that pay for tuition (and possibly housing) but *not* textbooks, and if you're taking 4-6 classes per term, each of which requires a $100 textbook, that's a lot of money per year. I think the initiative is good, but \n\n&gt; sick of students paying way too much for textbooks\n\nReally? After a quick search, I found that the undergraduates [tuition fees](http://www1.carleton.ca/about/facts/) in the (Canadian) university of this professor are of $6000-$9000 a year, and the residence seems to cost approximately as much.\n\nIf your professor is \"sick\" of seeing people pay $60-$100 for a data structure textbook, what does he thinks of what they pay to attend the university? Mega-sick? Instant death? Five books a semester at $1000 a year, or $4000 through 4 years of college.\n\nThat's insignificant to you? [deleted] I do. But often there are no used. Even older editions. And since library.nu is dead... finding it online is impossible. http://free-books.us.to/ That just takes me to google books. Do you think they'll have full textbooks there, or am I supposed to somehow make do with 32 pages of preview?\n\nSorry Professor, but you'll just have to give me an A on that because I don't have chapters 2 through 11, 13, or 17-22. Hmm, that's odd, here's what I get http://i.imgur.com/gQbKz.png\n\nI can only suppose that whereever you are is blocking it by redirecting to Google Books. Try a proxy. I tried it at home. I have Suddenlink cable. Same thing. Bounces to books.google.com. At work (a 4 year university) same thing. So what gives? One or the other I might believe, but both blocking it in the same manner? C'mon. Hmm, that's odd, here's what I get http://i.imgur.com/gQbKz.png\n\nI can only suppose that whereever you are is blocking it by redirecting to Google Books. Try a proxy. I think the initiative is good, but \n\n&gt; sick of students paying way too much for textbooks\n\nReally? After a quick search, I found that the undergraduates [tuition fees](http://www1.carleton.ca/about/facts/) in the (Canadian) university of this professor are of $6000-$9000 a year, and the residence seems to cost approximately as much.\n\nIf your professor is \"sick\" of seeing people pay $60-$100 for a data structure textbook, what does he thinks of what they pay to attend the university? Mega-sick? Instant death? I think the initiative is good, but \n\n&gt; sick of students paying way too much for textbooks\n\nReally? After a quick search, I found that the undergraduates [tuition fees](http://www1.carleton.ca/about/facts/) in the (Canadian) university of this professor are of $6000-$9000 a year, and the residence seems to cost approximately as much.\n\nIf your professor is \"sick\" of seeing people pay $60-$100 for a data structure textbook, what does he thinks of what they pay to attend the university? Mega-sick? Instant death?  Calling a book \"open source\" just because its source is available is retarded. Are we now going to call cakes \"open source\" if their recipe was found in some old cookbook ?  \n  \nJust call it a book. No one gives a shit about its source.  \n  \n-----------------------  \n**Edit**: Comment source:  \n  \nCalling a book \\\"open source\\\" just because its source is available is retarded. Are we now going to call cakes \\\"open source\\\" if their recipe was found in some old cookbook ?  \\n  \\n Just call it a book. No one gives a shit about its source.  Actually, I do. It was written in LaTeX. I give a shit about its source. If you were to tell me you examined the source of this book, you'd be lying. Calling a book \"open source\" just because its source is available is retarded. Are we now going to call cakes \"open source\" if their recipe was found in some old cookbook ?  \n  \nJust call it a book. No one gives a shit about its source.  \n  \n-----------------------  \n**Edit**: Comment source:  \n  \nCalling a book \\\"open source\\\" just because its source is available is retarded. Are we now going to call cakes \\\"open source\\\" if their recipe was found in some old cookbook ?  \\n  \\n Just call it a book. No one gives a shit about its source.  It's not called opened source for being available and free but because it's open sourced; The source is freely available (LaTex &amp; code sources) and more importantly, anyone can contribute (it's even on github) &gt; It's not called opened source for being available and free but because it's open sourced  \n  \nhttp://i.imgur.com/ErIup.png Calling a book \"open source\" just because its source is available is retarded. Are we now going to call cakes \"open source\" if their recipe was found in some old cookbook ?  \n  \nJust call it a book. No one gives a shit about its source.  \n  \n-----------------------  \n**Edit**: Comment source:  \n  \nCalling a book \\\"open source\\\" just because its source is available is retarded. Are we now going to call cakes \\\"open source\\\" if their recipe was found in some old cookbook ?  \\n  \\n Just call it a book. No one gives a shit about its source. ",
    "url" : "http://opendatastructures.org/"
  }, {
    "id" : 49,
    "title" : "Data Miners of r/CompSci what should I do for my final project?",
    "snippet" : "For our final project in my upper-level data mining class, we must collect our own data and use the techniques we have learned to try and learn something from it.\n\nI want to do something involving Reddit, what are your thoughts?\n\nPossible idea:\nLook into how different keywords in a username and analyze how that corresponds to upvotes and downvotes in comments.   What sort of scale? What are you interested in? Presentations are due on the 31st.\n\nI really want to do something that could help make Reddit a better place. Honestly, everything we have been doing in that class has fascinated me.\n\nThe current 1st place idea would be to augment Reddit search by analyzing the comments to infer more about the subject matter of the link. Presentations are due on the 31st.\n\nI really want to do something that could help make Reddit a better place. Honestly, everything we have been doing in that class has fascinated me.\n\nThe current 1st place idea would be to augment Reddit search by analyzing the comments to infer more about the subject matter of the link. Presentations are due on the 31st.\n\nI really want to do something that could help make Reddit a better place. Honestly, everything we have been doing in that class has fascinated me.\n\nThe current 1st place idea would be to augment Reddit search by analyzing the comments to infer more about the subject matter of the link.  I wish my school had a data mining class. I find it interesting although I've never studied it in depth. What textbook do you use for the class if you don't mind me asking? Is it any good? Probably going to do some self studying once I graduate. We have a book that we don't really use much, but my instructor's handouts are amazing.\n\nHe also posts them all online! Here's a link:\n[Notes](http://users.csc.calpoly.edu/~dekhtyar/466-Spring2012/)\n\nIt has all of the in-class handouts as well as labs an projects.",
    "url" : "http://www.reddit.com/r/compsci/comments/tfyvb/data_miners_of_rcompsci_what_should_i_do_for_my/"
  } ],
  "processing-time-source" : 72,
  "processing-result.title" : "compsci14_reddit.xml",
  "XmlDocumentSource.xml" : {
    "absolutePath" : "/Users/cole/code/cs467/r-topics/carrot2-cli-3.6.2/../data/compsci14_reddit.xml"
  }
}
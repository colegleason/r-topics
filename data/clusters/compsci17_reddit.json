{
  "processing-time-total" : 1837,
  "clusters" : [ {
    "id" : 0,
    "size" : 10,
    "score" : 61.82048482827894,
    "phrases" : [ "Programming Language" ],
    "documents" : [ 5, 8, 14, 15, 16, 18, 24, 34, 47, 49 ],
    "attributes" : {
      "score" : 61.82048482827894
    }
  }, {
    "id" : 1,
    "size" : 7,
    "score" : 73.87539177383762,
    "phrases" : [ "Mathematics for Computer Science" ],
    "documents" : [ 3, 7, 15, 18, 32, 44, 47 ],
    "attributes" : {
      "score" : 73.87539177383762
    }
  }, {
    "id" : 2,
    "size" : 6,
    "score" : 83.17634249657529,
    "phrases" : [ "Quick Sort" ],
    "documents" : [ 3, 15, 19, 37, 42, 49 ],
    "attributes" : {
      "score" : 83.17634249657529
    }
  }, {
    "id" : 3,
    "size" : 6,
    "score" : 48.11927500150175,
    "phrases" : [ "Time Complexity" ],
    "documents" : [ 3, 9, 18, 19, 27, 37 ],
    "attributes" : {
      "score" : 48.11927500150175
    }
  }, {
    "id" : 4,
    "size" : 5,
    "score" : 75.37733419909055,
    "phrases" : [ "Apply Functions to Sets of Functions" ],
    "documents" : [ 3, 11, 18, 37, 47 ],
    "attributes" : {
      "score" : 75.37733419909055
    }
  }, {
    "id" : 5,
    "size" : 5,
    "score" : 94.67715234358424,
    "phrases" : [ "Discrete Math" ],
    "documents" : [ 3, 15, 18, 44, 49 ],
    "attributes" : {
      "score" : 94.67715234358424
    }
  }, {
    "id" : 6,
    "size" : 5,
    "score" : 106.0747599144108,
    "phrases" : [ "School in CS" ],
    "documents" : [ 5, 8, 18, 47, 49 ],
    "attributes" : {
      "score" : 106.0747599144108
    }
  }, {
    "id" : 7,
    "size" : 5,
    "score" : 55.58728261997321,
    "phrases" : [ "Simulation Problem" ],
    "documents" : [ 2, 3, 10, 11, 47 ],
    "attributes" : {
      "score" : 55.58728261997321
    }
  }, {
    "id" : 8,
    "size" : 5,
    "score" : 103.00209352892759,
    "phrases" : [ "Text Book" ],
    "documents" : [ 4, 15, 32, 37, 39 ],
    "attributes" : {
      "score" : 103.00209352892759
    }
  }, {
    "id" : 9,
    "size" : 4,
    "score" : 119.77505451640141,
    "phrases" : [ "Randomly Generated" ],
    "documents" : [ 1, 9, 27, 47 ],
    "attributes" : {
      "score" : 119.77505451640141
    }
  }, {
    "id" : 10,
    "size" : 4,
    "score" : 59.73558192990474,
    "phrases" : [ "Trees" ],
    "documents" : [ 27, 28, 29, 31 ],
    "attributes" : {
      "score" : 59.73558192990474
    }
  }, {
    "id" : 11,
    "size" : 3,
    "score" : 66.73916667958795,
    "phrases" : [ "Conference" ],
    "documents" : [ 6, 13, 35 ],
    "attributes" : {
      "score" : 66.73916667958795
    }
  }, {
    "id" : 12,
    "size" : 3,
    "score" : 39.622348931306625,
    "phrases" : [ "Semantic" ],
    "documents" : [ 15, 37, 46 ],
    "attributes" : {
      "score" : 39.622348931306625
    }
  }, {
    "id" : 13,
    "size" : 3,
    "score" : 66.72810992765812,
    "phrases" : [ "Turing Machine" ],
    "documents" : [ 2, 10, 21 ],
    "attributes" : {
      "score" : 66.72810992765812
    }
  }, {
    "id" : 14,
    "size" : 3,
    "score" : 34.57085983607254,
    "phrases" : [ "Video" ],
    "documents" : [ 43, 44, 48 ],
    "attributes" : {
      "score" : 34.57085983607254
    }
  }, {
    "id" : 15,
    "size" : 2,
    "score" : 43.64819439143863,
    "phrases" : [ "Anybody" ],
    "documents" : [ 40, 48 ],
    "attributes" : {
      "score" : 43.64819439143863
    }
  }, {
    "id" : 16,
    "size" : 2,
    "score" : 39.978368910197915,
    "phrases" : [ "Blog Post" ],
    "documents" : [ 32, 49 ],
    "attributes" : {
      "score" : 39.978368910197915
    }
  }, {
    "id" : 17,
    "size" : 2,
    "score" : 125.51020685251879,
    "phrases" : [ "File Containing" ],
    "documents" : [ 26, 41 ],
    "attributes" : {
      "score" : 125.51020685251879
    }
  }, {
    "id" : 18,
    "size" : 2,
    "score" : 78.53810988003372,
    "phrases" : [ "Natural Language Processing" ],
    "documents" : [ 16, 18 ],
    "attributes" : {
      "score" : 78.53810988003372
    }
  }, {
    "id" : 19,
    "size" : 2,
    "score" : 65.45260685961685,
    "phrases" : [ "Quantum Computing" ],
    "documents" : [ 15, 36 ],
    "attributes" : {
      "score" : 65.45260685961685
    }
  }, {
    "id" : 20,
    "size" : 2,
    "score" : 72.36112330010751,
    "phrases" : [ "Rank Grade" ],
    "documents" : [ 0, 8 ],
    "attributes" : {
      "score" : 72.36112330010751
    }
  }, {
    "id" : 21,
    "size" : 10,
    "score" : 0.0,
    "phrases" : [ "Other Topics" ],
    "documents" : [ 12, 17, 20, 22, 23, 25, 30, 33, 38, 45 ],
    "attributes" : {
      "other-topics" : true,
      "score" : 0.0
    }
  } ],
  "processing-time-algorithm" : 1771,
  "documents" : [ {
    "id" : 0,
    "title" : "I'm doing a survey for computer science students or graduates, care to help me out?",
    "snippet" : "Hi everyone, here's a survey i need to do for a school project and i decided to do it about computer science students experience in the workplace and at school. Here's the link to the google doc form https://docs.google.com/spreadsheet/viewform?formkey=dE45MS0tTEs0X1V0QVBGQmxXUmFCS1E6MQ any help is appreciated. Thanks   You should probably discretize some of your data (e.g., male/female) and fix up your units. Do you mean hours/week? Things like that.\n\nAlso:\n\nDid you ever were on an internship during college? -&gt; Did you ever have an internship during college?  Done. My grades are from a UK university, so if you need any advice in percentages for them just respond to this and I'll do my best to help out Thanks for answering the survey, yeah i kinda do need help interpreting that grade percentage, how would you write them if 100 is the maximum grade? Thanks for answering the survey, yeah i kinda do need help interpreting that grade percentage, how would you write them if 100 is the maximum grade? Australia has two ranking systems that I am aware of (actual percentages may differ):\n\n* High Distinction: 85% or higher - rank/grade = 4 or 7\n* Distinction: 75%-84% - rank/grade = 3 or 6\n* Credit: 65%-74% - rank/grade = 2 or 5\n* Pass: 50%-64% - rank/grade = 1 or 4\n* Conceded pass: 45%-49% - rank/grade = N/A or 3\n* Fail: 35%-44% - rank/grade = 0 or 2\n* Major fail: lower than 34% - rank/grade = 0 or 1\n* Excluded: Didn't do enough work to pass: - rank/grade = 0 or x\n School sounds easy there, though I suppose the professor can make it difficult enough for that to be a reasonable grading scale. I don't know about Australia, but in the UK they tend to not use grading based off a curve. So that wouldn't be the top 15% getting a high distinction, it would just be anyone getting over 85% of the marks.\n\nIMO its a lot fairer to students than they use in the US, but i guess it probably leads to more people getting higher grades",
    "url" : "http://www.reddit.com/r/compsci/comments/ry5ps/im_doing_a_survey_for_computer_science_students/"
  }, {
    "id" : 1,
    "title" : "In genetic algorithms, what are the effects of using different types of gene mutations?",
    "snippet" : "I'm tinkering around with implementing GA's, and am trying to figure out how using different methods of mutating genes will effect the final outcome.\n\nSo, assuming I'm using int genes, what's the difference between, say, flipping a random bit in each mutation, replacing each gene with a totally random number, and adding a random number to the existing value of the gene?\n\nAnyone have experience here?   The differences are logical if you understand how GA's work.\nIf you flip one byte of a well performing individual, it's probably going to perform quite good afterwards, if you replace it with a new random number, the chances are that it will be crap.\n\nMore general: The general way to do things is:\n\n* Rate you individuals.\n* Keep the best 1% or something.\n* Let the 50% best have children, that is combine them in some way, eg. crossover.\n* Scrap the worst individuals, making room for new ones.\n* Possibly generate some totally random individuals to keep you gene pool fresh and diverse.\n\nThe thing is to think of this as evolution, how can i make my \"animals\" evolve as quickly as possible towards what I want?\nRule of thumb is that all changes to your gene pool etc. should be stochastic, when doing mutation walk through all the bits and with a small chance, flip each one. When doing crossover, walk over the pairs of bits, and with a small chance, swap them.\nAll the numbers in this answer are guesses and may change from implementation to implementation.\nAll the different techniques, crossover mutation etc. aren't needed, you can do fine with just totally random mutation also, but then your solution will converge a lot slower. If you see that some specific technique will make you solutions converge much quicker, then do that ;)  ",
    "url" : "http://www.reddit.com/r/compsci/comments/rxm6u/in_genetic_algorithms_what_are_the_effects_of/"
  }, {
    "id" : 2,
    "title" : "Simulating Turing Machines with Wang Tiles",
    "snippet" : "  This is a very cool article with a polished presentation. I wonder about hexagonal tiles or cubes...  Very cool article! There's only one thing I didn't understand -- the section on guaranteeing that there's only one head to the Turing machine. I'm going to draw my squares rotated by pi/4. So we have this kind of thing for the initial alphabet:\n\n    X *\n    * 0\n\nAnd this kind of thing for the head:\n\n    X  v\n    ^ A,0\n\n    X *\n    v 0\n\n    X ^\n    * 0\n\nSo that we can make a tile sequence like this:\n\n              X *\n              v 0\n        X  v\n        ^ A,0\n    X ^\n    * 0\n\nBut what's to stop us from duplicating this sequence, now, like this?\n\n                            X *\n                            v 0\n                      X  v\n                      ^ A,0\n                  X ^\n                  * 0\n              X *\n              v 0\n        X  v\n        ^ A,0\n    X ^\n    * 0\n\nedit: Okay, the paper sets things up a little differently. Anyway, I think one fix is to get rid of the * color entirely and just propogate \\^ and v colors all along the first column. So instead of this tile:\n\n    X *\n    v 0\n\nwe have this one:\n\n    X v\n    v 0\n\n(and similarly replace the * in the other one with a \\^). Interesting! You're absolutely right. In the original construction, they were looking at a version of the problem that constrained things to a one-way tape, and the 3-tile construction was my attempt to extend that to the bidirectional tape -- but in doing so it seems I haven't quite fixed the problem of having multiple heads.\n\nI guess the way to fix that would be to have two flavors of initial blank: one that goes above the tape head, and one that goes below. So to use your notation:\n\n                           X v\n                           v 0\n                       X v\n                       v 0\n                  X  v\n                  ^ A,0\n              X ^\n              ^ 0\n          X ^\n          ^ 0\n\nI *think* this would ensure that we only get one head in the first column. Nice catch!",
    "url" : "http://moyix.wordpress.com/2012/04/06/computing-with-tiles/"
  }, {
    "id" : 3,
    "title" : "Spaceship Navigation Problem",
    "snippet" : "Hey all! This is my first post in this subreddit so apologies if I do it wrong.\n\nI've been mulling over a problem in my head for a while now and I could use some advice on directions to go to solve it. I call it the **Spaceship Navigation Problem**.\n\nAssume you have a **2D space** with **N gravitational bodies**. The mass of each body is directly proportional to the diameter (meaning **all their densities are the same**), and the **objects are all perfect circles**. Collisions can be handled in any of the many typical methods used in N-Body simulations. I'll be \"fudging\" the gravity calculation at short distances to prevent singularities or near infinite forces being exerted on objects and simply allowing them to pass through one another. This part I have written and tested and it works great.\n\nHere comes the problem:\n\nPretend you have a spaceship starting at an arbitrary point, A, in the space. The ship has a certain ability to **exert a discrete amount of force at any time and in any direction summing to a certain net force** (think of it as fuel). It is attempting to **reach another arbitrary point**, B, in the space **without colliding** with any of the objects. \n\nSo given,\n\n* a set of N gravitational bodies\n* a start point, A\n* an end point, B\n* a total amount of force (or \"fuel\") which can be exerted\n\nfind the **set of decisions** (each comprised of a** time and a force**), S, such that the ship reaches point B in **minimal time** (and for bonus, with **minimal \"fuel\"** usage given a weight on both constraints) without colliding with any objects.\n\nI've tried simplifying the problem by removing the collisions part and allowing the ship to exert a force at only one time in the flight (making the solution a pair of values), but it still seems impossibly hard to compute in any reasonable amount of time.\n\nI'm currently trying a subset of the problem in which you pick a direction at time 0 in which a constant force is exerted and decide the direction which will allow you to reach your destination in minimal time.\n\nAny ideas on directions I can look to make this problem more workable other than just eliminating large chunks of the problem?\n\n**TL;DR** Find the set of discrete force exertions (time and intensity) on an object travelling through an N-Body gravitational system in motion from point A to point B which sum to less than a net force s.t. the object does not collide with any of the bodies and time and total force exerted are minimal.  honestly, with an infinite amount of solutions to try, why not set up a genetic algorithm.  have \"bots\" with random encodings of discrete force exertions (time, intensity, and direction), track how close each bot comes to the target, add fuel consumed and time taken, and weed out the algorithms that are the worst.  Take the best, make slight random modifications to those, and keep going until you get a reasonable solution. That's a really fun idea! Although I've only used generic algorithms in fairly simple problems... What wood the chromosomes actually represent? \n\nI've also considered adding some sort of neural net, but I'm not sure what the features would be. If you just did the inputs and outputs I don't see it picking up advanced techniques like slingshotting.\n\nWorth playing with though! Any suggestions about the generic approach would be great!  Picking up on the other comments I made.  The most basic genetic algorithm is a series of actions organized like this  Action of ( Time, thrust, vector).  Using a genetic algorithm, you can keep selecting move evolved lists until you get what you want FOR ONE list of gravity objects.  Move the stars around and the list is useless.  \n\n\nA more generic algorithm that will work reasonably well with any arrangement of gravity bodies is to create an algorithm that responds to input.  It has to know at any time, where the target is, the velocity vector what the local acceleration due to gravity is, and where and how far the nearest masses are.  You would have to put together a number of simple rules with variables that could still be tweaked by a genetic algorithm.  I don't know when it makes sense to fight against the gravity well of a star, and when it makes more sense to fall into that gravity well, and then accelerate to slingshot out, but a series of rules could be put in play,  (when gravity &gt;x and distance to star &lt;Y, then accelerate Z  (towards/away/ perpendicular/other/)  (OH, make that last bit  \"star vector) + vector V)    \n\n\nconsider rules like this.  When Star Vector - Acceleration due to gravity  Vector   &gt;  THETA degrees, then thrust vector = (...)\n\n\n\nThe smarter algorithms will figure out ways to stick to the Lagrange tubes between gravity wells,  and the really smart ones will learn to set up sling shots. honestly, with an infinite amount of solutions to try, why not set up a genetic algorithm.  have \"bots\" with random encodings of discrete force exertions (time, intensity, and direction), track how close each bot comes to the target, add fuel consumed and time taken, and weed out the algorithms that are the worst.  Take the best, make slight random modifications to those, and keep going until you get a reasonable solution.  This is really two problems in one: the simulation problem and the optimization problem.  The simulation problem is pretty straightforward, you just need to write a function that takes the engine firings as an input and outputs the closest distance you get to the target point.  Later you can add in time as an error variable.\n\nAt that point there are any number of optimization schemes which would be able to solve the problem (minimize the error in the simulation function).  Gradient approaches would probably get confused by the planet collisions (causing discontinuities in the error when the spaceship hits a planet).  So you might try metaheuristics like the particle swarm algorithm\n\n Agreed. I've solved the simulation component to my satisfaction, but the optimization is very difficult compared to the sorts of optimization problems I'm used to. It's a chaotic system so even small charges in initial conditions cause huge changes in results. How does one optimize when dealing with such a system reliably? (maybe that's the real problem! ) If you could compute derivaties over all parameters, it will help much. \n\nGenerally you should explore \"global optimization\" topic. I personally like approaches like genetic algorithms, or interval arithmetic methods, but for this particular problem probably even more will be needed to solve it.\n\nNormally, because planets are distant from each outher, trayectory can be planed in two phases: choicing sequence of planets to visit, optimizting detailed parameters of this visits. So to have global solution you basically need to have two solvers, one embeded in second, first is discrete one and checks multiple possible sequences, and second one is roubust continus solver. Leter one will also take into acount things like when too start (because planets are in constant motion, they relative positions changes).\n\nYou should also learn a manuver called Fly-By it is often used by spacecrafts.\n\nIf you cannot separate clearly planets from each other, then it starts to be extreamally hard.\n\nThere is actuall chalanges like this run by ESA and NASA.\n\nedit: typo\n This us extremely helpful! Thank you! I don't know why I assumed that just because I felt like I had a clear definition of the problem that it would be clearly solvable, but it's been wonderful to find out what a rich area of study this is! \n\nI finished making a Web-based N-body simulator in 2d which allows someone to input sequences of thruster fires (after having given them the initial parameters) and or manually control the ship in an attempt to get within X units of a point. \n\nI wonder about if there's a flaw in using an fixed coordinate system, but it seems practical. I'm using Newton's equations for gravity and Euler's method as efficiently as I could think to do it. I know Runge Kutta can be much faster, but I'm using 10-100 bodies and it's running fine. I'd love to research continuous solutions! \n\nI may post it up to get some criticisms if I convince myself people will want to see it. It'd be fun to see people's solutions! \n\nDo you have any references you think I'd enjoy looking at other than those already linked? Especially to those ESA/NASA events you spoke of!  I personally doesn't like Euler's method. Use at least Heun method, it is simple 2 step and 2nd order, which is VERY big difference compared to 1st order Euler. RK4 is standard in many simulations, but I think pointless (If you want real good ODE solver, then you will actually need something much more robust than RK4, including adaptative timestep, and this is time consumming to implement, so stick to Heun's method).\n\nUsing Eulers method you will easly create instabilities, and basically Energy and momentum and angular momentum will not be conserved, as well eliptical orbits will start changing positions without physical reason. Often this will throw planets from the whole system. You can solve it bay using Very small timestep, but then you introduce truncation errors and you will have performance problems. 2nd order methods like Heun or midpoint will solve both problems.\n\nhttp://www.esa.int/gsp/ACT/mad/op/GTOC/index.htm\n\n(last results http://www.esa.int/gsp/ACT/mad/op/GTOC/GTOC5/results.htm )\n\nthere is also group at JPL named \"Outer Planets Mission Analysis Group\", as well people in many other countries, some not even interested in space exploration per se, but just optimization. Russian, Italy, Spain, France, UK, USA, Japan,  China, Australia to name few. I was just reading some articles many of this groups published, and it struck how complicated problem is.\n\nsearch also for Spacecraft Trajectory Optimization By Bruce Conway\n\nSome other quickly searched resources:\n\n  * http://trs-new.jpl.nasa.gov/dspace/bitstream/2014/39674/1/05-2434.pdf\n  * http://users.ictp.it/~chelaf/LAPLACE.pdf\n  * http://www.technion.ac.il/~dssl/papers/Gurfil-59.pdf\n  * http://www.esa.int/gsp/ACT/doc/MAD/ACT-PRE-MAD-GTOC1-TUR.pdf\n  * http://130.203.133.150/showciting?cid=7899704\n  * http://130.203.133.150/viewdoc/summary?doi=10.1.1.51.8663\n\n\nIt is complex task, your is probably even more complex than this in real world despite being 2D (and real ones are still researched and refined, and improved). But it is really interesting to try, because it should be quite easly possible to create algorithm which at least find a sensible trajectory and then imrpvoves it. It is very interesting application of multiple methods.\n\nSome more general optimization chalanges: \nhttp://staff.ustc.edu.cn/~ketang/cec2012/lsgo_competition.htm\n I personally doesn't like Euler's method. Use at least Heun method, it is simple 2 step and 2nd order, which is VERY big difference compared to 1st order Euler. RK4 is standard in many simulations, but I think pointless (If you want real good ODE solver, then you will actually need something much more robust than RK4, including adaptative timestep, and this is time consumming to implement, so stick to Heun's method).\n\nUsing Eulers method you will easly create instabilities, and basically Energy and momentum and angular momentum will not be conserved, as well eliptical orbits will start changing positions without physical reason. Often this will throw planets from the whole system. You can solve it bay using Very small timestep, but then you introduce truncation errors and you will have performance problems. 2nd order methods like Heun or midpoint will solve both problems.\n\nhttp://www.esa.int/gsp/ACT/mad/op/GTOC/index.htm\n\n(last results http://www.esa.int/gsp/ACT/mad/op/GTOC/GTOC5/results.htm )\n\nthere is also group at JPL named \"Outer Planets Mission Analysis Group\", as well people in many other countries, some not even interested in space exploration per se, but just optimization. Russian, Italy, Spain, France, UK, USA, Japan,  China, Australia to name few. I was just reading some articles many of this groups published, and it struck how complicated problem is.\n\nsearch also for Spacecraft Trajectory Optimization By Bruce Conway\n\nSome other quickly searched resources:\n\n  * http://trs-new.jpl.nasa.gov/dspace/bitstream/2014/39674/1/05-2434.pdf\n  * http://users.ictp.it/~chelaf/LAPLACE.pdf\n  * http://www.technion.ac.il/~dssl/papers/Gurfil-59.pdf\n  * http://www.esa.int/gsp/ACT/doc/MAD/ACT-PRE-MAD-GTOC1-TUR.pdf\n  * http://130.203.133.150/showciting?cid=7899704\n  * http://130.203.133.150/viewdoc/summary?doi=10.1.1.51.8663\n\n\nIt is complex task, your is probably even more complex than this in real world despite being 2D (and real ones are still researched and refined, and improved). But it is really interesting to try, because it should be quite easly possible to create algorithm which at least find a sensible trajectory and then imrpvoves it. It is very interesting application of multiple methods.\n\nSome more general optimization chalanges: \nhttp://staff.ustc.edu.cn/~ketang/cec2012/lsgo_competition.htm\n I was wondering if you'd mind answering a dumb question about Heun's Method. It seems to ask for a \"prediction\" of the acceleration at the end of the time step. The way my simulation is structured, I'm not monitoring Jerk (maybe I should be). Should I just be assuming that the acceleration is constant? Wouldn't this mean I'm just using Euler's method to calculate the next velocity? I feel like I'm missing something simple about how this is meant to be implemented. You basically need to remember previous position and velocity.\n\n    x, v - current pos and vel\n\nyou currently doing something like this probably:\n\n    v += h*accel(x);\n    x += h*v;\n\n\nyou need to change this to:\n\n\n    a = accel(x);\n    v1 = v + h*a;\n    x1 = x + h*v;\n    \n    v = v + 0.5*h*(a + accel(x1)); // using a, to not calculate accel(x) twice\n    x = x + 0.5*h*(v1 + v);\n\n\nlast 2 equations can be changed to +=, of course v and x, are vectors (and each component-planet have 2 or 3 components-axis), as well accel function. You can also treet more uniformly both velocity (or momentum) and position, but it will probably complicate code slightly.\n\n\n    d = accel_and_speed(xv);\n    xv1 = xv + h*d;\n    xv = xv + 0.5*h*(d + accel_and_speed(xv1));\n\nhow you put velocity and position into xv, depends how you like. you can use first half for position and second half for position, or interleave things, etc. position_1, speed_1, position_2, speed_2, ..., depending what you want, or performance (half for speed, half for position, will be the best for cache and simplicity reasons).\n\nthen accel_and_speed, will just return accelerations as first half (derivative of speed), and speeds as second half (yes, second half is positions, but we return here a derivative of position, thus speed).\n Thanks for the reply! I think I've got all that, the main issue is that because of the nature of the system, I don't really have a straight-forward \"acceleration function\",\n\nor \n\n    accel(x) \n\nas you have it in your code, which takes an arbitrary position and produces an acceleration.\n\nI have a function which takes in the positions and masses of every particle and produces a triangular array of forces acting between each object. With a simple calculation of dividing/multiplying by the appropriate mass, I avoid about half the computation of individually calculating every object's interaction with every other object. Once that array is computed, I then calculate the next step relative to the current acceleration.\n\nAs it is now, I have a function I call which is, generally:\n\n    method(a,r,v,dt);\n\nFor example, my original euler implementation looked like this:\n\n    euler(a,r,v,dt){\n        var new_v = a*dt;\n        var new_r = v*dt;\n        return {r: new_r, v: new_v};\n    }\n\nUnfortunately this design doesn't work well if I need to dynamically recalculate an arbitrary acceleration for a given object. Do I need to consider a redesign? It would seem to me that in an n-body situation, you'd have to calculate \"x1\" and \"v1\" and apply it to the objects THEN recalculate EVERY acceleration given those new positions, THEN finish the method. This would mean double the acceleration calculations.\n\nIs the numerical stability worth this redesign? Calculating new accelerations given a position seems to be very expensive in an n-body simulation. Or maybe I'm misunderstanding something!\n\nThanks so much for the help! I understand using triangular array, it is common trick to calculate only half of forces, but actually you do not need triangular array, you can calculate vector of forces at once. \n\n    forces[] = [0,0,0];\n    foreach (i; 0 .. n) {\n    foreach (j; i .. n) {\n        var force_ij = force(pos[i],pos[j],m[i],m[j]);\n        forces[i] += force_ij/m[i];\n        forces[j] -= force_ij/m[j];\n    }\n    }\n    return forces;\n\nYou can precalculate 1/m[i] in other array invm[i], if you want to avoid division and change it to multiplication.\n\n\nI think redesign should be hard.   For a constant density, the mass would be proportional to the diameter squared.\n\nIt's a really difficult problem in general I think.  The technique of sling-shotting might be useful.  If there was just one body, you could solve it analytically, I guess for two bodies too.  After that you will need heuristics.  My guess is that in complicated scenarios, the optimal solution will look like a sequence of slingshots round a sequence of planets.  Perhaps it would be worth working out the maths of a single slingshot (the angular change is controlled by how close to the planet you get, the speed boost depends on that and the velocity of the planet), and then try to write code that chains them together.  I think if you are on a trajectory towards a planet, then it's likely that you can control it enough to arrange that you slingshot to a trajectory towards a different planet, so you don't necessarily have to plan the whole route up front.  This seems like it has a lot of needless complexity, in my opinion it is, as you have framed it, not really in the domain of computer science problems, but more something you would ask a physics simulation. \n\nTo boil it down to it's essential elements, you could compute the amount of fuel it would take to go from every planet to every \"adjacent\" planet, (for some notion of adjacency you define, perhaps that you can make a trip from one to the other without passing through third planet, in which case building this is O(n^2)). Then you have a weighted undirected graph, where you want to find the shortest path. There are plenty of good polytime algorithms for this (Dijkstra's, Bellman-Ford, etc). \n\nThis is a planet hopping solution, which would preform pretty poorly on sparse solar systems (imagine one planet that is way out of your way, you need potentially near 0 fuel, to just *glide across*, but this algorithm has you visit the planet). \n\nPerhaps you could frame it as a partitioning problem, where you specify a maximum number of direction changes, and partition the planets into n groups (need an approximation here, as finding the best partition would be exponential time), then you can find the path which maintains the maximum distance to all the planets in each partition with dynamic programming. \n\nJust some thoughts: the TL;DR is you need to distill this a little better before it really comes in the area of compsci. It's pretty certain that you will need to settle for a suboptimal solution, but you may get better approximation from taking a physics approach then a compsci one, I really couldn't say.  Thanks so much for the reply!\n\n&gt;you will need to settle for a suboptimal solution\n\nThis is more and more the conclusion I am coming to. I considered a planet hopping approach, but this is severely limiting the nature of the destination point. I'm not sure I want to modify the meaning of the problem in that direction.\n\nAs it's phrased now, I'm viewing it as an extremely complex constraint satisfaction problem. The inputs being the system variables (N-Body initial state, fuel capacity, start point, end point), the constraints being no collisions, minimal time and minimal fuel usage, and the output being a decision set. The issue with treating it this way is that the search domain is *fantastically* large. I could imagine modeling the N-Body simulation as a 3-D array of some extremely large size (2 spacial dimensions and 1 time dimension), but even if I had the memory for that (I obviously don't), searching for an optimal path would not only likely reveal a limitedly correct result, but would take an incredible amount of time to compute. Perhaps that's a more interesting problem though!\n\nEither way, like you said, it looks like I'll have to limit the problem fairly extensively. This is, as you might expect, personally disappointing but not surprising. The reason I find it disappointing is because it *feels* like a very practical problem as it's described now.\n\nThanks for your thoughts! Do you have any advice on another subreddit in which I might seek advice on ways to modify the problem? I thought about /r/Physics or /r/Math, but I'm not sure where it applies (I'm CS so that's naturally where I figured I'd start). &gt;I could imagine modeling the N-Body simulation as a 3-D array of some extremely large size (2 spacial dimensions and 1 time dimension)  \n\nI was thinking the third dimension could represent various motion vectors, since the craft has restrictions on the amount of thrust it may provide, there is a restriction on how one moves between these vectors. The biggest problem you face is that you have to perform an exhaustive search because AFAIK, the most efficient path-finding algorithms wont allow your spacecraft to appear to go too far out of it's way so it can slingshot off a conveniently placed body.  \n\nTo minimise the time/memory of the exhaustive search, you could do several passes with differing granularity, you will most likely miss the optimal edge case, but it will give a generally good solution.  \n\nThe ideal solution to the problem is going to be calculus heavy, good luck. Yeah using the vectors would probably make more sense... And you're right, the somewhat chaotic nature of the system (chaos in the mathematical sense, not popular sense) also raises issues with time resolution producing ever increasing error regardless of what I do (a problem with any N-Body simulator) - beyond just requiring an exhaustive search.  Damn, I totally forgot about Lorentz attractors! Haha I only have a very general understanding of any of that realm of dynamics. I bought a textbook on it all, but there's just so much there! I really love the idea that a small change in the decision set could yield an impressive improvement in score. It, unfortunately (or maybe fortunately?), scoffs in the face of every algorithm I know for optimization.\n\nCertainly a problem I'm going to enjoy investigating more! I'm thinking about just writing up a little \"game\" to see how well I can perform. Then I'll add in a panel where you can write your own javascript code to solve the problem. It'd be fun to try a bunch of solutions and see which one does the best in terms of score and performance (memory and time). Crowd source it. Put the simulation online and let people compete for the highest score.",
    "url" : "http://www.reddit.com/r/compsci/comments/rx4z1/spaceship_navigation_problem/"
  }, {
    "id" : 4,
    "title" : "Anyone know of good history of software development books?",
    "snippet" : "I'm wanting to start reading a book about the history of software development and programming. If anyone can point me into a direction, that'd be awesome. Thanks.\n\nEDIT: To be a bit more specific, I don't just want a text book, but rather a good non-fiction story. I'm also really wanting something that chronicles the development of computers and/or software. I've found a couple:\n\n[Campbell-Kelly &amp; Aspray, Computer: A history...](http://www.amazon.com/Computer-History-Information-Machine-Technology/dp/0813342643/ref=tmm_pap_title_0)\n\n[Eric Raymond, Cathedral and the Bazaar](http://www.amazon.com/The-Cathedral-Bazaar-Accidental-Revolutionary/dp/0596001088)\n\n[Steven Levy, Hackers](http://www.amazon.com/Hackers-Computer-Revolution-Steven-Levy/dp/0141000511)\n\n  [The Cuckoo's Egg](http://www.amazon.com/Cuckoos-Egg-Clifford-Stoll/dp/0671726889) is a good true account of a system administrator chasing a hacker in the late 80's.   I read that several times when I was in high school and loved it, the [TED talk he did was rather amusing too.](http://www.ted.com/talks/clifford_stoll_on_everything.html)   You might need to be more specific.  At a human level \"hackers\" by Stephen Levy is *the* book You might need to be more specific.  At a human level \"hackers\" by Stephen Levy is *the* book I've already read this and absolutely loved it. I was just wondering if there are more like it, and if so, the titles. As for being more specific, I'm looking for a book that encompasses the whole \"computer revolution\"...so, 1940s to 1990s would be something that I'm really interested in reading about. I've found this, and heard good reviews: http://www.amazon.com/Computer-History-Information-Machine-Technology/dp/0813342643/ref=tmm_pap_title_0\n\nI also would love to find something about the free/open source software movement. I already know of Raymond's Cathedral and the Bazaar, and also his book on Art of Unix Programming.     ",
    "url" : "http://www.reddit.com/r/compsci/comments/rwda5/anyone_know_of_good_history_of_software/"
  }, {
    "id" : 5,
    "title" : "Getting accepted into UNIQ (Oxford Summer School in CS)",
    "snippet" : "Hey /r/compsci,\n\n\nLet me introduce myself. I'm a 16 year old, about to leave year 11, and go into sixth form next September. I've been interested in Computers since I was about 8, but for the past 2 years, my focus has shifted to Computer Science.\n\n\nAfter discovering UNIQ (http://www.ox.ac.uk/admissions/undergraduate_courses/working_with_schools_and_colleges/uniq//), I've set it as my 'target' as it were, to apply and be accepted. What I'm wondering is, how can I show them that I have a passion (and have taken means to) learning about the subject?\n\n\nSo far, what I've done is -\n\n* Learn Python (using LearnPythonTheHardWay), whilst coding small problem solving activities (such as finding the square root of a number using bisection search).\n\n* Learn briefly about OOO (encapsulation, polymorphism and hierarchy).\n\n* Began to follow MIT OpenCourseWare's 'Introduction to Computer Science and Programming'. The problem I've found however, is that my Maths isn't good enough to keep up with this :/\n\n* Use iTunes U to learn about computer networks (more as a secondary aspect).\n\nCould anyone please recommend ways of impressing the board when I apply next year, or any reading material worth looking at, or concepts worth coding (I considered trying the Knights Tour at some point, with each path using a separate thread, but I'm not entirely sure where to start). Please?\n\n\n**Any help, opinions or advice would be really, really appreciated.**\n\nThanks!   I applied for Cambridge for compsi this year and got accepted (doing A levels this year) so I suppose I can tell you about my experience. \n\nMy school had never sent anyone in for compsi but plenty for maths so that's all they did really. \n\nI don't think learning programming languages and compsi stuff is really going to help you. I know a couple of languages but I never learnt them for any other reason than to do projects. \n\nA couple of my teachers at school at tutors at Oxford uni for maths and do interviews there so I had good preparation for a maths interview. \n\nMy advice is just to make sure you're really good at maths and do further maths a level. Do things such as project Euler to help at problem solving but overall just be really passionate about it - which it seems you are. \n\nDoing the MIT compsci course isn't all that useful I think because after all you want to do it at oxford where you will be taught everything. They will never ask you anything course or knowledge specific at interview. \n\n**Edit:** Sorry, didn't realise you were talking about the summer course and not university application.  Yup. I agree. I was at Cambridge (did maths, not compsci though) and was a student guide for interviews. Certainly for an admissions interview, make certain that your maths is up to scratch. It's just the sort of thing that they like to test in anything science or maths based. \n\nShouldn't imagine they'll be doing interviews for this summer school though. But I imagine OP'll be planning to apply to Oxbridge, so I hope this is still useful. On the topic of which, rejection from a Uni's summer school definitely doesn't mean that you won't get accepted there for a degree, so don't worry if you miss your 'target'. In my interview it turned out not to be that maths orientated at all. I had questions on finite state machines, CPU and memory architecture, a logic problem involving a pipe system, and one maths questions which was simplifying a summation formula. \n\n  sounds like you are doing all the right things.  one thing to ask yourself though, is whether you really need to pay for a formal cs education at your age.  many good programmers are self-taught. and many brilliant computer scientists can't program. I find that assertion surprising, and it doesn't coincide with my experience. Would you care to give some examples? and many brilliant computer scientists can't program. what's the point of saying something like this?  i wasn't implying that good programmers aren't educated, i was just pointing out that a lot of them taught themselves how to program.  i don't think you can realistically become a good programmer without teaching yourself a lot.  not familiar with oxford, but if it's anything like the US ivies, you will need something extraordinary to stand out. many kids can learn code, you need something concrete to stick out to admissions. Possibly writing an app, or creating your own software. taking advanced college comp sci courses wouldn't be enough (there are too many smart kids taking APs IBs etc.).\n\nperhaps a software to help kids learn / tutoring, software for your school administration.",
    "url" : "http://www.reddit.com/r/compsci/comments/rw98v/getting_accepted_into_uniq_oxford_summer_school/"
  }, {
    "id" : 6,
    "title" : "Conference Reviewing Considered Harmful (Thomas Anderson)",
    "snippet" : "    PDF!!! Sorry, is it bad form to submit PDFs? at least put a warning in the title of your post next time why? What's wrong with pdfs? Having to open a potentially memory-hogging PDF viewer is annoying as shit. Plugin-container's memory usage jumped 5 MB when I opened it.\n\nTHE HORRAR!!! at least put a warning in the title of your post next time I for one wish someone would warn me for HTML files, downloading all those JavaScript and CSS files separately is so inconvenient and inefficient. I do understand the concern about pdfs. \n\nI remember working on my university linux box, waiting for adobe acrobat reader to scrape itself together off the hard disk, usually crashing the whole browser in the process. Ugh... ",
    "url" : "http://pages.cs.wisc.edu/~dusseau/Classes/CS739/anderson-model.pdf"
  }, {
    "id" : 7,
    "title" : "I'm finding useful links for students interested in students Computer Science. Feedback welcome!",
    "snippet" : "  For the more mathematically inclined, you can include these:\n\n[Mathematics for Computer Science](http://courses.csail.mit.edu/6.042/spring12/mcsfull.pdf)\n\n[Logic for Computer Science](http://www.cis.upenn.edu/~jean/gbooks/logic.html)  I would suggest:\n\n* http://stackoverflow.com/\n\n* http://www.wibit.net/ \n\nThe latter is more of a podcast but it is very informative and easy to listen to. Wibit is amazing. Thank you! Great, glad you liked it! Also, great links you've got so far!!!  I'd be weary of putting too much work into Delicious; my account was completely erased in the transfer from Yahoo.     ",
    "url" : "http://www.delicious.com/stacks/view/PGJgxY"
  }, {
    "id" : 8,
    "title" : "r/compsci, how would you say the computer science major at an average university ranks amongst all majors in terms of difficulty?",
    "snippet" : "Was just thinking about this tonight after my last exam. Absolutely got my ass handed to me and im struggling quite a bit. It's only my second class for the major, so i feel a bit.. embarassed i guess. Stupid. I want to see what you guys think about this major in relevance to other majors.  If you're feeling bad about your performance on the exam, go seek extra help. Don't look for an easy way out. If you want to do CS, take the time to learn, find the TAs, or get tutoring to fill in the gaps in your knowledge. Do what you find interesting and what gives you satisfaction, not just what is easy. CS is not a simple subject and not everybody understands it immediately.\n\nDon't look for us to tell you how much harder CS is than everything else, because it isn't. CS seems to be about the same difficulty as most of the other hard science / engineering majors at most schools. I found even my easiest CS classes significantly harder than even the \"hardest\" business classes I took, and hard in different ways than most humanities classes. (I am, of course, biased being a CS major)\n If you're feeling bad about your performance on the exam, go seek extra help. Don't look for an easy way out. If you want to do CS, take the time to learn, find the TAs, or get tutoring to fill in the gaps in your knowledge. Do what you find interesting and what gives you satisfaction, not just what is easy. CS is not a simple subject and not everybody understands it immediately.\n\nDon't look for us to tell you how much harder CS is than everything else, because it isn't. CS seems to be about the same difficulty as most of the other hard science / engineering majors at most schools. I found even my easiest CS classes significantly harder than even the \"hardest\" business classes I took, and hard in different ways than most humanities classes. (I am, of course, biased being a CS major)\n Good advice here. My professor often tells me i need to learn the material on my own, however. He likes to use the \"teach a man how to fish\" analogy. Good advice here. My professor often tells me i need to learn the material on my own, however. He likes to use the \"teach a man how to fish\" analogy.  I started studying CS back in the Fall of 2007 and I got a 56/100 on my first CS test ever. The class average was in the high 90s and I was absolutely crushed. I eventually had to drop the course as things were getting harder and I just couldn't grasp things quickly enough.\n\nI explicitly remember the walk back to my dorm room. I was terrified, upset at myself and just overall shamed. I hated feeling like this so much that I said to myself that I'd graduate with a degree in CS. \n\nFast forward 4 years later (2011) and I did exactly this. A lot of things have happened since and I've grown as a person, but looking back I probably would not have majored in Computer Science; a minor would've been just fine with me. It began half way into junior year. I started losing interest in CS as I realized that algorithms, formal languages, and operating systems weren't really for me. Sure I liked understanding how they worked at a high level, but diving into the nitty gritty was not for me. But I felt it was too late at that point to switch so I stuck with it. I guess what I'm trying to say with this rant is that be acutely aware of your level of interest. Try to be as clear as possible to the reasons behind studying CS. These tidbits may help. Let me know if you have any more questions or need any further info. \n\n****\n**Do not let your emotions make the decision for you**\n\n* You are vulnerable at this current state. Back when I was a freshman I had a knee jerk reaction to failure and said \"I will absolutely major in CS\". Looking back, this was stubbornness and a hurt ego talking, not me. Don't immediately dismiss CS either by saying \"CS absolutely isn't for me.\" It may or may not be. Right now the aim is to just be as aware of your situation as possible. Relax things will be fine. Then ask yourself some of these questions and be honest with yourself. \n\n**Questions**\n\n* Are you sure you aren't studying CS b/c of some external factor? Maybe the faculty is really cool. Maybe you heard the money is great. Maybe you're family/friends insist that you do CS? \n* What exactly drew you into CS? Saying I'm good at computers or I like computers is not good enough. A better reason might be \"I like building computer software and want to learn more about computation/math/etc. to get better at this. Notice the differentiation between liking and liking to build. Lastly, this quote comes to mind.\n\n&gt;Never be afraid to fall apart, because it is an opportunity to rebuild yourself  the way you wish you had been all along. \n- Rae Smith This post was a great read! Thank you very much. I am actually a sophomore, I didn't declare a computer science major until the end of my freshman year.\n\nI wanted to take compsci for a multitude of reasons: I had always been interested in computers in general, but i was more fascinated with the software than the hardware. I love gaming and always had a dream in the back of my head to be a game designer one day. I find programming to be pretty fun sometimes, and it's interesting to learn about how the languages (i've only learned Java so far) operate within the computer and what they actually do. I find it extremely difficult, and it's discouraging to see what other people have said in this thread about CS being one of the easier majors, but like you said: I won't let that get to me. I think difficulty may be a relative term.. some people are just naturally good at some things that others are not.\n\nI've grown a lot since school started too, I've gotten a bit better at studying (though i have a long way to go) and i'm not afraid to fail an exam or even a class because it means i can still learn more and keep trying, Sounds like you're on the right path! So I'm working on an independent project after my experiences to help other students better understand what each major is about - basically a starting point of research. \n\nI compiled a bunch of links about Computer Science using social bookmarking tools. I started off with [Delicious](http://www.delicious.com/stacks/view/PGJgxY), but have since discovered [Diigo](http://www.diigo.com/list/ghettoversity/computer-science).\n\nWhich site do you like better? Also, I'd love to hear any feedback as I'll be doing this for other majors as well. \n\nCheers!  [deleted]   I would say CS is definitely one of the easier majors. My undergrad is in math and I am currently in grad school for CS. I would say roughly 10% (could be way off) of my fellow grad students were math majors as well. When the grad students sit around and talk about undergraduate war stories it is pretty clear that what the CS students consider difficult is pretty laughable. Also, for some reason the former math majors always get the highest grades even when they, such as my self, have only been study CS for a very short time.\n\nTL;DR: CS is a pretty soft degree in comparison with a math degree. Also, I seem to remember reading somewhere that the CS undergrads had the lowest average IQ in the sciences. As someone studying joint honours Math/Compsci I find this to be a huge generalization. Anyone who thinks CS is considerably easier than the maths basically doesn't understand the CS enough. I would say CS is definitely one of the easier majors. My undergrad is in math and I am currently in grad school for CS. I would say roughly 10% (could be way off) of my fellow grad students were math majors as well. When the grad students sit around and talk about undergraduate war stories it is pretty clear that what the CS students consider difficult is pretty laughable. Also, for some reason the former math majors always get the highest grades even when they, such as my self, have only been study CS for a very short time.\n\nTL;DR: CS is a pretty soft degree in comparison with a math degree. Also, I seem to remember reading somewhere that the CS undergrads had the lowest average IQ in the sciences. Well, so i'm just stupid. Thanks for clarifying that. Well, so i'm just stupid. Thanks for clarifying that. ",
    "url" : "http://www.reddit.com/r/compsci/comments/rvxoq/rcompsci_how_would_you_say_the_computer_science/"
  }, {
    "id" : 9,
    "title" : "Test your ability to generate a truly random sequence of 0's and 1's. Can you make one that rivals a computer generated one?",
    "snippet" : "  This isn't really checking randomness, just distribution. Copy-paste 00011101 over and over again and you satisfy all of its requirements. \n\nFor random numbers you have to check for repeating patterns/predictability. The repeating patterns that are available are enough to show that any *honest* human generated sequence isn't random. That's the only point of this. If you copy-paste or look at the results while you generate the sequence, it will look random in this setup but that's defeating the purpose of trying to generate your own anyway. Even with your eyes closed, it's not testing *randomness*. You can have random values within a distribution. The real flaw in human generated random numbers isn't their distribution, it's their periodicity and predictability. It's literally impossible to test randomness. There's no test you can do that can conclude \"this sequence is almost certainly random\". The best we can do is a test that can conclude \"this sequence is almost certainly not random\". Then we throw every such test we can think of at it and see if it fails them all. That's what the Diehard tests are, for instance. Relevant: http://dilbert.com/strips/comic/2001-10-25/ Even with your eyes closed, it's not testing *randomness*. You can have random values within a distribution. The real flaw in human generated random numbers isn't their distribution, it's their periodicity and predictability. Even with your eyes closed, it's not testing *randomness*. You can have random values within a distribution. The real flaw in human generated random numbers isn't their distribution, it's their periodicity and predictability. The repeating patterns that are available are enough to show that any *honest* human generated sequence isn't random. That's the only point of this. If you copy-paste or look at the results while you generate the sequence, it will look random in this setup but that's defeating the purpose of trying to generate your own anyway. I generated this sequence using a simple enough algorithm and seems quite random :D\n\n0000000100100011010001010110011110001001101010111100110111101111 Is it just I that find the 0:s to diminish as I read on? This isn't really checking randomness, just distribution. Copy-paste 00011101 over and over again and you satisfy all of its requirements. \n\nFor random numbers you have to check for repeating patterns/predictability. What does \"checking randomness\" even mean?\n\nFor a sequence of length n, any single combination of 1's and 0's is just as probable as any other, assuming the chance of spitting out either is 50% (or close enough). Claiming that one sequence is \"more random\" than another means absolutely nothing.\n\nChecking for repeating patterns/predictability is leaning toward the law of averages, which we all know is not true :)\n\nOne can only examine an RNG by looking at how it generates, not its output. What does \"checking randomness\" even mean?\n\nFor a sequence of length n, any single combination of 1's and 0's is just as probable as any other, assuming the chance of spitting out either is 50% (or close enough). Claiming that one sequence is \"more random\" than another means absolutely nothing.\n\nChecking for repeating patterns/predictability is leaning toward the law of averages, which we all know is not true :)\n\nOne can only examine an RNG by looking at how it generates, not its output.  Some randomness tests that are more interesting then this:\n\n* [Rock, Paper or Scissors](http://www.nytimes.com/interactive/science/rock-paper-scissors.html)\n* [Mind Reader](http://seed.ucsd.edu/~mindreader/)\n Thanks for those. I've seen the rock, paper, scissors one before. I'm pretty sure it's not a randomness test because the computer is learning based on your previous moves. \n\nThe last time i played I was able to stay ahead of the computer because I was deciding my moves based on the moves it made which were based on my previous moves. Basically, all you have to do is stay two steps ahead of yourself. If the player was truly random, the computer would have nothing to learn -- that is, the computer would still \"find patterns,\" but its guesses about future moves based on said patterns would be right only as often as random guessing. No more, no less. &gt; If the player was truly random, the computer would have nothing to learn\n\nExactly this. OP seems to misunderstand what randomness means. I think you misunderstand what randomness means. Randomness does not mean a lack of patterns. Any specific string has exactly 0 chance of being chosen, it's completely not random. Even a string with a repeating pattern can still be randomly generated. Randomness just means you can never be sure a pattern will continue, not that there won't be any.  Yes, I understand. Randomness tests test *algorithms* for generating sequences, not sequences themselves. Which is why Rock, Paper, Scissors games are randomness tests (because they test your ability to pick many 'random' moves), and why the link the OP posted is *not* a randomness test.\n\nAlso, wat:\n\n&gt; Any specific string has exactly 0 chance of being chosen\n\nAre you talking about strings of unbounded length? There's not even a uniform distribution over those. Yes, I'm talking about strings of unbounded length. Assuming you have at least 2 symbols (i.e. you have more than one string) then you can obviously, at the minimum (i.e. with only finite symbols), assign each string to a real number and when picking a random real number, the chance of picking any specific real number is zero. \n\nOkay, I misunderstood what you were saying and so I feel I must, at least partially, agree with you. However, I guess if you had a finite number of symbols (not sure how well it would work with an infinite number....) you could create a way to write numbers in whatever-nary and then take the outputs mod 3 to get a specific move in RPS, but there are definitely different types and degrees of randomness and uniformity. Plus, well, you'd already need the ability to create a truly random sequence (to feed in to RPS as the opponent) and even then you'd still need to be careful to define exactly what you mean by randomness, as there are definitely different types of randomness, even for randomly generated strings. I mean, I'm not too far in to my studies of statistics, but randomness is not necessarily a well-defined term (unless it's given context).\n\nBut, in the end, that's all kinda pointless because, as I said, randomness can very well have patterns in it and so the RPS opponent (who is apparently not completely random, but learns from opponents \"moves\") could easily get lucky and take advantage of local patterns. I mean, all randomness should have local patterns, but if the RPS opponent is not truly random and \"learns\" from the main player, then it's definitely not a random test as complete randomness is not the best choice in ALL series of games of RPS, but only when facing an opponent who is itself truly random. So, if we don't know enough about the learning algorithm, true randomness may not be easily detectable based upon wins and losses. At least, that's my understanding of how RPS could be a randomness (based roughly on my understanding of the specific definition we're dealing with) test, but isn't necessarily inherently one. \n\nAlso, if you consider all strings which are simply (infinite) sequences of (finite for sure, not sure about infinite sets of symbols) you can, as I said, treat them like a specific base representation of the reals and there is definitely a uniform distribution over the reals, so you sir, are wrong.  &gt; Yes, I'm talking about strings of unbounded length. Assuming you have at least 2 symbols (i.e. you have more than one string) then you can obviously, at the minimum (i.e. with only finite symbols), assign each string to a real number and when picking a random real number, the chance of picking any specific real number is zero.\n\n&gt; ...\n\n&gt; Also, if you consider all strings which are simply (infinite) sequences of (finite for sure, not sure about infinite sets of symbols) you can, as I said, treat them like a specific base representation of the reals and there is definitely a uniform distribution over the reals, so you sir, are wrong.\n\nWhat? No. There is no uniform distribution over the natural numbers. It does not make sense to say \"pick a natural number uniformly at random\". The probability is not zero, it is not defined.\n\nYou cannot map strings to real numbers in this way, because as soon as you move from finite-length strings to infinite-length strings you end up with an uncountable set.\n\nI assume you're an undergrad..? Please speak to a professor to clarify these issues. This is pretty basic stuff...\n\nSorry, but when you say stuff like this:\n\n&gt; But, in the end, that's all kinda pointless because, as I said, randomness can very well have patterns in it and so the RPS opponent (who is apparently not completely random, but learns from opponents \"moves\") could easily get lucky and take advantage of local patterns.\n\nIt makes it really clear that you do not understand the math or complexity theory behind this topic. Alright, sorry about the RPS thing, I will clarify what I meant there:  \nI was being a contradictory ass and was being silly with my abuse of statistical measures of randomness based upon a finite number of tests. Of course modern random tests have edge cases (i.e. a truly random RNG could still output only 1's for all eternity) and just putzing around with... Well, I apologize. I wrote the comment in several separate sittings (kept getting called off to do stuff) and so I admit the writing and coherence were rather embarrassing and I was wrong. Or at least being an ass who was technically correct only in the worst way.\n\n\n\nOn the issue of the real numbers though, I am going to have to assume YOU are the one who doesn't know what they're talking about.  \nNow, I'm honestly not sure why you're confused at all, but nowhere did I ever say or imply that the set of the infinite sequences (aka strings) was countable.  \nIn fact, by your argument, the reals ARE equivalent to the naturals apparently, since every single real number has an n-ary representation (potentially infinite) and there are an uncountable number of real numbers and thus there are an uncountable number of such representations (even before you count duplicates, such a .9999... and 1.0 in decimal or .3333... and 1.0 in ternary).  \nNow, to make things simpler, as you probably know, the interval [0,1) is uncountable and so we can just consider representation on one side of the decimal point (since all members of that interval can be represented as 0.whatever) and assign each one of those n-ary representations to a string we can think of those simply as strings of (countably) infinite length made up of n+1 symbols.  \nSince, as I just showed, that set of strings (which you for some reason assumed had a bijection to the naturals) is uncountable, by your logic uncountable sets are the same size as countable (or countably infinite if that's your preferred term) sets.  \nDo you understand now?  \nIf not, perhaps you should look up an example of a proof for why the Cantor Ternary set is uncountable, as it uses a similar proof based upon construction of the binary representations of numbers in the interval [0,1) (or was it [0,1]? Meh, doesn't really matter) and recognize that the techniques used in that proof are how you can show that any set of all infinite length strings made up of a finite number of symbols is equivalent to some n-ary representation of numbers in [0,1] and thus uncountable. \n\n\nPerhaps YOU need to brush up on your basic math as the above proof is pretty damn elementary and the fact that there will always be corner cases and thus your truly completely random RNG could spit out sequences, even many sequences, all consisting of only, say, Paper, and thus the non-random learning RPS would simply choose Scissors every time and win. Now, those corner cases are kinda me being an ass about how probability works, but they do exist and thus, without making sure to define your randomness test only in terms of probability of being random/not random, you can never be sure you've got no false negatives (or perhaps positives... After all, you could just feed the digits of Pi and so, from one perspective, such an input has a very low  entropy as it can simply be defined by the rule: decimal expansion of Pi, and thus is completely predictable, but may look naively random, since Pi is likely to be normal and thus should give you a normal distribution, which is arguably one definition of randomness, yes?) Thanks for those. I've seen the rock, paper, scissors one before. I'm pretty sure it's not a randomness test because the computer is learning based on your previous moves. \n\nThe last time i played I was able to stay ahead of the computer because I was deciding my moves based on the moves it made which were based on my previous moves. Basically, all you have to do is stay two steps ahead of yourself. The optimal strategy in RPS is completely random selection. Any other strategy can be exploited with a non-random counter-strategy. The fact that your computer opponent couldn't find that strategy merely indicates that it isn't an optimum player, not that your non-random strategy was superior to a random strategy.\n\nPut another way, the computer only tries to find non-random strategies so it can exploit humans' inability to generate plausible randomness. Exploitive strategies are non-optimal. The optimal strategy in RPS is completely random selection if and only if that's also your opponent's strategy.  To see that what you said isn't quite right, suppose that my strategy is to always play Rock.  Then your optimal strategy is to always play Paper, not to choose uniformly at random.\n Some randomness tests that are more interesting then this:\n\n* [Rock, Paper or Scissors](http://www.nytimes.com/interactive/science/rock-paper-scissors.html)\n* [Mind Reader](http://seed.ucsd.edu/~mindreader/)\n    A friend once asked me to write a \"random\" sequence of 0's and 1's using my brain and to generate one by flipping a coin, since he knew some tricks for telling the difference.  I used the odd or evenness of decimals in pi (I knew a lot of pi at the time).  He could easily tell that the pi expansion wasn't random.  I thought that was interesting.  I don't know if this was just coincidental - that the first hundred pi digits' odd-or-evenness doesn't look random, but that most runs of one hundred do - or if pi's decimal expansion as a sequence isn't doesn't really look as random as it does to me. *pi* is a [normal number](http://en.wikipedia.org/wiki/Normal_number), which implies that its distribution of digits is uniform. It does not imply that its distribution of digits is random or approximates randomness. *pi* is a [normal number](http://en.wikipedia.org/wiki/Normal_number), which implies that its distribution of digits is uniform. It does not imply that its distribution of digits is random or approximates randomness. Its widely believed that pi is a normal number, but not proven.   The presumption that a \"computer generated\" sequence of randomness is somehow superior to any other source belies a lack of understanding of (1) how difficult it is to generate random numbers, (2) how flawed many RNG schemes are, and (3) how easy it is to have detectable non-randomness in your entropy source even if your algorithm is good, because the proper operation of extant computing machinery depends upon determinism while true randomness requires nondeterminism.  000000000000000000000000000000\n\nif your RNG cant generate that, it's not truly random  Binary solo! Binary solo!   ",
    "url" : "http://www.khanacademy.org/labs/explorations/frequency-stability"
  }, {
    "id" : 10,
    "title" : "Left-reset turing machine",
    "snippet" : "How would you show that a turing machine that can only move right and reset head position to the beginning of the tape (no moving to the left) is equivalent to a normal left/right turing machine?  Tag system, queues...\n\nPretty much, you just have to show that it's possible to simulate a left-move with just right moves and resets. Use some special markers (add them to the tape alphabet) to help you figure out where you were before the reset. I realized this was the general idea, but I don't see what to do with tags. Ie, if instead of moving left, I decide to tag where I am and reset to the start of the tape, then start moving right, by the time I reach the mark, I would have already passed the cell one to the left of where I started (which is where I want to be). I tried thinking about variations on this but I couldn't get anywhere. Well, this is really inefficient idea, but: First, you can construct second label for every original from tape alphabet S, i.e. Sx{0,1} where (s,0) represent the original and (s,1) is \"marked\".\n\nThen you can mark your position, then reset and mark all the position up to that last one. Then you can reset, replace one mark, go to the very end of the input, and count how many marks you removed. By the time you remove all the marks, you have the original tape and the \"index\" of the original position. Then simply subtract one from the counter, and move that many steps from left (you can do this again by marking one at a time and reseting).\n\nI hope it's at least a bit clear.\n\nHowever, much simpler is to directly simulate a Tag system (that *is* a thing, look it up at wiki). It is already shown that Tag system is equivalent to TM, so if you can simulate TS, it can in turn simulate any TM, so you have universal computation. \n\nA \"push-down\" automaton with queue instead of stack is also equivalent to TM, so you can try to simulate that as well.  Is this for homework?  Be honest! Nope. It's from Sipser, but it wasn't an assignment. I asked my instructor about it after class but he wasn't sure about the solution off the top of his head. So I'm going to show it's equivalent to a Left/Right TM (and everybody knows those are equiv to Left/Right/Stationary, which is I think what Sipser uses.\n\nAnyways, right transitions are the same (iir the problem c).  Expand ur tape alpha bet to \\Sigma \\cup \\Sigma ^*, where \\Sigma ^* is disjoint from \\Sigma , and is merely the alphabet with a * on top of each one.  Then, whenever you would have a left transition, put a star on top of whatever you write on it, and left reset.  Then, check if the leftmost was starred, if it was either terminate or act on that bit of the tape - dunno what sipser does on left transition on leftmost bit.  Anyways, if it wasn't *, then write a * on it, and transition right.  Is the one next a star, cool do B, otherwise do A.\n\nA.  Left reset.  Go right until you see a star, then erase that star, and go to the right.  Star that.  Then go to the right.  Is that starred, if so do B, otherwise do A.\n\nB.  Erase the star and left reset.  Go back up to the star.  That starred letter is the one to the left of your first, act on it as you would it's unstarred version. \\Sigma* might be an unfortunate choice of notation as it would easily be misinterpreted as a Kleene star. So I'm going to show it's equivalent to a Left/Right TM (and everybody knows those are equiv to Left/Right/Stationary, which is I think what Sipser uses.\n\nAnyways, right transitions are the same (iir the problem c).  Expand ur tape alpha bet to \\Sigma \\cup \\Sigma ^*, where \\Sigma ^* is disjoint from \\Sigma , and is merely the alphabet with a * on top of each one.  Then, whenever you would have a left transition, put a star on top of whatever you write on it, and left reset.  Then, check if the leftmost was starred, if it was either terminate or act on that bit of the tape - dunno what sipser does on left transition on leftmost bit.  Anyways, if it wasn't *, then write a * on it, and transition right.  Is the one next a star, cool do B, otherwise do A.\n\nA.  Left reset.  Go right until you see a star, then erase that star, and go to the right.  Star that.  Then go to the right.  Is that starred, if so do B, otherwise do A.\n\nB.  Erase the star and left reset.  Go back up to the star.  That starred letter is the one to the left of your first, act on it as you would it's unstarred version.",
    "url" : "http://www.reddit.com/r/compsci/comments/rvdq0/leftreset_turing_machine/"
  }, {
    "id" : 11,
    "title" : "A bin packing problem with a wrinkle - how would you approach it?",
    "snippet" : "(This is a simplification of a problem I'm dealing with at work)\n  \nYou are at a carnival and go to play a game.  The barker gives you n balls and shows you n bins, each of which can hold n balls.  The balls are all different sizes, colors, and weights.  He asks you to throw the balls into the bins however you choose. Once you have thrown all n balls he will apply the same function (based on properties like size, color, and weight) to each bin to derive a value - you will win the sum of the values of the bins. \n  \nYou will win money based on your allocation.  He tells you that:\n  \n1) If you throw each ball in a single bin you win $0. Only combos are worth money (and many combos are worth &lt;$0).\n  \n2) Before you play he will let you try every one of the 2^n combination of the balls into a single bin if you want (in order to try to deduce the formula he is using).\n  \nHow do you maximize your winnings?\n  \nIt's a bin packing problem but you're not trying to *minimize the number of bins*, you're trying to find the allocation with the maximum value based on some function.  The \"balls\" are vectors all of the same size for a given instance of  the problem but different instances might have different size vectors. \n  \nIt's obviously NP complete because it's analogous to finding a set cover (which is NP complete) but worse than that you're looking for the *optimal* set cover.\n  \nI haven't been able to find this variation discussed in the literature - have any of you?\n  It doesn't even seem like there's much structure to exploit. It also only seems related to bin packing in the sense that you've explained it in terms of bins. You can quite easily rephrase: Given unknown function f(x), where x is a vector. Find the x vector that maximizes f(x) where each x_i is integer, x_i &gt;= 0 and sum x_i &lt; n. Which is basically an integer programming problem.\n\nYou might want to look at linear regression, integer programming, simulated annealing, or branch and bound techniques as ways to solve your problem. I don't think you're characterizing the problem quite right, or I am misunderstanding you.  There are 2^n possible configurations for each of the n bins. I want to maximize the sum of the values of the bins, not maximize the value of a single bin.\n  \nIf you think of it like set partition I want to find the way to partition a set of size N into m distinct subsets where each M[i] transforms to a real numbers.  I want to carve the set up to maximize the sum of the subsets.\n Sorry, I was a bit inspecific, I didn't mean one bin, I meant over all bins:\nYou're looking to maximize the sum over all bins, of f(x_i) where x_i is the number of balls in bin i. Do I have that correct? You could also just as easily say you're looking to maximize one function: g(x) = \\sum_{i=1}^n f(x_i).\n\nBasically, your problem comes down to searching for the right set of x_i's that maximize the function (or sum of functions). The reason you aren't finding anything in the literature on bin packing in this manner is because this problem is more general than bin packing, and is addressed by techniques like integer programming set cover (as you note), and many other approaches to solving NP problems.\n\nThere's also heuristic search techniques like simulated annealing which might give you good results for this problem.\n\nAnother approach would be to model the function and find roots of it's derivative, to find a maxima. (regression)\n\nSorry if this just confuses things more, I just wanted to drop some keywords that you might find helpful in solving your problem. :) Yes you are correct.  I've been wandering all over the literature looking at knapsack, set cover, bin packing  - they're all related.  I just couldn't come up with a transform between what I want and what other problems are like. For example set cover typically focuses on finding if a set cover exists given a set of pieces - in my case I know a huge number of them exist.  \n  \nI've used Simulated annealing in the past and my hesitation here is that the search space is so vast I have little confidence in getting near a \"good\" answer in a reasonable amount of time.\n  \nThank you for the help - I will continue plugging away. Yes you are correct.  I've been wandering all over the literature looking at knapsack, set cover, bin packing  - they're all related.  I just couldn't come up with a transform between what I want and what other problems are like. For example set cover typically focuses on finding if a set cover exists given a set of pieces - in my case I know a huge number of them exist.  \n  \nI've used Simulated annealing in the past and my hesitation here is that the search space is so vast I have little confidence in getting near a \"good\" answer in a reasonable amount of time.\n  \nThank you for the help - I will continue plugging away.  It seems like the problem is finding a maximum score in an extremely massive search space, (each ball is different, different combinations give different values, and different bins use different combinations for adding value? is this right?). Our goal should then be to only look in promising areas of the search space.\n\nI'd look into forming an evolutionary algorithm to attack this problem. The problem can be represented fairly simply, allowing it to be manipulated easily. There are n balls that must be assigned to bins. Bins range from bin1 to binN. We would just have a list of length n (to represent the n balls). The value stored in each element of the list would be the bin number that the corresponding ball is in. IE: (2, 3, 1, 1) would mean ball1 is in bin2, ball2 is in bin3, and balls 3&amp;4 are in bin1. \n\nFrom an evolutionary algorithm point of view, this is easy to manipulate with crossover and mutation. With a representation model that can accommodate all possible combinations as well as simple manipulation possible, we can move to the next hurdle. We can examine how good a solution is by examining the list and attributing a score to it. Specifics for how to score a possible solution are not explicitly given, so I cannot really proceed.\n\nThis problem is very well suited for attack by evolutionary/genetic algorithms. If this isn't homework, I could work on creating a program that would attack this program with you. If it is homework, I'd be glad to show you genetic algorithm design in more detail and show you how this problem can be attacked. I've already gone down this road and built an implementation based on a genetic algorithm.  The problem I had was flailing around the search space without a lot of confidence of approaching the optimum.  \n  \nThat said I like your representation.  I was using bitmasks for each bin so if bin1 has value 7 it means balls 1,2,3 and 3 are stored in it.  I think your representation might be easier to work with.  EDIT: Canonicalization of this representation will be problematic.  For instance [1,1,2,3] is really equivalent to [2,2,1,4] because they both encode {{1,2}, {3}, {4}}.    I think your best bet (for finding the best solution, not necessarily quickly) is to try each of the 2^n combinations on the one bin and record the results.\n\nEven without deducing a formula, you do get an evaluation function which gives you an exact value of a bin given its contents, and you can write an algorithm to maximize the value of all the bins with the n \"balls\" you are given. I am doing that, although a) it's not as much help as you think as we're still in a \"Bell Number\"-scale combinatorial explosion and b) 2^n also gets big pretty quickly.  I was leaning toward the evolutionary approach at first to try and avoid even calculating the 2^n and maybe stay in pseudo-polynomial territory.  I also have been trying to work out the function to see if we can rule out combinations. So, for example, if (A,B) is positive and (B,C) is positive can we infer anything about (A,C) or (A,B,C)?  unfortunately the answer is not as linear as I might like.\n\nEdit:  \"you can write an algorithm to maximize the value of all the bins with the n balls you are given.\"  That's the $64,000 NP-complete question right thar. :) Perhaps you can come up with various measures of similarity between combinations, determine which one is best suited for predicting values of bins, and use the combination of a sample of values and your similarity measure to come up with a heuristic for your search?\n\nAnother possibility is that more knowledge of the specific problem you're trying to solve will help. How are you getting the values of the bins? Are you sending off a solution to another party and they are coming back to you, or is what you posted a simplification of something like optimizing a data structure where your values indicate speed of lookups? These are indeed all things I'm looking at.  What I'm really interested in (more than concrete advice) is pointers to papers considering this or similar problems.  It's a fairly obvious analog to set cover, bin packing or even the knapsack problem but I can't find any papers dealing with it.",
    "url" : "http://www.reddit.com/r/compsci/comments/rums4/a_bin_packing_problem_with_a_wrinkle_how_would/"
  }, {
    "id" : 12,
    "title" : "quantifying regular expressions in standard notation (i.e. not using perl or POSIX notation) (xpost r/HomeworkHelp)",
    "url" : "http://www.reddit.com/r/HomeworkHelp/comments/rurhk/quantifying_regular_expressions_in_standard/"
  }, {
    "id" : 13,
    "title" : "I found this: Symbolic Partial Fraction Decomposition",
    "snippet" : "  I'm pretty interested in symbolically evaluating mathematical expressions, a natural outgrowth of learning the pencil and paper method first. I got my TI-89, and I have been endlessly fascinated with it ever since.\n\nAlso of note: the paper was written in '71, and presented at a conference. That's why some of the symbols look handwritten. ",
    "url" : "http://groups.csail.mit.edu/mac/users/gjs/6.945/readings/simplification/horowitz-ratint.pdf"
  }, {
    "id" : 14,
    "title" : "Want to learn more algorithms, don't know where to start.",
    "snippet" : "I'm proficient at programming, though there are still things I'm not entirely comfortable with (function pointers, what?), but the largest gap in my knowledge is of algorithms. I competed in ACM my freshman year of college and did pretty well (second place at a small competition, 5th at a slightly larger one), but that was largely because I had a teammate who had a huge binder of algorithms for the complicated problems. I've gotten a little burned out on figuring out my own solutions to problems I know have been solved, and I want to learn some of those solutions. What should I learn?\n\n(I know sorting algorithms, and I've heard a little about fancier things like edit distance and such. I'm interested in data processing currently; smoothing, approximating, best-fitting, etc. Currently trying to work on something to predict/learn trends--how it's changing, how fast it's changing, and how long that change will last. I'm interested in algorithms in general though.)  Introduction to Algorithms by Cormen et al.\n\nIt's a good book. Introduction to Algorithms by Cormen et al.\n\nIt's a good book. Introduction to Algorithms by Cormen et al.\n\nIt's a good book. Introduction to Algorithms by Cormen et al.\n\nIt's a good book. Introduction to Algorithms by Cormen et al.\n\nIt's a good book. This one is on my shelf now, but I regretted not buying it for my comp sci degree. It's kind of expensive so maybe look for a used copy. Although, I'd be surprised if many people actually wanted to sell their copy. I might get buried with mine, along with my copy of APUE. False. It's about $60 on Amazon for a hardcopy now. False. It's about $60 on Amazon for a hardcopy now. $60 is expensive for some people. Have a heart. Introduction to Algorithms by Cormen et al.\n\nIt's a good book. Purchased! You will never regret it! CLSR saved my ass when it came to job interview time. ahhem, CLRS. You will never regret it! CLSR saved my ass when it came to job interview time. I had a friend who had a google interview - he was asked to code mergesort in Java, he forgot how to make an array! I had a friend who had a google interview - he was asked to code mergesort in Java, he forgot how to make an array! I had a friend who had a google interview - he was asked to code mergesort in Java, he forgot how to make an array! Introduction to Algorithms by Cormen et al.\n\nIt's a good book. Introduction to Algorithms by Cormen et al.\n\nIt's a good book.  Check out this [previous thread](http://www.reddit.com/r/compsci/comments/rjc77/can_anyone_recommend_some_good_reading_for/) with some algorithms textbook references.\n\nMy recommendation is [Jeff Erickson's algorithms notes](http://compgeom.cs.uiuc.edu/~jeffe/teaching/algorithms/) (online and free). His notes are the golden standard. Especially the ones on dynamic programming. Rethinking DP in terms of just _smart recursion_ rather than how to create your table for memoization really should be the way its taught. I know next to no dynamic programming but the source for DP algorithms that I've seen make my head explode. For the record, Dynamic Programming is a style of algorithmic design in which the problem you are trying to solve consists of many (sometimes overlapping) subproblems.  The reason this is faster than a brute-force algorithm is that the results of the subproblems are saved (memo-ized) so they do not need to be recomputed.\n\nIf you're still in college, I really recommend taking a class on analysis of algorithms. I'm at a community college, I've literally taken almost all the programming classes they offer; the ones I haven't taken are repeats of what I have using different languages. I'm interested in learning about it, but I'd need to find a book. I'm at a community college, I've literally taken almost all the programming classes they offer; the ones I haven't taken are repeats of what I have using different languages. I'm interested in learning about it, but I'd need to find a book. I'm at a community college, I've literally taken almost all the programming classes they offer; the ones I haven't taken are repeats of what I have using different languages. I'm interested in learning about it, but I'd need to find a book. Check out this [previous thread](http://www.reddit.com/r/compsci/comments/rjc77/can_anyone_recommend_some_good_reading_for/) with some algorithms textbook references.\n\nMy recommendation is [Jeff Erickson's algorithms notes](http://compgeom.cs.uiuc.edu/~jeffe/teaching/algorithms/) (online and free). From chapter 1, \"Recursion: Simplify and Delegate\":\n\n\"I used to refer to ‘elves’ instead of the Recursion Fairy, referring to the traditional fairy tale about an old shoemaker who leaves his work unfinished when he goes to bed, only to discover upon waking that elves have finished everything overnight. Someone more entheogenically experienced than I might recognize them as Terence McKenna’s ‘self-adjusting machine elves’.\"\n\nnot only does this dude have a clear and understandable style, he's got a sense of humor.\n Check out this [previous thread](http://www.reddit.com/r/compsci/comments/rjc77/can_anyone_recommend_some_good_reading_for/) with some algorithms textbook references.\n\nMy recommendation is [Jeff Erickson's algorithms notes](http://compgeom.cs.uiuc.edu/~jeffe/teaching/algorithms/) (online and free).  http://www.amazon.com/gp/aw/d/0387948600 -- great book. I used it when studying for my Google interview. The first quarter or so is theory and the rest is just a BUNCH of algorithms, what problems they are good for, what their trade offs are, etc. I'd recommend it.      This is not as good as the books already suggested, but Wikipedia's list_of_algorithms is cool to browse through for an afternoon.  Function pointers are a way to define on run time what function to call.  \nLet's say you want to write a report based on the choice of a user. WriteReport is a function pointer, and WriteReportA and WriteReportB are two functions.  \nYou could do:\n\n    if (a) {\n        WriteReportA();\n    } else if (b) {\n        WriteReportB();\n    }\n\nOr you could do:\n\n    if(a) {\n        WriteReport = WriteReportA;\n    } else if (b) {\n        WriteReport = WriteReportB;\n    }\n    WriteReport();\n\nIf you used the first way and wanted to write the chosen report later again, you would need to perform the verification again. But the second way, if you want to write that report, all you have to do is call WriteReport() and it will call the function assigned to it previously.  \n\nNow about remembering the syntax, good luck with that, lad. You will travel the seven seas and conquer many kingdoms before using function pointers without some kind of reference. Yeah, it's more remembering the syntax than the concept. I've never really practiced using them so I can't remember it at all. \n\n    int *(*foo)(int)\n\nwat Yeah, it's more remembering the syntax than the concept. I've never really practiced using them so I can't remember it at all. \n\n    int *(*foo)(int)\n\nwat    No \"the art of programming\" - Donald Knuth anyone?     &gt; (function pointers, what?)\n\nThis is why everyone should learn some assembly language (preferably not x86). Although I agree that every novice programmer should learn a bit of assembly, I don't think it's a prerequisite for understanding function pointers. Just look at C, or any language which supports first-class functions. &gt; I don't think it's a prerequisite for understanding function pointers\n\nIt's not a prerequisite, but it makes it so obvious that you barely even need to think about them.\n\n&gt; Just look at C, or any language which supports first-class functions.\n\nI'm not sure what you're trying to say here. C does not have first class functions. I can't think of a language that does that also has function pointers, although I'd be happy to be pointed at one. &gt; C does not have first class functions\n\nTrue. Perhaps we could say it has function objects? But it doesn't! It has function pointers. Why try and dress that up any other way? &gt; I don't think it's a prerequisite for understanding function pointers\n\nIt's not a prerequisite, but it makes it so obvious that you barely even need to think about them.\n\n&gt; Just look at C, or any language which supports first-class functions.\n\nI'm not sure what you're trying to say here. C does not have first class functions. I can't think of a language that does that also has function pointers, although I'd be happy to be pointed at one. &gt; (function pointers, what?)\n\nThis is why everyone should learn some assembly language (preferably not x86). I understand the concept, that makes perfect sense, but I can never remember the funky syntax (at least for C++, and if I understand correctly Java doesn't support them unless you do crazy hacky stuff).",
    "url" : "http://www.reddit.com/r/compsci/comments/rrfws/want_to_learn_more_algorithms_dont_know_where_to/"
  }, {
    "id" : 15,
    "title" : "What is happening on a binary level when you write an \"if\" statement?",
    "snippet" : "I am looking around for information on how the original computer programs were written, like even before computers came out. I am very comfortable with binary numbers and a newbie programmer but cant for the life of me understand how binary is used to actually make things happen. \nI know that this probably has no practical use now but i am really curios.\n\n(Also any good books or websites that would have information about this)\n\nThanks  Binary has no semantics - no meaning - other than being a convenient encoding of symbols.  The \"magic\" is in how the hardware interprets those symbols.\n\n[The Elements of Computing Systems: Building a Modern Computer from First Principles](http://www.amazon.com/The-Elements-Computing-Systems-Principles/dp/026214087X) walks you all the way from logic gates all the way to a working computer system with an OS and a programming language. Cheapest used copy is $143. \n\nWas just thinking I'd probably be happy with a compsci text book from the 40s, do you know where I can get one of those? That edition is just out of print. Here's a link to a much cheaper copy.\n\nhttp://www.amazon.com/The-Elements-Computing-Systems-Principles/dp/0262640686/ref=tmm_pap_title_0 Thank you. I just ordered it. All but 3 of the chapters are available free online. http://www1.idc.ac.il/tecs/plan.html Cool, thanks.\n\nThough the book on the second link above is less than $20 and honestly, if someone puts out a good piece of work and charges a fair price, I'd dont mind paying.  The book is a piece of teaching Art. I didnt know anything about PC's on the level of Logic Gates, and after readin trough half the book I am basicly able to do a very primitive PC. Atleast the hardware part. After exams im probably going to finish reading it, as I want to continue in direction of microprocessors. Its truly worth the money. I just started reading it. Will they really walk me through building an actual computer? Yes, and at the end youll even have the power of making your own primitive computer with its own OS &amp; I think part of the book is about making Pong for the computer. The book wont walk you trought it, it will just give you the basics and anything you need to know, and then make you figure out how to make it. Which, in my point of view, is the best way of teaching. Learning trought making. All but 3 of the chapters are available free online. http://www1.idc.ac.il/tecs/plan.html Cheapest used copy is $143. \n\nWas just thinking I'd probably be happy with a compsci text book from the 40s, do you know where I can get one of those? AFAIK Computer science was not well enough developed in the 40's to have text books.  Why from the 40's?  A logic gate is a logic gate whether it's realized in a vacuum tube, discrete transistor, or an integrated circuit. I thought of that date because I thought that was when the first computers were made. Although I know that people were writing computer programs for years before that. It was in no way supposed to be definitive, sorry if it came across like that. \n\nWhat got me interested in this was a short conversation I overheard between a younger programmer and an older guy who apparently used to be a programmer. The younger fellow was describing quantum computing and how instead of each bit having two states it could now have 3. The older guy was blown away and after a few seconds of contemplation he said something to the effect of \"this is going to change everything. even basic If statements are going to be redone\" In this short conversation, the younger one provided an oversimplification of quantum computing. Three-state logic isn't terribly interesting and readily doable (just use two bits and throw one state away). Quantum computing is much more. With that kind of information, the older guy can not come to any useful conclusion. On top of that I'm pretty sure somewhere like Russia developed a 3-state transistor early on. It didn't catch on though. In this short conversation, the younger one provided an oversimplification of quantum computing. Three-state logic isn't terribly interesting and readily doable (just use two bits and throw one state away). Quantum computing is much more. With that kind of information, the older guy can not come to any useful conclusion. The conversation not with standing, I have read that programs are being written today for Quantum computers even though there is no way to really test them yet.\n\nI inferred from there that the basic underlying structure of Quantum computer language is going to be different than what  is available today.\n\nDid I read wrong, did I infer wrong or am I missing your point? I thought of that date because I thought that was when the first computers were made. Although I know that people were writing computer programs for years before that. It was in no way supposed to be definitive, sorry if it came across like that. \n\nWhat got me interested in this was a short conversation I overheard between a younger programmer and an older guy who apparently used to be a programmer. The younger fellow was describing quantum computing and how instead of each bit having two states it could now have 3. The older guy was blown away and after a few seconds of contemplation he said something to the effect of \"this is going to change everything. even basic If statements are going to be redone\"  i think that you are asking about the machine-level implementation, so i will speak to that.\n\nthe root of the conditional statement is a comparison. for example,  \n    \n    if ( a &gt; b )\n        a = 5\n    else\n        b = 6\n\nwill decide whether to execute \"a=5\" or \"b=6\" depending on the comparison \"a&gt;b\".\n\nto accomplish this, several things are needed. the first is the ability to select between two different branches of instruction. this amounts to changing the program counter to point to either the instruction to load 5 into a, or 6 into b. the next thing that is needed is the actual comparison of the two values stored in a and b. we need to load the value of a into a working register, load the value of b into a working register, and compare them. \n\na simple way to compute \"a&gt;b\" is to instead perform \"(a - b)&gt; 0\". this is done by taking the 2's compliment of b, adding it to a, and then determining if the sign bit is negative after dropping the excess bit (when necessary)\n\nat the binary (digital logic) level, the bits are determined via XOR, AND, and OR operations in some form of full adder (carry look-ahead, whatever). it's actually pretty easy. \n\nthe circuitry to build AND, OR, XOR, etc gate is made by daisy-chaining a couple of transistors such that the output voltage of a gate reads either logic high or logic low. \n\nboom. there you go. btw, a cheaper book would be \"Computer Engineering: Hardware Design\" by Morris Mano.  it is usually less then 5 bucks, and it is the book i use to teach my digital logic class. &gt;a simple way to compute \"a&gt;b\" is to instead perform \"(a - b)&gt; 0\". this is done by taking the 2's compliment of b, adding it to a, and then determining if the sign bit is negative after dropping the excess bit (when necessary)\n\nI still dont understand how you can get a conditional statement out of this? At the end of the day you still have to say\n \n    if (bit is positive) \n        a=5\n    else\n        a=6\nhow can you represent a condition with pure math?\n\n(a shaky solution that i can think of is somehow encapsulating the\n    a=5\ntogether with the conditional and adding some sort of pointer in that encapsulation to skip over whatever register contains the \"else\" value.\nThis way if the condition is true, a=5 will run and b=6 will be skipped. But if the condition is false then the entire encapsulation will be skipped and the computer will continue on with the next working registry (which in this case we'll have to assure is b=6)\n\n\n&gt;at the binary (digital logic) level, the bits are determined via XOR, AND, and OR operations in some form of full adder (carry look-ahead, whatever). it's actually pretty easy.\nthe circuitry to build AND, OR, XOR, etc gate is made by daisy-chaining a couple of transistors such that the output voltage of a gate reads either logic high or logic low.\n\nSorry, I dont follow. Care to elaborate?\n\nThanks for bearing with me\n It depends on the processor - each has its own instruction set.  For a simple example we'll convert \n\nif (a != 0)\n  a = 5\nelse \n  a = 6\n\nMost CPUs have a TEST instruction which says set a bunch of bits in a flag register based on the contents of a register.  One of the flags is a zero flag.\n\nIf a is in memory address 9999 then the code would look something like\n    \n    1000 MOV R1, [9999]\n    1001 TEST R1\n    1002 JMPZ 1005\n    1003 MOV [9999], 5\n    1004 JMP 1006\n    1005 MOV [9999], 6\n    1006 .. rest of program\n    \nAn instruction pointer (IP) points to the next instruction - in this case 1000.  The first instruction says to get the contents of a and put it into the register R1 (registers are local data to the CPU that can be accessed quickly).  It does a TEST instruction which will set the zero bit in a flags register. The JMPZ instruction says to set the instruction pointer to 1005 if the zero bit in the flag register is set.  If it is not set (i.e. the value was not zero) it will execute the next instruction at 1003 which moves 5 into the memory location of a.  It then does a JMP to 1006 - set the instruction pointer to 1006.  The 1003 instruction only gets executed if the value for a is non zero and the 1005 instruction only gets executed when the value for a is zero.\n\nBut this is extremely simplified.  You also have compiled vs interpreted languages vs virtual machine code which all behave completely differently, yada, yada, yada\n I like this, thank you.\n\nStill dont understand though\n\n&gt;The JMPZ instruction says to set the instruction pointer to 1005 if the zero bit in the flag register is set.\n\nSo essentially you still need a conditional. Your breaking down an \n\n    if...else...\n\ninto\n\n    if...\n    if...\n\nI still dont understand how you make a conditional with numbers. \n The flag is used as a selector in a [multiplexer](http://en.wikipedia.org/wiki/Multiplexer), that chooses whether to pick 1005 or 1006, the input signals of the multiplexer. I like this, thank you.\n\nStill dont understand though\n\n&gt;The JMPZ instruction says to set the instruction pointer to 1005 if the zero bit in the flag register is set.\n\nSo essentially you still need a conditional. Your breaking down an \n\n    if...else...\n\ninto\n\n    if...\n    if...\n\nI still dont understand how you make a conditional with numbers. \n You might view it as if the CPU has a very basic way of doing an `if`. For our computers to work like they do, they need *at least* one, basic way of doing an `if`.\n\nOnce you have that *one, puny, simple, basic, tiny, little* way of doing an `if`, you can extend it to all you know from C, Python and the high level languages.\n\nFortunately for us, there are ways to implement a basic `if` in hardware, using electrical signals. The numbers themselves don't do this, but when we stick them into logic gates, different numbers comes out.\n\n----\n\nWhat fascinates me, however, is a branch of mathematics called *lambda calculus*. The building block of lambda calculus is function application, and with the help of that you can construct a lot of conditionals. It's a little off-topic here, but it really fascinates me in its simplicity, so you might want to look it up when you've got spare times. &gt;Once you have that one, puny, simple, basic, tiny, little way of doing an if, you can extend it to all you know from C, Python and the high level languages.\n\nthis! \n\nI will definitely look into lambda calculus. I took a semester of calculus in college and did very well, hopefully i'll be able to remember what i learned and pick up from there. Lambda calculus has nothing to do with differential/integral calculus. hmmmm..ok\n\nI'll check it out anyway. \nthanks though I like this, thank you.\n\nStill dont understand though\n\n&gt;The JMPZ instruction says to set the instruction pointer to 1005 if the zero bit in the flag register is set.\n\nSo essentially you still need a conditional. Your breaking down an \n\n    if...else...\n\ninto\n\n    if...\n    if...\n\nI still dont understand how you make a conditional with numbers. \n I like this, thank you.\n\nStill dont understand though\n\n&gt;The JMPZ instruction says to set the instruction pointer to 1005 if the zero bit in the flag register is set.\n\nSo essentially you still need a conditional. Your breaking down an \n\n    if...else...\n\ninto\n\n    if...\n    if...\n\nI still dont understand how you make a conditional with numbers. \n I like this, thank you.\n\nStill dont understand though\n\n&gt;The JMPZ instruction says to set the instruction pointer to 1005 if the zero bit in the flag register is set.\n\nSo essentially you still need a conditional. Your breaking down an \n\n    if...else...\n\ninto\n\n    if...\n    if...\n\nI still dont understand how you make a conditional with numbers. \n   This book(Code: This hidden language of computer hardware and software): http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?ie=UTF8&amp;qid=1333490779&amp;sr=8-1\n\nIt explains it all! The title makes it sound like it is about code, but it is really about how a computer works(code is of course a part of it). It is very easy to read and does not really require any prior knowledge, it actually starts by explaining how a flashlight works and builds on that.\n\nI simply can't describe how awesome it is, you should really read it! This book(Code: This hidden language of computer hardware and software): http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?ie=UTF8&amp;qid=1333490779&amp;sr=8-1\n\nIt explains it all! The title makes it sound like it is about code, but it is really about how a computer works(code is of course a part of it). It is very easy to read and does not really require any prior knowledge, it actually starts by explaining how a flashlight works and builds on that.\n\nI simply can't describe how awesome it is, you should really read it!    Have a look at http://cs.otago.ac.nz/cosc243. The lecturer has put up his slides, and takes you from first principles (logic gates etc.) through to a fully functioning CPU.    On an assembly level, an if is translated to something like a \"Branch if equal\" (BEQ), if the two variables (registers) have equivalent values, it jumps to a different line (assembly instruction). Something like if (a==b) would require a BNE (branch if not equal) so it jumps AFTER the close brace if they are not equal.\n\nFor more information on this, learn any assembly language.\n\nOn a machine level, there is a register that holds the current \"line\" in the code (read: instruction), on a BEQ (or any branch), it changes the PC register (the register that holds the line, called a program counter) to the destination address.\n\nThe PC is important because it always points to the next line of code (instruction). The way all instructions work is, simply, load the instruction pointed at by PC, increment PC, execute instruction. If the executed instruction is a branch, then it might change what the PC points to (or a jump, which is equivalent to a goto, or a ret, which is similar to a return, etc), then, when it repeats the cycle, it just loads the address you changed it to point to.\n\nIf you wish for me to elaborate a little (I tried to keep things short), tell me and I will. Break? No... it's Branch. Branch not equal. On an assembly level, an if is translated to something like a \"Branch if equal\" (BEQ), if the two variables (registers) have equivalent values, it jumps to a different line (assembly instruction). Something like if (a==b) would require a BNE (branch if not equal) so it jumps AFTER the close brace if they are not equal.\n\nFor more information on this, learn any assembly language.\n\nOn a machine level, there is a register that holds the current \"line\" in the code (read: instruction), on a BEQ (or any branch), it changes the PC register (the register that holds the line, called a program counter) to the destination address.\n\nThe PC is important because it always points to the next line of code (instruction). The way all instructions work is, simply, load the instruction pointed at by PC, increment PC, execute instruction. If the executed instruction is a branch, then it might change what the PC points to (or a jump, which is equivalent to a goto, or a ret, which is similar to a return, etc), then, when it repeats the cycle, it just loads the address you changed it to point to.\n\nIf you wish for me to elaborate a little (I tried to keep things short), tell me and I will. On an assembly level, an if is translated to something like a \"Branch if equal\" (BEQ), if the two variables (registers) have equivalent values, it jumps to a different line (assembly instruction). Something like if (a==b) would require a BNE (branch if not equal) so it jumps AFTER the close brace if they are not equal.\n\nFor more information on this, learn any assembly language.\n\nOn a machine level, there is a register that holds the current \"line\" in the code (read: instruction), on a BEQ (or any branch), it changes the PC register (the register that holds the line, called a program counter) to the destination address.\n\nThe PC is important because it always points to the next line of code (instruction). The way all instructions work is, simply, load the instruction pointed at by PC, increment PC, execute instruction. If the executed instruction is a branch, then it might change what the PC points to (or a jump, which is equivalent to a goto, or a ret, which is similar to a return, etc), then, when it repeats the cycle, it just loads the address you changed it to point to.\n\nIf you wish for me to elaborate a little (I tried to keep things short), tell me and I will. This is a really good answer.\n\nJust to connect the dots, assembly level is two steps above binary:\n\n* Level 2: Assembly\n\n* Level 1: Hex\n\n* Level 0: Binary \n\nThe assembly instructions are converted into hex, which are then converted into binary. This is all done by the compiler.      ",
    "url" : "http://www.reddit.com/r/compsci/comments/rrst5/what_is_happening_on_a_binary_level_when_you/"
  }, {
    "id" : 16,
    "title" : "I'm interested in Natural language processing, and relationship building systems. ",
    "snippet" : "I'm interested in a building a system that can do natural language processing, and then build relationships on the data that it gathers. Maybe scanning websites looking for information, forming relationships from the data, and then postulating ideas from those relationships. I've read about the NLP libraries from Stanford, but I'm not sure if there are other components that would help with the relationship building part. I was thinking some sort of logic programming (Prolog)? Any experience you have I would be interested to hear about it. Thanks      ",
    "url" : "http://www.reddit.com/r/compsci/comments/rsil9/im_interested_in_natural_language_processing_and/"
  }, {
    "id" : 17,
    "title" : "Automata Study Tips/Books",
    "snippet" : "Studying Automata right now and it's kicking my butt.  Doing PDAs and how to generate context free grammar (and vice versa) and I'm looking for an alternative explanation to *Introduction to Languages and the Theory of Computation*, John C. Martin, which is rather dry and formal.\n\nI found a copy of Aho's *Principles of Compiler Design* (**The Green Dragon Book**) in the library and am currently reading through that now.  Even though a lot of practical examples are in Pascal, I seem to find it much easier to follow, but the sections on Languages and Grammars are quite brief.\n\nI'm not the best student ever.  Seeing the same thing from multiple angles tends to help me understand it better, and my memory's not great either so understanding is better than rote memorization of theorems.  And Martin's is REALLY theorem heavy.  Some great lectures here: http://www.aduni.org/courses/theory/index.php?view=cw\n\nHave you tried Sipser's book? It's great and very readable: http://www.amazon.co.uk/Introduction-Theory-Computation-Michael-Sipser/dp/0619217642\n\nGood luck !",
    "url" : "http://www.reddit.com/r/compsci/comments/rrydu/automata_study_tipsbooks/"
  }, {
    "id" : 18,
    "title" : "What are good math courses for a compsci student?",
    "snippet" : "I am a first year computer science student who wants to be a software engineer in the future. Im doing pretty well in most of my intro-level classes, but have always felt as though im lacking when it comes to mathematics. What are some good math courses that supplement compsci well (if any)?  Discrete Math, Linear Algebra, Calculus, Differential Equations, ... ALL OF THEM. You said all of them but you left out Prob &amp; Statistics from the named ones.  Very important. Also numerical analysis was very valuable for understanding how functions such as sin(x) and others are implemented and evaluated. Number theory.  Many of the algorithms and theorems taught in number theory are much easier and more efficient to implement on a computer than the typical or intuitive methods. You said all of them but you left out Prob &amp; Statistics from the named ones.  Very important. Also numerical analysis was very valuable for understanding how functions such as sin(x) and others are implemented and evaluated. Discrete Math, Linear Algebra, Calculus, Differential Equations, ... ALL OF THEM. I really don't think Differential Equations are necessary. Calculus of course, but DiffiEQs? Cmon.\n\nOP would be better off taking more Linear Algebra or Discrete Math. I really don't think Differential Equations are necessary. Calculus of course, but DiffiEQs? Cmon.\n\nOP would be better off taking more Linear Algebra or Discrete Math. I agree, but I'd go one step further. Why does a future software engineer need calculus or linear algebra?\n\nDiscrete math is useful because it teaches you how to count, which is pretty valuable for understanding computational complexity. \n\nLinear algebra and calculus are useful for scientific computing, but how often do cs students actually end up using those tools? I agree, but I'd go one step further. Why does a future software engineer need calculus or linear algebra?\n\nDiscrete math is useful because it teaches you how to count, which is pretty valuable for understanding computational complexity. \n\nLinear algebra and calculus are useful for scientific computing, but how often do cs students actually end up using those tools? Linear algebra is used extensively in a wide variety of computing problems, including search, graphics, machine learning, natural language processing, finance, etc. Calculus is useful as a mathematical and mental tool for just about anything, if you ask me.\n\nOf course, you can be a programmer without using much math at all. I've encountered programmers who know how to code within complex frameworks, but don't know you can nest for() loops, or how to do basic complexity reduction. &gt; Calculus is useful as a mathematical and mental tool for just about anything, if you ask me.\n\nYou can say the same about any field of math. Calculus is just the traditional introduction to mathematical thought, a position it has no inherent claim to. Linear algebra is used extensively in a wide variety of computing problems, including search, graphics, machine learning, natural language processing, finance, etc. Calculus is useful as a mathematical and mental tool for just about anything, if you ask me.\n\nOf course, you can be a programmer without using much math at all. I've encountered programmers who know how to code within complex frameworks, but don't know you can nest for() loops, or how to do basic complexity reduction. good point but bad example, as complexity reduction and un-nesting for loops arent may be completely unrelated to math. They are intrinsically related to math... So when you're un-nesting loops, are you performing a mathematical calculation with pen and paper, or just re-organizing your code in a different architecture that doesnt require an inner loop?  I dont need to bust outba calculator just to apply object-oriented or functional programming architectures, or refactoring to Separate the Concerns, or apply DRY. good point but bad example, as complexity reduction and un-nesting for loops arent may be completely unrelated to math. I don't understand. In my Design and Analysis of Algorithms class, we had to calculate the efficiency of non-recursive algorithms, which did require math.  I agree, but I'd go one step further. Why does a future software engineer need calculus or linear algebra?\n\nDiscrete math is useful because it teaches you how to count, which is pretty valuable for understanding computational complexity. \n\nLinear algebra and calculus are useful for scientific computing, but how often do cs students actually end up using those tools? I agree, but I'd go one step further. Why does a future software engineer need calculus or linear algebra?\n\nDiscrete math is useful because it teaches you how to count, which is pretty valuable for understanding computational complexity. \n\nLinear algebra and calculus are useful for scientific computing, but how often do cs students actually end up using those tools? I really don't think Differential Equations are necessary. Calculus of course, but DiffiEQs? Cmon.\n\nOP would be better off taking more Linear Algebra or Discrete Math. I've made several models over my career and my differential equations experience definitely lent itself to those tasks.  Maybe DEQs isn't useful if you're just moving data in and out of a database or creating simple forms-based applications, but I've found that the more math you know, the more interesting the work is that you get to do.  Learn all the math you can. What kinds of kinds of applications did you work on that required diff 'eqs?  Scientific?  Physics based? I really don't think Differential Equations are necessary. Calculus of course, but DiffiEQs? Cmon.\n\nOP would be better off taking more Linear Algebra or Discrete Math. I really don't think Differential Equations are necessary. Calculus of course, but DiffiEQs? Cmon.\n\nOP would be better off taking more Linear Algebra or Discrete Math. Discrete Math, Linear Algebra, Calculus, Differential Equations, ... ALL OF THEM. Also if you plan on studying theoretical comp sci. a math course which teaches you how to write good proofs is very helpful. At my school the course was called Math Reasoning. I took a course like that as an elective. It turned out to be more useful and enlightening than any of the required math courses. Discrete Math, Linear Algebra, Calculus, Differential Equations, ... ALL OF THEM. Discrete Math, Linear Algebra, Calculus, Differential Equations, ... ALL OF THEM. Discrete Math, Linear Algebra, Calculus, Differential Equations, ... ALL OF THEM. These will all be required if the program is half decent. This question is unnecessary.  No, it's not unnecessary.  Some people are limited in where they can go, like I was.\n\nMy comp sci program required discrete structures, calc 1 and 2, and stats. also,they just changed the requirement eliminating the requirement for linear algebra.\n\nI took linear algebra anyways and recommended as many as my peers do so, cuz it was awesome. Discrete Math, Linear Algebra, Calculus, Differential Equations, ... ALL OF THEM. Calculus is of secondary importance for the core of CS, but of importance for some applications of CS. Of course, just about everything is of importance for some applications of CS. Calculus gives you the mathematical maturity to handle math which is useful for CS.   If you are thinking of only one course then go for:\n\n**Graph Theory**\n\nNot only is it fun; its applications are something you come across time after time in Software development.\nQuestions from allocation of resources, the best order to do things, matching problem, etc...etc..\n\nNext in order would be: \n\n* Discrete Mathematics, \n\n* Probability and Statistics.\n\nBut do not forget to take a basic class in **Algorithms** at the very least to have a general idea of mathematical methods to use.\n\n    Combinatorics is important in some applications.  So is linear algebra. I would say combinatorics is the most important if anyone wants to go anywhere near Theoretical CS. I would say combinatorics is the most important if anyone wants to go anywhere near Theoretical CS.  In addition to those already mentioned:\n\nset theory, category theory, logic, topology, finite model theory, theory of computation, probability, ... these are great math courses but not sure they would all take the same priority as say, statistics, in the limited amount of college time one is afforded. In addition to those already mentioned:\n\nset theory, category theory, logic, topology, finite model theory, theory of computation, probability, ... Can you provide examples of applications of topology or category theory for Computer Science? Category theory has innumerable applications in Programming Language research.\n\nA question in algebraic topology was recently used as an approach to solve P vs NP. This is terrible advice unless he is planning to become an academic after university, and even then it's questionable.\n\n&gt; Category theory has innumerable applications in Programming Language research.\n\nHow many software engineers work in programming language research?\n\n&gt; A question in algebraic topology was recently used as an approach to solve P vs NP.\n\nYeah, because software engineers have to do that all the time... &gt; This is terrible advice unless he is planning to become an academic after university, and even then it's questionable.\n\nIt's not questionable.  If you want to be a theoretician, you better learn category theory.\n\nEDIT: actually, I may have misread you.  I agree that it's terrible advice to eschew more foundational courses and jump right into the likes of category theory and universal algebra.  The whole beauty of category theory is that is provides a unified language for talking about other branches of mathematics.  Naturally, without knowledge of branches of more concrete mathematics, the point of category theory will be lost. &gt; If you want to be a theoretician, you better learn category theory.\n\nWhere did the OP say he wanted to be a theoretician?  He said he wanted to be a software engineer.  Theoreticians are less than 0.1% of software engineers. He didn't.  You presupposed it in your use of \"if he wants to be an academic ...\".  I just ran with that. Ok, although academic != theoretician All the best ones are :p &gt; This is terrible advice unless he is planning to become an academic after university, and even then it's questionable.\n\nIt's not questionable.  If you want to be a theoretician, you better learn category theory.\n\nEDIT: actually, I may have misread you.  I agree that it's terrible advice to eschew more foundational courses and jump right into the likes of category theory and universal algebra.  The whole beauty of category theory is that is provides a unified language for talking about other branches of mathematics.  Naturally, without knowledge of branches of more concrete mathematics, the point of category theory will be lost. Category theory has innumerable applications in Programming Language research.\n\nA question in algebraic topology was recently used as an approach to solve P vs NP. &gt;A question in algebraic topology was recently used as an approach to solve P vs NP.\n\nSo which one is it? http://www.cse.buffalo.edu/~regan/papers/pdf/Reg02MSFD.pdf Category theory has innumerable applications in Programming Language research.\n\nA question in algebraic topology was recently used as an approach to solve P vs NP. &gt;Category theory has innumerable applications in Programming Language research.\n\n&gt;A question in algebraic topology was recently used as an approach to solve P vs NP.\n\nCan you list some notable results? I went through a relatively theory heavy undergrad and graduate computer science program and haven't seen any applications of category theory (or maybe I didn't know it was from that field). Also, I hope you aren't saying that P =? NP is solved with your latter statement. Pretty much all of theoretical computer science is becoming \"categorified\", as its a universal language through which we can talk to each other.\n\n(I'm a theoretical computer scientist, and I'd estimate 50% of all talks I go to assume a working knowledge of category theory.) Is this in Programming Languages/ Type Theory? Are there non-trivial applications in, say, complexity theory? I certainly agree with the ''universal language'' bit, as an algebraist. Pretty much all of theoretical computer science is becoming \"categorified\", as its a universal language through which we can talk to each other.\n\n(I'm a theoretical computer scientist, and I'd estimate 50% of all talks I go to assume a working knowledge of category theory.) It makes sense; it lets us use results from Mathematics. I just defended a dissertation that was all about group theory. Interesting.  What subject?  The only serious use of group theory I've come across in theoretical CS is symmetry reduction for controlling state space explosions in model checkers. &gt;Category theory has innumerable applications in Programming Language research.\n\n&gt;A question in algebraic topology was recently used as an approach to solve P vs NP.\n\nCan you list some notable results? I went through a relatively theory heavy undergrad and graduate computer science program and haven't seen any applications of category theory (or maybe I didn't know it was from that field). Also, I hope you aren't saying that P =? NP is solved with your latter statement. Monads are a category theoretic concept, and are deeply tied to Haskell (note ghc won the SIGPLAN Programming Languages Software Award this last year).\n\nIndeed, it's used often in most ML, detailed here: http://www.cs.man.ac.uk/~david/categories/book/book.pdf\n\nAlso, I posted this above, but http://www.cse.buffalo.edu/~regan/papers/pdf/Reg02MSFD.pdf is the approach using the Zariski topology.  Certainly P vs NP isn't solved, but it's been used as an approach, and some person who was pretty notable (don't know his name, a quick google will find it) thought he got it using this approach, but a hole was found. Category theory has innumerable applications in Programming Language research.\n\nA question in algebraic topology was recently used as an approach to solve P vs NP.    Don't forget optimization courses.  Take a class on linear programming if you can.  This opens the door to all kinds of things... semidefinite optimization, combinatorial optimization, operations research.  A good chunk of CS-types end up moving in these directions in grad studies and industry.\n\nIn general it's useful to have a solid background in rigourous mathematics... stuff like real analysis or ring/field theory can help a lot in teaching you what to learn.\n\nProbability and combinatorics are also extremely important.\n\nThe stuff I would worry less about are things like differential equations, etc. Don't forget optimization courses.  Take a class on linear programming if you can.  This opens the door to all kinds of things... semidefinite optimization, combinatorial optimization, operations research.  A good chunk of CS-types end up moving in these directions in grad studies and industry.\n\nIn general it's useful to have a solid background in rigourous mathematics... stuff like real analysis or ring/field theory can help a lot in teaching you what to learn.\n\nProbability and combinatorics are also extremely important.\n\nThe stuff I would worry less about are things like differential equations, etc.  Take the ones that are the most interesting to you. Prereqs should cover the bare necessities. You can get a job with about as much or as little math as you like. The more background you have, the more doors open up. You'll want an interesting job, so learn interesting math. Find the jobs that you want and figure out what they require. \n\nMore concretely, take the more theoretically-oriented classes (abstract algebra, analysis, discrete, etc.). That way, you don't memorize a bunch of formulas you may/may not use, but you have definitely learned how to think well.        what exactly is \"linear algebra\". If i passed calc 2 would it be unbelievably simple, or would it actually teach me something. what exactly is \"linear algebra\". If i passed calc 2 would it be unbelievably simple, or would it actually teach me something. FYI, you're getting some terrible advice here, people are throwing out all kinds of crap that you don't need to know - they're basically doing their best to intimidate you.\n\nI graduated Compsci with honors from one of the top courses in the UK having not really learned much more math than I went into university with (honors math from an Irish secondary school).  I've had a a very successful career as a software engineer since (run my own companies, raised millions in venture capital, etc).\n\nThe only area of math I wished I had a better understanding of was statistics and probability, and that is because I do a lot of work in machine learning and predictive analytics, but I've picked up what I need along the way. It depends on what he wants to do. Research is a different thing than writing code. what exactly is \"linear algebra\". If i passed calc 2 would it be unbelievably simple, or would it actually teach me something. what exactly is \"linear algebra\". If i passed calc 2 would it be unbelievably simple, or would it actually teach me something. what exactly is \"linear algebra\". If i passed calc 2 would it be unbelievably simple, or would it actually teach me something.  Khanacademy.org     All of them. What a completely unhelpful answer.    i'm always confused as why i see this kind of question so often. don't people's CS curricula already have a set of required math classes? Yes they do. But there are two factors to such questions:\n\n* Most people are afraid/unsure of math, and hence are looking for guidance on what will align best with their career objectives.\n\n* Because Computer Science is basically a math field -- most curricula in United States specify a set of mandatory electives of basics -- which also tend to have a lot of choice in them. And then most advanced Computer Science courses are out there that are taken either by a niche research group or people who go in with a clear objective.\n\nSo understandably things get a little confused for beginners. The beauty is that if a school's curriculum is properly arranged, by the time the student graduates they end up realizing how all the different fields interconnect with each other and they are offshoots of just basic branches of mathematics with focus on different parts.   Is Discrete Math something that's required in order to succeed in theoretical CS classes, or does it actually make one a better programmer? It will definitely improve your programming. First and most obvious example is sets - they make performing the operations they were designed for much easier. Such as;\n\n* remove all duplicates from a collection (by definition)\n* Get all items that occur in two collections (intersection)\n* Test if all elements in B occur in A (subset and superset)\n\nEven if you don't use it in your day to day life, it's not like it's worth not knowing :) Is Discrete Math something that's required in order to succeed in theoretical CS classes, or does it actually make one a better programmer?    It depends entirely on what you want to do after your degree.  Mathematics content of the various branches of computer science is extremely subject specific.  I'm an academic that hasn't touched an integral, or solved a differential equation, in years.\n\nSo, what do you plan on doing after your degree? I simply want to design software, I couldn't tell you anything more specific. My dream one day is to get into the video-game design business, but only after a good amount of experience elsewhere.   BIG O MAKES MY DICK ROCK HARD",
    "url" : "http://www.reddit.com/r/compsci/comments/rq60i/what_are_good_math_courses_for_a_compsci_student/"
  }, {
    "id" : 19,
    "title" : "How does parallel programming affect an algorithm's time complexity?",
    "snippet" : "I just started reading a [book](http://www.amazon.com/Introduction-Parallel-Programming-Peter-Pacheco/dp/0123742609/ref=sr_1_1?ie=UTF8&amp;qid=1333417731&amp;sr=8-1) on parallel programming, and I immediately began to wonder how it affects time complexity.\n\nI understand that theory and programming are not the same, but time complexity seems to rely on the notion that the algorithm is executed serially—will the mathematics to describe algorithms need to change to accomodate this?\n\nFor example, a binary search algorithm could be written such that each element in the set could be assigned to CPU core (or something similar). Then, wouldn't the algorithm essentially be constant time, O(1), instead of O(log n).     \n      \nThanks for any insightful response.    You'd be interested, then, in the notion of splitting up time complexity into work and span.\n\nWork is the traditional time complexity that everyone is used to: the order of magnitude of the number of elementary operations an algorithm will run in. Binary search, for example, will take an order of O(log n) comparisons.\n\nSpan is a more novel concept. This is the longest chain of dependencies that an algorithm has, also described in Big O notation. To understand this, let us consider the binary search example: The algorithm for binary search is essentially to check the middle element each tick, narrow the search down by half, and repeat. Unfortunately for parallelism, this is bad: each tick of the algorithm **depends** on the prior tick. Therefore, the longest chain of dependencies is O(log n). Binary search, as far as I know, is not very parallelizable. \n\nThe algorithm OP describes is, however, a legitimate algorithm. It is not binary search, but a linear search. It will have O(n) work, but its span is O(1) as OP described. This is because checking each element can be checked independently. In other words, we can check if each element without waiting for a prior step to finish. Therefore, the longest chain of dependencies or span is O(1). \n\nIn summary, yes, there is analysis to describe parallel algorithms. I'm not an expert, so I'm not sure about the real world implications of parallelism. I do know that, in real life, there are many overheads to achieving parallelism in addition to the fact that there are a finite number of processors.\n\n[Read more here.](http://software.intel.com/en-us/articles/what-the-is-parallelism-anyhow-1/) I didn't find this link or anything really until after I wrote this. It looks like a better and more thorough explanation. You'd be interested, then, in the notion of splitting up time complexity into work and span.\n\nWork is the traditional time complexity that everyone is used to: the order of magnitude of the number of elementary operations an algorithm will run in. Binary search, for example, will take an order of O(log n) comparisons.\n\nSpan is a more novel concept. This is the longest chain of dependencies that an algorithm has, also described in Big O notation. To understand this, let us consider the binary search example: The algorithm for binary search is essentially to check the middle element each tick, narrow the search down by half, and repeat. Unfortunately for parallelism, this is bad: each tick of the algorithm **depends** on the prior tick. Therefore, the longest chain of dependencies is O(log n). Binary search, as far as I know, is not very parallelizable. \n\nThe algorithm OP describes is, however, a legitimate algorithm. It is not binary search, but a linear search. It will have O(n) work, but its span is O(1) as OP described. This is because checking each element can be checked independently. In other words, we can check if each element without waiting for a prior step to finish. Therefore, the longest chain of dependencies or span is O(1). \n\nIn summary, yes, there is analysis to describe parallel algorithms. I'm not an expert, so I'm not sure about the real world implications of parallelism. I do know that, in real life, there are many overheads to achieving parallelism in addition to the fact that there are a finite number of processors.\n\n[Read more here.](http://software.intel.com/en-us/articles/what-the-is-parallelism-anyhow-1/) I didn't find this link or anything really until after I wrote this. It looks like a better and more thorough explanation.  Time complexity is not necessarily related to time of execution (e.g. insertion sort can be faster than quick sort if the array is small enough, even though quick sort has a lower time complexity.) Time complexity depends on the amount of **work** that needs to be done. Even if you had enough hardware to compute the answer in a single clock cycle, the amount of work (comparisons) needed to be done is still log(n). So no, parallel programming does not affect an algorithm's time complexity. Time complexity is not necessarily related to time of execution (e.g. insertion sort can be faster than quick sort if the array is small enough, even though quick sort has a lower time complexity.) Time complexity depends on the amount of **work** that needs to be done. Even if you had enough hardware to compute the answer in a single clock cycle, the amount of work (comparisons) needed to be done is still log(n). So no, parallel programming does not affect an algorithm's time complexity. Hm, I like this idea of time complexity representing the amount of work being done, but I can't find a definition that supports it. The gist seems to be the amount of time taken by an algorithm based on a given input size.     Look at the definition of big-O. f(n) \\in O(g(n)) iff. exists c, n_0 s.t. for all n &gt;= n_0 f(n) &lt;= c*g(n). Parallelizing an algorithm is roughly equivalent to reducing that constant c to c/d where d is the number of cores/execution-units. The complexity remains O(g(n)). I dont think that is necessarily true. Some algorithms have super-linear speedup when parallelism is introduced (see parallel backtracking algorithms).  ",
    "url" : "http://www.reddit.com/r/compsci/comments/rqim5/how_does_parallel_programming_affect_an/"
  }, {
    "id" : 20,
    "title" : "Blog: Greedy/Lagrangian-relaxation algorithms using method of conditional probabilities",
    "snippet" : " ",
    "url" : "http://greedyalgs.info/blog/about/"
  }, {
    "id" : 21,
    "title" : "Mind vs. Machine: Interesting article about Turing Tests and what it means to be human.",
    "snippet" : " ",
    "url" : "http://www.theatlantic.com/magazine/archive/2011/03/mind-vs-machine/8386/1/"
  }, {
    "id" : 22,
    "title" : "Ping Filesystem",
    "snippet" : "   Q: Where's our data?\n\nA: In the cloud.  what am I reading what am I reading I felt a great disturbance in the Force, as if millions of routers suddenly cried out in terror and were suddenly silenced. what am I reading   Wait, this site was actually black listed from Google for its content?  That's both awesome and frightening! [Blackholed](http://en.wikipedia.org/wiki/Black_hole_%28networking%29) ≠ blacklisted.\n\nUsing PingFS against a server or network is essentially [ping flooding](http://en.wikipedia.org/wiki/Ping_flood) it, which is likely to get your IP address blackholed as a defensive measure. Wait, this site was actually black listed from Google for its content?  That's both awesome and frightening!  How do I use this? Nothing is explained :&lt; How do I use this? Nothing is explained :&lt; * mkdir testdir\n* python ping_fuse.py testdir/\n* # play with your new virtual disk [mounted at testdir] ImportError: No module named ping_raid\n Fixed. Ping_Raid still on site, but commented out. Now I get a different error. Posted here: http://pastebin.com/XtXu3NmN The exception you're seeing was verifying the ID pingfs received back. That suggests the field used for IDs isn't being preserved. Some routers and firewalls will do that. Shoot me an email and I'll help troubleshoot. I got it working on another box (Archlinux). Thanks.\nI did have one question: By spoofing the senders ip, could it be possible to sends files by \"bouncing\" it off a server? Yes, but that might accidentally a covert channel. You should make it. * mkdir testdir\n* python ping_fuse.py testdir/\n* # play with your new virtual disk [mounted at testdir] ",
    "url" : "http://www.blindcode.net"
  }, {
    "id" : 23,
    "title" : "Difficulty with Compsci concepts - Lower Bound Theory and Oracles and Adversary Arguments",
    "snippet" : "The Algorithm concepts Lower Bound Theory and Oracles and Adversary Arguments find it difficult to digest.\n\nCould anyone point to some good materials.",
    "url" : "http://www.reddit.com/r/compsci/comments/rp4me/difficulty_with_compsci_concepts_lower_bound/"
  }, {
    "id" : 24,
    "title" : "Rule-Based Programming in Interactive Fiction",
    "snippet" : "   What's wrong with Prolog? IF is much weaker; this LtU discussion talks about whether or not the language does resolution (http://lambda-the-ultimate.org/node/1554). Arguably this makes it easier to understand; general logic programming is pretty stiff stuff.",
    "url" : "http://eblong.com/zarf/essays/rule-based-if/"
  }, {
    "id" : 25,
    "title" : "Esoteric queue scheduling disciplines",
    "url" : "http://blog.databigbang.com/esoteric-queue-scheduling-disciplines/"
  }, {
    "id" : 26,
    "title" : "The Gzip Homorphism. Why does it work?",
    "snippet" : "I am working on a small project, and stumbled into something\ninteresting.\n\nUsing ++ to describe file concatenation.\nLet a.txt and b.txt be files, and g(x) be a gzip compression \nfunction such that g : File -&gt; File\n\nFor the few cases I tried gzip seems to be homomorphic; i.e.:\n\ng(a.txt ++ b.txt) = g(a.txt) ++ g(b.txt)      If that were always the case, wouldn't gzip be unable to compress at all? Consider \"a.txt\", a 1-byte file containing the letter 'a'. Clearly g(a.txt) must use at least one byte. Now if g(a.txt++a.txt) == g(a.txt) ++ g(a.txt), then by induction gzip can never compress a string of \"a\"s.",
    "url" : "http://www.reddit.com/r/compsci/comments/rmrlt/the_gzip_homorphism_why_does_it_work/"
  }, {
    "id" : 27,
    "title" : "How feasible would it be to write a basic interactive AI chess game for my AI class's final project? I have a little over a month to do it. Any other suggestions?",
    "snippet" : "I would really love to do this, but I fear that it would be too involved. I was also considering doing the same for a connect-4 game. Anyone have any thoughts on how involved either of these would be to implement? Or if anyone has any other ideas for what I could do, I would really appreciate it.  It is definitely entirely feasible. A minimax search with alpha-beta pruning and some nifty heuristics can be written without *too* much effort. If you search a bit on the web, you'll find multiple reports from students at some university that has a chess AI tournament; they are a good introductory for some of the techniques involved. There is also a chess programming wiki somewhere, with excellent information in the area. It is definitely entirely feasible. A minimax search with alpha-beta pruning and some nifty heuristics can be written without *too* much effort. If you search a bit on the web, you'll find multiple reports from students at some university that has a chess AI tournament; they are a good introductory for some of the techniques involved. There is also a chess programming wiki somewhere, with excellent information in the area. We had 2 weeks to write an AI that checked ~ 8 moves deep for a pretty basic programming course, I don't imagine this would be tough for you to get done at all.  It is definitely entirely feasible. A minimax search with alpha-beta pruning and some nifty heuristics can be written without *too* much effort. If you search a bit on the web, you'll find multiple reports from students at some university that has a chess AI tournament; they are a good introductory for some of the techniques involved. There is also a chess programming wiki somewhere, with excellent information in the area. I decided to go with chess. Thanks to you and everyone else here who suggested it. \n\nNow, if it turns out to be a huge pain in the ass, heads will roll. Be forewarned. But I'm excited! Good luck :) Thanks. Have you ever done a chess game before out of curiosity? Yeah, I wrote a chess AI for an AI course that I was taking last semester. The alpha-beta pruning is slightly theoretically cumbersome, but I don't think you'll have too much trouble. :) Nice. How did it turn out? Are you still working on it? Has it beat anyone? Are there any little things you learned along the way that might not be apparent at first (and I might be able to benefit from)?\n\nAlso, how hard was implementing the actual rules of chess into the game? Another commenter said that that was going to be extremely tedious. As I was reading this message and contemplating answers for the questions you posed, I realized... I didn't implement a chess AI, I implemented a Reversi (AKA Othello) AI. Sorry for the confusion. The techniques are very similar, however, and many of the sources I used were on chess rather than Reversi (which is probably why I made this brainfart).\n\nWith this in mind, I will attempt to answer your questions.\n\nFirst, I feel I must provide some background... The AI was written for a tournament amongst the AIs that we (the students taking this particular course) had written. The tournament never actually happened (for various reasons), so I never got to see how it fared against the other AIs.\n\n&gt;How did it turn out? \n\nIt was fun, but I can't speak of its performance. In truth, I never even played against it (since this was meant to be a tournament amongst AIs, there was no front-end, and I didn't write one).\n\n&gt;Are you still working on it?\n\nNo. AFAIK, they are trying the tournament again next year, and I might dust off the AI and enter it next year instead.\n\n&gt;Has it beat anyone?\n\nNobody human, at least (since it hasn't ever played a human).\n\n&gt;Are there any little things you learned along the way that might not be apparent at first (and I might be able to benefit from)?\n\nWrite a naive minimax (or negamax) search first and ensure that it works. When you later add alpha-beta pruning, compare the results with the original naive minimax version to ensure they come to the same result. I spent a lot of time debugging the alpha-beta search that I had initially written.\n\n&gt;Also, how hard was implementing the actual rules of chess into the game?\n\nThis I obviously can't answer... since I didn't implement them. :) For Reversi, the rules are very simple (if you haven't played it). I implemented the actual game logic using bitboards, leading to very fast move generation (finding all valid moves for a certain configuration is only a few bitshifts back and forth). I know that you can do something similar to this with chess (the wiki I mentioned in my earlier post is the chessprogramming wiki I saw someone else post; they have examples of bitboards for chess I believe). I don't think it'll be overly tedious.\n\nGood luck once again, and sorry for misleading you. If you have any other questions I'll answer them to the best of my abilities.   You could always take the crazy stochastic approach and have the AI play multiple random games starting with each legal move it can make for the next turn. The move with the highest number of better boards at the end of the random game pool is the move the AI makes.\n\nNote: I have not actually tried this approach, just heard about it in passing. This is the essence of the MCTS (Monte-carlo tree search) approach that has been used to make world-class Go programs. You could always take the crazy stochastic approach and have the AI play multiple random games starting with each legal move it can make for the next turn. The move with the highest number of better boards at the end of the random game pool is the move the AI makes.\n\nNote: I have not actually tried this approach, just heard about it in passing. You could always take the crazy stochastic approach and have the AI play multiple random games starting with each legal move it can make for the next turn. The move with the highest number of better boards at the end of the random game pool is the move the AI makes.\n\nNote: I have not actually tried this approach, just heard about it in passing. If you could run thousands of simulations ahead of time and save the data this could be feasible. But if you are actively running the simulations every time this would be too slow to be practical.  You could always take the crazy stochastic approach and have the AI play multiple random games starting with each legal move it can make for the next turn. The move with the highest number of better boards at the end of the random game pool is the move the AI makes.\n\nNote: I have not actually tried this approach, just heard about it in passing. [deleted]  I suck at chess, but this [js1k entry](http://js1k.com/2010-first/demo/750) can totally beat me.  I know that crazy optimized code size != indication of complexity of algorithm, but it still makes me feel like it should feasible do do something *okay* without going insane.  Maybe you could limit it to a selection of endgames?        If you do go for a Chess AI, I recommend you have a quick look at GNUChess, not to gleam AI insight from, but rather to gleam it's protocol.\n\nThere are many, many chess board programs designed to work with GNUChess, many of which are quite capable of pitting different AI's against each other. I suggest it as this is a fair portion of code that has already been written for you, and you can concentrate solely on the AI portion of the task.    A chess AI would probably be too involved: the number of possible moves at each game state is so large that advanced techniques are required to do anything interesting.  \n\nA connect-4 AI, on the other hand, sounds manageable (and interesting!).  The key here is that connect-4 is a much simpler game with a smaller game tree.  Coming up with heuristic values for each state shouldn't be to o bad, either. &gt;A chess AI would probably be too involved: the number of possible moves at each game state is so large that advanced techniques are required to do anything interesting.\n\nLike what kind of techniques?\n\n&gt;A connect-4 AI, on the other hand, sounds manageable (and interesting!). The key here is that connect-4 is a much simpler game with a smaller game tree. Coming up with heuristic values for each state shouldn't be to o bad, either.\n\n\nWhat exactly do you mean by generating heuristic values for each state? Is that like the static evaluation function?  A chess AI would probably be too involved: the number of possible moves at each game state is so large that advanced techniques are required to do anything interesting.  \n\nA connect-4 AI, on the other hand, sounds manageable (and interesting!).  The key here is that connect-4 is a much simpler game with a smaller game tree.  Coming up with heuristic values for each state shouldn't be to o bad, either. Connect-4 has been solved. Is Connect-N solved for a MxM board? I recall hearing that the answer is yes, but I have absolutely no proof for you. Sorry. :( I don't think connect 4 could be solved in a 4x4 so its probably not the case A chess AI would probably be too involved: the number of possible moves at each game state is so large that advanced techniques are required to do anything interesting.  \n\nA connect-4 AI, on the other hand, sounds manageable (and interesting!).  The key here is that connect-4 is a much simpler game with a smaller game tree.  Coming up with heuristic values for each state shouldn't be to o bad, either. A chess AI would probably be too involved: the number of possible moves at each game state is so large that advanced techniques are required to do anything interesting.  \n\nA connect-4 AI, on the other hand, sounds manageable (and interesting!).  The key here is that connect-4 is a much simpler game with a smaller game tree.  Coming up with heuristic values for each state shouldn't be to o bad, either. &gt; A chess AI would probably be too involved: the number of possible moves at each game state is so large that advanced techniques are required to do anything interesting.\n\nDepends on how good it needs to be.  Also note that the really high-end machines like Deep Blue actually do an exhaustive search, they just do it incredibly quickly. How would \"high-end machines like Deep Blue\" from the late 90s compare to a standard house-hold computer from 2012?\n\n[Wikipedia](http://en.wikipedia.org/wiki/Deep_Blue_\\(chess_computer\\)) offers some specs, but I don't think you can compare them directly....\n\n&gt; It was a massively parallel, RS/6000 SP Thin P2SC-based system with **30 nodes, with each node containing a 120 MHz P2SC microprocessor** for a total of 30, enhanced with **480 special purpose VLSI chess chips**. Its chess playing program was written in C and ran under the AIX operating system. It was capable of **evaluating 200 million positions per second**           The most tedious part is likely to be programming the chess move rules. This is the bit that would push me to do a Connect-N-style game instead.  (Disclaimer: I've never implemented Chess--maybe I'm giving it a bad rap.) Implementing the rules isn't particularly mentally stimulating in either case, but at least it's dead-simple for Connect-N.  Then you can get onto the fun part--minimax or monte carlo.  Or both.\n\nOthers point out that Connect-4 is solved... but does that matter? What is the goal for you, personally?  Chess?  AI?  I wrote a minimax tic-tac-toe player, even though that's one of the worst ways to implement tic-tac-toe.  The point was to learn minimax, and tic-tac-toe was a perfectly fine vehicle.\n\nNow I'm thinking of the best way to implement Chess rules... I'll have to look into that. :-)  Fun all around! I am skeptical that the rules would be harder than a programming a strategy capable of beating a 10 year old who has played 20 times.  ",
    "url" : "http://www.reddit.com/r/compsci/comments/rlrbz/how_feasible_would_it_be_to_write_a_basic/"
  }, {
    "id" : 28,
    "title" : "Are there any non-tree-based filing systems?",
    "snippet" : "As a project, I want to develop a computer file system where any folder could have multiple parents.  In other words, if I have folders A B and C, then folder C could b located inside A or B, and a file inside C could have possible paths:\nA/C/file  or B/C/file\n\nMy questions:\n1. Is there anything like this already?\n2. Are there any obvious problems I am overlooking?  Well, I think a normal unix filesystem will let you hard-link a directory to more than one parent. Also, this tag-based fs comes to mind: http://www.tagsistant.net/component/content/article/10-what-is-tagsistant  I had the same kind of idea a few years ago, so I googled it and I found that it already exists (so I didn't do anything about it). Here are some relevant links:\n\n* http://semanticweb.org/wiki/SemFS\n* https://code.google.com/p/tagfs/\n* https://code.google.com/p/dhtfs/\n* http://www.tagsistant.net/\n* https://code.google.com/p/labelfs/    I see no obvious problems.  The only difference between what you're describing and what's commonly used is that most file systems use a tree, but yours has cycles.  Most modern filesystems allow you to create hardlinks which do something similar to what you're talking about, or symlinks which only look like they do something similar to what you're talking about. That's not totally true. You typically can't make hard links to directories, only files, so \"real\" cycles don't actually happen.",
    "url" : "http://www.reddit.com/r/compsci/comments/rly19/are_there_any_nontreebased_filing_systems/"
  }, {
    "id" : 29,
    "title" : "Please help me understand red black trees",
    "snippet" : "I am having a hard time understanding red black trees in general, but specifically, I do not understand this statement\n\n'Every simple path from a node to a descendant leaf contains the same number of black nodes.'\n\n1. What does a simple path mean. Does it mean non-circular path, or does it mean shortest path.\n\n2. Are we talking about every simple path from the same node to the same descendant leaf, or are we talking about the path from the same node to any of it's descendant leafs, or are we talking about the path from any node to any of it's descendant leafs ?\n\n3. Why is it important to have the same number of black nodes from a node to it's descendants ?\n\n4. The documentation says that a red nodes children have to be black, but what are the rules for the children of a black node ?  Red-black trees are isometric to [2-3 trees](http://en.wikipedia.org/wiki/2-3_tree) and the following site gives a good explanation of 2-3 trees: [http://algs4.cs.princeton.edu/33balanced/](http://algs4.cs.princeton.edu/33balanced/) (the site also covers red-black trees.)\n\nEDIT:\nThe red-black tree on the previous website isn't really a [red-black tree](http://en.wikipedia.org/wiki/Red-black_tree), it is a representation of a [AA tree](http://en.wikipedia.org/wiki/AA_tree), which is a variation/simplification of a red-black tree.\nRed-black trees are isometric to [2-3-4 trees](http://en.wikipedia.org/wiki/2-3-4_tree) and AA trees are isometric to 2-3 trees, as noted by authorblues and dhruvbird as comments on this post.\n\nPlease correct me if I'm wrong. I didn't know this. I thought the isometry only applied to [2-3-4 trees](http://en.wikipedia.org/wiki/2-3-4_tree) (B-trees of order 4). TIL. IIRC, RB-Trees are an isomorphism of 234-Trees and AA-Trees are akin to 23-Trees.       Related:  I had an interesting problem for an algorithm class that might help you understand them some more.\n\nGiven a valid red-black tree, without rearranging any nodes, efficiently recolor the tree such that the number of red nodes is maximized.   ",
    "url" : "http://www.reddit.com/r/compsci/comments/rksmf/please_help_me_understand_red_black_trees/"
  }, {
    "id" : 30,
    "title" : "Wavelettransformation on 3D-Volume Data, suggestions?",
    "snippet" : "I am working on my bachelor's thesis with focus on implementing an efficient wavelettransformation for 3D-volume data on CUDA. Which in my case would be data from a computational fluid dynamics simulation with 3D grids for pressure values, velocity in x/y/z). \n\nMy question is, since there are so many kinds of wavelets (wavelets sperable/nonsep. , curvelets, shearlets, morphological wavelets,...) I am not sure which would yield the best result to approximate the data, since edge detection or an accurate representation of fine details are important. So I would welcome every suggestion I can get.\n\nAre there any suggestions on reading material, papers or a general hint on which kind of wavelets I should focus on? My professor is giving me a lot of material, but implementing them all, testing  and comparing them all would probably take to much time.\n\nThanks in advance ",
    "url" : "http://www.reddit.com/r/compsci/comments/rkqsa/wavelettransformation_on_3dvolume_data_suggestions/"
  }, {
    "id" : 31,
    "title" : "Software for drawing trees",
    "snippet" : "Hi guys, I have an assignment where I have to draw a large number of trees (BST, AVL, 2-3 etc) by hand and its very tedious. I'm looking for a program that could help me draw them faster. My google-fu proved fruitless. Any ideas?   Maybe take a look at [graphviz](http://www.graphviz.org/), I wrote [a website](http://graphs.grevian.org/) for one of my math classes that gives a few tutorials and a simple web interface that should let you tinker with it, and should adapt easily enough to trees (Since they are specialized graphs after all)  Maybe take a look at [graphviz](http://www.graphviz.org/), I wrote [a website](http://graphs.grevian.org/) for one of my math classes that gives a few tutorials and a simple web interface that should let you tinker with it, and should adapt easily enough to trees (Since they are specialized graphs after all)  Maybe take a look at [graphviz](http://www.graphviz.org/), I wrote [a website](http://graphs.grevian.org/) for one of my math classes that gives a few tutorials and a simple web interface that should let you tinker with it, and should adapt easily enough to trees (Since they are specialized graphs after all)  Maybe take a look at [graphviz](http://www.graphviz.org/), I wrote [a website](http://graphs.grevian.org/) for one of my math classes that gives a few tutorials and a simple web interface that should let you tinker with it, and should adapt easily enough to trees (Since they are specialized graphs after all)  Hey, this looks promising. Thanks alot! Maybe take a look at [graphviz](http://www.graphviz.org/), I wrote [a website](http://graphs.grevian.org/) for one of my math classes that gives a few tutorials and a simple web interface that should let you tinker with it, and should adapt easily enough to trees (Since they are specialized graphs after all)   I would use TikZ/PGF, especially if you are writing your assignment in LaTeX. I would use TikZ/PGF, especially if you are writing your assignment in LaTeX.  Try [Dia](http://live.gnome.org/Dia) - it's multi-platform and FOSS!  GraphViz; http://www.graphviz.org/.\n\nFor the assignment, you're better just drawing them by hand. GraphViz; http://www.graphviz.org/.\n\nFor the assignment, you're better just drawing them by hand. That depends.\n\nI've generally found that at least for finite state machines, I don't save *that* much time doing them by hand vs dot, and I'm ultimately being asked for a PDF anyway (not paper).\n\nBut I've also come to appreciate the value of having a whiteboard around for brainstorming stuff that's easier to brainstorm by hand than with typing. I agree completely. Dot is perfect for making graphics to throw in LaTeX. But if the OP is just learning these structures, learning Dot will probably just make his life harder.       Not to be a dick, but I think drawing them out by hand helps you to learn the concept of how each type of tree works. Especially if you do it step by step. I'm not looking for a way around that. I'm still going to be doing it step by  step I just want software to help me draw lines, circles, copy, paste, undo etc. Google docs drawing tools are almost good enough but the canvas size is very limited.  I'm not looking for a way around that. I'm still going to be doing it step by  step I just want software to help me draw lines, circles, copy, paste, undo etc. Google docs drawing tools are almost good enough but the canvas size is very limited.  In that case, use OmniGraffle on Mac or Visio on Windows. Everyone else here is suggesting automatic methods. (except for Inkscape)      ",
    "url" : "http://www.reddit.com/r/compsci/comments/rjyo4/software_for_drawing_trees/"
  }, {
    "id" : 32,
    "title" : "Can anyone recommend some good reading for algorithm theory/analysis?",
    "snippet" : "Looking for some informative/insightful reading on the topic, including anything from an elementary algorithms-for-dummies level to more advanced applications. \n\nEDIT: Thanks for all the thoughtful suggestions and annotations!   Algorithm Design by Kleinberg and Tardos  Introduction to Algorithms by Cormen, Leiserson, Rivest, and Stein, third\nedition, 2009\n\nis a pretty good introductory Algorithm text. Introduction to Algorithms by Cormen, Leiserson, Rivest, and Stein, third\nedition, 2009\n\nis a pretty good introductory Algorithm text. CLRS is a great textbook for an graduate level algorithms class. If that's what you're looking for, I don't know of an alternative.\n\nIt is not, however, \"a[n] [...] introductory algorithms text\" suitable for most people. See also: [this blog post](http://www.billthelizard.com/2008/12/books-programmers-dont-really-read.html) CLRS is a great textbook for an graduate level algorithms class. If that's what you're looking for, I don't know of an alternative.\n\nIt is not, however, \"a[n] [...] introductory algorithms text\" suitable for most people. See also: [this blog post](http://www.billthelizard.com/2008/12/books-programmers-dont-really-read.html) CLRS is a great textbook for an graduate level algorithms class. If that's what you're looking for, I don't know of an alternative.\n\nIt is not, however, \"a[n] [...] introductory algorithms text\" suitable for most people. See also: [this blog post](http://www.billthelizard.com/2008/12/books-programmers-dont-really-read.html) CLRS is a great textbook for an graduate level algorithms class. If that's what you're looking for, I don't know of an alternative.\n\nIt is not, however, \"a[n] [...] introductory algorithms text\" suitable for most people. See also: [this blog post](http://www.billthelizard.com/2008/12/books-programmers-dont-really-read.html) Introduction to Algorithms by Cormen, Leiserson, Rivest, and Stein, third\nedition, 2009\n\nis a pretty good introductory Algorithm text.  There's this book: [Algorithms](http://www.cs.berkeley.edu/~vazirani/algorithms.html)\n\nIt really is an introductory book and a light read too. The proofs are less formal but still rigorous.\n\nThis is what they say about their style:\n&gt;Playing on the strengths of our students (shared by most of today’s undergraduates in Computer Science), instead of dwelling on formal proofs we distilled in each case the crisp mathematical idea that makes the algorithm work. In other words, we emphasized rigor over formalism. We found that our students were much more receptive to mathematical rigor of this form.\n\nIt also stresses some aspects not emphasised by CLRS such as algorithms for bignum arithmetic and FFT right in the first chapter.\n   Sipser's intro to theory of computation.  May be farther down the abstract path than you are looking for, but one of the best CS books out there non-the-less. That book is really really good. I'd say it's easily the most clear and understandable book on the subject.  relevant flair *flair Sipser's intro to theory of computation.  May be farther down the abstract path than you are looking for, but one of the best CS books out there non-the-less. Sipser's intro to theory of computation.  May be farther down the abstract path than you are looking for, but one of the best CS books out there non-the-less.   if(Great Book &amp;&amp; About Algorithms)\n{\n     this statement never executed in my program;\n} [deleted]",
    "url" : "http://www.reddit.com/r/compsci/comments/rjc77/can_anyone_recommend_some_good_reading_for/"
  }, {
    "id" : 33,
    "title" : "China's Not-So-Super Computers ",
    "snippet" : "   Why doesn't the article name one of those local projects the supercomputers are used for?  Possibly related to the technological complexes in the neighborhoods. It's a bit hasty to just say they are useless because of non-national focus, I would think.\n\nEdit:\n\nAlso this seems to be a bit hasty judgment: \"Chinese scientists also lack the funding, and freedom, to explore technologies that haven't already been endorsed by the government, which can keep them well behind the cutting edge.\"\n\nI don't see how that directly follows.\n\nEdit edit:\n\nI see they do mention one, possibly the most controversial example they could find: \"a plan to use the Nebulae to improve health care services in South China—a socially important goal but not one that makes use of the power of what is ranked as the world's fourth-fastest supercomputer.\"\n\nIf the power is not used, there is always a way to use the remainder of the power to calculate something else.  That's not surprising since China's progress is spurred by the massive chip on its shoulder.  It may not seem like a sustainable or healthy strategy in our Western eyes, but if modernization is anything like the Olympics, it just might work.",
    "url" : "http://online.wsj.com/article/SB10001424052702303812904577298062429510918.html"
  }, {
    "id" : 34,
    "title" : "Programming languages in other languages?",
    "snippet" : "This may sound silly, but are there programming languages out there that have a non-English syntax? \nI was just thinking of how difficult it must be for non-English speakers to learn a programming language.    ",
    "url" : "http://www.reddit.com/r/compsci/comments/rie9f/programming_languages_in_other_languages/"
  }, {
    "id" : 35,
    "title" : "one of the biggest compsci conferences seems to be a scam running since 2006",
    "snippet" : "  A touch of hyperbole I suspect as this conference isn't even in the top 1000 according to http://www.cs.iit.edu/~xli/CS-Conference-Journals-Impact.htm it's also been said in other places.    Conferences are always scams to get your employer to send you somewhere and pay for your booze for a weekend.  Visweek?  SIGGRAPH?  Those are some of the most important publication venues in their areas, and they're both conferences. And they are both easy ways to expense large amounts of alcohol :-D  ",
    "url" : "http://copy-shake-paste.blogspot.de/2012/02/fake-conference-worldcomp.html"
  }, {
    "id" : 36,
    "title" : "Q: How can quantum computers break encryption?",
    "snippet" : "  I'm just going to leave this excerpt from my Theory of Computations and Algorithms notes:\n\n&gt; ...contrary to some of the popular media says, quantum computation is not (proved or believed to be) the same as non-deterministic computation.\n\nAnd non-deterministic computation is what would be required to solve this problem in an efficient way.",
    "url" : "http://www.askamathematician.com/2011/02/q-how-can-quantum-computers-break-ecryption/"
  }, {
    "id" : 37,
    "title" : "Big-Oh = vs. \\in",
    "snippet" : "Sorry for what I think is the third thread on big-oh notation in a short time, but I was just wondering, does anyone know where the idea to \"overload\" the meaning of = for things like`[;  3n^2 + 4 = O(n^2) ;]` came from? (as opposed to just saying `[; 3n^2 + 4 \\in O(n^2) ;]`)\n\nedit: Okay, I think I get it now/am satisfied with the formality using these definitions:\n\n`[; O(f(n)) = \\{g(n) \\mid \\exists k,n_0 \\ge 0 \\mbox{ st. } \\forall n \\ge n_0, g(n) \\le kf(n) \\} ;]`\n\nif [;g(n);] is a function, [;g(n) = O(f(n)) ;] means [; g(n) \\in O(f(n)) ;]\n\nif [;A;] is a set of functions, [;A = O(f(n)) ;] means [;A \\subseteq O(f(n));]\n\nand we also use the idea that if [;A;] is a set of functions, `[;f(A) = \\{f(g(n)) \\mid g(n) \\in A\\};]`\n\nedit2: I also like [mamjjasond's suggestion](http://www.reddit.com/r/compsci/comments/rg3ft/bigoh_vs_in/c45jwi8) of using [;\\propto;] instead of =, since this makes the non-reflexivity clearer imo.  CLRS, a classic text that certainly influenced many people's style, has the following to say:\n\n&gt; \"Because \\theta(g(n)) is a set, we could write $f(n) \\in \\theta(g(n))$ to indicate that f(n) is a member of $\\theta(g(n))$. Instead, we will usually write $f(n) = \\theta(g(n))$ to express the same notation. You might be confused because we abuse equality in this way, but we shall see later in this section that doing so has its advantages\"\n\nand later in the section:\n\n&gt; When the asymptotic notation stands alone (That is, not within a larger formula) on the right-hand side of an equation (or inequality), as in $n = O(n^2 )$, we have already defined the equal sign to mean set membership. In general, however, when asymptotic notation appears in a formula, we interpret it as a standing for some anonymous function that we do not care to name. For example, the formula $2n^2 + 3n + 1 = 2n^2 + \\theta(n)$ means $2n^2 + 3n + 1 = 2n^2 + f(n)$, where f(n) is some function in the set $\\theta(n)$.\n\n&gt; Using asymptotic notation in this manner can help eliminate inessential detail and clutter in an equation.\n\n[edit: Transcribing from books is boring. I should have found an ebook.] I don't see why the first (unnecessary) abuse is needed to justify the second (useful) one. The substitution of an order notation into an equation for a function makes sense if we consider the function equal to its order notation set. But we could also do that if we just allow ourselves to apply functions to sets of functions, without creating odd results like n = O(n) = n+1 =&gt; 0 = 1, couldn't we? If you make sure your big O is always on the right side of whatever expression you're substituting, you should never run into this problem.\n\nThe equals sign in these cases is merely a short-hand notation for *both* $\\in$ and $\\subset$, and should only be used as such. Yeah, I guess if we define the special = in the case of big-oh to be non-reflexive, I guess it works out. I just don't get why we can't just use \\in to avoid the potential ambiguity. Though I'm starting to see some situations where this might not necessarily be a clearer notational choice. The beauty is that in this case, = is also a substitute for $\\subset$: O( n ) = O( n^2 ) means O( n ) \\subset O( n^2 ).\n\nAs to why the equals sign is used (as opposed to another symbol like ~ or \\in) in these cases, I don't have a good answer. &gt; O( n ) = O( n^2 )\n\nI would never call that beauty, since = has very strong connotations with being symmetric.\n\nOf all the reasons put forward here, I cannot see one that is worth the confusion this causes for students. I think I'm becoming fond of [mamjjasond's](http://www.reddit.com/r/compsci/comments/rg3ft/bigoh_vs_in/c45jwi8) suggestion of using [; \\propto ;]. (which I've never seen used before with big-oh). I feel like that would be a nice way of implying both [; \\in ;] and [; \\subseteq ;] without implying reflexivity. But we could also do that if we just allow ourselves to apply functions to sets of functions, without creating odd results like n = O(n) = n+1 =&gt; 0 = 1, couldn't we? The substitution of an order notation into an equation for a function makes sense if we consider the function equal to its order notation set. CLRS, a classic text that certainly influenced many people's style, has the following to say:\n\n&gt; \"Because \\theta(g(n)) is a set, we could write $f(n) \\in \\theta(g(n))$ to indicate that f(n) is a member of $\\theta(g(n))$. Instead, we will usually write $f(n) = \\theta(g(n))$ to express the same notation. You might be confused because we abuse equality in this way, but we shall see later in this section that doing so has its advantages\"\n\nand later in the section:\n\n&gt; When the asymptotic notation stands alone (That is, not within a larger formula) on the right-hand side of an equation (or inequality), as in $n = O(n^2 )$, we have already defined the equal sign to mean set membership. In general, however, when asymptotic notation appears in a formula, we interpret it as a standing for some anonymous function that we do not care to name. For example, the formula $2n^2 + 3n + 1 = 2n^2 + \\theta(n)$ means $2n^2 + 3n + 1 = 2n^2 + f(n)$, where f(n) is some function in the set $\\theta(n)$.\n\n&gt; Using asymptotic notation in this manner can help eliminate inessential detail and clutter in an equation.\n\n[edit: Transcribing from books is boring. I should have found an ebook.] This seems to be what [fmota was saying](http://www.reddit.com/r/compsci/comments/rg3ft/bigoh_vs_in/c45j5u6), yes? I still don't see what the point of using = is. If we define O(f(n)) as a set, all we need to do is define what happens when we apply a function to a set of function (see my response to fmota), then we can just say `[; 2n^2 + 3n + 1 \\in 2n^2 + \\theta(n) ;]` (we apply the function `[; f(m) = 2n^2 + m ;]` to the set [; \\theta(n) ;]).\n\nAlthough I do see why it might be a bit more natural to say things like [; 3n + log(n) = O(n) + O(log(n)) ;] (as opposed to [; 3n + log(n) \\in O(n) + O(log(n)) ;]). This seems to be what [fmota was saying](http://www.reddit.com/r/compsci/comments/rg3ft/bigoh_vs_in/c45j5u6), yes? I still don't see what the point of using = is. If we define O(f(n)) as a set, all we need to do is define what happens when we apply a function to a set of function (see my response to fmota), then we can just say `[; 2n^2 + 3n + 1 \\in 2n^2 + \\theta(n) ;]` (we apply the function `[; f(m) = 2n^2 + m ;]` to the set [; \\theta(n) ;]).\n\nAlthough I do see why it might be a bit more natural to say things like [; 3n + log(n) = O(n) + O(log(n)) ;] (as opposed to [; 3n + log(n) \\in O(n) + O(log(n)) ;]). Quickly, `$3 n + \\log(n) \\in O(n) + O(\\log(n))$` doesn't make sense even with the set interpretation, it'd at best mean the union of those two sets.\n\nMore importantly however is that often we do handle algebraic manipulations alongside big-O notation. This is super common in asymptotic arguments in statistics/optimization where you want to remember that one side also has some constant or low-order terms but don't want to keep writing them out.\n\nIt's entirely notational convenience. It'd be easy enough quantification lines in all derivations like \"For all `f \\in O(g(n))`...\" but the \"inline\" notation is very nice and easy enough to track once you practice it a bit. Yeah, I'm seeing how sticking with set notation might get a bit complicated (it should be `[; 3n + \\log(n) \\subseteq O(n) + O(\\log(n)) ;]`, I guess). I'd suggest `3n + \\log(n) \\in O(n)` alone. It's also be in `O(n + \\log(n))` but `O(n + \\log(n)) = O(n)`, so you should just write the simpler one. Since `3n + \\log(n) \\in O(n)` then it's surely in `O(n) \\union O(\\log(n))`, but you don't gain anything writing that. Actually, wait, what's wrong with `[; 3n + \\log(n) \\in O(n) + O(\\log(n));]`? `[;O(n) + O(\\log(n)) = \\{f(n) + g(n) \\mid f(n) \\in O(n) \\mbox{ and } g(n) \\in O(\\log(n)) \\};]`. [;3n + \\log(n);] in this set with [;f(n) = 3n \\in O(n);] and [;g(n) = \\log(n) \\in O(\\log(n)) ;].   I think the biggest reason to use \"=\" is because it is easy to use big-Oh (and big-Theta, big-Omega, little-oh, little-omega) in arithmetic expressions. For example, we could express\n\n`[; n! = 2^{\\omega(n)} ;]`\n\nusing set notation, but we either invent a new variant of little-omega, or we express everything by long hand:\n\n`[; \\forall c. \\exists n_0. \\forall n \\geq n_0. n! \\geq 2^{cn}. ;]`\n\nAnother example: we can express a polynomial bound as\n\n`[; T(n) = n^{O(1)} ;]`\n\nrather than spelling everything out as above. Well, we could just as easily consider `[; \\omega(n) ;]` to be a set, and then `[; n! \\in 2^{\\omega(n)} ;]`. We could describe polynomials as `[; T(n) \\in n^{O(1)} ;]`. My question is why we don't do this, instead of using the current notation of \"overloading\" =.\n\nedit: Well, this requires a bit of an extra definition in terms of what happens when we apply a function to a set of functions, but we can do this: if [; A ;] is a set of functions, `[;f(A) = \\{f(g(n)) \\mid g(n) \\in A\\};]` The notation you describe in your edit is quite standard: http://en.wikipedia.org/wiki/Set_notation#Metaphor_in_denoting_sets I don't think there were any examples in that section of applying a function to a set of functions. Ie, if [; f \\colon X \\rightarrow Y ;] and [; A \\subseteq X ;], [; f(A) ;] is clearly `[; \\{f(a) \\mid a \\in A\\} ;]`, because [;A;] is a subset of the function's domain. But if [;B;] is a set of functions, we can't simply apply [;f;] to a function, since [;f;] is not defined for functions. This is why I just explicitly explained the notation. The end of the second paragraph:\n\n&gt; Alternatively, a single symbol for the set of even numbers is 2Z. Likewise, since any odd number must have the form 2a + 1 for some integer a, the set of odd numbers may be denoted 2Z+1.\n\nIt's not about applying functions, its about using operators from an [algebraic structure](http://en.wikipedia.org/wiki/Algebraic_structures) on sets instead elements (function application is also an operator). This is generally understood as the set you obtain by applying the operator on all members. Wait, why would we limit it do algebraic operators? Can't you consider `[; 2^S = \\{2^n \\mid n \\in S\\} ;]` just as easily as `[; 2\\mathbb{Z} = \\{2n \\mid n \\in \\mathbb{Z} \\} ;]`? Well, we could just as easily consider `[; \\omega(n) ;]` to be a set, and then `[; n! \\in 2^{\\omega(n)} ;]`. We could describe polynomials as `[; T(n) \\in n^{O(1)} ;]`. My question is why we don't do this, instead of using the current notation of \"overloading\" =.\n\nedit: Well, this requires a bit of an extra definition in terms of what happens when we apply a function to a set of functions, but we can do this: if [; A ;] is a set of functions, `[;f(A) = \\{f(g(n)) \\mid g(n) \\in A\\};]` You're just substituting one metaphor for another. :-)\n\n- In my usage above, which I think is pretty standard, you treat `[; O(f(n)) ;]` as some arithmetic expression that is assymptotically bounded by `[; f(n) ;]`. In other words, the `[; O(f(n)) ;]` is an anonymous variable dependent on `[; n ;]`.\n- In your usage, you overload operations on elements as operations on sets of elements.\n\nThe biggest issue that I can think of with your notation is that there are many set operations that use the same notation as number operations. For example, the set `[; A^B ;]` denotes the set of functions from `[; B ;]` to `[; A ;]`. So, the set `[; 2^{\\omega(n)} ;]` is actually the set of functions from `[; \\omega(n) ;]` to `[; 2 = \\{0, 1\\} ;]`.\n\nEither way, neither solution is perfect, and it comes down to this: how do *you* want to write it? Write it that way. Just be clear what you mean. Hm, wouldn't `[;2^{\\omega(n)};]` also be the powerset of [;\\omega(n);]? I guess I always considered notation like that to be slightly less standard than applying functions to sets. Though you're right, there's a bit of a conflict there, too. O(f(n)) just seems to so naturally be describing a set, and it seems to make much more sense to think about it like that and use \\in as opposed to inventing new usage of =. `[;2^{\\omega(n)};]` as the powerset of `[;\\omega(n);]` is naturally isomorphic to `[;2^{\\omega(n)};]` as the set of functions from `[;\\omega(n);]` to `[;2;]`, which is why you see the same notation being used in different ways, there.\n\nAs for inventing new ways to use the `=` ... it's only been in usage for hundreds of years or so... and, and Knuth uses it that way. So it's okay. Plus, commutation of `=` is really not as important as transitivity and reflexity, which are still satisfied. [/first world excuses] &gt; transitivity and reflexity, which are still satisfied.\n\nWhat do you think about when people write 2n = O(n) = O(n^2)? They're using the transitive property of \"`=` in the presence of O\". Obviously, if you switched these things around, you'd wind up in trouble.\n Not only transitivity, but also the symmetry. I think using = for an non-symmetric relation is what bothers me most. Yep. But ... you get over it. Well, we could just as easily consider `[; \\omega(n) ;]` to be a set, and then `[; n! \\in 2^{\\omega(n)} ;]`. We could describe polynomials as `[; T(n) \\in n^{O(1)} ;]`. My question is why we don't do this, instead of using the current notation of \"overloading\" =.\n\nedit: Well, this requires a bit of an extra definition in terms of what happens when we apply a function to a set of functions, but we can do this: if [; A ;] is a set of functions, `[;f(A) = \\{f(g(n)) \\mid g(n) \\in A\\};]` How would you interpret `[; n^2+O(n)=O(n^2) ;]` with sets? I think the biggest reason to use \"=\" is because it is easy to use big-Oh (and big-Theta, big-Omega, little-oh, little-omega) in arithmetic expressions. For example, we could express\n\n`[; n! = 2^{\\omega(n)} ;]`\n\nusing set notation, but we either invent a new variant of little-omega, or we express everything by long hand:\n\n`[; \\forall c. \\exists n_0. \\forall n \\geq n_0. n! \\geq 2^{cn}. ;]`\n\nAnother example: we can express a polynomial bound as\n\n`[; T(n) = n^{O(1)} ;]`\n\nrather than spelling everything out as above.   Maybe this is a stupid question, but I am curious ... when you guys are saying \"\\in\" are you referring to the \"is an element of\" symbol (that looks like the end of pitchfork turned sideways, or an E with a curved side), or do you literally mean the string '\\in' ?  \n\n(I always used lower case alpha, \"is proportional to\".) Maybe this is a stupid question, but I am curious ... when you guys are saying \"\\in\" are you referring to the \"is an element of\" symbol (that looks like the end of pitchfork turned sideways, or an E with a curved side), or do you literally mean the string '\\in' ?  \n\n(I always used lower case alpha, \"is proportional to\".) That's LaTeX notation.  Checkout [TexTheWorld](http://userscripts.org/scripts/show/92758). It's a browser plugin that'll actually parse the LaTeX strings for you.  Makes things a little easier to read.\n\nThey use it all the time over in [/r/math](/r/math). Wow nice.  Just installed it and I am digging it a lot.  Thank you! No, don't use TexTheWorld, it's unmaintaned and slow. Use this instead: http://userscripts.org/scripts/show/108770~~~~ Maybe this is a stupid question, but I am curious ... when you guys are saying \"\\in\" are you referring to the \"is an element of\" symbol (that looks like the end of pitchfork turned sideways, or an E with a curved side), or do you literally mean the string '\\in' ?  \n\n(I always used lower case alpha, \"is proportional to\".)   I must be missing something.. because this question seems completely pedantic. Overloading happens all the time in the correct context to simplify notation and expressions. Can't it just be that simple?   I think the real thing is that using equality allows us a certain amount of leniency in prose. For instance, we could say that the runtime of Insertion Sort on an array A[1..n] is [;n + Inv(A) \\in O(n ^ 2);], but since [;n + Inv(A) = O(n ^ 2);], we can state that the runtime is [;O(n ^ 2);] since those two things are equivalent. It cuts down on a bit of the clutter of having to say \"the runtime is a member of [;O(n ^ 2);]\" everytime. I don't see why we can't still say \"the runtime is `[;O(n^2);]`\". I mean, we say things like \"5 is an integer\", not \"5 is a member of the integers\". Saying \"the runtime is `[;O(n^2);]`\" as opposed to \"the runtime is a member of `[;O(n^2);]`\" doesn't seem that far off. &gt; \"5 is an integer\"\n\nYour use of \"an\" is equivalent, semantically, to set membership. \"I am **a** boy\" doesn't mean \"I am eqivalent to the set of all boys\". It means I am in that group. But you would write that authorblues [;\\in;] boys. Fine, we can say things like \"5 is integral\", so if we like, we can say \"n^2 is O(n^2 )\".\n\nAt any rate, I'm not so concerned with how well the common language for discussing the concepts relates to the formal concepts. Ie, I don't think it's such a big deal if we take short cuts in common language that don't match up perfectly with the mathematical language. Me either. I was being pedantic. What is important is that the notation was overloaded to allow for ease. Is [;O(f(n));] a set? Is it a congruence class? Is it something more abstract? It isn't really important. But when it comes to computational complexity, [;=;] *does* have a well-defined meaning. An expression [;g(n) = O(f(n));] if [;\\lim_{x \\rightarrow \\inf} \\frac{g(x)}{f(x)} = c;].\n\n(That last expression may not be formally correct, but that is the implication in my mind. I'm looking for a source that might back me up right now...)    I would say \"=\", since the definition is a limit between the asymptotic values of two functions, not really something defined between an element and a set.\n\nFWIW, all of my professors also use \"=\". That is incorrect. It is certainly acceptable (if not standard) to view O(f(x)) as the set of functions g(x) such that for some c, g(x) &lt;= c * f(x) I would say \"=\", since the definition is a limit between the asymptotic values of two functions, not really something defined between an element and a set.\n\nFWIW, all of my professors also use \"=\".",
    "url" : "http://www.reddit.com/r/compsci/comments/rg3ft/bigoh_vs_in/"
  }, {
    "id" : 38,
    "title" : "There was a thread on Big O that was deleted, but I had a question. ",
    "snippet" : "Can I view Big O as function with an output? Furthermore can I view the set {n, 2n, 3n, ...} as a level set L^n (O(x))?  FWIW I always thought saying things like 3n^2 + 4 = O(n^2 ) was silly. O(n^2 ) is like the set of all functions that are quadratic or slower, and 3n^2 + 4 is in the set O(n^2 ).\n\n~~edit: I realize O(n^2 ) is not \"the set of all functions that are quadratic\", I just didn't know how to say it properly without being redundant (\"O(n^2 ) is the set of all functions that are O(n^2 )\"), and I didn't feel like using the formal definition for O(f(n)).~~\n\nedit2: fine, let's be formal:\n\n`[; O(f(n)) = \\{g(n) \\mid \\exists k,n_0 \\ge 0 \\mbox{ st. } \\forall n \\ge n_0, g(n) \\le kf(n) \\}  ;]`, so saying stuff like `[; 5n^2 + 3 = O(n^2) ;]` is just a silly way of saying `[; 5n^2 + 3 \\in O(n^2) ;]`\n\nOut of curiosity, does anyone know where the idea of \"overloading\" = to mean [; \\in ;] in the case big-oh notation came from? Why fight against already-existing notation that can express exactly what you mean? That's almost right. You definitely have the right intuition and are thinking about things correctly, but technically O(n^2 ) is larger than the set of all quadratic functions. O(n^2 ) is the set of all functions that are eventually bounded by n^2, so it is the set of all constant functions, linear functions, and quadratic functions (and all linear combinations). So, for example, n is O(n^2 ) and so is n^2 + 3n + 4.\n\nYour statement is correct for big theta notation, which encapsulates eventual upper and lower bounds, in which case it is no longer true that  n is bigtheta(n^2 ). &gt;but technically O(n^2 ) is larger than the set of all quadratic functions. O(n^2 ) is the set of all functions that are eventually bounded by n^2, so it is the set of all constant functions, linear functions, and quadratic functions (and all linear combinations).\n\nThat's the set of all O(n^2 ) *polynomials*, but there are other O(n^2 ) functions.  Like, say ln(x) or,\n\nf(x) = x (x is odd) or x^2 (x is even).\n\n Oh yes, you are certainly correct. This raises an interesting question: is there a finite basis for O(n^2 )?\n\nActually, inspired by your second example, there certainly is not. Let f_k(n) be a function that is 0 for n = 0,..,k-1 and is n^2 for n &gt;= k. These functions are all O(n^2 ) and are linearly independent. Erm, you asked for a finite basis then produced an infinite set.  And I'm not convinced they span O(n^2 ), e.g., you might miss ln(x). That's almost right. You definitely have the right intuition and are thinking about things correctly, but technically O(n^2 ) is larger than the set of all quadratic functions. O(n^2 ) is the set of all functions that are eventually bounded by n^2, so it is the set of all constant functions, linear functions, and quadratic functions (and all linear combinations). So, for example, n is O(n^2 ) and so is n^2 + 3n + 4.\n\nYour statement is correct for big theta notation, which encapsulates eventual upper and lower bounds, in which case it is no longer true that  n is bigtheta(n^2 ). I'm going to TL;DR what pablo78 said: O(n^2 ) is the set of all functions that grow no larger than some quadratic function. this is a very good example of TL;DR Thank you very much. FWIW I always thought saying things like 3n^2 + 4 = O(n^2 ) was silly. O(n^2 ) is like the set of all functions that are quadratic or slower, and 3n^2 + 4 is in the set O(n^2 ).\n\n~~edit: I realize O(n^2 ) is not \"the set of all functions that are quadratic\", I just didn't know how to say it properly without being redundant (\"O(n^2 ) is the set of all functions that are O(n^2 )\"), and I didn't feel like using the formal definition for O(f(n)).~~\n\nedit2: fine, let's be formal:\n\n`[; O(f(n)) = \\{g(n) \\mid \\exists k,n_0 \\ge 0 \\mbox{ st. } \\forall n \\ge n_0, g(n) \\le kf(n) \\}  ;]`, so saying stuff like `[; 5n^2 + 3 = O(n^2) ;]` is just a silly way of saying `[; 5n^2 + 3 \\in O(n^2) ;]`\n\nOut of curiosity, does anyone know where the idea of \"overloading\" = to mean [; \\in ;] in the case big-oh notation came from? Why fight against already-existing notation that can express exactly what you mean? [deleted] FWIW I always thought saying things like 3n^2 + 4 = O(n^2 ) was silly. O(n^2 ) is like the set of all functions that are quadratic or slower, and 3n^2 + 4 is in the set O(n^2 ).\n\n~~edit: I realize O(n^2 ) is not \"the set of all functions that are quadratic\", I just didn't know how to say it properly without being redundant (\"O(n^2 ) is the set of all functions that are O(n^2 )\"), and I didn't feel like using the formal definition for O(f(n)).~~\n\nedit2: fine, let's be formal:\n\n`[; O(f(n)) = \\{g(n) \\mid \\exists k,n_0 \\ge 0 \\mbox{ st. } \\forall n \\ge n_0, g(n) \\le kf(n) \\}  ;]`, so saying stuff like `[; 5n^2 + 3 = O(n^2) ;]` is just a silly way of saying `[; 5n^2 + 3 \\in O(n^2) ;]`\n\nOut of curiosity, does anyone know where the idea of \"overloading\" = to mean [; \\in ;] in the case big-oh notation came from? Why fight against already-existing notation that can express exactly what you mean?    &gt; Can I view Big O as function with an output?\n\nYes. O(f(x)) is the upper bound of f(x). So you can write g(x) = O(f(x)) where f is the upper bound of g for sufficiently large x.\n\nSo O is a function that takes another function as its argument and has as an output another function.\n\n----\n\nIf I understand what you mean by a Level set, then also yes. Assuming you are talking about the way that this is typically used in computer science, O(k * f(x)) = O(f(x)). This also implies that O(f(x)+c) = O(f(x)), since for sufficiently large x f(x)+c will be bounded by k * f(x).\n\nExample: Say you have a square matrix of n by n. Some function f has to iterate over all elements in this matrix, and then iterate over each row, and finally perform some task which doesn't depend on n (ie. it's constant). The first step is O( n^2 ), and the second step is O(n), and the third step is O(1). So O(f) = n^2 + n + 1.\n\nNow, notice that with sufficiently large values of n, f(n) = n^2 + n + 1 will converge to k * (n^2), so we write O(f) = n^2.\n\nIn other words, since O is often concerned with 'worst-case' behavior, we often limit our discussion to the worst-performing aspect of a given function/algorithm. This is kind of hand-wavy, but O itself is a bit hand-wavy since we are only trying to establish a general, worst-case, upper-bound.\n\nEDIT: Sorry for the mistakes in this...fixed em. You have that backwards. In the expression g(x) = O(f(x)), f is the upper bound on g, not vice versa. When we write O( n^2 ) = n^2 + n + 1, what we really mean is that n^2 + n + 1 ∈ O( n^2 ), because n^2 is an upper bound (with appropriate constants). O( n^2 ) is a set of functions, basically anything that grows quadratically or slower. Huh, that's funny. Is that how it's usually taught in the US? I mean, that O(n^2 ) is a set of functions. Or maybe it's a comp-sci thing.\n\nI say that because I'm French, and a math student, and I've always seen f=O(g) explained as either\n\n* there exists a constant C such that for n large enough, f(n) &lt;= C g(n), or\n* f(n) = g(n)h(n) for some bounded function h and for n large enough.\n\nNote that the second definition justifies the use of the \"=\" sign; you just  don't give a name to the bounded function because you don't care what it is, just that it's bounded. You are right, the usual definition makes no explicit mention of sets.\n\nBut it is clear from the definition that the = sign in the relation f = O(g) is something other than equality, and for any g is clearly a unique set of functions F such that f = O(g) iff. f ∈ F. So even if we don't define O(g) to be a set per se, there is a natural set of functions that corresponds to \"being O(g).\"\n\nThe world would be a lot simpler if instead of some ad-hoc notation borrowing the = sign, we said\n\n    O(g) = { f | ∃ C, k such that f(n) &lt;= g(n) for all n &gt; k }\n\nand then just used standard inclusion to say h ∈ O(g). &gt; Can I view Big O as function with an output?\n\nYes. O(f(x)) is the upper bound of f(x). So you can write g(x) = O(f(x)) where f is the upper bound of g for sufficiently large x.\n\nSo O is a function that takes another function as its argument and has as an output another function.\n\n----\n\nIf I understand what you mean by a Level set, then also yes. Assuming you are talking about the way that this is typically used in computer science, O(k * f(x)) = O(f(x)). This also implies that O(f(x)+c) = O(f(x)), since for sufficiently large x f(x)+c will be bounded by k * f(x).\n\nExample: Say you have a square matrix of n by n. Some function f has to iterate over all elements in this matrix, and then iterate over each row, and finally perform some task which doesn't depend on n (ie. it's constant). The first step is O( n^2 ), and the second step is O(n), and the third step is O(1). So O(f) = n^2 + n + 1.\n\nNow, notice that with sufficiently large values of n, f(n) = n^2 + n + 1 will converge to k * (n^2), so we write O(f) = n^2.\n\nIn other words, since O is often concerned with 'worst-case' behavior, we often limit our discussion to the worst-performing aspect of a given function/algorithm. This is kind of hand-wavy, but O itself is a bit hand-wavy since we are only trying to establish a general, worst-case, upper-bound.\n\nEDIT: Sorry for the mistakes in this...fixed em. &gt; So O is a function that takes another function as its argument and has as an output another function.\n\nThis isn't totally accurate (and the confusion is mostly the fault of stupid notation). If this were true, then we have n = O(n), and also n + 1 = O(n), so (if O really just maps functions to functions), n = n+1, and so 0 = 1 (therefore the sun is made of peanut-butter). &gt; Can I view Big O as function with an output?\n\nYes. O(f(x)) is the upper bound of f(x). So you can write g(x) = O(f(x)) where f is the upper bound of g for sufficiently large x.\n\nSo O is a function that takes another function as its argument and has as an output another function.\n\n----\n\nIf I understand what you mean by a Level set, then also yes. Assuming you are talking about the way that this is typically used in computer science, O(k * f(x)) = O(f(x)). This also implies that O(f(x)+c) = O(f(x)), since for sufficiently large x f(x)+c will be bounded by k * f(x).\n\nExample: Say you have a square matrix of n by n. Some function f has to iterate over all elements in this matrix, and then iterate over each row, and finally perform some task which doesn't depend on n (ie. it's constant). The first step is O( n^2 ), and the second step is O(n), and the third step is O(1). So O(f) = n^2 + n + 1.\n\nNow, notice that with sufficiently large values of n, f(n) = n^2 + n + 1 will converge to k * (n^2), so we write O(f) = n^2.\n\nIn other words, since O is often concerned with 'worst-case' behavior, we often limit our discussion to the worst-performing aspect of a given function/algorithm. This is kind of hand-wavy, but O itself is a bit hand-wavy since we are only trying to establish a general, worst-case, upper-bound.\n\nEDIT: Sorry for the mistakes in this...fixed em. ",
    "url" : "http://www.reddit.com/r/compsci/comments/rffwk/there_was_a_thread_on_big_o_that_was_deleted_but/"
  }, {
    "id" : 39,
    "title" : "What advantages are their to using a D or SR flip-flop circuit over a JK flip-flop?",
    "snippet" : "Oddly specific, but my text book on circuits raises the question and I can't come up with an answer. As far as I can see the JK flip-flop can do everything a D flip-flop or an SR flip-flop can do, so why haven't the D or SR flip-flops been replaced by JKs?  One possibility - JK uses more gates, and so more transistors. Thus, more expensive to produce. Always this.  Remember that real world designs always have to get past the accountant's desk.\n\nSimpler devices are often smaller too. One possibility - JK uses more gates, and so more transistors. Thus, more expensive to produce. [deleted]  Well, an SR only works well if you don't allow for the possibility of an input of S=1 R=1, which would make it the same as a JK (but with fewer gates, as agmatine noted).\n\nA D flip flop design might be better in terms of other logic, depending on what you're making - although I can't particularly think of anything off the top of my head except for asynchronous counters. And even then, you could use the JK as a T flip-flop.\n\nOverall, I'd say it just depends on design. [deleted]     ",
    "url" : "http://www.reddit.com/r/compsci/comments/rf6r7/what_advantages_are_their_to_using_a_d_or_sr/"
  }, {
    "id" : 40,
    "title" : "Anybody into formal methods/specifications/VDMsl?",
    "snippet" : "I'm studying this sort of thing in a big way at university at the moment, and I was wondering if any other Redditors enjoy this more mathematical aspect of design, logic and program proving and so on. \n\nAnd perhaps, does anybody use these kind of methods in a real world context? I'd be interested to hear about that.  ",
    "url" : "http://www.reddit.com/r/compsci/comments/rfl15/anybody_into_formal_methodsspecificationsvdmsl/"
  }, {
    "id" : 41,
    "title" : "Researchers; how do you gather results efficiently?",
    "snippet" : "Hi there compsci.\n\nIn short: When developing a new method within your field, how do you structure your results gathering as to be most efficient?\n\nI'm currently doing my masters thesis in computer science. I'm struggling with how I gather results. I have created a new method within image processing. The base method is close to done, but there's still a few aspects I'd like to try out (if time allows it) and I currently have quite a few tuning parameters. I use version control on my source code.\n\nSo, my problem is that I tend to override previous results, and at times I tend to tune more than 1 parameter at once (stupid I know). However I really lack some way to structure my result gathering of the method on various images, with various parameters, with different implementations. I have so many tuning options I constantly loose oversight.\n\nCurrently I try to tie up results with a revision from the version control, while noting down parameters but it's quite time consuming, to an extend where I tend to lapse back into my bad habits of quickly editing a parameter or two and running the method, overriding the earlier result. Even worse is when I modify the source code with a few minor changes and testing. Several times I've done a few changes in a row without a single commit to the version control, yielding bad results and having to backtrack everything to the last revision.\n\nI realise I ought to know how to structure this by now. However, I ask now to try to remedy this situation before it's too late.\n\nThanks in advance    Do you know about [Sumatra](http://software.incf.org/software/sumatra)?\n\nI don't use it, but only because by the time I found out about it I had already reimplemented most of it for myself.\n\nEvery experiment run saves its results to a new directory. Inside this directory, a configuration file contains everything needed to reproduce the results, including the specific repository version for the source code used. There's an optional \"comments\" file where I can enter free-form text about the experiment results. In addition to being very useful when you need to understand why you ran that experiment months ago, it's also indexed and thus great for search. I \"tag\" interesting/important results using symlinks.\n\nFor handling experiment parameters, I've coded my programs to accept a YAML file containing all the parameters. Then, a helper script creates the directory, creates the YAML file and finally runs the code. By forcing parameters to be read only from a file, I make sure that every parameter is actually documented in that file.\n\nI find that keeping *every* single results you've ever produced actually helps you in the long run. It does use a *lot* of disk space depending on how much data your experiments produce, though. [deleted]",
    "url" : "http://www.reddit.com/r/compsci/comments/re3sk/researchers_how_do_you_gather_results_efficiently/"
  }, {
    "id" : 42,
    "title" : "Sorting algorithm to minimize head movement?",
    "snippet" : "This thought was inspired by having to alphabetically sort papers and may not be very useful (maybe something similar with distributed sorting?) but seemed interesting to think about.\n\nBasically, a fundamental assumption for a lot of sorting algorithms is that we have random access to any index. What happens when this assumption isn't true, and that is the main thing we want to minimize? More concretely, imagine that our memory is a tape with the list elements on it, and it can move the head left or right 1 element at a time. If we can hold a constant amount of memory in the state independently of the tape (e.g. we don't have to store, say, the current index on the tape itself), how can we minimize the number of head movements?\n\nDoes anyone have any ideas of how this might be done, what completely obvious solution I've missed, or any references to things that have addressed this concern?   This doesn't answer your question so much as avoids it...but the best way (imo) to sort something like papers or a deck of cards isn't to minimize head movements but to increase the number of heads.\n\nEnter: [quick sort](http://en.wikipedia.org/wiki/Quicksort)\n\nAlgorithmically, it's simple enough for a human to do. By continually splitting the size of your list in half, you avoid needing to 'index' particular elements. By the time you get to sorting specific elements your indexes will be 0 or 1. In practice, when doing this by hand you only need to split papers enough that you can quickly sort whatever pile you have in front of you.\n\nExample:  \nStart with A-Z in pile 1.  \nGo through pile 1, setting A-K in pile 1', L-Z in 2'  \nSplit pile 1' into A-F in pile 1'', G-K in 2''  \nContinue in this manner until you have just 4 or 5 papers in each pile, sort those by insertion, swapping, or whatever else.\n\nEDIT:\n\nWanted to add a few things. Another option is something like a selection sort. Run through the papers, find the lowest, put it at the top of the pile. In this case it might be easiest to put it at the end...since if you do this then you will have your sorted elements at the bottom, and when it's all sorted your bottom becomes your top. The problem here is that it requires human memory to answer the question \"what is the first in this list\".\n\nSo you have a stack of papers, but you aren't sure which student has the first name alphabetically. You run into an Adams, then Adama, then you see Alberts, and finally Abel. You can see how this would get confusing after some time. You have to constantly keep the minimum value in your head, and then compare that to each new name you see. But the minimum can change during the process...so it gets to be taxing (again, in my experience). Quicksort has a handy property of offloading all of that memory by increasing the space you need on your desk/floor. The piles serve as your memory, and in each pile you perform the same comparison \"is this greater than or less than my splitting value?\"\n\n\nEDITED AGAIN: That's quicksort...not merge sort. It's not exactly quicksort either, because quicksort sorts an array in place, whereas your paper sort is a kind of bulk selection merge sort that uses n extra storage or dynamic extra storage or similar. But that's all very inexpensive in desk space.  This doesn't answer your question so much as avoids it...but the best way (imo) to sort something like papers or a deck of cards isn't to minimize head movements but to increase the number of heads.\n\nEnter: [quick sort](http://en.wikipedia.org/wiki/Quicksort)\n\nAlgorithmically, it's simple enough for a human to do. By continually splitting the size of your list in half, you avoid needing to 'index' particular elements. By the time you get to sorting specific elements your indexes will be 0 or 1. In practice, when doing this by hand you only need to split papers enough that you can quickly sort whatever pile you have in front of you.\n\nExample:  \nStart with A-Z in pile 1.  \nGo through pile 1, setting A-K in pile 1', L-Z in 2'  \nSplit pile 1' into A-F in pile 1'', G-K in 2''  \nContinue in this manner until you have just 4 or 5 papers in each pile, sort those by insertion, swapping, or whatever else.\n\nEDIT:\n\nWanted to add a few things. Another option is something like a selection sort. Run through the papers, find the lowest, put it at the top of the pile. In this case it might be easiest to put it at the end...since if you do this then you will have your sorted elements at the bottom, and when it's all sorted your bottom becomes your top. The problem here is that it requires human memory to answer the question \"what is the first in this list\".\n\nSo you have a stack of papers, but you aren't sure which student has the first name alphabetically. You run into an Adams, then Adama, then you see Alberts, and finally Abel. You can see how this would get confusing after some time. You have to constantly keep the minimum value in your head, and then compare that to each new name you see. But the minimum can change during the process...so it gets to be taxing (again, in my experience). Quicksort has a handy property of offloading all of that memory by increasing the space you need on your desk/floor. The piles serve as your memory, and in each pile you perform the same comparison \"is this greater than or less than my splitting value?\"\n\n\nEDITED AGAIN: That's quicksort...not merge sort.  I use merge sort.\n\nSort small groups, then gather them back once you have the sorted piles ready.\n\nAlso helpful if you have friends that would assist you in sorting the small groups.  ",
    "url" : "http://www.reddit.com/r/compsci/comments/rd0wo/sorting_algorithm_to_minimize_head_movement/"
  }, {
    "id" : 43,
    "title" : "Intro to comp arch (videos under handouts + labs build up to a RISC based cpu)",
    "snippet" : "  [deleted]",
    "url" : "http://6004.lcs.mit.edu/"
  }, {
    "id" : 44,
    "title" : "Mathematics for Computer Science",
    "snippet" : "  To put this in context, this is the textbook for the introductory math course in MIT's Computer Science department. Students typically take it in their Sophomore year, though it doesn't require any math past Single Variable Calculus (18.01 for MIT folks). \n\nThe intention of 6.042 is to fill in holes in student's math backgrounds. It is assumed that the student is going to take Introduction to Algorithms (6.006) soon after this course, and is thus going to need to know at least the basics of the various fields of discrete math. \n\nAdditionally, the class has a strong focus on how to find and then write proofs in math, which students need by the time they get to the second level algorithms class (6.046). \n\n**Here are the OCW links to the relevant classes:**\n\n[6.042](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/index.htm) Has problem sets to go along with the readings in the OP. No solutions, though the OCW version from [2005](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-spring-2005/) does have problems with solutions. Not sure how well the 2005 version of 6.042 matches up though.\n\n[6.006](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-spring-2008/) Lecture notes along with assigned readings from [CLRS](http://mitpress.mit.edu/algorithms/) (which I highly recommended, though I don't believe it's legally available for free online)\n\n[6.046](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/) Video Lectures, Problem Sets with Solutions, and assigned readings from CLRS. Here be maths (6.046 is taught jointly with the Math Dept).\n\n[18.01](http://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/) Video lectures and assignments, though no solutions. It's just Single Variable Calculus.\n\n       PDF warning  800+ pages of math, uh I'm not sure if I'm ready for this. 800+ pages of math, uh I'm not sure if I'm ready for this.  ",
    "url" : "http://courses.csail.mit.edu/6.042/spring12/mcsfull.pdf"
  }, {
    "id" : 45,
    "title" : "[bitc-dev] Retrospective Thoughts on BitC",
    "url" : "http://www.coyotos.org/pipermail/bitc-dev/2012-March/003300.html"
  }, {
    "id" : 46,
    "title" : "Combining semantic web and computer vision?",
    "snippet" : "I am taking a semantic web class this semester and instead of having a final exam, we are going to do a project. The project can either be an application/system of some sort or it can be theoretical.\n\nIn order for me to feel motivated to do these types of projects I try to relate them as best as I can to my research area: computer vision; specifically, image retrieval. But I am having a really, really hard time trying to think of a project that combines the two together.\n\nI have searched around a little bit and come across Semantic Image Annotation and Retrieval (http://www.svcl.ucsd.edu/projects/imgnote/), but that is the most I have found.\n\nCan anyone offer some suggestions?  Have a look at [image net](http://www.image-net.org/). It contains images ordered by semantic class, and it has links to algorithms and classification metrics used on the dataset.  &gt; In order for me to feel motivated to do these types of projects I try to relate them as best as I can to my research area: computer vision; specifically, image retrieval. But I am having a really, really hard time trying to think of a project that combines the two together.\n\nIf Your *research area* is image retrieval, your imagination must be really, really poor if you can't see how a semantic ordering might benefit image retrieval.\n\nActually, I don't know. I'm not that familiar with those fields, but ideas started popping in my head the moment I read your post! For example, take some vocabulary that when used directly as keywords doesn't bring up much relevant images. For your project, try to improve on those image results by using your semantic web to first decompose that vocabulary into a less abstract set of words with similar semantics, and then do a search with that set of words and compare with the direct method. Or better yet, improve your semantic web by using image search results as the feedback component of a supervised learning algorithm for semantic equivalences. There's a whole bunch of fascinating stuff there I'm sure. I appreciate your suggestions and yes, I have thought about those types of projects, but the problem is that there is essentially no computer vision involved. You are not analyzing any of the images, you aren't applying any algorithms to describe the images (in terms of histograms, local invariant descriptors, etc). You are simply analyzing the text or annotations of an image (such as the text surrounding an image on a webpage).\n\nHowever, you did touch on something important: relevance feedback. I think creating a \"search by example\" image retrieval system and then asking for keywords would help with the process. I will definitely keep that in mind.",
    "url" : "http://www.reddit.com/r/compsci/comments/rbb79/combining_semantic_web_and_computer_vision/"
  }, {
    "id" : 47,
    "title" : "Computer Science research topics for master's ",
    "snippet" : "Hey all CS people,\n\nI'll be starting a computer science master's program in the fall. I'm trying to decide on a thesis research topic as I would rather do research over the applied cs. Some of my interests are database, security, and forensics. Our departments forensics program is just beginning but it has grown quickly in just a semester. \n\nSo what are some possible research topics that you other CS master's have done, possibly in these topics? I'm not asking for in depth information, but just the general topics to kind of help me drill down into some options. \n\nThanks  Mine's still fuzzy, but my MS CS thesis topic is on thermal aware scheduling algorithms for server provisioning in data centers. The simple run down is: \n\nSay you have a data center full of servers that you can let go into 'sleep' modes. As you get more and more requests coming in from the internet, which servers should you wake to handle the workload? I'm trying to optimize on cooling energy.  Figuring out which servers contribute to 'thermal hotspots' in the room is the biggest issue to solve. Computational Fluid Dynamic simulators such as OpenFOAM and FloVent are useful for modeling the thermal side of things. \n\nThere's another related topic of \"heat capturing\" in data centers. Essentially, there're these devices called peltier chips that convert heat *gradients into electricity. What do data centers spend a lot of money doing? Removing heat from their rooms and buying electricity to run their servers. Sounds like there's a nice synergy here. . . (whoever solves this and makes a device is basically a multi-millionaire. No joke.)\n\nedit: corrected peltier device to specify heat *gradient Wow that sounds amazing. I love the scheduling wake ups. I haven't ever heard of this. Is it something commonly used?\n\nThe other idea would be great too. I'm not really a hardware guy though so it's way out of my league. I would definitely enjoy reading more into it though.  Wow that sounds amazing. I love the scheduling wake ups. I haven't ever heard of this. Is it something commonly used?\n\nThe other idea would be great too. I'm not really a hardware guy though so it's way out of my league. I would definitely enjoy reading more into it though.  Mine's still fuzzy, but my MS CS thesis topic is on thermal aware scheduling algorithms for server provisioning in data centers. The simple run down is: \n\nSay you have a data center full of servers that you can let go into 'sleep' modes. As you get more and more requests coming in from the internet, which servers should you wake to handle the workload? I'm trying to optimize on cooling energy.  Figuring out which servers contribute to 'thermal hotspots' in the room is the biggest issue to solve. Computational Fluid Dynamic simulators such as OpenFOAM and FloVent are useful for modeling the thermal side of things. \n\nThere's another related topic of \"heat capturing\" in data centers. Essentially, there're these devices called peltier chips that convert heat *gradients into electricity. What do data centers spend a lot of money doing? Removing heat from their rooms and buying electricity to run their servers. Sounds like there's a nice synergy here. . . (whoever solves this and makes a device is basically a multi-millionaire. No joke.)\n\nedit: corrected peltier device to specify heat *gradient Do you have any preprints or working papers? This is an interesting topic! Mine's still fuzzy, but my MS CS thesis topic is on thermal aware scheduling algorithms for server provisioning in data centers. The simple run down is: \n\nSay you have a data center full of servers that you can let go into 'sleep' modes. As you get more and more requests coming in from the internet, which servers should you wake to handle the workload? I'm trying to optimize on cooling energy.  Figuring out which servers contribute to 'thermal hotspots' in the room is the biggest issue to solve. Computational Fluid Dynamic simulators such as OpenFOAM and FloVent are useful for modeling the thermal side of things. \n\nThere's another related topic of \"heat capturing\" in data centers. Essentially, there're these devices called peltier chips that convert heat *gradients into electricity. What do data centers spend a lot of money doing? Removing heat from their rooms and buying electricity to run their servers. Sounds like there's a nice synergy here. . . (whoever solves this and makes a device is basically a multi-millionaire. No joke.)\n\nedit: corrected peltier device to specify heat *gradient That sounds fucking amazingly interesting. Just starting out on my MS myself with concentration in ML. Not quite sure on thesis topic yet, though.\n\nBest of luck! Mine's still fuzzy, but my MS CS thesis topic is on thermal aware scheduling algorithms for server provisioning in data centers. The simple run down is: \n\nSay you have a data center full of servers that you can let go into 'sleep' modes. As you get more and more requests coming in from the internet, which servers should you wake to handle the workload? I'm trying to optimize on cooling energy.  Figuring out which servers contribute to 'thermal hotspots' in the room is the biggest issue to solve. Computational Fluid Dynamic simulators such as OpenFOAM and FloVent are useful for modeling the thermal side of things. \n\nThere's another related topic of \"heat capturing\" in data centers. Essentially, there're these devices called peltier chips that convert heat *gradients into electricity. What do data centers spend a lot of money doing? Removing heat from their rooms and buying electricity to run their servers. Sounds like there's a nice synergy here. . . (whoever solves this and makes a device is basically a multi-millionaire. No joke.)\n\nedit: corrected peltier device to specify heat *gradient That sounds like a fucking brilliant thesis idea. I'm interested in seeing how this works out. Remember to post this to [/r/compsci](/r/compsci), ok? Mine's still fuzzy, but my MS CS thesis topic is on thermal aware scheduling algorithms for server provisioning in data centers. The simple run down is: \n\nSay you have a data center full of servers that you can let go into 'sleep' modes. As you get more and more requests coming in from the internet, which servers should you wake to handle the workload? I'm trying to optimize on cooling energy.  Figuring out which servers contribute to 'thermal hotspots' in the room is the biggest issue to solve. Computational Fluid Dynamic simulators such as OpenFOAM and FloVent are useful for modeling the thermal side of things. \n\nThere's another related topic of \"heat capturing\" in data centers. Essentially, there're these devices called peltier chips that convert heat *gradients into electricity. What do data centers spend a lot of money doing? Removing heat from their rooms and buying electricity to run their servers. Sounds like there's a nice synergy here. . . (whoever solves this and makes a device is basically a multi-millionaire. No joke.)\n\nedit: corrected peltier device to specify heat *gradient \"peltier chips that convert heat *gradients* into electricity\".... Yep, you're right. The key is getting a big temp difference. Water cooling is a big part of that. I would've thought you'd need to be in Saskatoon or something to make that economically viable. For the record, the mechanical/materials engineering side of this project isn't my part. With that out of the way:\nRight now, yes (**Canada, land of data centers** would be pretty awesome.), but in the future, no. There are some materials groups that know the utility of these chips and are working hard to increase the efficiency and drop the required delta_temp. On top of it, CPUs are getting denser and creating more heat. Our proof of concept right now is to use a [lithium bromide chiller](http://en.wikipedia.org/wiki/Absorption_refrigerator). We hope to be able to move to peltier in the future though! \n\nWhat is your background in all of these things? Mine's still fuzzy, but my MS CS thesis topic is on thermal aware scheduling algorithms for server provisioning in data centers. The simple run down is: \n\nSay you have a data center full of servers that you can let go into 'sleep' modes. As you get more and more requests coming in from the internet, which servers should you wake to handle the workload? I'm trying to optimize on cooling energy.  Figuring out which servers contribute to 'thermal hotspots' in the room is the biggest issue to solve. Computational Fluid Dynamic simulators such as OpenFOAM and FloVent are useful for modeling the thermal side of things. \n\nThere's another related topic of \"heat capturing\" in data centers. Essentially, there're these devices called peltier chips that convert heat *gradients into electricity. What do data centers spend a lot of money doing? Removing heat from their rooms and buying electricity to run their servers. Sounds like there's a nice synergy here. . . (whoever solves this and makes a device is basically a multi-millionaire. No joke.)\n\nedit: corrected peltier device to specify heat *gradient  I'm right now finishing up my Master's Thesis on \"Eliminating SQL Injection and Cross Site Scripting Using Aspect Oriented Programming\". If you're interested, in security, forensics and stuff you can look into AOP and see how you can apply it to some interesting areas.  Nice. Are you focusing on one language or framework (php, asp.net, ...)? I started looking through some research papers and found some on Forensics with Databases and Data mining. It looks like a pretty interesting field.  I'm creating a program that will generate aspects for eliminating security vulnerabilities identified by source code analyzers. For my project, it's only Java applications but I plan on expanding it to other languages as well.  That's actually really cool. That would be useful. In our program it takes a lot for the professor to drill \"parameter-ize your queries\" into some of the heads of the students. I honestly don't know much Java, although I'm a TA for our Java classes. We only use it as an introductory language and then use C#, C++, and PHP heavily depending on concentrations.  Yep. You'd be amazed how many people don't do simple things like parameterized queries in the professional world. Grinds my gears.  What do you mean? They use the query \"as is\" from the client?? At a guess he means \"select x from y where z=\" + user_entered_string - the \"user_entered_string\" comes straight from a web form...\n\nAnd yeah, I've seen that a fair amount in the professional world.\n Thats a slight simplification of the actual concept of parameterized queries. In many languages, there's a couple ways to go about creating a query:\n\n1. String based, no sql-escapes (really bad)\n\n        sql = \"SELECT * FROM myTable WHERE name = '$userInput'\"\n        runQuery(sql)\n\n2. String based, manually sql-escaped (bad, but not horrible and sometimes necessary)\n\n        sql = \"SELECT * FROM myTable WHERE name = '[sqlEscape $userInput]'\"\n        runQuery(sql)\n\n3. Parameterized\n\n        sql = \"SELECT * FROM myTable WHERE name = ?\"\n        runQuery($sql, $userInput)\n\nThe second has the coder escaping the value manually. The third tells the system that there is a value to be placed \"at this point\" in the query, hands the value along with the query, and lets the underlying system escape it. \n\nThe third approach can provide additional benefits beyond the risk of forgetting to escape the values, such as query caching (which, if you're running the same query over and over, can result in significant speed benefits) What do you mean? They use the query \"as is\" from the client?? I'm right now finishing up my Master's Thesis on \"Eliminating SQL Injection and Cross Site Scripting Using Aspect Oriented Programming\". If you're interested, in security, forensics and stuff you can look into AOP and see how you can apply it to some interesting areas.  Do you have a website, or some published papers? If not, would you share your thesis when you can? I'm right now finishing up my Master's Thesis on \"Eliminating SQL Injection and Cross Site Scripting Using Aspect Oriented Programming\". If you're interested, in security, forensics and stuff you can look into AOP and see how you can apply it to some interesting areas.  I dont know if this is possible, but I'd love to read your thesis one day.  Sure thing. I have to defend it in front of my committee in a couple weeks and after that it should be published and available to the public.  Very cool, can you keep us informed when it will be available?  Good luck ! Sure thing. I have to defend it in front of my committee in a couple weeks and after that it should be published and available to the public.  As part of your thesis, are you going to go in depth on the concepts of the actual injections?  I've always found those fascinating.  Can't wait to to see it. I am taking three enterprise level open source java applications, doing source code analysis on all of them using Fortify, and then generating aspects based off of that. Once the aspects are generated, I'm weaving them into the byte code of the applications and basically intercepting any potentially malicious content and verifying it.   Upvote because I'm doing a CS master's next year too. I'll be interested in what people have to say.  My masters was in CS Algorithms, so sounds like a totally different topic, but you can browse it if you like.\n\nhttps://theses.lib.sfu.ca/thesis/etd6191 I'm doing an IT concentration. Math, by no means, is my strong suit. I know that sounds terrible for a computer scientists but I admit it as a fault.   I just finished my masters in CS, and I did my final project on migrating biospecimen legacy databases to a new system by mapping and transforming the data from the source system to the target.\n\n**In the end, I was able to migrate about 250,000 biospecimen records from the old system to the new in less than 4 minutes.** I just finished my masters in CS, and I did my final project on migrating biospecimen legacy databases to a new system by mapping and transforming the data from the source system to the target.\n\n**In the end, I was able to migrate about 250,000 biospecimen records from the old system to the new in less than 4 minutes.**   I'm currently in my MS program and am planning to do a programming language theory thesis. Possibly bringing functional programming into a C-style language (imagine using C but it being functional). I'm part time so I still have a long time to figure it out. Right now I'm taking a course in High Performance Computing using C... I don't really want to imagine C at all :) hahaha touche! I'm currently in my MS program and am planning to do a programming language theory thesis. Possibly bringing functional programming into a C-style language (imagine using C but it being functional). I'm part time so I still have a long time to figure it out. What would the point/advantages of such a language be? \n\nI mean would you mix the imperative and the functional paradigmes? Or design a language that looks like C syntax-wise but is functional? It would look like C syntax wise but would be 100% functional. The reason would be as a bridge between the imperative and functional world. I know that a lot of people are put off by the syntax of functional languages and never learn them. This way they could keep their syntax but gain the benefits of a functional language. It would look like C syntax wise but would be 100% functional. The reason would be as a bridge between the imperative and functional world. I know that a lot of people are put off by the syntax of functional languages and never learn them. This way they could keep their syntax but gain the benefits of a functional language.  My master's thesis was on live forensics and anti-forensic techniques for Android devices.  If you're interested in forensics, there are many devices which are very understudied, but could contain useful information for investigators. My master's thesis was on live forensics and anti-forensic techniques for Android devices.  If you're interested in forensics, there are many devices which are very understudied, but could contain useful information for investigators. I have some real-life applications for that research.  Can you link your thesis? (or PM if you don't want a public link).  &gt; database, security, and forensics\n\nMy advisor and some other students in my lab are working on this concept of [undo computing](http://css.csail.mit.edu/undo/), in which one can trace the data dependencies of an attack (at the database operation, system call, etc. level) and basically just revert the attack and get data integrity back. I'm probably not doing it justice; see one of the papers for more details.\n\nMy own master's thesis topic is application sandboxing via lightweight virtual machines. [libvirt-sandbox](http://berrange.com/tags/libvirt-sandbox/), which was announced like two months ago, seems to have several of the same ideas and is actually in a usable state unlike my code. You don't work with Samuel King at UIUC do you?\n\nedit: Guess I could've just moused over your link to see it's linked to MIT.  King has done some neat work with time traveling virtual machines to unwind/replay attacks. No, but now I have a link to a handful of interesting papers to read next time I'm bored. Thanks! :-)  Prove P = NP.  Not only will you get your Masters but you'll get $6mil in cash and the entire world beating a path to your door.\n\nOn the other hand there's also studying cascade failure in cloud services.  That's always fun too.  If you want to get messy you can drag in hardware cascade failures like the power grid is known for and show how it can happen to a cloud service. Don't even attack that problem. Just nibble on the edges. e.g. Which problems in NP might also be in P with some conditions set. That's the problem with it, you have to go ultra-specific. It's interesting though because I learned that sudoku and n-queens are the same class of problem along with database transaction scheduling. I've been wanting to see if you can translate a transaction scheduling problem into a sudoku and then just solve it like that :p Prove P = NP.  Not only will you get your Masters but you'll get $6mil in cash and the entire world beating a path to your door.\n\nOn the other hand there's also studying cascade failure in cloud services.  That's always fun too.  If you want to get messy you can drag in hardware cascade failures like the power grid is known for and show how it can happen to a cloud service. That's my topic, bitch :-p Prove P = NP.  Not only will you get your Masters but you'll get $6mil in cash and the entire world beating a path to your door.\n\nOn the other hand there's also studying cascade failure in cloud services.  That's always fun too.  If you want to get messy you can drag in hardware cascade failures like the power grid is known for and show how it can happen to a cloud service.  Research is good if you plan on doing research, or something kind of resembling research (open-specified problems, little direction, etc) later.  Otherwise, applied CS is probably more marketable and makes a better portfolio.\n\nThat said, I did a MS CS with a thesis.  I generally fell into a topic.  I imagine many people are the same way. I'm not really sure on a career path. I've been talking with one of the people with the MySQL division of Oracle for a position with them but that would require a major move and I don't know if it's something that I'm interested in. Dreamjob wise, my career would have nothing to do with computer science. \n\nI'm doing research based because I would rather go in depth into one subject rather than do the general software engineering route again (our undergrad course is heavily software engineering project based.) No offense but if you already think your dream job is not in CS you need to make sure you truly do enjoy the field. You are so early in your career, perhaps think about CS in combination with another field that would be closer to a more fun job (biology, geology, physics,...). That being said, perhaps your dream job is entirely unrelated to an office job... Oh it is completely unrelated. Dream job would be writing/directing. Practically I know that I have a much better chance of success in computer science compared to film studies. I enjoy the field and I'm good at it so it was a better option. I went through so many ideas before I went to computer science (secondary education, engineering, and film studies).           Unrelated: If your eventual goal is a cs phd, is it more common to do bs -&gt; ms -&gt; phd, or just bs -&gt; phd? bs -&gt; ms -&gt; phd\n\nYou can't just jump from a bs to doctorate program. Sometimes you can go into a random masters program if you have a bs is something else but doctorate is a different thing altogether.  &gt;You can't just jump from a bs to doctorate program.\n\nI'm guessing you mean in your current country, no? Oh, I'm in the US. As far as I know, no one (not even exchange students) can go from just a bs to a phd program. I know that if you show sufficient knowledge and have a bs is some other field then you have a possibility of making into an ms program. Say if you have a degree in mathematics and want to go into a computer science ms. If you showed that you had sufficient programming knowledge then you could make it. \n\nBut, for the US at least, a phd program is a much higher level that I don't think you can skip a step to get to. I don't know of anyone that has gone from bs -&gt; phd. But there is a teacher in our department that has his bs in physics and his ms in computer science.  I am in the US also, California actually.  You can join a PhD program straight out of undergrad.  However, on the way of working through your PhD you will earn a masters degree.  You don't need to wait until after you get your MS to apply for a PhD program. Oh, I'm in the US. As far as I know, no one (not even exchange students) can go from just a bs to a phd program. I know that if you show sufficient knowledge and have a bs is some other field then you have a possibility of making into an ms program. Say if you have a degree in mathematics and want to go into a computer science ms. If you showed that you had sufficient programming knowledge then you could make it. \n\nBut, for the US at least, a phd program is a much higher level that I don't think you can skip a step to get to. I don't know of anyone that has gone from bs -&gt; phd. But there is a teacher in our department that has his bs in physics and his ms in computer science.  Oh, I'm in the US. As far as I know, no one (not even exchange students) can go from just a bs to a phd program. I know that if you show sufficient knowledge and have a bs is some other field then you have a possibility of making into an ms program. Say if you have a degree in mathematics and want to go into a computer science ms. If you showed that you had sufficient programming knowledge then you could make it. \n\nBut, for the US at least, a phd program is a much higher level that I don't think you can skip a step to get to. I don't know of anyone that has gone from bs -&gt; phd. But there is a teacher in our department that has his bs in physics and his ms in computer science.  &gt;You can't just jump from a bs to doctorate program.\n\nI'm guessing you mean in your current country, no? Is there any country where a phd program doesn't require a ms? I would surprised if that is the case, my thought is it would more or less render that countrys students worthless on an international market. There are plenty of schools in the US that admit students into a PhD program without getting a masters, including MIT, Stanford, Cornell, etc. I'm willing to bet that their alumni are regarded internationally as exceptionally good at what they do.\n\nMy impression is that MS programs in the US are treated more as professional training in a specific subject rather than rigorous research program. Generally, in pursuing a PhD you meet the requirements for whatever professional training is necessary in your subject and receive a MS anyway. I think this is the reason why Master's degrees in things such as the biological sciences are considered to be somewhat worthless. It says 'I know how science works, but I don't know how to do it.' Thanks for a good answer. Out of curiosity; does this mean you spend ~2 years less than if you completed the full MS before pursuing a PhD or can MS students take credit for some of their studies? bs -&gt; ms -&gt; phd\n\nYou can't just jump from a bs to doctorate program. Sometimes you can go into a random masters program if you have a bs is something else but doctorate is a different thing altogether.  I know tons of people (in CS as well as other fields) who enter into PhD programs straight out of undergrad. This is probably more common at my college (in the US, extremely good physics, geology, math, chemical engineering, top 20 CS department) than it is for people to enter into MS programs. Some PhD programs will give you an MS on the way to a PhD, others will only grant you an MS if you drop out of the PhD program after a certain number of years (so you only leave with 1 degree, either a PhD or a MS). I know of a few people that entered into PhD programs with the idea of dropping out after 2 or 3 years with an MS mostly because its a free MS (almost all hard science PhD programs pay grad students a 20k-30k yearly stipend). bs -&gt; ms -&gt; phd\n\nYou can't just jump from a bs to doctorate program. Sometimes you can go into a random masters program if you have a bs is something else but doctorate is a different thing altogether. ",
    "url" : "http://www.reddit.com/r/compsci/comments/r8y4y/computer_science_research_topics_for_masters/"
  }, {
    "id" : 48,
    "title" : "Jitter in distributed systems as a grad research topic",
    "snippet" : "I was watching [Scalability at YouTube](http://youtu.be/G-lGCC4KKok), and Mike Solomon mentions that with a distributed system as large as YouTube, they have to add jitter to prevent creating \"thundering herds.\" He introduces the example of a cache expiration causing everyone trying to view a video to synchronize on waiting for a fresh version of the video, where \"everyone is actually removing entropy from the system, so you have to put some back in.\"\n\nI'm sure YouTube isn't the first system to come across this, but I thought this need for jitter was a pretty cool concept, and it could result in some interesting grad work. Is anybody doing this?   &gt;  Mike Solomon mentions\n\nat 18:15.\n\n...\n\nHere's a vaguely related paper: [desynchronization in wireless networks](http://www.eecs.harvard.edu/ssr/papers/ipsn07-degesys.pdf). I think \"desynchronization\" might be a good keyword here, but it's not enough on it's own -- I hope someone else can give you better references. I actually worked on that concept and implemented stuff on the wireless sensors... :P   Luiz Andre Barroso from Google talks about this problem in http://dl.acm.org/citation.cfm?id=2019527&amp;bnc=1  ",
    "url" : "http://www.reddit.com/r/compsci/comments/r95yd/jitter_in_distributed_systems_as_a_grad_research/"
  }, {
    "id" : 49,
    "title" : "Is it possible to join the field of CompSci after receiving an unrelated undergrad degree?",
    "snippet" : "Edit: By CompSci I guess I mean software and web development, not academic work.\nMy degree is in Design and Architecture, I'm 23 and graduated last year, but am not really excited about the future career prospects, salary, and cut-throat environment of Design.\n\nWould I have to go back for a second bachelors in compsci in order to land a job in the field? Masters? Is it possible to teach myself? Is there an area of programming that would be easiest to break into? Web development? My friends who work for software companies, google, etc. seem to have pretty great lives even if the work isn't always socially meaningful.\n\nAre you professionals happy with your career? Any insights would be much appreciated!  Short answer: Yes.\n\nLong answer: A college degree is not required. It might look good if you're trying to work at Google, but there are tons of smaller companies that will grade you on your programming chops, not your degree (or lack thereof).\n\nLearn to program, and the jobs will come. I don't think company size necessarily matters, one of my coworkers at Citrix had only a high school diploma. I don't think company size necessarily matters, one of my coworkers at Citrix had only a high school diploma. I know my company (defense contractor, 100k+) wouldn't consider someone for a programming job that didn't have a math/science degree. I know my company (defense contractor, 100k+) wouldn't consider someone for a programming job that didn't have a math/science degree.   I have a degree in psychology and am currently working for my second software company. I taught myself. \n\nAt my first software company, I was doing xml work, which was pretty entry level but was surrounded by developers who got me started with the basics. I eventually started writing basic C# scripts for internal use. I took a couple courses at the local community college, found an internship and now I am a salaried software developer. Thanks for the info! and you're happy with your decision, job, pay, etc.?  Don't get a 2nd bachelors degree. It is a waste of time and money (you don't get financial aid for a second bachelors in the States). If you want to go back to school, then get a Masters. Otherwise, just start cranking out some code and projects and put them up for all to see. Create a blog and write about the code you have written or about how you solved particular problems. I'm currently getting financial aid for a second bachelor's degree in the United States. Really? I was told I wouldn't be eligible. Government or private financial aid?  Weird.\n\nI think it's government.  I'm pretty sure it's subsidized.  I have maybe 12k in loans from the first B.A. outstanding.  I'm drawing another $5,000/yr now (so not a lot!).\n\n---\n\nI did a quick search - you're not eligible for *grants*, but you are eligible for *loans* as long as you haven't hit a cap of some sort.  Not sure where the cap is, but it does change based on your situation. Interesting. Are you going to a public school or private?\n  \n  Half of my loans are subsidized the other are not but still from the government which gives more leniency than private loans. Public school! Ah. And thanks for the edit above. Maybe the the person got it wrong that I talked to. Either way, I'm glad she talked me out of a BA and into the MS. \n  \n  I'm curious: what is your first BA in and what are you getting now? History and Computer Science Coolio! Any particular reason why you didn't do a MS instead of a bachelors? Don't get a 2nd bachelors degree. It is a waste of time and money (you don't get financial aid for a second bachelors in the States). If you want to go back to school, then get a Masters. Otherwise, just start cranking out some code and projects and put them up for all to see. Create a blog and write about the code you have written or about how you solved particular problems. \"financial aid\"\n\nSo this is what you call being bankrupt after college in the US, i see haha. I'm thankful for it. At $2,000 a class, I wouldn't be able to afford grad school without it (even working full time). [deleted] 'Cause America education is damn worth it duh! /sarcasm\n  \n  Well, for me it is a private university, so that increases the price. Second, it seems to be the status quo in the States. If they charge any less than they (a) get perceived as a inferior school and (b) lose out on money they could've gotten from students. \n  \n  Oh and it gets worse. My school is on quarters, so I take 1 class a quarter (I'm part time) so I pay $8,000 a year (more like $10k since summer session is split up). One reason why I'm willing to pay so much is that I know that I'll be able to make up the difference relatively easily once I graduate. Now, if I was getting an MA in English, that would be a totally difference story.\n  \n  &gt;I pay 90€... per semester\n  \n  That's awesome. I'm seriously considering moving to Europe for a PhD if I end up going that route. In the states you don't pay to do a PhD. They allow you to do a PhD at their institution, give you a stipend, and they (faculty) basically get super technical work from extremely bright people for next to nothing. A PhD shouldn't cost you anything but time and opportunity. In the states you don't pay to do a PhD. They allow you to do a PhD at their institution, give you a stipend, and they (faculty) basically get super technical work from extremely bright people for next to nothing. A PhD shouldn't cost you anything but time and opportunity. That's true for STEM fields, but not for all other PhDs (humanities, social sciences). I was implicitly assuming STEM since this is the compsci subreddit. But my understanding is that this is true in general at any competitive institution. I've never heard of any reputable PhD program that expects the candidate to pay for their program (humanities or otherwise). Due to the length and nature of a PhD that sounds unreasonable. Can you give me some examples of where this is untrue? (genuinely curious) Many education PhDs I've met, who received their degrees while teaching (grade school level). \n\nI also met several PhD candidates who were unfunded, while I was working in the history department of a large state university (as a student assistant).\n\nGoogle \"unfunded PhD\" there are plenty of examples. [deleted] How much is your education subsidized by taxes? A lot....as well as our healthcare Right. We, on the other hand, have chosen to take less taxes and have a massive military. &gt;have chosen to take less taxes and have a massive military.\n\nWhich is why we are broke! Right. We, on the other hand, have chosen to take less taxes and have a massive military. Screw that geeky crap, we want to kill people - America (and yes, I'm American) [deleted] Don't get a 2nd bachelors degree. It is a waste of time and money (you don't get financial aid for a second bachelors in the States). If you want to go back to school, then get a Masters. Otherwise, just start cranking out some code and projects and put them up for all to see. Create a blog and write about the code you have written or about how you solved particular problems. A second undergrad degree is essentially never a good idea.  Yes! I got my BS in Mass Communications last year and I'm currently enrolled in a post-bac CS certificate program while working in a field related to my degree.\n\nI'll finish my certificate in 2 years, after which I'll be able to get a job at my current company as a programmer.  Making you earn a 2 year certificate to be hired at a company you already work at is... interesting.  They're going to end up doing a lot of on the job training, certificate or not.  Absolutely. I hold a Bachelor's degree in the humanities. With half a year to go before finishing my BS I decided that CS is more for me. What I did was to find a Master's program geared towards people with no CS background, and worked my ass off, skipping as many easy classes as possible in order to take the harder courses.\n\nI will be defending my MS. thesis in April, and I had several companies interested in hiring me, but decided to pursue a PhD (where I also got accepted to several top schools).\n\nI have several classmates with similar backgrounds who have all managed to find programming jobs as well. Do note that landing a job at something like Google is a different story. For that you would have to work your ass off, acquire good programming skills quite fast, and become good at algoritchmics and such. I started as a CS major, then switched to Philosophy, because I found that I love it too. I want to go to graduate school in CS. What's the attitude of MS programs toward humanities undergrads?  Was it hard to get in? I didn't find it hard to get into the MS program, but it was also geared towards people like me. For a normal CS master's program it might be harder, I don't really know. \nI did successfully go from this program to a normal CS PhD program though. What university is this? What do I look for to see if an MS program is geared toward non-CS undergrads? See my response to Streetdude's question for some elaboration on my University.\n\nWhat to look for depends on your non-CS background, but I would say that it needs to have the following: A CS style discrete math course and an algorithms and data structures course. These are both courses that would normally be covered in undergrad. Beyond these basic necessities I would aim for something with a lot of flexibility. Obviously a 2 year degree, where half a year is spent on thesis writing, can't cover everything that a full CS undergrad will, and therefore I would try to cover the basics mentioned above, and then try to specialize relatively fast. This is basically what I've done, I have fairly strong knowledge in AI and algorithms, whereas my knowledge on topics like networks or compilers is downright pitiful. This has not been an obstacle so far, but it does mean that I will need to do a little catching up for some of my phd breadth courses.\n\nAnother piece of advice is to work hard and study on your own. One thing that you should not do is spend a whole semester on intro to programming/CS style courses, teach yourself that sort of thing before starting the MS. I also taught myself most of linear algebra and calc, which is another thing that it might be hard to find time for with only 3 semesters. I've got plenty of self-teaching, a software dev internship down (and another upcoming), and I switched from CS to philosophy after a couple years. I'm trying to decide between double majoring w/ CS (giving me less free time), versus just doing a phil major. But one of the phil profs is convinced and seems to have convinced me that I'll have much better chances if I double-major, so that's what I guess I'll do. Absolutely. I hold a Bachelor's degree in the humanities. With half a year to go before finishing my BS I decided that CS is more for me. What I did was to find a Master's program geared towards people with no CS background, and worked my ass off, skipping as many easy classes as possible in order to take the harder courses.\n\nI will be defending my MS. thesis in April, and I had several companies interested in hiring me, but decided to pursue a PhD (where I also got accepted to several top schools).\n\nI have several classmates with similar backgrounds who have all managed to find programming jobs as well. Do note that landing a job at something like Google is a different story. For that you would have to work your ass off, acquire good programming skills quite fast, and become good at algoritchmics and such. Can I ask what Masters program you went into? Feel free to PM it to me if you don't want to post personal information. I'm in a similar situation and a Masters geared towards people without a CS background sounds amazing. I'm sure I'm not the only one who would be interested. Absolutely. I hold a Bachelor's degree in the humanities. With half a year to go before finishing my BS I decided that CS is more for me. What I did was to find a Master's program geared towards people with no CS background, and worked my ass off, skipping as many easy classes as possible in order to take the harder courses.\n\nI will be defending my MS. thesis in April, and I had several companies interested in hiring me, but decided to pursue a PhD (where I also got accepted to several top schools).\n\nI have several classmates with similar backgrounds who have all managed to find programming jobs as well. Do note that landing a job at something like Google is a different story. For that you would have to work your ass off, acquire good programming skills quite fast, and become good at algoritchmics and such.  Architects/Designers make fantastic computer scientists. Do it.      Yes, I suggest you start programming now. Decide what kind of programming you want to do, and learn about it. (It sounds like you want to do web design?) Make your own website, program your own game, etc. Build up a portfolio to show employers. \n\nNo matter what direction of programming you choose, I suggest learning about Data Structures.\n\nHead on over to r/programming and read articles you find interesting.  do you have any suggestions at what type of programming is both relatively interesting and can also be learned within a couple years? do you have any suggestions at what type of programming is both relatively interesting and can also be learned within a couple years? do you have any suggestions at what type of programming is both relatively interesting and can also be learned within a couple years?  [deleted] I am currently getting my second bachelors, this time in CS. Already had a BS in psychology. It'll end up taking me 2 years to finish, I have a feeling that it will be worth it. what school are you at? or at least what type of school are you at now? a regular 4 year? what school are you at? or at least what type of school are you at now? a regular 4 year?      I assume you're really looking for a programming job. This happens all the time, though not usually with someone who decided to enter the field at 23. Usually it's with someone who had taught themselves programming in their early teens but wasn't particularly interested in going through the rigor of a CS program. They get a Business or Communication Studies degree then continue to programming/development work afterwards. At this point they have almost 10 years of informal experience though.\n\nI think the quickest way for you to switch careers would be to get a 2nd Baccalaureate or a Masters in CS.    Point of contention, the field of CS is academia. The science of computing &amp; computability\n\nThat out of the way, what you are looking for is probably programming. In which case [I'd advise this book](http://www.headfirstlabs.com/books/hfprog/) CS grads make good programmers though, especially in demanding areas like graphics, algorithms, compilers, systems work, etc. If you want to write web pages, by all means learn to hack. Otherwise a good CS background goes a long way toward creating value.        If you don't have programming experience, you don't even know if you'll like programming. Biggest problem I have seen working with first years is they drop or change courses part way through a semester. I'd suggest spending a month and teach yourself the entire C language, enough to get a real feel for the language. From there you can decide if you really want to spend the rest of your life running through millions of lines of code to fine that one null pointer. thanks for the response, not gonna lie this is an issue I'm dealing with. \n\nDefinitely going to dip my toe in the water before diving in [deleted] thanks for the response, not gonna lie this is an issue I'm dealing with. \n\nDefinitely going to dip my toe in the water before diving in      ",
    "url" : "http://www.reddit.com/r/compsci/comments/r8hhd/is_it_possible_to_join_the_field_of_compsci_after/"
  } ],
  "processing-time-source" : 66,
  "processing-result.title" : "compsci17_reddit.xml",
  "XmlDocumentSource.xml" : {
    "absolutePath" : "/Users/cole/code/cs467/r-topics/carrot2-cli-3.6.2/../data/compsci17_reddit.xml"
  }
}
{
  "processing-time-total" : 2123,
  "clusters" : [ {
    "id" : 0,
    "size" : 8,
    "score" : 121.72673807514747,
    "phrases" : [ "Deleted Deleted" ],
    "documents" : [ 2, 7, 23, 28, 40, 41, 42, 49 ],
    "attributes" : {
      "score" : 121.72673807514747
    }
  }, {
    "id" : 1,
    "size" : 6,
    "score" : 66.19968754791918,
    "phrases" : [ "Data Structures" ],
    "documents" : [ 16, 20, 28, 34, 41, 42 ],
    "attributes" : {
      "score" : 66.19968754791918
    }
  }, {
    "id" : 2,
    "size" : 6,
    "score" : 31.035854848436266,
    "phrases" : [ "Library" ],
    "documents" : [ 5, 7, 16, 32, 42, 43 ],
    "attributes" : {
      "score" : 31.035854848436266
    }
  }, {
    "id" : 3,
    "size" : 6,
    "score" : 54.5209158792343,
    "phrases" : [ "Logical Thought" ],
    "documents" : [ 10, 28, 37, 40, 41, 42 ],
    "attributes" : {
      "score" : 54.5209158792343
    }
  }, {
    "id" : 4,
    "size" : 6,
    "score" : 83.34177529605005,
    "phrases" : [ "Turing Machine" ],
    "documents" : [ 7, 10, 15, 28, 29, 38 ],
    "attributes" : {
      "score" : 83.34177529605005
    }
  }, {
    "id" : 5,
    "size" : 6,
    "score" : 80.76467713736099,
    "phrases" : [ "Type System" ],
    "documents" : [ 16, 19, 20, 22, 37, 42 ],
    "attributes" : {
      "score" : 80.76467713736099
    }
  }, {
    "id" : 6,
    "size" : 5,
    "score" : 66.15573139321369,
    "phrases" : [ "Capable" ],
    "documents" : [ 7, 10, 13, 37, 42 ],
    "attributes" : {
      "score" : 66.15573139321369
    }
  }, {
    "id" : 7,
    "size" : 5,
    "score" : 76.33296012092,
    "phrases" : [ "Consists mostly of Programming" ],
    "documents" : [ 16, 28, 29, 42, 47 ],
    "attributes" : {
      "score" : 76.33296012092
    }
  }, {
    "id" : 8,
    "size" : 5,
    "score" : 88.26066879573564,
    "phrases" : [ "Exactly Computer Science" ],
    "documents" : [ 10, 28, 29, 38, 42 ],
    "attributes" : {
      "score" : 88.26066879573564
    }
  }, {
    "id" : 9,
    "size" : 5,
    "score" : 147.38710164757808,
    "phrases" : [ "Second Algorithms" ],
    "documents" : [ 7, 30, 31, 41, 45 ],
    "attributes" : {
      "score" : 147.38710164757808
    }
  }, {
    "id" : 10,
    "size" : 4,
    "score" : 47.76430822833167,
    "phrases" : [ "Area" ],
    "documents" : [ 39, 42, 45, 46 ],
    "attributes" : {
      "score" : 47.76430822833167
    }
  }, {
    "id" : 11,
    "size" : 4,
    "score" : 66.12224766384278,
    "phrases" : [ "Graphs" ],
    "documents" : [ 6, 12, 30, 40 ],
    "attributes" : {
      "score" : 66.12224766384278
    }
  }, {
    "id" : 12,
    "size" : 3,
    "score" : 58.3219331881606,
    "phrases" : [ "Function in this Class a Given Member" ],
    "documents" : [ 15, 16, 42 ],
    "attributes" : {
      "score" : 58.3219331881606
    }
  }, {
    "id" : 13,
    "size" : 3,
    "score" : 35.592251220250134,
    "phrases" : [ "Information from the Video Page" ],
    "documents" : [ 42, 45, 49 ],
    "attributes" : {
      "score" : 35.592251220250134
    }
  }, {
    "id" : 14,
    "size" : 3,
    "score" : 77.8154706761104,
    "phrases" : [ "Known in Advance for Languages" ],
    "documents" : [ 10, 28, 35 ],
    "attributes" : {
      "score" : 77.8154706761104
    }
  }, {
    "id" : 15,
    "size" : 3,
    "score" : 59.07764737072798,
    "phrases" : [ "Selection" ],
    "documents" : [ 0, 20, 42 ],
    "attributes" : {
      "score" : 59.07764737072798
    }
  }, {
    "id" : 16,
    "size" : 2,
    "score" : 27.281142091732594,
    "phrases" : [ "Billion" ],
    "documents" : [ 8, 11 ],
    "attributes" : {
      "score" : 27.281142091732594
    }
  }, {
    "id" : 17,
    "size" : 2,
    "score" : 79.83837210692222,
    "phrases" : [ "CS Degree isn't Software Engineering" ],
    "documents" : [ 28, 42 ],
    "attributes" : {
      "score" : 79.83837210692222
    }
  }, {
    "id" : 18,
    "size" : 2,
    "score" : 60.27374716946155,
    "phrases" : [ "Grace Hopper" ],
    "documents" : [ 18, 47 ],
    "attributes" : {
      "score" : 60.27374716946155
    }
  }, {
    "id" : 19,
    "size" : 2,
    "score" : 43.59646633938924,
    "phrases" : [ "Matrix" ],
    "documents" : [ 12, 30 ],
    "attributes" : {
      "score" : 43.59646633938924
    }
  }, {
    "id" : 20,
    "size" : 2,
    "score" : 48.87704340102982,
    "phrases" : [ "Weird Relations" ],
    "documents" : [ 7, 12 ],
    "attributes" : {
      "score" : 48.87704340102982
    }
  }, {
    "id" : 21,
    "size" : 15,
    "score" : 0.0,
    "phrases" : [ "Other Topics" ],
    "documents" : [ 1, 3, 4, 9, 14, 17, 21, 24, 25, 26, 27, 33, 36, 44, 48 ],
    "attributes" : {
      "other-topics" : true,
      "score" : 0.0
    }
  } ],
  "processing-time-algorithm" : 2020,
  "documents" : [ {
    "id" : 0,
    "title" : "How selective are Comp Sci REUs in general?",
    "snippet" : "I'm very interested in participating in one next summer.  Selective. Want to know an easier route that yields the same result? Go up to a professor and tell them you are interested in doing research with them. I did and it led to recommendation letters, a publication + research grants.    I did the Computer Vision one at UCF.  From my experience, not that selective.  Some of my friends applied and most got in.  We're all from MIT.\n\nThat said, it was pretty fun and definitely makes you a better researcher.   What's an REU? What's an REU?",
    "url" : "http://www.reddit.com/r/compsci/comments/1293c0/how_selective_are_comp_sci_reus_in_general/"
  }, {
    "id" : 1,
    "title" : "e-Petition Reminder: Put Alan Turing on the next £10 note -- 20% of the way there!",
    "snippet" : "  20% of the way to being *considered for debate*    ",
    "url" : "http://epetitions.direct.gov.uk/petitions/31659"
  }, {
    "id" : 2,
    "title" : "\"How unimportant it is whether submarines can swim\" (EWD1056)",
    "snippet" : "  [deleted] [deleted]",
    "url" : "http://members.chello.nl/hjgtuyl/computing/EWD1056.html"
  }, {
    "id" : 3,
    "title" : "Ecology and evolution through the algorithmic lens",
    "url" : "http://cstheory.stackexchange.com/q/14103/1037"
  }, {
    "id" : 4,
    "title" : "Sketch of the Day: HyperLogLog — Cornerstone of a Big Data Infrastructure",
    "url" : "http://blog.aggregateknowledge.com/2012/10/25/sketch-of-the-day-hyperloglog-cornerstone-of-a-big-data-infrastructure/"
  }, {
    "id" : 5,
    "title" : "SSL certificate validation is completely broken in many security-critical applications and libraries",
    "snippet" : "  ",
    "url" : "https://crypto.stanford.edu/~dabo/pubs/abstracts/ssl-client-bugs.html"
  }, {
    "id" : 6,
    "title" : "Generating random k-regular graphs",
    "snippet" : "  I'm missing something simple, can someone explain to me why the obvious/naive approach not work? \n\n    Pick a random vertex without replacement and \n        (from the remaining vertices), \n        choose k random neighbors. \n    Repeat n times.  So the problem here is, say you pick k random neighbors for vertex v_1. Let one of those neighbors be v_2. Now go to v_2 and pick k random neighbors, it is not necessary that v_1 will be chosen as one of v_2's neighbors. In this case, v_2 will have more than k neighbors, making the graph not k-regular. What if I picked k - degree(current_vertex) neighbors?  This would not be unbiased. Remember, the golden standard is picking 1 graph uniformly at random from all k-regular graphs. So in order to show that your procedure works, you need to show that it not only produces a k-regular graph but that the distribution over k-regular graph produced by random coin tosses of your algorithm is uniform, or extremely close to uniform. Any idea how one would go about proving/demonstrating the distribution bias of this algorithm?  What if I picked k - degree(current_vertex) neighbors?  What if I picked k - degree(current_vertex) neighbors?  I'm trying to figure this out. I don't know exactly, but my inclination would lead me to believe that graphs generated in this way would not necessarily be unbiased. What if I picked k - degree(current_vertex) neighbors?   ",
    "url" : "http://egtheory.wordpress.com/2012/03/29/random-regular-graphs/"
  }, {
    "id" : 7,
    "title" : "runtime of 2^n",
    "snippet" : "So there are several ways to find 2^n (without using any libraries). There is the obvious loop n times and multiply by 2 each time. \n\n    int poweroftwo(int n) {\n\n      int sum = 1;                \n      for (int i = 0; i &lt; n; n++){\n        sum = sum*2;\n      }            \n      return sum;\n    }\n\nruntime - O(n)\n\nit can also be done recursively\n\n    int poweroftwo(int n) {\n      if (n &lt; 1)\n        return 1;\n\n      int x = poweroftwo(floor(n/2));\n      if (n % 2 == 0) {\n        return x*x;\n      } else { \n        return x*x*2;\n      }\n    }\n\nthis will give a log(n) running time\n\nbut when I asked the question to someone this is what they gave me and claimed constant running time\n\n    int poweroftwo(int n) {\n      return 1 &lt;&lt; n;\n    }\n\n\nIs this actually constant running time? or is it still a linear running time because it takes n iterations to move n bits?\n\nThanks!  It's comparing apples and oranges.\n\nThe first two runtimes are algorithmic runtimes -- the worst case behavior for a number n *of any size*. The last one talks about actual runtime -- how many CPU cycles are needed. Those are different things.\n\nYes, the last one is O(1) if you look at an actual CPU. But the reason is not that it's a better algorithm, the reason is that you are limiting the size of the input. You assume n fits in a single CPU register (e.g 64 bits); if it doesn't, it's not O(1) any more.\n\nHowever, any algorithm is O(1) if the input is bounded by a fixed number. So if n can never be longer than 64 bits, then the first and the second algorithms are O(1), too.\n\n\n It's comparing apples and oranges.\n\nThe first two runtimes are algorithmic runtimes -- the worst case behavior for a number n *of any size*. The last one talks about actual runtime -- how many CPU cycles are needed. Those are different things.\n\nYes, the last one is O(1) if you look at an actual CPU. But the reason is not that it's a better algorithm, the reason is that you are limiting the size of the input. You assume n fits in a single CPU register (e.g 64 bits); if it doesn't, it's not O(1) any more.\n\nHowever, any algorithm is O(1) if the input is bounded by a fixed number. So if n can never be longer than 64 bits, then the first and the second algorithms are O(1), too.\n\n\n It's comparing apples and oranges.\n\nThe first two runtimes are algorithmic runtimes -- the worst case behavior for a number n *of any size*. The last one talks about actual runtime -- how many CPU cycles are needed. Those are different things.\n\nYes, the last one is O(1) if you look at an actual CPU. But the reason is not that it's a better algorithm, the reason is that you are limiting the size of the input. You assume n fits in a single CPU register (e.g 64 bits); if it doesn't, it's not O(1) any more.\n\nHowever, any algorithm is O(1) if the input is bounded by a fixed number. So if n can never be longer than 64 bits, then the first and the second algorithms are O(1), too.\n\n\n Then for the first algorithm if n is the number of bits it would take O(n^2) since multiplication is O(n) and it does n multiplications. \nand for the last algorithm it would take simply O(n). How is the last one not a better algorithm?  Yes, if the CPU contains a hardware shifter unit then the operation is done in constant time. See http://en.wikipedia.org/wiki/Barrel_shifter A barrel shifter is really O(log n).  It's a fully unrolled version of the \"Russian peasant\" algorithm in OP's post.\n\nHowever, the shift will take less than one clock cycle which makes it constant time in practice, unless you're designing the barrel shifter itself. Isn't it O(log n) in the number of bits?\n\nThus, it's a constant run time for any integer of constant size. It's O(log n) if you're dealing with multi-precision integers, which you probably aren't. It's O(log n) if you're dealing with multi-precision integers, which you probably aren't. Isn't it O(log n) in the number of bits?\n\nThus, it's a constant run time for any integer of constant size. A barrel shifter is really O(log n).  It's a fully unrolled version of the \"Russian peasant\" algorithm in OP's post.\n\nHowever, the shift will take less than one clock cycle which makes it constant time in practice, unless you're designing the barrel shifter itself. [deleted] To the software guy, it's constant time.\n\nThe actual physical propagation time through the hardware is logarithmic.  You need an extra mux for each bit. Well, when you talk about actual hardware, then you have a maximum bit width, which means asymptotic performance isn't meaningful (i.e. everything is constant time, once you put an upper bound on n). I suppose it could be meaningful if you're writing an HDL library or something, such that you need a barrel shifter of arbitrary bit width.  Dunno if current HDLs allow for that sort of meta-programming.  I'm sure it will happen in the future; (digital) hardware design is getting more and more like software development. Verilog lets you do stuff like that. Not sure about VHDL, but I'd imagine it would, too. To the software guy, it's constant time.\n\nThe actual physical propagation time through the hardware is logarithmic.  You need an extra mux for each bit. To the software guy, it's constant time.\n\nThe actual physical propagation time through the hardware is logarithmic.  You need an extra mux for each bit.  Shifting bits is a single instruction in common CPUs. It takes up only one clock cycle. That’s why it’s constant.\n\n*Edit:* That’s how it’s wired: http://www.realworldtech.com/includes/images/articles/fo4-fig10.gif   Why are all these people saying it's a single operation?\n\nWhat about a 9000-bit number? This is compsci, not compengineering; consider the general case. I think the whole question would better fit into /r/programming since it seems very C-specific and at the very least it is architecture dependant.\n\nOn a theoretical turing machine it could take O(1), O(log(n)) or maybe even O(n) depending on what kind of transitions you allow. \n\nIf you take an arbitrarily large number you have to choose whether you can write this number as one single symbol or if you only allow 0s and 1s. Then you need to decide what kind of transitions you allow, just NAND or everything including shifts.\n\nShifts and substitutions are very easy and simple to do in hardware, in the minimalistic variant, you just need to cross some wires. The computational theory however doesn't really fit the hardware capabilities in this case. Well... You cannot have a Turing machine that can write an arbitrary number of symbols to the tape. Turing machines can only write symbols from a finite alphabet (by definition).\n\nA Turing machine to calculate 2^x would require at least x operations. I would say that you can have an arbitrary but fixed number of symbols, so if you have an arbitrary but fixed upper limit for the number, you could read in one step.\n\nThe issue I have is mostly this:\n\nOn an actual CPU you can read a 8, 16, 32, 64, ... bit number in one go. So assuming *n* is the native width of a number, and *+* is a one instruction operation *2^n + 2^n* would be constant.\n\nBut on a theoretical turing machine with no native integers, *2^n + 2^n* would be O(n). On this machine there would be no operation that takes less than O(n) since you normaly need to at least read the input.\n\nWhile this is a great theoretical construct, it complicates all complexity analysis, since for every operation you need to consider the time it takes to read the data. If you get even more theoretical and only allow NAND operations, simple multiplication can soon be O( n^2 ) or worse. And you will find it difficult to get sorting algorithms with a complexity of less than O( n^2 * log(n) ). Well... You cannot have a Turing machine that can write an arbitrary number of symbols to the tape. Turing machines can only write symbols from a finite alphabet (by definition).\n\nA Turing machine to calculate 2^x would require at least x operations. [deleted]  It is a trick to make it look like constant time, but in this case the constant is bound by the size (# of bits) of an integer.  Even if the CPU can perform this operation in a single instruction it is still taking advantage of fixed size integers. If integers weren't fixed bit width, then this wouldn't be constant time.   It requires constant running time with most architectures, but some can require O(n) number of cycles. See [this discussion](http://stackoverflow.com/questions/9083743/is-bit-shifting-o1-or-on) for details.\n\nKeep in mind that none of your code will work correctly when n is greater than 64 (or 32, if you have a 32-bit machine). Since n is bounded, you could argue that shifting is O(1), but that'd be a weird argument to make since I assume that you are thinking about applying these approaches to arbitrary precision arithmetic.\n\nOne more thing to note is the absence of *implied constants* in asymptotic analysis. If you are on an architecture where your shifting algorithm is O(n), then that is asymptotically \"slower\" than your recursive O(log(n)) algorithm, but practically speaking the shifting algorithm will be significantly faster. Why? Well, think about the cost of integer multiplication versus integer bit shifting.        Using shifts is very popular for architectures with limited resources to optimize performance critical parts. Think 3D engines, embedded media player, video codecs, ...\n\nThis is espically usefull, if your chip can do shifts in hardware but can't do multiplications or division. Even if you have to do several single shifts, it is often faster than multiplication.\n\n\nThere are some caveats, mostly related to signed and unsigned numbers. For example\n\n    // This won't work\n    int divideByTwo(int x) {\n      return (x &gt;&gt; 1);\n    }\n    \n    // This will work\n    unsigned int divideByTwo(unsigned int x) {\n      return (x &gt;&gt; 1);\n    }\n\nYou can to some very nice and fast math using this for example\n\n    bool isEvenNumber(int x){\n      return (x &amp; 1);\n    }\n\n    int moduloSixteen(int x){\n      return (x &amp; 0x10);\n    }\n\n    unsigned int calculateSevenEights(unsigned int x){\n      return (x - (x &gt;&gt; 3));\n    }\n\n    unsigned int multiplyByThirteen(unsigned int x){\n      return (x + (x &lt;&lt; 2) + (x &lt;&lt; 3));\n    } Using shifts is very popular for architectures with limited resources to optimize performance critical parts. Think 3D engines, embedded media player, video codecs, ...\n\nThis is espically usefull, if your chip can do shifts in hardware but can't do multiplications or division. Even if you have to do several single shifts, it is often faster than multiplication.\n\n\nThere are some caveats, mostly related to signed and unsigned numbers. For example\n\n    // This won't work\n    int divideByTwo(int x) {\n      return (x &gt;&gt; 1);\n    }\n    \n    // This will work\n    unsigned int divideByTwo(unsigned int x) {\n      return (x &gt;&gt; 1);\n    }\n\nYou can to some very nice and fast math using this for example\n\n    bool isEvenNumber(int x){\n      return (x &amp; 1);\n    }\n\n    int moduloSixteen(int x){\n      return (x &amp; 0x10);\n    }\n\n    unsigned int calculateSevenEights(unsigned int x){\n      return (x - (x &gt;&gt; 3));\n    }\n\n    unsigned int multiplyByThirteen(unsigned int x){\n      return (x + (x &lt;&lt; 2) + (x &lt;&lt; 3));\n    } &gt;int moduloSixteen(int x) { return (x &amp; 0x10); }\n\nThis is equivalent to (x/16)%2 which doesn't sound like the function name.\n\nAlso I don't think there are many \"3D engines, embedded media player, video codecs\" running on chips that don't have hardware multipliers.  When I took your code for the recursive version of poweroftwo, it did not work. It returned 2, 4, 4, 16, 16, 16, 16, 256, 256... when I looped through the first few powers.\n\n\nIt ends up returning 2^k, where k is some power of 2, until n equals the next power of 2. So from n = 4 to n = 7, it returns 2^4. Once n = 8, it returns 2^8 all the way to n = 15.   How would you actually describe a program that \"moved every bit n to the right\"?\n\nFirst you read the bit in the 1st position, and write it to some other memory at position n+1, then do the same for the bit in the 2nd position. So on and so forth...\n\nNow how many bits are there for a number n? log_2(n). \n\nThis algorithm has the same run time as (and actually is doing effectively the same thing as) the recursive algorithm. Except it's usually done in HW using a LSL or ASL instruction. Which can't be done on an arbitrarily large number (is implementation specific).\n\nAs said elsewhere, this is /r/compsci not /r/computerengineering\n Is 2's compliment notation then computer engineering as well and have no place in Computer Science?  2's complement addition doesn't work with arbitrary large numbers as it relies on rollover and a fixed bitwidth.\n\nKnowing how to leverage bitwise operations is critical though for someone trying to write an optimized compiler (Compiler theory is strongly in the CS realm IMO).  Completely disregarding a solution that has a well understood limitation that applies to the other solutions given as well (they use int explicitly, not BigInteger) is stupid IMO.\n\nThe question was specifically \"Is this constant runtime\" and the answer is \"on most computers, yes, with the limitation that it only works up to 2^(N-1) where N is the number of bits of your processor's registers.\"  I'll concede that it is constant \"runtime\" for the fuzzy definition used colloquially. But by the formal definition of order notation it is still O(log(n)). Which can't be done on an arbitrarily large number (is implementation specific).\n\nAs said elsewhere, this is /r/compsci not /r/computerengineering\n How would you actually describe a program that \"moved every bit n to the right\"?\n\nFirst you read the bit in the 1st position, and write it to some other memory at position n+1, then do the same for the bit in the 2nd position. So on and so forth...\n\nNow how many bits are there for a number n? log_2(n). \n\nThis algorithm has the same run time as (and actually is doing effectively the same thing as) the recursive algorithm.  [deleted]  [deleted]",
    "url" : "http://www.reddit.com/r/compsci/comments/11yql6/runtime_of_2n/"
  }, {
    "id" : 8,
    "title" : "How do cache-timing attacks work?",
    "snippet" : "I'm reading an article that talks about ciphers that are susceptible to cache-timing attacks. I'm having a hard time understanding what they are. \n\nThe article defines a cache-timing attack as \"when an attacker attempts to recover parts of a cryptosystem state by observing the variance in timing measurements due to processor data caching effects.\"\n\nI don't understand when these observations of timing measurements occur. Is the hacker \"watching\" the message get encrypted? How would a hacker be able to observe processor data caching effects of a message that has already been encrypted?\n\nAny clarification or knowledge of this subject is greatly appreciated.  First you'd need to know what data caching is. In this case it's when addresses of the same prefix occur. I.e. for a size 16 cache, address of 1, 17, 33 will hit the same cache location because 1 mod 16 = 1, 17 mod 16 = 1, and  33 mod 16 = 1.\n\nThis is important in knowing what happens when the program is trying to reference something that has been seen relatively recently. Take this example, \n\n    x = 1 //a\n    \n    x = 0 //b\n    \n    y = 2 //c\n    \n    x = 3 //d\n\nAnd, using the 16 size cache mentioned earlier, have x map to on address 1 and y map to address 17. Looking at the cache just after (a) runs, we'll see a mapping of cache[&amp;x % 8] = (address, value) = (&amp;x,1). When (b) hits, we check the mapping of cache[&amp;x%8] = (&amp;x,_) and the address inside this cache value is the same as the address for the variable being referenced, so we can update the cache, no worries cache[1] = (&amp;x, 0), making the data *dirty* (data in the cache is out of sync with upper layers). When c gets executed, and we do the address check, cache[&amp;y%8]  which will return the address of x which isn't the variable we want to reference. This is known as a *cache collision*, so now, because the data is dirty, we have to write the contents of this cache (in this case the value of x) out to the next layer of memory, is this is a outer most layer of cache, it'll go to main memory (ram). After writing out the, the cache[1] can now be assigned the values of y, i.e. (&amp;y, 2). A similar collision occurs on (d), but no writing back to main memory is necessary because the contents of y haven't changed, i.e. not dirty.\n\nNow back to the main question of topic. The Cache-timing attack deals with the cases that the cache becomes corrupted, therefore a delay is presented when looking up data. For AES encryption, most software uses a lookup table for its encryption algorithm using values of *n* XOR *k*, where *n* is the data to send and *k* is your AES key. If you know what software the server is using for the AES encryption, you could run the algorithm yourself with known data and determine based on your input of *n* and find the delay given by the cache for given n^i(keep in mind you'd need to run billions of these transfers in order to get a good baseline in case you get a lot of non-collision cache hits). After mapping delays to given n^i, you could then use this to find the value of *k* given known values *n* and *n* XOR *k*.\n\nTo watch the message, you see the delay in message passing between A and B and after many *n*'s, you can determine AES time for each n^i. Using this data, if you know which *n^i* causes the biggest time spike and you know its *n^i* XOR *k^i*, you can determine the *k^i*\n\nGuh... typed too much.\n\n[AES cache-timing attack](https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;ved=0CDIQxQEwAQ&amp;url=https%3A%2F%2Fdocs.google.com%2Fviewer%3Fa%3Dv%26q%3Dcache%3ATyYZ0oBb8IoJ%3Acr.yp.to%2Fantiforgery%2Fcachetiming-20050414.pdf%2B%26hl%3Den%26gl%3Dus%26pid%3Dbl%26srcid%3DADGEESjSFaUIG3iGAuayfrRQY-nzDJm4jbQRsYrLK2POWgiGwgvS_sLFy7cVzuHtJ7P3ddpVNseOMRAk0Idj9Nf2HkIwAPOT0mSvBHs6D6ZRJLl1xhw-jXqfRw5MqhlgDYA1rTGlkEtV%26sig%3DAHIEtbStxfPxdJMLfyrZIfiTPdP86lgTcA&amp;ei=01KHUJHuEa7cigKex4HoDA&amp;usg=AFQjCNGClS9obSfy85owlFi5QSK_HDyXIw&amp;cad=rja)\n\nEDIT: clearity  So let's say you had some code in your crypto algorithm like this:\n\n    if (keybit[30]):\n        foo()\n    else:\n        bar()\n\nwith 'foo' and 'bar' being some code that is large enough that they occupy different cache lines.\n\nAfter executing this, the processor task switches to the attacker who runs:\n\n    start_time = now()\n    foo()\n    foo_time_taken = now() - start_time\n\n    start_time = now()\n    bar()\n    bar_time_taken = now() - start_time\n\n    if (foo_time_taken &lt; bar_time_taken)\n        print(\"foo was recently in memory so keybit 30 is more likely to be true\")\n    else \n        print(\"bar was recently in memory so keybit 30 is more likely to be false\")\nWhichever function was already in the cache would execute faster, revealing a bit of the state of the cryptosystem. Now, of course, you don't usually have things as clear as 'if bit 30 then do x', but even a small amount of probabilistic data goes a long way at reducing the keyspace. especially when you can just run it 100,000 times. Thank you, this response was extremely helpful.\n\nIs there a clear way to explain how exactly \"the processor task switches to the attacker\"? I guess I don't understand how an attacker would have access to the cache memory of the crypto algorithm. ",
    "url" : "http://www.reddit.com/r/compsci/comments/11z7tz/how_do_cachetiming_attacks_work/"
  }, {
    "id" : 9,
    "title" : "The Ph.D. Grind: Lead From Below",
    "url" : "http://cacm.acm.org/blogs/blog-cacm/155690-the-phd-grind-lead-from-below/fulltext"
  }, {
    "id" : 10,
    "title" : "Your thoughts on how quantum computing will change computer science from a CS students perspective.",
    "snippet" : "Most universities today don't teach quantum computing. They teach computing as it has been for the past decade or more. Will the value of today's computer science degree diminish with the advent of quantum computing? At what level of abstraction will computer science change? Will programs written in the most common languages like Java and C++ run on quantum computers? Will programmers have to think differently or will only computer architects have to think differently? Will the von Neumann model still hold?  Quantum computing works fundamentally differently than traditional computing.\n\nIn fact, unless it turns out differently than expected (which it very well might) it won't replace traditional computing for most every-day tasks.\n\nSo, no, you probably won't be running any C or Java on a quantum computer. I'm a CS professor in a small liberal-arts school.  I have thought about this a lot lately.  At what point am I going to need to learn quantum computing to continue teaching undergraduate-level courses?  It's not clear that it will ever be a core class, but I'd rather be ready than be caught off-guard. So incredibly unlikely within the next 20 years it's not worth worrying about IMO. Not only would there have to be quantum computers that are fast, reliable, and (relative to universities and corporations, similar to clusters, HPC) affordable, but there'd have to be some major breakthroughs in the field of quantum computing itself. There are *some* algorithms for QC, but nobody really has any idea what it'd look like in actual code, and the pool of algorithms is obviously nowhere near as broad as that of classical computers. It's nothing more than a theoretical challenge right now, and even with ubiquitous quantum computers, it's very unlikely that it'd make an appreciable dent in the conventional CS curriculum, though I can see mandatory QC courses tacked on, or a new QCS major.\n\nAnybody with more experience in the field, feel free to correct me, but I haven't seen any indicators that one or two QC breakthroughs would overthrow traditional CS. While I do agree with you that no, we do not need to learn Quantum Computing any time soon, I will say this: I know a quantum programmer. \n\nA friend of mine spent a few years working at D-Wave. They have papers out now that finally do vindicate that what they are doing is actually real, and not a scam, like some people claimed they were. \n\nThey aren't using the classic idea of a universal Quantum Computer, but they are using an adiabatic quantum optimizer to solve some NP-Hard problems faster than was previously believed possible. Mind you, I think they're still working in the low-numbers of q-bits. \n\nIt's interesting stuff, but I suspect that when it becomes a real thing, it'll be a bit like Amazon Web Services, or Google's Compute Engine, where one company has the QCs, and the rest of us pay them for time to use them for the specific purposes that we need them for.  While I do agree with you that no, we do not need to learn Quantum Computing any time soon, I will say this: I know a quantum programmer. \n\nA friend of mine spent a few years working at D-Wave. They have papers out now that finally do vindicate that what they are doing is actually real, and not a scam, like some people claimed they were. \n\nThey aren't using the classic idea of a universal Quantum Computer, but they are using an adiabatic quantum optimizer to solve some NP-Hard problems faster than was previously believed possible. Mind you, I think they're still working in the low-numbers of q-bits. \n\nIt's interesting stuff, but I suspect that when it becomes a real thing, it'll be a bit like Amazon Web Services, or Google's Compute Engine, where one company has the QCs, and the rest of us pay them for time to use them for the specific purposes that we need them for.  While I do agree with you that no, we do not need to learn Quantum Computing any time soon, I will say this: I know a quantum programmer. \n\nA friend of mine spent a few years working at D-Wave. They have papers out now that finally do vindicate that what they are doing is actually real, and not a scam, like some people claimed they were. \n\nThey aren't using the classic idea of a universal Quantum Computer, but they are using an adiabatic quantum optimizer to solve some NP-Hard problems faster than was previously believed possible. Mind you, I think they're still working in the low-numbers of q-bits. \n\nIt's interesting stuff, but I suspect that when it becomes a real thing, it'll be a bit like Amazon Web Services, or Google's Compute Engine, where one company has the QCs, and the rest of us pay them for time to use them for the specific purposes that we need them for.  Solving instances of NP-hard problems quickly is not new.  Last I knew, there was no quantum algorithm for solving NP-hard problems in polynomial time.  (For the record, I don't know what an \"adiabatic quantum optimizer\" is.) Quantum computers cannot solve NP-hard problems in polynomial time (as far as we know), but they *can* solve BQP class problems in polynomial time (*bounded error quantum polynomial time*). Nobody knows what the relationship between NP and BQP is, but it is almost certain that BQP is larger than P, i.e., there are more solvable problems in BQP than in P alone.\n\nFor more information, see http://en.wikipedia.org/wiki/BQP. While I do agree with you that no, we do not need to learn Quantum Computing any time soon, I will say this: I know a quantum programmer. \n\nA friend of mine spent a few years working at D-Wave. They have papers out now that finally do vindicate that what they are doing is actually real, and not a scam, like some people claimed they were. \n\nThey aren't using the classic idea of a universal Quantum Computer, but they are using an adiabatic quantum optimizer to solve some NP-Hard problems faster than was previously believed possible. Mind you, I think they're still working in the low-numbers of q-bits. \n\nIt's interesting stuff, but I suspect that when it becomes a real thing, it'll be a bit like Amazon Web Services, or Google's Compute Engine, where one company has the QCs, and the rest of us pay them for time to use them for the specific purposes that we need them for.  I'm a CS professor in a small liberal-arts school.  I have thought about this a lot lately.  At what point am I going to need to learn quantum computing to continue teaching undergraduate-level courses?  It's not clear that it will ever be a core class, but I'd rather be ready than be caught off-guard. I'm a CS professor in a small liberal-arts school.  I have thought about this a lot lately.  At what point am I going to need to learn quantum computing to continue teaching undergraduate-level courses?  It's not clear that it will ever be a core class, but I'd rather be ready than be caught off-guard. Probably better spending the time on figuring out how to implement parallel computing in more courses and earlier in your students degree. The reality is that we will spend the next many decades using multi core processors before anything like QC is available. Probably better spending the time on figuring out how to implement parallel computing in more courses and earlier in your students degree. The reality is that we will spend the next many decades using multi core processors before anything like QC is available. Quantum computing works fundamentally differently than traditional computing.\n\nIn fact, unless it turns out differently than expected (which it very well might) it won't replace traditional computing for most every-day tasks.\n\nSo, no, you probably won't be running any C or Java on a quantum computer. Quantum computing works fundamentally differently than traditional computing.\n\nIn fact, unless it turns out differently than expected (which it very well might) it won't replace traditional computing for most every-day tasks.\n\nSo, no, you probably won't be running any C or Java on a quantum computer. Assuming quantum computing ever because something available to the masses, I imagine it will augment the traditional computer and be available to programmers not unlike how CPU and GPU tasks can be separated today. Quantum computing works fundamentally differently than traditional computing.\n\nIn fact, unless it turns out differently than expected (which it very well might) it won't replace traditional computing for most every-day tasks.\n\nSo, no, you probably won't be running any C or Java on a quantum computer.   It's a non-issue.  Worry more about learning parallel computing.\n\nQuantum computing, if it's ever a thing that ends up on a typical machine at all, will be a specialized component of the machine that changes nothing at all about programming.  It probably won't even be a general purpose computing device (like GPUs are now,) but instead specialized to one or two specific tasks.  You'll no more write quantum programs than you'll write \"quartz programs\" for consulting the real time clock.\n\nThe only way anyone will end up writing quantum programs is if (a) we get several absurdly unlikely breakthroughs in what quantum computers are even theoretically capable of, or (b) it turns out that building quantum circuits is outright trivial and totally cheap, on par with silicon! (btw, would you like to buy a bridge?)\n\nDon't get me wrong, quantum computing is interesting.  But it's interesting because it's one of the best ways we have to probe quantum physics right now (besides the LHC) and it's incredible tool for making advances in algorithms, including some results that don't even directly involve quantum algorithms.  But not because it's going to change computing.  Dijkstra had this to say about what computers mean to Computer Science:\n\n*\"As a result, the topic became —primarily in the USA— prematurely known as \"computer science\" —which, actually is like referring to surgery as \"knife science\"— and it was firmly implanted in people's minds that computing science is about machines and their peripheral equipment.\"*\n\nThe nature of information and abstract thought will not change if we somehow achieve processors capable of new things. All that will happen is that our limitations manipulating the world will be lessened. People will still have to learn most of what we learn today - maybe with a different focus, much like how we (\"higher-level\" focused people) nowadays learn about transistors and logic gates, as when compared to the eletrical engineerings and their 'electronic brains'in the 50's. The next people to come along will just have to learn *extra* things - much like we learn about big data storage and access.\n\n**Computer Science is as much about Computers as Astronomy is about telescopes** is the quote often attributed do Djistkra, and even though it might just be folklore, I thing it summarizes well his intent with the actual quote.\n\nThere is a very famous gif I can't quite find right now of an \"Introduction to Computer Science\" class where the lecturer writes Computer Science in the blackboard, then proceeds to cross first Science, then Computer. It's not a (at least not natural) Science. And it's not (just) about computers. Dijkstra said a lot of things. He said that if you learn BASIC as your first programming language, you can never learn how to program properly. And I guess surgeons spend a ton of time talking about knives then too. \n\nI don't see at all how your assertion that it has practically nothing to do with real computers is borne out. Quantum computers change the complexity of problems in a mathmatical sense. Surely you're aware that in a quantum computer, searching for an item in an unsorted array goes from Theta (N) to Theta (N^1/2 ), right? It's just another model of computation. Nondeterministic Turing Machines can solve NP-complete problems in polynomial time, deterministic Turing Machines (probably) cannot. But it is a model of computation that can (hopefully) be reified*.\n\n*in a natural sense (e.g. without the combinatorial explosion via simulation). Yeah, but it doesn't change \"the complexity of problems in a mathematical sense\", P problems are still in P, PSPACE problems are still in PSPACE, EXPTIME in EXPTIME, etc. QTMs add a whole new hierarchy of complexity classes which we know almost nothing about, IMO that's even more exciting. It's just another model of computation. Nondeterministic Turing Machines can solve NP-complete problems in polynomial time, deterministic Turing Machines (probably) cannot. Dijkstra said a lot of things. He said that if you learn BASIC as your first programming language, you can never learn how to program properly. And I guess surgeons spend a ton of time talking about knives then too. \n\nI don't see at all how your assertion that it has practically nothing to do with real computers is borne out. Quantum computers change the complexity of problems in a mathmatical sense. Surely you're aware that in a quantum computer, searching for an item in an unsorted array goes from Theta (N) to Theta (N^1/2 ), right? Sure, a lot of things will change, maybe most of it - maybe everything, in practice - but will it change what *is* data? What is a process? What is sequential-ism, what is parallelism? What is access, what is an array, what is memory, what is a numerical base, what is a logical operation...\n\nIt's absolutely going to change everything that has to do with how the abstract entities of computation are turned into real physical processes - much like the change from the abacus to the modern computer. Won't change the fact that we are mapping concepts observed in the 'universe' of what we study to an arbitrary machine that can itself be replaced. \n\nThe assertion that computer science has nothing to do with computers is misguided in this way, yeah, I think what I'm trying to say (at least I, shouldn't really assume about Dijstrka) is that it has to do with **every imaginable computer**, not just the ones we have *right now*.\n\nP.S.: Come to think of it, the simple fact that we are discussing quantum computing in a computer science forum is empirical proof that our field encompasses more than that which is real (at least *right now*). Sure, a lot of things will change, maybe most of it - maybe everything, in practice - but will it change what *is* data? What is a process? What is sequential-ism, what is parallelism? What is access, what is an array, what is memory, what is a numerical base, what is a logical operation...\n\nIt's absolutely going to change everything that has to do with how the abstract entities of computation are turned into real physical processes - much like the change from the abacus to the modern computer. Won't change the fact that we are mapping concepts observed in the 'universe' of what we study to an arbitrary machine that can itself be replaced. \n\nThe assertion that computer science has nothing to do with computers is misguided in this way, yeah, I think what I'm trying to say (at least I, shouldn't really assume about Dijstrka) is that it has to do with **every imaginable computer**, not just the ones we have *right now*.\n\nP.S.: Come to think of it, the simple fact that we are discussing quantum computing in a computer science forum is empirical proof that our field encompasses more than that which is real (at least *right now*). Dijkstra said a lot of things. He said that if you learn BASIC as your first programming language, you can never learn how to program properly. And I guess surgeons spend a ton of time talking about knives then too. \n\nI don't see at all how your assertion that it has practically nothing to do with real computers is borne out. Quantum computers change the complexity of problems in a mathmatical sense. Surely you're aware that in a quantum computer, searching for an item in an unsorted array goes from Theta (N) to Theta (N^1/2 ), right? Dijkstra had this to say about what computers mean to Computer Science:\n\n*\"As a result, the topic became —primarily in the USA— prematurely known as \"computer science\" —which, actually is like referring to surgery as \"knife science\"— and it was firmly implanted in people's minds that computing science is about machines and their peripheral equipment.\"*\n\nThe nature of information and abstract thought will not change if we somehow achieve processors capable of new things. All that will happen is that our limitations manipulating the world will be lessened. People will still have to learn most of what we learn today - maybe with a different focus, much like how we (\"higher-level\" focused people) nowadays learn about transistors and logic gates, as when compared to the eletrical engineerings and their 'electronic brains'in the 50's. The next people to come along will just have to learn *extra* things - much like we learn about big data storage and access.\n\n**Computer Science is as much about Computers as Astronomy is about telescopes** is the quote often attributed do Djistkra, and even though it might just be folklore, I thing it summarizes well his intent with the actual quote.\n\nThere is a very famous gif I can't quite find right now of an \"Introduction to Computer Science\" class where the lecturer writes Computer Science in the blackboard, then proceeds to cross first Science, then Computer. It's not a (at least not natural) Science. And it's not (just) about computers. I think you may be misunderstanding the the very shift in the math and logic Quantum Computing would require. I don't think the shift in math and logic is so far out of grasp that the scope of traditional computer science classes can't reach it.\n\nMechanically and physically, quantum computers are novel and fundamentally different from the implementations of classical computers, but we have a set of axioms to describe the power of computing \"devices\" and that set of axioms does not fail to represent the abilities of quantum computers. &gt; I don't think the shift in math and logic is so far out of grasp that the scope of traditional computer science classes can't reach it.\n\nAs a computer science student who has taken a number of courses on quantum mechanics and quantum computing, and who plans to study go on to study quantum computing, I disagree. At a fundamental level the *logic* of quantum mechanics is different. [Seriously](http://en.wikipedia.org/wiki/Quantum_logic). \n\nIt's not beyond most computer science students, but even understanding the basic idea of the simplest quantum algorithm takes at least a semester to even see why it is possible. And even then, a lot of things would need to be glossed over.\n\nSo sure, classes certainly *can* reach it, but the difference is probably a whole lot larger than you might think. I don't think the shift in math and logic is so far out of grasp that the scope of traditional computer science classes can't reach it.\n\nMechanically and physically, quantum computers are novel and fundamentally different from the implementations of classical computers, but we have a set of axioms to describe the power of computing \"devices\" and that set of axioms does not fail to represent the abilities of quantum computers. I agree with tylerni7 on this one.  Try writing down [Shor's algorithm](http://en.wikipedia.org/wiki/Shor%27s_algorithm) for factoring in your own words and get back to us.  The thing about it is that it requires some real leaps (of luck IMO) to come up with something like this. Applying the axioms to get results is *extremely* non-trivial. I agree with tylerni7 on this one.  Try writing down [Shor's algorithm](http://en.wikipedia.org/wiki/Shor%27s_algorithm) for factoring in your own words and get back to us.  The thing about it is that it requires some real leaps (of luck IMO) to come up with something like this. Applying the axioms to get results is *extremely* non-trivial. Until this past semester, I had absolutely no knowledge of quantum mechanics or quantum computing.  I didn't even know what Schrodinger cat was.  I'm taking a quantum complexity theory class this semester however in which we covered Shor's algorithm about a quarter of the way through - and I'm definitely not the only person in the class with this type of background.  So I think quantum computing is pretty accessible to those with just a background in computer science. I think you may be misunderstanding the the very shift in the math and logic Quantum Computing would require. Can you elaborate?  I'm rather unfamiliar with the topic but extremely curious I think you may be misunderstanding the the very shift in the math and logic Quantum Computing would require. No, that's my point. Things have been shifting a lot the last half century, and we have been adapting.\n\nI really don't see people 'ditching' what we now call Computer Science. on a 'worst case scenario', people who focus on the new Quantum Computing field will need to know it, if only because it is history - like we all learn classical Darwinism before learning the modern theory of evolution.\n\nAs a last comment, I don't see what you mean as 'shift in math and logic'. Will geometry not be relevant anymore? Predicate logic? Or will they be less necessary because of a change in approach? I can't see how. \n\nI'd bet QC will be a change in paradigm, and that's all there is to it. As big a change as the change from abacuses to von neuman's architecture machines, or even greater maybe, but still \"just\" a paradigm change. Learning numerical bases (abacus-knowledge) is not irrelevant today in the age of the internet, and will probably never be, regardless of the advent of quantum computing. No, that's my point. Things have been shifting a lot the last half century, and we have been adapting.\n\nI really don't see people 'ditching' what we now call Computer Science. on a 'worst case scenario', people who focus on the new Quantum Computing field will need to know it, if only because it is history - like we all learn classical Darwinism before learning the modern theory of evolution.\n\nAs a last comment, I don't see what you mean as 'shift in math and logic'. Will geometry not be relevant anymore? Predicate logic? Or will they be less necessary because of a change in approach? I can't see how. \n\nI'd bet QC will be a change in paradigm, and that's all there is to it. As big a change as the change from abacuses to von neuman's architecture machines, or even greater maybe, but still \"just\" a paradigm change. Learning numerical bases (abacus-knowledge) is not irrelevant today in the age of the internet, and will probably never be, regardless of the advent of quantum computing. Dijkstra had this to say about what computers mean to Computer Science:\n\n*\"As a result, the topic became —primarily in the USA— prematurely known as \"computer science\" —which, actually is like referring to surgery as \"knife science\"— and it was firmly implanted in people's minds that computing science is about machines and their peripheral equipment.\"*\n\nThe nature of information and abstract thought will not change if we somehow achieve processors capable of new things. All that will happen is that our limitations manipulating the world will be lessened. People will still have to learn most of what we learn today - maybe with a different focus, much like how we (\"higher-level\" focused people) nowadays learn about transistors and logic gates, as when compared to the eletrical engineerings and their 'electronic brains'in the 50's. The next people to come along will just have to learn *extra* things - much like we learn about big data storage and access.\n\n**Computer Science is as much about Computers as Astronomy is about telescopes** is the quote often attributed do Djistkra, and even though it might just be folklore, I thing it summarizes well his intent with the actual quote.\n\nThere is a very famous gif I can't quite find right now of an \"Introduction to Computer Science\" class where the lecturer writes Computer Science in the blackboard, then proceeds to cross first Science, then Computer. It's not a (at least not natural) Science. And it's not (just) about computers. [The gif in question](http://i.imgur.com/l9bLx.jpg) (Not exactly a gif, but you get the point)\n Dijkstra had this to say about what computers mean to Computer Science:\n\n*\"As a result, the topic became —primarily in the USA— prematurely known as \"computer science\" —which, actually is like referring to surgery as \"knife science\"— and it was firmly implanted in people's minds that computing science is about machines and their peripheral equipment.\"*\n\nThe nature of information and abstract thought will not change if we somehow achieve processors capable of new things. All that will happen is that our limitations manipulating the world will be lessened. People will still have to learn most of what we learn today - maybe with a different focus, much like how we (\"higher-level\" focused people) nowadays learn about transistors and logic gates, as when compared to the eletrical engineerings and their 'electronic brains'in the 50's. The next people to come along will just have to learn *extra* things - much like we learn about big data storage and access.\n\n**Computer Science is as much about Computers as Astronomy is about telescopes** is the quote often attributed do Djistkra, and even though it might just be folklore, I thing it summarizes well his intent with the actual quote.\n\nThere is a very famous gif I can't quite find right now of an \"Introduction to Computer Science\" class where the lecturer writes Computer Science in the blackboard, then proceeds to cross first Science, then Computer. It's not a (at least not natural) Science. And it's not (just) about computers. Dijkstra had this to say about what computers mean to Computer Science:\n\n*\"As a result, the topic became —primarily in the USA— prematurely known as \"computer science\" —which, actually is like referring to surgery as \"knife science\"— and it was firmly implanted in people's minds that computing science is about machines and their peripheral equipment.\"*\n\nThe nature of information and abstract thought will not change if we somehow achieve processors capable of new things. All that will happen is that our limitations manipulating the world will be lessened. People will still have to learn most of what we learn today - maybe with a different focus, much like how we (\"higher-level\" focused people) nowadays learn about transistors and logic gates, as when compared to the eletrical engineerings and their 'electronic brains'in the 50's. The next people to come along will just have to learn *extra* things - much like we learn about big data storage and access.\n\n**Computer Science is as much about Computers as Astronomy is about telescopes** is the quote often attributed do Djistkra, and even though it might just be folklore, I thing it summarizes well his intent with the actual quote.\n\nThere is a very famous gif I can't quite find right now of an \"Introduction to Computer Science\" class where the lecturer writes Computer Science in the blackboard, then proceeds to cross first Science, then Computer. It's not a (at least not natural) Science. And it's not (just) about computers. I know dudes fucking love that quote, but I don't think the analogy works that well anymore. In fact, I think it can be downright dangerous. It is just a fun way of feeling superior to other people if you do CS. I'll give a few examples about how the analogy breaks down.  \n\nHardcore CS work can be informed by modern machines. Power consumption is a serious problem in computing today. We have nice ways of defining the time and space complexity of an algorithm, but what about the power complexity? This sort of thing is fundamentally tied to physical implementations since none of our mathematical models of computation have any notion of \"power cost\" and abstract away things like caches which dramatically change how much power you use. Can you write a sorting function that still runs in O(n logn) but uses less power on average than quicksort? \n\nIt isn't all that hard to come up with examples like these where CS and computers are fundamentally tied. You will miss out on some really interesting research ideas by simply dismissing anything that touches a physical machine. \n\nIt is important that we change the perception of CS away from things like Microsoft Certification but we don't need to shift so hard that we miss things.         The von Neumann model hasn't held for over 10 years, considering superscalar architectures and, e.g., cache oblivious algorithms.\n\nAFAICT, D-Wave will turn out to have been a slimy curiosity, but I wouldn't let that detract from the beauty of quantum computation / information theory. &gt;The von Neumann model hasn't held for over 10 years\n\nhttp://en.wikipedia.org/wiki/Harvard_architecture  People always say, \"Moore's Law *has* to break soon, [valid reason that is seemingly convincing]!\" They said it 20 years ago, they said it 10 years ago, they're saying it now about atom-sized transistors (where quantum physics takes over). But that's the magic of Moore's Law. Somewhere, about a year after the last improvement, some guy is going to formulate a solution and make it 2x better than the last guy. What's the link between Quantum CS and Moore's law ? We know some algorithms would be easier with QCs, but we cannot speak of a general speed up. If you want to be pedantic, Moore's law has absolutely nothing [necessarily] to do with quantum computing. The law specifically describes component density on integrated circuits. People always say, \"Moore's Law *has* to break soon, [valid reason that is seemingly convincing]!\" They said it 20 years ago, they said it 10 years ago, they're saying it now about atom-sized transistors (where quantum physics takes over). But that's the magic of Moore's Law. Somewhere, about a year after the last improvement, some guy is going to formulate a solution and make it 2x better than the last guy. People always say, \"Moore's Law *has* to break soon, [valid reason that is seemingly convincing]!\" They said it 20 years ago, they said it 10 years ago, they're saying it now about atom-sized transistors (where quantum physics takes over). But that's the magic of Moore's Law. Somewhere, about a year after the last improvement, some guy is going to formulate a solution and make it 2x better than the last guy.     Computational theory is the same irrespective of the devices that implement the computations. Computational theory predates the computer age, in fact. But at a less fundamental level, I think CS would change dramatically. As a rule, algorithms do not have the same time complexity in Quantum Computing. Computational theory is overhauled by this. So your statement is false and this is why you're getting downvotes.",
    "url" : "http://www.reddit.com/r/compsci/comments/11wx6a/your_thoughts_on_how_quantum_computing_will/"
  }, {
    "id" : 11,
    "title" : "Billion Laughs",
    "snippet" : "  I've seen a similar attack on sites which allow you to compile code, where you submit C code with an exponential amount of #defines:\n#define A B B\n#define B C C\n...\n#define Z printf (\"0\");\nA lot of sites forget to cap the compile time... I have never capped compile time on any builds, is this something one scripts or are there command line parameters to facilitate this?  Interesting, tangentially related fact: Due to the fact that browsers accept gzipped content, you can send a zip bomb as a response* to a HTTP request. I thought zip bombs usually relied on a similar trick shown in a billion laughs, namely, recursively including several copies of a single zipped file, which itself is many copies of a single zip file, etc. Since (presumably) browsers only unroll one level of the recursion, is this actually a real problem? Not safe for unsaved work.\n\n[Yes](http://www.reddit.com/r/netsec/comments/n8idh/in_the_9_months_since_i_last_crashed_your_browser/)\n\nUsing HTTP chunked transfer encoding Gzip you can potentially destabilise the browser/OS.\n Heh, it only crashed the tab I used to open it. Yay Chrome! That's interesting. One person on that thread talked about it bringing down their system using Chromium. Another mentioned that Chrome always kills the tab when it gets to 2 gigs.\n\nSo I guess that's an advantage of a 32-bit Chrome on Windows/Mac? By default, a 32-bit Windows process can't use more than 2 gigs of RAM, so this crashes that tab before it can do more than that.\n\nOn 64-bit Linux, Chrome is 64-bit, so this had my system pretty unresponsive (it was pushing stuff into swap). Fortunately, it registered my \"close tab\" click before it had used enough swap for the OOM-killer to kick in, but that could've been bad. In my experience infinite loops/recursion and fork bombs in JavaScript can really bring down a browser like chrome or Firefox (especially).  These browsers have a \"stop unresponsive script\" feature that does absolutely nothing.  Why can't browsers get it right?  Why does the very feature designed to solve the problem only make it more annoying? Really? In my experience:\n\n&gt; These browsers have a \"stop unresponsive script\" feature that does absolutely nothing.\n\nWorks perfectly fine for me. Do you have an example of this not working? Well, in Firefox, I usually get the dialog, say stop, and the browser continues to be unresponsive, usually popping up the dialog again after a minute.  You can't close the tab or the browser without killing the process while this is happening.  This is on Windows 7 64bit, but I've seen it on every (Windows) system I've ever used.  In fact, I can't remember this dialog ever, ever doing what it supposed to. I thought zip bombs usually relied on a similar trick shown in a billion laughs, namely, recursively including several copies of a single zipped file, which itself is many copies of a single zip file, etc. Since (presumably) browsers only unroll one level of the recursion, is this actually a real problem?  Reminds me of fork-bombing. From now on whenever I fork bomb a friend's computer I promise to use \"lol\" instead of ':'.  (via https://news.ycombinator.com/item?id=4677364 )  I wonder how many parsers will catch a circular entity reference?\n\n    &lt;?xml version=\"1.0\"?&gt;\n    &lt;!DOCTYPE lolz [\n     &lt;!ENTITY lol1 \"&amp;lol2;&amp;lol2;\"&gt;\n     &lt;!ENTITY lol2 \"&amp;lol1;&amp;lol1;\"&gt;\n    ]&gt;\n    &lt;lolz&gt;&amp;lol1;&lt;/lolz&gt;\n\nWill that be an error, or will it be an infinitely large document? I wonder how many parsers will catch a circular entity reference?\n\n    &lt;?xml version=\"1.0\"?&gt;\n    &lt;!DOCTYPE lolz [\n     &lt;!ENTITY lol1 \"&amp;lol2;&amp;lol2;\"&gt;\n     &lt;!ENTITY lol2 \"&amp;lol1;&amp;lol1;\"&gt;\n    ]&gt;\n    &lt;lolz&gt;&amp;lol1;&lt;/lolz&gt;\n\nWill that be an error, or will it be an infinitely large document?  ",
    "url" : "http://en.wikipedia.org/wiki/Billion_laughs"
  }, {
    "id" : 12,
    "title" : "A graph classification strategy: Applied to subreddits. ",
    "snippet" : "  Interesting stuff.\n\nMost of the data seems correct, althought there are some weird relations in there that might not represent a majority of users (pokemon&lt;-&gt;drugs, ladyboners&lt;-&gt;Christianity...).\n\nI'll be waiting for the results on the larger dataset. It's a shame though that arbor.js doesn't work that well.  I thought the result very impressive, although [JD557](http://www.reddit.com/r/compsci/comments/11uz6o/a_graph_classification_strategy_applied_to/c6qhayt) has highlighted there are some weird relations.\n\nI would like to see a bechmark against other classification strategies, such as [K-Means](http://en.wikipedia.org/wiki/K-means_clustering).\n\nConcerning the visualization, arbor.js is fairly bad for large data (as in the example): that would be nice if other visualization formats (as a matrix or plain text file) were available.",
    "url" : "http://www.hiiamchris.com/wordpress/?p=76"
  }, {
    "id" : 13,
    "title" : "Comparing ACLs and Capabilities",
    "url" : "http://www.eros-os.org/essays/ACLSvCaps.html"
  }, {
    "id" : 14,
    "title" : "Help with typing lambda terms",
    "snippet" : "I am taking a lambda calculus class and am a linguistics grad student not a comp sci grad student.  I have kept up with the class for the most part until we reached the typing section.  I understand how to do the typing derivations because it is essentially pattern matching with the type assignment rules.  What I cannot seem to understand is how to assign a type to a particular term T.  \n\nFor example with the Church encoded 2: \\x.\\y.x(xy) the type (I'm pretty sure) is (b1 -&gt; b2) -&gt; (b1 -&gt; b2).\n\nHow can you know the type just from looking at the term?  How to do type inference manually:\n\n    \\f. \\x. f (f x)\nWe will start out by adding dummy types to every expression:\n\n    \\f:a. \\x:b. f (f x : c) : d\n\nWe look at every part, starting at this:\n\n    (f x : c)\n\nThis tells us at if we give `f` a `b`, then we will get a `c`, in other words, `f : b -&gt; c`. Since `f : a` and that there is no subtyping in simply typed lambda calculus, we can set `a = b -&gt; c`. Then we end up with\n\n    \\f:b -&gt; c. \\x:b. f (f x : c) : d\n\nNext, see that we also give `f` a `c`:\n\n    f (f x : b)\n\n This means that `b = c`:\n\n    \\f:b -&gt; b. \\x:b. f (f x : b) : d\n\nLastly, observe that `f` returns a `d`:\n\n    f (f x : b) : d\n\nSince we already know that `f : b -&gt; b`, `d = b`.\n\n    \\f:b -&gt; b. \\x:b. f (f x : b) : b\n\nIn other words, the overall type is\n\n    (\\f:b -&gt; b. \\x:b. f (f x : b) : b) : (b -&gt; b) -&gt; (b -&gt; b) This is wonderful, thank you.  I understood you perfectly until the last step when you translated the various types of the individual terms to a type for the whole term.  Could you maybe use indicies so that I understand which parts of overall type correspond to the typing of the individual terms? Okay, so the after the second-last step:\n\n    \\f:b -&gt; b. \\x:b. f (f x : b) : b\n\nWe will start out by looking at the type of \n\n     \\x:b. f (f x : b) : b\n\nThe type of its input, `x` is `b`:\n\n    \\x:b.\n\nThe type of its result is `b`:\n\n    f (f x : b) : b\n\nSo its type is `b -&gt; b`, a function that takes a `b` and returns a `b`:\n\n    \n     (\\x:b. f (f x : b) : b) : b -&gt; b\n\nThis means that now, the term is\n\n    \\f:b -&gt; b. (\\x:b. f (f x : b) : b) : b -&gt; b\n\nExercise: explain how the type of the rest is `(b -&gt; b) -&gt; (b -&gt; b)`. ",
    "url" : "http://www.reddit.com/r/compsci/comments/11uksn/help_with_typing_lambda_terms/"
  }, {
    "id" : 15,
    "title" : "What is the biggest class of programs for which extensional equality is decidable?",
    "snippet" : "[Extensionality](http://en.wikipedia.org/wiki/Extensionality)\n\nEdit: Okay, biggest class so far is [functions from a compact domain to a discrete codomain](http://math.andrej.com/2007/09/28/seemingly-impossible-functional-programs/).  If we consider programs as black box functions, then\n\n(a) every function from a finite domain to a discrete codomain has decidable equality by simply enumerating the finite domain and comparing the output on each input.\n\n(b) more generally, [every function from from a compact domain to a discrete codomain has decidable equality](http://math.andrej.com/2007/09/28/seemingly-impossible-functional-programs/).\n\nI'm not aware of any broader class. I believe the functions have to be total too? Good point.\n\nWithout totality, Rice's theorem applies implies no class of programs has decidable extensional equality.\n\nProof.  Consider any partial function f.  Now consider the set of partial functions that are extensionally equal to f.  This set is non-empty since it contains f.  It also doesn't contain every function because consider two non-extensionally equal functions g and h;  At most one of them is a member of our set.  Therefore Rice's theorem says that membership in our set is undecidable. I don't understand what you wrote there, but that is wrong by simple counterexample, wherein the domain is booleans and the codomain is booleans (and, of course, not halting): in this class, we have only two functions, the identity function which halts after zero steps and the hanging function which never halts. To identify which function in this class a given member f is, execute a single step and check if it halted.\n\nThis is not different if we require that every program that is extensionally equivalent to a program in our class is also in the class. Consider the class of the identity function, along with a function that returns true when given false and hangs when given true. These functions can be identified by calling them with false. &gt; To identify which function in this class a given member f is, execute a single step and check if it halted.\n\nNope.  There are also a partial function that halts after 2 steps, and a partial function that halts after 3 steps, and so forth. If we consider programs as black box functions, then\n\n(a) every function from a finite domain to a discrete codomain has decidable equality by simply enumerating the finite domain and comparing the output on each input.\n\n(b) more generally, [every function from from a compact domain to a discrete codomain has decidable equality](http://math.andrej.com/2007/09/28/seemingly-impossible-functional-programs/).\n\nI'm not aware of any broader class. &gt; If we consider programs as black box functions, \n\nIndeed, if you make as many of your modules not have any couplings, and just have proper functions that take in data and return results, one can seemingly decompose almost any sized program of such type into independent testable units that can be verified on their own.\n\nAnd thus test driven development is born! :-D (And a lot of other related fields!)\n\nIf anything else, all answers to this question will serve as yet more reasons why reducing coupling is good.\n\nIn real life, some very large programs are said to have extensional equality. That is basically what proper APIs create, a black box of \"functioning\". As for testing all inputs, well, as you said:\n\n&gt; (a) every function from a finite domain to a discrete codomain has decidable equality by simply enumerating the finite domain and comparing the output on each input.\n\nAnd that indeed seem like the proper answer!\n\nReally it is the same as being able to test any other piece of software. If you can get 100% coverage of your inputs, and have a way to verify all outputs, then you can prove your program is correct. Unfortunately the set of useful programs for which inputs are reasonable sized finite is quite small (in contrast to \"finite in terms of any user input up to 1 megabyte of text\"). When dealing with larger finite sets the main limitations become processing time for execution and even disk space for results (unless you can verify in place and not after the fact, in which case, you are just limited by CPU!).\n\nBut if all the stars line up right and one does ever end up in a scenario where there is a program with a reasonble sized set of inputs, and outputs that can be automatically verified in some reasonable fashion, then indeed, one can drop in another program that is supposed to work the exact same way as the first, run the same test suite on it and verify the second program.\n\nI actually got to use a variant of this once when testing an assembler! Since I had a known working disassembler I had a function and its inverse! I likely won't ever get to see that scenario again. (Obviously I was just testing 1 instruction at a time and not the complete set of all possible assembly programs!)  Regular languages, by construction of minimal automaton/transition monoid.\n\nFor CFL, even the problem of deciding whether a language is trivial is impossible. But doesn't inspection of the two minimal automata violate extensionality? \n\nAlso, do you know which computing devices more powerful than a DFA still have decidable (intensional) equivalence? I tried searching for whether rational transducers have a decidable equality function but that literature is impenetrable to me.  Then I probably didn't understand your question. If you're not allowed to look at the \"source\", and the two programs behave the same, then it is impossible to decide if they are the same.\n\nIf DFA equiv. cannot be decided, no more powerful device can be decided either. Thinking about it some more, I suspect *I've* misunderstood the question. I guess extensional equivalence just means \"sure inspect the implementations, but will they always behave identically on identical inputs?\"\n\nOtherwise you're probably stuck with finite mappings. \n Regular languages, by construction of minimal automaton/transition monoid.\n\nFor CFL, even the problem of deciding whether a language is trivial is impossible. What do you mean by triviality? Emptiness for CFLs is decidable. But it's undecidable to determine whether a CFL's complement is empty.  Hmm. I assume you are defining the problem as: take as input a description of two Turing Machines, and accept if they always have the same behavior on the same inputs, or reject otherwise. Right? Yes. Hmm, ok. The first positive result I can think of is that we should be able to *recognize* the language of two *unequal* TMs if they are both deciders (that is, both guaranteed to halt). Just run them together on every input and accept if they differ.\n\nI am pretty sure that we can decide extensional equality given two PDAs. Because of the pumping lemma, we should be able to figure out a sort of maximum length and test everything up to that point; if they agree there, they will agree on all pumped versions. (Not positive but seems plausible.)\n\nIt seems to me that in general, we'll have to be able to solve two problems. The first is computing the busy-beaver function on these two inputs. The second is knowing, given the size of the machines, what the \"maximum\" number of inputs to try should be. I'm not sure about this one, but I kind of have the feeling that it would follow from knowing the BB.\n\nSo I guess my (shaky) conjecture is that the problem may be reduced to the busy-beaver problem. What do you think? Well, that's solving the problem in general, but I'm asking about the biggest class of Turing-machine for which one can solve it. Also, the general problem is trivially reducible to the busy-beaver problem, as the halting problem is trivially reducible to the busy beaver problem and the \"equality problem\" is trivially reducible to the halting problem. Well, if you think your problem is trivially reducible to BB or halting, then the correct answer is \"the class of programs for which BB/halting is decidable.\"\n\nI don't know how to describe it, but that is maybe a more direct question that someone has already answered. Either I misunderstood the word \"reducible\", or you did. It's probably me, but I thought that \"A is reducible to B\" meant that if you can solve \"B\", then you can solve \"A\"? Oh, I see what you mean. Yes, I mistyped.\n\nI was thinking of the statement, \"your problem can be solved for a class of TMs if *and only if* BB can be solved on it.\"  So I think BB could be reduced to your problem somehow. Yes, in the general case (which is not what I am talking about) HP, BB and this problem are equivalent. That is not neccesarily the case if one restricts the problem space to a smaller class of programs.\n\nTake, for instance, the class of always-terminating programs. In this class, HP and BB are trivial, but I see no easy solution to my problem. Is BB trivial for always-terminating programs? How do you do it?",
    "url" : "http://www.reddit.com/r/compsci/comments/11sr0x/what_is_the_biggest_class_of_programs_for_which/"
  }, {
    "id" : 16,
    "title" : "closures vs classes?",
    "snippet" : "I don't understand much of the benefits of using either a closure as opposed to using a class in languages that support both.\n\nI'll use PHP as an example since that's the language I'm using most these days.\n\n    $two = 2;\n    $addFromTwo = function ($number) use ($two) {\n        return $two + $number;\n    };\n\nIn the case of the closure, you have access to a variable, in this case $two, in the scope outside of the closure. It's mutable. After the closure is created, I could alter it's behavior by doing something like...\n\n    $two = 3;\n\nWith a class, I can do\n\n    class Number {\n        public $number;\n        public function __construct($number) {\n            $this-&gt;number = $number;\n        }\n        public function add($otherNumber) {\n           return $this-&gt;number + $otherNumber;\n        }\n    }\n    $number = new Number(2);\n    \nIn the class instance, I can do the following to change the behavior of the add method...\n\n    $number-&gt;number = 3;\n\nClasses add the benefit of letting me group functional mutations to a set of data, and letting me create several interfaces to a set of data. I can do the same thing with closures, but not in a way that expresses the enclosed data quite as well. Closures, in the meantime, require less written code than a class.\n\nCould someone help me understand which structure is better for different scenarios and why?  Mostly convenience, though likely not related to your example. Compare:\n\nJava (at least, Java 7) does not have closures. To take an extremely simple example, here is how to sort a List in Java:\n\n    import java.util.*; // Don't do this, let Eclipse manage imports, but\n                        // this is nice for a demo\n\n    class Test {\n        public static void main(String[] args) {\n            List&lt;String&gt; list = new ArrayList&lt;String&gt;(Arrays.asList(new String[]{\"ALPACA\",\"syntax\",\"astrology\",\"LLAMA\"}));\n            Collections.sort(list);\n            System.out.println(list);\n        }   \n    }   \n\nIf you run this, it generates the following:\n\n    [ALPACA, LLAMA, astrology, syntax]\n\nThat's probably not what you wanted. It's sorting by some absolute value of the character, so capital letters are always before lowercase. You probably wanted a case-insensitive sort. You could do something like this first:\n\n    for(int i=0; i&lt;list.size(); ++i) {\n        list.set(i, list.get(i).toLowerCase());\n    }\n\nBut this probably still isn't what you want:\n\n    [alpaca, astrology, llama, syntax]\n\nYou wanted to preserve the case they had, but sort them in a case-insensitive way. In other words, you wanted to sort them some other way than the default.\n\nI'll leave aside a few other, increasingly more tortured possibilities. The obvious thing you want is to provide some additional *behavior* that a sorter could use.\n\nYou could, I suppose, use inheritance to do this. It's not hard to imagine -- create a class with a sort() method and a compare() method, then override the compare() method in a subclass, maybe a CaseInsensitiveSorter. But that violates the [open/closed principle](http://en.wikipedia.org/wiki/Open/closed_principle), so I won't go into the details of that. (Besides, it would be a great exercise for the reader -- implement a class that does Quicksort, and let it override some method so it sorts by some other criterion.)\n\nThe solution the Java Collections library chose is to let you pass in an object called a Comparator, which lets you compare things. [Here is the interface for Comparator](http://docs.oracle.com/javase/7/docs/api/java/util/Comparator.html):\n\n    public interface Comparator&lt;T&gt; {\n        public int compare(T left, T right);\n    }\n\nYou can override this, so you'd probably do this:\n\n    public class CaseInsensitiveComparator implements Comparator&lt;String&gt; {\n        @Override\n        public int compare(String left, String right) {\n            return left.toLowerCase().compareTo(right.toLowerCase());\n        }\n    }\n\nHere, I'm using the fact that String has a compareTo method that's more or less the same idea. Basically, Java doesn't have operator overloading, so instead of being able to test things like this:\n\n    a &lt; b\n\nUnless it's a primitive numeric type, you can do this to get the same result:\n\n    a.compareTo(b) &lt; 0\n\nSimilarly, a==b becomes a.compareTo(b) == 0, and so on. If you have a comparator (call it c), you would instead do:\n\n    c.compare(a, b) &lt; 0\n\nI probably haven't done the best job explaining this part, so let me know if this is confusing.\n\nSo anyway, back to our example. Java actually doesn't make this quite as painful as all that. There are *anonymous classes,* so I can just make a brand new object with a proper compare() method on the spot. So here's my complete insensitive sort example:\n\n    import java.util.*;\n    \n    class Test {\n        public static void main(String[] args) {\n            List&lt;String&gt; list = new ArrayList&lt;String&gt;(Arrays.asList(new String[]{\"ALPACA\", \"syntax\",\"astrology\",\"LLAMA\"}));\n            Collections.sort(list, new Comparator&lt;String&gt;(){\n                @Override\n                public int compare(String left, String right) {\n                    return left.toLowerCase().compareTo(right.toLowerCase());\n                }\n            });\n            System.out.println(list);\n        }\n    }\n\nThis outputs what we want:\n\n    [ALPACA, astrology, LLAMA, syntax]\n\nThat's starting to look more like a closure. Really, it's a functor -- a function object -- really, all I want to do is pass a function pointer to Collections.sort(), but Java doesn't have function pointers, so instead I'm faking it with an object.\n\nThis example is a bit contrived -- apparently, people want to compare ignoring case often enough that not only is there also a compareToIgnoreCase() method on string, but String.CASE_INSENSITIVE_ORDER is a comparator that uses it. So I could have written this as:\n\n    Collections.sort(list, String.CASE_INSENSITIVE_ORDER)\n\nStill, I hope you get the idea. This is the kind of thing you have to do if the behavior you want isn't built in somehow.\n\nNow, I don't have Java 8 on my system, and I'm too lazy to fetch it for this example, and I don't trust myself to write good Java 8 from scratch. So instead, here's the same example in some other languages. In JavaScript:\n\n    var list = ['ALPACA','syntax','astrology','LLAMA'];\n    list.sort(function(a,b) {\n        a = a.toLowerCase();\n        b = b.toLowerCase();\n        if (a &lt; b) {\n            return -1;\n        } else if (a &gt; b) {\n            return 1;\n        } else {\n            return 0;\n        }\n    });\n    alert(list);\n\nThis pops up a window that says \"ALPACA,astrology,LLAMA,syntax\". The comparison function is a bit longer, but that may be just because I don't know offhand of any functions to return that magical -1, 0, or 1 that I need here.\n\nStill, I contend that it's more readable. That extra cruft of an interface and a whole extra class that exists only to pass in a function is gone. I'm just passing a function.\n\nIn Ruby:\n\n    list = ['ALPACA','syntax','astrology','LLAMA']\n    list.sort! {|a,b| a.downcase &lt;=&gt; b.downcase}\n    p list\n\nAnd the output:\n\n    [\"ALPACA\", \"astrology\", \"LLAMA\", \"syntax\"]\n\nBut notice, that sort method call is *ridiculously* better than the Java version -- it's even a readable one-liner! I also avoided some rubyisms, to make it a bit more comparable, but we can do better than that:\n\n    list.sort_by! {|x| x.downcase}\n\nAnd since we're only calling a single method there, there's another shortcut that's definitely cheating:\n\n    list.sort_by!(&amp;:downcase)\n\nIdiomatic Ruby would probably avoid the bang method (by convention, in Ruby, methods that end in bangs (!) are \"dangerous\", and usually modify the object in question). Plus, it's so short already, I'd probably just do this:\n\n    list = ['ALPACA','syntax','astrology','LLAMA']\n    p list.sort_by(&amp;:downcase)\n\nAnyway, this isn't supposed to be about Ruby being better at everything, but while I'm on the subject, Ruby makes closures so easy to write that they've actually solved some *other* problems Java 8 is trying to solve. For example, if you open a file, how do you make sure it's closed? In C++, you'd use a destructor -- whenever the object falls out of scope, the destructor is called, and that closes the file. But in garbage-collected languages, like pretty much all the other languages we've mentioned, you have to make sure to close the file, even if an exception is thrown. In Java, that means you're doing a lot of this:\n\n    try {\n      SomeInputStream input = whatever...\n      do some stuff with input...\n    } finally {\n      input.close();\n    }\n\nSimilar things have to happen for transactions. Try the operation, if there are errors, catch and rollback, otherwise commit.\n\nIn Ruby, those blocks mean you can practically make up control structures. So you can add new sorts of loops, and you can abstract away all that messy exception handling:\n\n    open 'foo.txt' do |file|\n      file.each_line do |line|\n        puts \"Read line: #{line.chomp}\"\n      end\n    end\n\nNot only do you get that clever each_line construct, but the 'open' method promises to handle closing the file for you, even if there are exceptions involved. You can *almost* pretend it's garbage-collected.\n\nOf course, the old way is also in Ruby, because sometimes files don't nest quite so neatly -- you might need to open file a, then file b, then close file a, work with file b some more, and finally close file b. But it works well enough often enough that it still saves me a ton of time.\n\nClosures aren't the only way to solve problems like this. Lisp-style macros are even more powerful when it comes to replacing keywords with user-defined stuff (custom loops and such). I actually like the C++ destructor concept a lot more for resource cleanup (other than memory), I'm just not willing to give up garbage collection to be able to use it.\n\nThe main thing, though, is convenience. You *could* do any of the above in other ways. I even showed you the other ways. But honestly, would you rather write a try/catch block around every file open, or would you rather pass a block to 'open'? Would you rather create a Comparator (even an anonymous one), or would you rather pass a block instead? Which is more readable?\n\nAs for when classes are better: Probably when you actually are conceptually creating an object and maintaining state. Your example is a good one. I could make a counter like this (JS):\n\n    function Counter() {\n      var num = 0;\n      return function() {\n        return (num += 1);\n      };\n    }\n    \n    var x = Counter();\n    x();  # 1\n    x();  # 2\n    ...\n\nAnd that's a neat trick, but if it gets much more complicated than a simple counter or a single variable, it's probably better to just make a class. I tend to use closures more when they fit neatly into the flow of the program, and when I would otherwise just be passing a function pointer that really is just a function pointer. Ruby is cool. But don't forget about C# with its anonymous methods, lambda expression trees, and using blocks.\n\nAnd does anything else have the anything comparable to the new \"await\" stuff? Mostly convenience, though likely not related to your example. Compare:\n\nJava (at least, Java 7) does not have closures. To take an extremely simple example, here is how to sort a List in Java:\n\n    import java.util.*; // Don't do this, let Eclipse manage imports, but\n                        // this is nice for a demo\n\n    class Test {\n        public static void main(String[] args) {\n            List&lt;String&gt; list = new ArrayList&lt;String&gt;(Arrays.asList(new String[]{\"ALPACA\",\"syntax\",\"astrology\",\"LLAMA\"}));\n            Collections.sort(list);\n            System.out.println(list);\n        }   \n    }   \n\nIf you run this, it generates the following:\n\n    [ALPACA, LLAMA, astrology, syntax]\n\nThat's probably not what you wanted. It's sorting by some absolute value of the character, so capital letters are always before lowercase. You probably wanted a case-insensitive sort. You could do something like this first:\n\n    for(int i=0; i&lt;list.size(); ++i) {\n        list.set(i, list.get(i).toLowerCase());\n    }\n\nBut this probably still isn't what you want:\n\n    [alpaca, astrology, llama, syntax]\n\nYou wanted to preserve the case they had, but sort them in a case-insensitive way. In other words, you wanted to sort them some other way than the default.\n\nI'll leave aside a few other, increasingly more tortured possibilities. The obvious thing you want is to provide some additional *behavior* that a sorter could use.\n\nYou could, I suppose, use inheritance to do this. It's not hard to imagine -- create a class with a sort() method and a compare() method, then override the compare() method in a subclass, maybe a CaseInsensitiveSorter. But that violates the [open/closed principle](http://en.wikipedia.org/wiki/Open/closed_principle), so I won't go into the details of that. (Besides, it would be a great exercise for the reader -- implement a class that does Quicksort, and let it override some method so it sorts by some other criterion.)\n\nThe solution the Java Collections library chose is to let you pass in an object called a Comparator, which lets you compare things. [Here is the interface for Comparator](http://docs.oracle.com/javase/7/docs/api/java/util/Comparator.html):\n\n    public interface Comparator&lt;T&gt; {\n        public int compare(T left, T right);\n    }\n\nYou can override this, so you'd probably do this:\n\n    public class CaseInsensitiveComparator implements Comparator&lt;String&gt; {\n        @Override\n        public int compare(String left, String right) {\n            return left.toLowerCase().compareTo(right.toLowerCase());\n        }\n    }\n\nHere, I'm using the fact that String has a compareTo method that's more or less the same idea. Basically, Java doesn't have operator overloading, so instead of being able to test things like this:\n\n    a &lt; b\n\nUnless it's a primitive numeric type, you can do this to get the same result:\n\n    a.compareTo(b) &lt; 0\n\nSimilarly, a==b becomes a.compareTo(b) == 0, and so on. If you have a comparator (call it c), you would instead do:\n\n    c.compare(a, b) &lt; 0\n\nI probably haven't done the best job explaining this part, so let me know if this is confusing.\n\nSo anyway, back to our example. Java actually doesn't make this quite as painful as all that. There are *anonymous classes,* so I can just make a brand new object with a proper compare() method on the spot. So here's my complete insensitive sort example:\n\n    import java.util.*;\n    \n    class Test {\n        public static void main(String[] args) {\n            List&lt;String&gt; list = new ArrayList&lt;String&gt;(Arrays.asList(new String[]{\"ALPACA\", \"syntax\",\"astrology\",\"LLAMA\"}));\n            Collections.sort(list, new Comparator&lt;String&gt;(){\n                @Override\n                public int compare(String left, String right) {\n                    return left.toLowerCase().compareTo(right.toLowerCase());\n                }\n            });\n            System.out.println(list);\n        }\n    }\n\nThis outputs what we want:\n\n    [ALPACA, astrology, LLAMA, syntax]\n\nThat's starting to look more like a closure. Really, it's a functor -- a function object -- really, all I want to do is pass a function pointer to Collections.sort(), but Java doesn't have function pointers, so instead I'm faking it with an object.\n\nThis example is a bit contrived -- apparently, people want to compare ignoring case often enough that not only is there also a compareToIgnoreCase() method on string, but String.CASE_INSENSITIVE_ORDER is a comparator that uses it. So I could have written this as:\n\n    Collections.sort(list, String.CASE_INSENSITIVE_ORDER)\n\nStill, I hope you get the idea. This is the kind of thing you have to do if the behavior you want isn't built in somehow.\n\nNow, I don't have Java 8 on my system, and I'm too lazy to fetch it for this example, and I don't trust myself to write good Java 8 from scratch. So instead, here's the same example in some other languages. In JavaScript:\n\n    var list = ['ALPACA','syntax','astrology','LLAMA'];\n    list.sort(function(a,b) {\n        a = a.toLowerCase();\n        b = b.toLowerCase();\n        if (a &lt; b) {\n            return -1;\n        } else if (a &gt; b) {\n            return 1;\n        } else {\n            return 0;\n        }\n    });\n    alert(list);\n\nThis pops up a window that says \"ALPACA,astrology,LLAMA,syntax\". The comparison function is a bit longer, but that may be just because I don't know offhand of any functions to return that magical -1, 0, or 1 that I need here.\n\nStill, I contend that it's more readable. That extra cruft of an interface and a whole extra class that exists only to pass in a function is gone. I'm just passing a function.\n\nIn Ruby:\n\n    list = ['ALPACA','syntax','astrology','LLAMA']\n    list.sort! {|a,b| a.downcase &lt;=&gt; b.downcase}\n    p list\n\nAnd the output:\n\n    [\"ALPACA\", \"astrology\", \"LLAMA\", \"syntax\"]\n\nBut notice, that sort method call is *ridiculously* better than the Java version -- it's even a readable one-liner! I also avoided some rubyisms, to make it a bit more comparable, but we can do better than that:\n\n    list.sort_by! {|x| x.downcase}\n\nAnd since we're only calling a single method there, there's another shortcut that's definitely cheating:\n\n    list.sort_by!(&amp;:downcase)\n\nIdiomatic Ruby would probably avoid the bang method (by convention, in Ruby, methods that end in bangs (!) are \"dangerous\", and usually modify the object in question). Plus, it's so short already, I'd probably just do this:\n\n    list = ['ALPACA','syntax','astrology','LLAMA']\n    p list.sort_by(&amp;:downcase)\n\nAnyway, this isn't supposed to be about Ruby being better at everything, but while I'm on the subject, Ruby makes closures so easy to write that they've actually solved some *other* problems Java 8 is trying to solve. For example, if you open a file, how do you make sure it's closed? In C++, you'd use a destructor -- whenever the object falls out of scope, the destructor is called, and that closes the file. But in garbage-collected languages, like pretty much all the other languages we've mentioned, you have to make sure to close the file, even if an exception is thrown. In Java, that means you're doing a lot of this:\n\n    try {\n      SomeInputStream input = whatever...\n      do some stuff with input...\n    } finally {\n      input.close();\n    }\n\nSimilar things have to happen for transactions. Try the operation, if there are errors, catch and rollback, otherwise commit.\n\nIn Ruby, those blocks mean you can practically make up control structures. So you can add new sorts of loops, and you can abstract away all that messy exception handling:\n\n    open 'foo.txt' do |file|\n      file.each_line do |line|\n        puts \"Read line: #{line.chomp}\"\n      end\n    end\n\nNot only do you get that clever each_line construct, but the 'open' method promises to handle closing the file for you, even if there are exceptions involved. You can *almost* pretend it's garbage-collected.\n\nOf course, the old way is also in Ruby, because sometimes files don't nest quite so neatly -- you might need to open file a, then file b, then close file a, work with file b some more, and finally close file b. But it works well enough often enough that it still saves me a ton of time.\n\nClosures aren't the only way to solve problems like this. Lisp-style macros are even more powerful when it comes to replacing keywords with user-defined stuff (custom loops and such). I actually like the C++ destructor concept a lot more for resource cleanup (other than memory), I'm just not willing to give up garbage collection to be able to use it.\n\nThe main thing, though, is convenience. You *could* do any of the above in other ways. I even showed you the other ways. But honestly, would you rather write a try/catch block around every file open, or would you rather pass a block to 'open'? Would you rather create a Comparator (even an anonymous one), or would you rather pass a block instead? Which is more readable?\n\nAs for when classes are better: Probably when you actually are conceptually creating an object and maintaining state. Your example is a good one. I could make a counter like this (JS):\n\n    function Counter() {\n      var num = 0;\n      return function() {\n        return (num += 1);\n      };\n    }\n    \n    var x = Counter();\n    x();  # 1\n    x();  # 2\n    ...\n\nAnd that's a neat trick, but if it gets much more complicated than a simple counter or a single variable, it's probably better to just make a class. I tend to use closures more when they fit neatly into the flow of the program, and when I would otherwise just be passing a function pointer that really is just a function pointer. whistles, now that's detail. An upvote for you sir.  Thanks. I eventually just had to stop...\n\nBut I'm missing stuff! Like, I mentioned Java 8 is trying to solve the problem of automagically closing files and such, but I can't remember what that feature actually looks like in Java. I know they had something that was targeted specifically at that, but wasn't closures. Mostly convenience, though likely not related to your example. Compare:\n\nJava (at least, Java 7) does not have closures. To take an extremely simple example, here is how to sort a List in Java:\n\n    import java.util.*; // Don't do this, let Eclipse manage imports, but\n                        // this is nice for a demo\n\n    class Test {\n        public static void main(String[] args) {\n            List&lt;String&gt; list = new ArrayList&lt;String&gt;(Arrays.asList(new String[]{\"ALPACA\",\"syntax\",\"astrology\",\"LLAMA\"}));\n            Collections.sort(list);\n            System.out.println(list);\n        }   \n    }   \n\nIf you run this, it generates the following:\n\n    [ALPACA, LLAMA, astrology, syntax]\n\nThat's probably not what you wanted. It's sorting by some absolute value of the character, so capital letters are always before lowercase. You probably wanted a case-insensitive sort. You could do something like this first:\n\n    for(int i=0; i&lt;list.size(); ++i) {\n        list.set(i, list.get(i).toLowerCase());\n    }\n\nBut this probably still isn't what you want:\n\n    [alpaca, astrology, llama, syntax]\n\nYou wanted to preserve the case they had, but sort them in a case-insensitive way. In other words, you wanted to sort them some other way than the default.\n\nI'll leave aside a few other, increasingly more tortured possibilities. The obvious thing you want is to provide some additional *behavior* that a sorter could use.\n\nYou could, I suppose, use inheritance to do this. It's not hard to imagine -- create a class with a sort() method and a compare() method, then override the compare() method in a subclass, maybe a CaseInsensitiveSorter. But that violates the [open/closed principle](http://en.wikipedia.org/wiki/Open/closed_principle), so I won't go into the details of that. (Besides, it would be a great exercise for the reader -- implement a class that does Quicksort, and let it override some method so it sorts by some other criterion.)\n\nThe solution the Java Collections library chose is to let you pass in an object called a Comparator, which lets you compare things. [Here is the interface for Comparator](http://docs.oracle.com/javase/7/docs/api/java/util/Comparator.html):\n\n    public interface Comparator&lt;T&gt; {\n        public int compare(T left, T right);\n    }\n\nYou can override this, so you'd probably do this:\n\n    public class CaseInsensitiveComparator implements Comparator&lt;String&gt; {\n        @Override\n        public int compare(String left, String right) {\n            return left.toLowerCase().compareTo(right.toLowerCase());\n        }\n    }\n\nHere, I'm using the fact that String has a compareTo method that's more or less the same idea. Basically, Java doesn't have operator overloading, so instead of being able to test things like this:\n\n    a &lt; b\n\nUnless it's a primitive numeric type, you can do this to get the same result:\n\n    a.compareTo(b) &lt; 0\n\nSimilarly, a==b becomes a.compareTo(b) == 0, and so on. If you have a comparator (call it c), you would instead do:\n\n    c.compare(a, b) &lt; 0\n\nI probably haven't done the best job explaining this part, so let me know if this is confusing.\n\nSo anyway, back to our example. Java actually doesn't make this quite as painful as all that. There are *anonymous classes,* so I can just make a brand new object with a proper compare() method on the spot. So here's my complete insensitive sort example:\n\n    import java.util.*;\n    \n    class Test {\n        public static void main(String[] args) {\n            List&lt;String&gt; list = new ArrayList&lt;String&gt;(Arrays.asList(new String[]{\"ALPACA\", \"syntax\",\"astrology\",\"LLAMA\"}));\n            Collections.sort(list, new Comparator&lt;String&gt;(){\n                @Override\n                public int compare(String left, String right) {\n                    return left.toLowerCase().compareTo(right.toLowerCase());\n                }\n            });\n            System.out.println(list);\n        }\n    }\n\nThis outputs what we want:\n\n    [ALPACA, astrology, LLAMA, syntax]\n\nThat's starting to look more like a closure. Really, it's a functor -- a function object -- really, all I want to do is pass a function pointer to Collections.sort(), but Java doesn't have function pointers, so instead I'm faking it with an object.\n\nThis example is a bit contrived -- apparently, people want to compare ignoring case often enough that not only is there also a compareToIgnoreCase() method on string, but String.CASE_INSENSITIVE_ORDER is a comparator that uses it. So I could have written this as:\n\n    Collections.sort(list, String.CASE_INSENSITIVE_ORDER)\n\nStill, I hope you get the idea. This is the kind of thing you have to do if the behavior you want isn't built in somehow.\n\nNow, I don't have Java 8 on my system, and I'm too lazy to fetch it for this example, and I don't trust myself to write good Java 8 from scratch. So instead, here's the same example in some other languages. In JavaScript:\n\n    var list = ['ALPACA','syntax','astrology','LLAMA'];\n    list.sort(function(a,b) {\n        a = a.toLowerCase();\n        b = b.toLowerCase();\n        if (a &lt; b) {\n            return -1;\n        } else if (a &gt; b) {\n            return 1;\n        } else {\n            return 0;\n        }\n    });\n    alert(list);\n\nThis pops up a window that says \"ALPACA,astrology,LLAMA,syntax\". The comparison function is a bit longer, but that may be just because I don't know offhand of any functions to return that magical -1, 0, or 1 that I need here.\n\nStill, I contend that it's more readable. That extra cruft of an interface and a whole extra class that exists only to pass in a function is gone. I'm just passing a function.\n\nIn Ruby:\n\n    list = ['ALPACA','syntax','astrology','LLAMA']\n    list.sort! {|a,b| a.downcase &lt;=&gt; b.downcase}\n    p list\n\nAnd the output:\n\n    [\"ALPACA\", \"astrology\", \"LLAMA\", \"syntax\"]\n\nBut notice, that sort method call is *ridiculously* better than the Java version -- it's even a readable one-liner! I also avoided some rubyisms, to make it a bit more comparable, but we can do better than that:\n\n    list.sort_by! {|x| x.downcase}\n\nAnd since we're only calling a single method there, there's another shortcut that's definitely cheating:\n\n    list.sort_by!(&amp;:downcase)\n\nIdiomatic Ruby would probably avoid the bang method (by convention, in Ruby, methods that end in bangs (!) are \"dangerous\", and usually modify the object in question). Plus, it's so short already, I'd probably just do this:\n\n    list = ['ALPACA','syntax','astrology','LLAMA']\n    p list.sort_by(&amp;:downcase)\n\nAnyway, this isn't supposed to be about Ruby being better at everything, but while I'm on the subject, Ruby makes closures so easy to write that they've actually solved some *other* problems Java 8 is trying to solve. For example, if you open a file, how do you make sure it's closed? In C++, you'd use a destructor -- whenever the object falls out of scope, the destructor is called, and that closes the file. But in garbage-collected languages, like pretty much all the other languages we've mentioned, you have to make sure to close the file, even if an exception is thrown. In Java, that means you're doing a lot of this:\n\n    try {\n      SomeInputStream input = whatever...\n      do some stuff with input...\n    } finally {\n      input.close();\n    }\n\nSimilar things have to happen for transactions. Try the operation, if there are errors, catch and rollback, otherwise commit.\n\nIn Ruby, those blocks mean you can practically make up control structures. So you can add new sorts of loops, and you can abstract away all that messy exception handling:\n\n    open 'foo.txt' do |file|\n      file.each_line do |line|\n        puts \"Read line: #{line.chomp}\"\n      end\n    end\n\nNot only do you get that clever each_line construct, but the 'open' method promises to handle closing the file for you, even if there are exceptions involved. You can *almost* pretend it's garbage-collected.\n\nOf course, the old way is also in Ruby, because sometimes files don't nest quite so neatly -- you might need to open file a, then file b, then close file a, work with file b some more, and finally close file b. But it works well enough often enough that it still saves me a ton of time.\n\nClosures aren't the only way to solve problems like this. Lisp-style macros are even more powerful when it comes to replacing keywords with user-defined stuff (custom loops and such). I actually like the C++ destructor concept a lot more for resource cleanup (other than memory), I'm just not willing to give up garbage collection to be able to use it.\n\nThe main thing, though, is convenience. You *could* do any of the above in other ways. I even showed you the other ways. But honestly, would you rather write a try/catch block around every file open, or would you rather pass a block to 'open'? Would you rather create a Comparator (even an anonymous one), or would you rather pass a block instead? Which is more readable?\n\nAs for when classes are better: Probably when you actually are conceptually creating an object and maintaining state. Your example is a good one. I could make a counter like this (JS):\n\n    function Counter() {\n      var num = 0;\n      return function() {\n        return (num += 1);\n      };\n    }\n    \n    var x = Counter();\n    x();  # 1\n    x();  # 2\n    ...\n\nAnd that's a neat trick, but if it gets much more complicated than a simple counter or a single variable, it's probably better to just make a class. I tend to use closures more when they fit neatly into the flow of the program, and when I would otherwise just be passing a function pointer that really is just a function pointer.   [Relevant reading](http://www.ibm.com/developerworks/java/library/j-jtp04247/index.html)\n\nOne example: with closures and first class functions, you can actually build a function up at runtime, without knowing the variables at compile time. This means you can do cool things like memoize arbitrary functions *at runtime*. \n\nTo do this with an object you'd need to know something about the structure of the function you're going to memoize. Not so with closures; we capture the environment when we close over the function.\n\nBut that's an esoteric example. The real reason you'd want to use a closure is because some things are just a lot easier to do with them. It takes a functional mindset to understand when and why closures are useful. People who start with OOP languages often miss the power of closures, because we get used to the idea of returning 'objects or literals' not returning blocks of executable code as data.  I'm using closures in object-oriented PHP code primarily for filter and sort functions, while most everything else is a class, and isolate mutability to data access objects.\n\nEdit: I also hate most every OOP language's DateTime class for being mutable.\n\nAs with memoizing arbitrary functions at runtime, I'm wondering if serializing an arbitrary object at runtime is similar. I'll have to read your relevant reading to understand more about your point, but for now I've got to get off Reddit :(  &gt; function ($number) use ($two)  \n&gt; After the closure is created, I could alter it's behavior by doing something like...\n\nThis is actually called [dynamic scoping](https://en.wikipedia.org/wiki/Scope_%28computer_science%29#Dynamic_scoping) and it's *evil*.  As you present them, they are two different approaches to the same problem. They stem from two widely different ideologies in programming: Classes comes from object oriented programming, while closures come from functional programming.\n\nWhether you use classes or closures is up to you, really, but I would recommend consistency. If your project uses lots of OOP, you should go for classes. If your project uses lots of functional programming, you should go for closures.\n\nIn the end, though, it's entirely up to you. You nailed it down pretty well in your post, actually. Classes are \"smarter\" in a way, but closures require less code. What is important to you? I disagree.\n\nI mean, to an extent, yes, if the entire project is using closures *instead of* classes, that makes sense. But I think they go well together. For example, idiomatic Ruby tends to be classes for actual objects and state, and closures for replacing flow-control constructs. The obvious example: While Ruby does have a foreach keyword, most enumerable objects also have an 'each' method which can take a closure as an argument.  Think of closures as an elegant way of defining a functor, and it may help you out.\n\nYou're unlikely to want to modify a functor after receiving it; the use case for functor reuse after mutation is generally slim and uncommon. Where such a case would needed to be done it's likely you'll have some secondary mechanism to generate a new one.\n\nWhen you're sharing functors and closures what you're often wanting to send off to a receiver is not a 'thing' so much as it is a 'behaviour'. Tossing objects around, mutating them in clients, calling their functor behaviour, et cetera; it's not particularly as elegant as simply wrapping up a closure and sending it along as though it's any other *opaque* function.\n\nAs you note, there's not particularly a strong pro or con to either, but I find that the more I write software in scheme the less I enjoy writing software in C++ or Java. They're just... inelegant. ;)\n\nIn fact, Scheme has given me a much stronger appreciation for C. Go figure. Functors in which sense?  C++, prolog, ML, Haskell, Category Theory?\n\nOne of the nice things about closures is that functions *compose*.  In haskell, you can write e.g. a function composition operator *once*, and use it with any closure.  It's similar to the benefits of using Tuples:  when you use a more structural type system, you can more easily reuse glue code then when everything is nominal.\n\nAlso, closures let you partially apply arguments in whatever order you want.  For example, you can create a function that adds \"foo\" to whatever set you pass in. Think of closures as an elegant way of defining a functor, and it may help you out.\n\nYou're unlikely to want to modify a functor after receiving it; the use case for functor reuse after mutation is generally slim and uncommon. Where such a case would needed to be done it's likely you'll have some secondary mechanism to generate a new one.\n\nWhen you're sharing functors and closures what you're often wanting to send off to a receiver is not a 'thing' so much as it is a 'behaviour'. Tossing objects around, mutating them in clients, calling their functor behaviour, et cetera; it's not particularly as elegant as simply wrapping up a closure and sending it along as though it's any other *opaque* function.\n\nAs you note, there's not particularly a strong pro or con to either, but I find that the more I write software in scheme the less I enjoy writing software in C++ or Java. They're just... inelegant. ;)\n\nIn fact, Scheme has given me a much stronger appreciation for C. Go figure. I am curious about that last statement. What makes you appreciate C more? Is it just a dislike for the design C++? There's a distinct freedom to be found in a simple but adequately expressive language.\n\nIE, Most of the time, I don't need much more than a record (struct), why am I burdening myself with an object system? I guess \"adequately expressive\" is in the eye of the beholder.\n\nIt's true that much of the time, I don't need more than a struct, so I'd ideally like struct-like things to be easy to define. But I'm also often working with relatively high-level concepts, like abstract data types -- lists (rather than fixed-length arrays), associative arrays, and so on.\n\nC can do all of this, it just gets ugly. Though I guess if you're comparing it to C++ or Java... Well, it's telling that I still code in Scheme. Just because I appreciate C more doesn't mean I necessarily prefer to use it. ;) Appreciate it more than you did before, or more than scheme? Maybe that's what I was missing?\n\nI do see what you mean, though. C is a language that's actually small enough to hold in your head. Think of closures as an elegant way of defining a functor, and it may help you out.\n\nYou're unlikely to want to modify a functor after receiving it; the use case for functor reuse after mutation is generally slim and uncommon. Where such a case would needed to be done it's likely you'll have some secondary mechanism to generate a new one.\n\nWhen you're sharing functors and closures what you're often wanting to send off to a receiver is not a 'thing' so much as it is a 'behaviour'. Tossing objects around, mutating them in clients, calling their functor behaviour, et cetera; it's not particularly as elegant as simply wrapping up a closure and sending it along as though it's any other *opaque* function.\n\nAs you note, there's not particularly a strong pro or con to either, but I find that the more I write software in scheme the less I enjoy writing software in C++ or Java. They're just... inelegant. ;)\n\nIn fact, Scheme has given me a much stronger appreciation for C. Go figure. &gt; In fact, Scheme has given me a much stronger appreciation for C. Go figure.\n\nBut C++ actually gives you classes, where you can fake closures with constructor and member variables. In C, how do you fake closures more elegantly? &gt; In fact, Scheme has given me a much stronger appreciation for C. Go figure.\n\nBut C++ actually gives you classes, where you can fake closures with constructor and member variables. In C, how do you fake closures more elegantly? &gt; In fact, Scheme has given me a much stronger appreciation for C. Go figure.\n\nBut C++ actually gives you classes, where you can fake closures with constructor and member variables. In C, how do you fake closures more elegantly? &gt; In fact, Scheme has given me a much stronger appreciation for C. Go figure.\n\nBut C++ actually gives you classes, where you can fake closures with constructor and member variables. In C, how do you fake closures more elegantly?    ",
    "url" : "http://www.reddit.com/r/compsci/comments/11rg83/closures_vs_classes/"
  }, {
    "id" : 17,
    "title" : "Is there a diagram anywhere that displays a broad history (with short detail) of most of the major microarchitectures anywhere? ",
    "snippet" : "For a semi-beginner wanting to get a better visual understanding of how all of them play together and when they were released.  ",
    "url" : "http://www.reddit.com/r/compsci/comments/11rfde/is_there_a_diagram_anywhere_that_displays_a_broad/"
  }, {
    "id" : 18,
    "title" : "Happy Ada Lovelace Day - Celebrating important women in CS",
    "snippet" : "   With another Ada Lovelace Day passed,  it's a good time to think about women in technology. There are some great success stories, but our field has a long way to go before it is truly accepting of (and welcoming to) women.\n\nWith that in mind, here are a couple articles that will be eye-openers for many.\n\nTess Rinearson's [On Technical Entitlement.](http://tessrinearson.com/blog/?p=400)\n\nand\n\nKatie Cunningham's [Lighten Up.](http://therealkatie.net/blog/2012/mar/21/lighten-up/) With another Ada Lovelace Day passed,  it's a good time to think about women in technology. There are some great success stories, but our field has a long way to go before it is truly accepting of (and welcoming to) women.\n\nWith that in mind, here are a couple articles that will be eye-openers for many.\n\nTess Rinearson's [On Technical Entitlement.](http://tessrinearson.com/blog/?p=400)\n\nand\n\nKatie Cunningham's [Lighten Up.](http://therealkatie.net/blog/2012/mar/21/lighten-up/)   She [was not](http://en.wikipedia.org/wiki/Ada_Lovelace#Controversy_over_extent_of_contributions) the first programmer, btw. They should perhaps have named it the [Grace Hopper](http://en.wikipedia.org/wiki/Grace_Hopper) day instead. Grace hopper isn't as romantic and seductive, but she has many notable contributions that can't possibly be attributed to someone else.\n\nNevertheless, Ada Lovelace was definitely smart and inquisitive, and might still hold the record as being one of the first authors to publish about computing and programming. Ada Lovelace was an important women promoting science in a society that made any form of participation difficult for women. This controversy on whether or not she was \"the first programmer\" is ridiculous, this term being in fact impossible to define objectively. It would be inane to refuse to use Ada Lovelace's name just because some people exaggerated the claims about her actually writing computer programs.\n\nThere are numerous women that have made very important contributions to science in equally male-dominated times: Emmy Noether and Marie Curie just to cite a few, and their contributions are certainly much more important than those of Ada Lovelace.  \nBut don't be fooled: the people that whine today about Ada Lovelace \"not being the first programmer\" are just angry, for some reason, about efforts to increase involvement of women in scientific fields. Name it the Grace Hopper day and you'll have them claiming that the compiler she wrote was actually very primitive, that COBOL is a horrible programming language, and that she left her mathematics teaching position to work for the military. &gt;But don't be fooled: the people that whine today about Ada Lovelace \"not being the first programmer\" are just angry, for some reason, about efforts to increase involvement of women in scientific fields.  \n\nNo, it's because they are, technically, correct. This is ad hominem, also a strawman since no one here is arguing against Grace Hopper's contributions. Sorry if you think Lovelace should be above scrutiny because you think it helps introducing women to programming. As you can see in the Wikipedia entry, Babbage takes credit for \"the first program\". We don't need your patronizing attitude in trying to get more women into programming. I do agree that claiming that Ada \"is not the first programmer\" is technically correct. On the other side, I think that claiming that Babbage wrote \"the first program\" is not technically correct (because I think you could go further and call programs the iterative algorithms that the mathematicians developed and understood; sure, they weren't executed on machines at the time, but that doesn't make them \"not programs\").\n\nI essentially don't care about what Ada Lovelace did. I like to have a day to celebrate women's contribution in science -- and in this day I would like to talk about women whose contribution matter to me, and I would probably pick someone else than Ada Lovelace.\n\nBut I can't help wondering why you seem to have this strange obsession with \"setting the record straight\" (while, as I say, everybody can understand that \"being the first programmer\" is so fuzzy a concept that it's not an objective statement about Ada Lovelace, or Babbage for that matter). I don't think that cringing to a claim in all circumstances makes the world a better place; you have all year to advance your personal obsession with the contribution on Ada Lovelace (by all means, do), why choose precisely this day if not to weaken the effect of the Ada Lovelace Day, if only by drawing the discussion to minutiae of the Lovelace/Babbage relation instead of actually speaking about advances of women in science?\n\nI'm genuinely interested in this question. What do you see as your motivation here? &gt; I'm genuinely interested in this question. What do you see as your motivation here?\n\n[Jumping in the middle of a conversation here.]\n\nI think it's the same reason why some people have to mention that Columbus didn't actually discover America, on Columbus day. They are missing the point, but lets not leap to why. While I understand your point, I think the criticism of Columbus on Columbus day is different in that he was actually a really horrible person. He didn't just not discover America, but also committed some pretty heinous acts against the Native Americans who were already there. Did you just nitpick me on my point about how people nitpick details when the point is more important   ? ಠ_ಠ\n Haha, I guess I did. When I wrote it the differences seemed significant enough to me to warrant a comment. I do think the two situations are not really comparable, but that is a side-tracking so sorry about that! &gt; I'm genuinely interested in this question. What do you see as your motivation here?\n\n[Jumping in the middle of a conversation here.]\n\nI think it's the same reason why some people have to mention that Columbus didn't actually discover America, on Columbus day. They are missing the point, but lets not leap to why. I would agree It was inappropriate of me to make claims about the *intent* of the people having this opinion -- I don't know what their intent was. But the observed effect is the same : they lessen discussion about women in science with this obsessive (and I think, often just as irrational and biased) discussion of Ada Lovelace's contribution. In the end it doesn't matter much if it's done with intentions I would agree with or not.\n\nAnd we need to look at the big picture. Reactions that individually are reasonable and of good faith can become a nuisance when repeated or made at just the inappropriate time and place. I'm disappointed by those comments that I find out-of-place. I mostly agree with with you are saying, but for the last part. As bad as the the comments about Ada Lovelace contribution are, equally non productive are comments which try to shut them down by discerning sexism in their intent. A better approach would have been to point out Ms Lovelace's contributions, and how we are celebrating important women in CS, not important women in a semantically narrow definition of 'programming'.  Now you (and I) have made this molehill into a mountain - thankfully the thread is buried, and won't derail the actual conversation. \n\nI agree that a lot of people disguise sexism as criticism, (or racism as state rights etc etc), but we still need to deal with each comment individually. Of course, your experiences might have biased you to a heuristic discrimination, and it's hard to argue against that. But I still had to say my piece :). Cheers.  \n\n[Edit: Just saw your history and saw that you are a haskeller. Yeah, this discussion is going nowhere :) ] My entirely personal guess at the situation (with no claim of objective accuracy: it's how I perceived the people starting this discussion) is that nawitus has a grudge against the Ada Lovelace Day. See his or her equally noisy comments on [the other Ada Lovelace Day thread](http://www.reddit.com/r/linux/comments/11litu/fsf_on_ada_lovelace_day_though_the_number_of/). On the other hand, I understood that trannygirl was honestly and in good faith trying to appease potential trolls by suggesting a different name. I did not initially respond to nawitus' remark, but found it important to point out to trannygirl that, given the fact that some people doubting Ada Lovelace's contributions in fact had an anti-women-celebration agenda, changing the name of the event would not appease critics but mostly shift discussion to some other anti-women-celebration topic.\n\nThis is still my belief about nawitus, and therefore characterized in my eyes 100% of the people expressing themselves on the matter thus far in the comments, which lead to my unfortunate generalization. As you said, \"heuristic discrimination\". I hope it was clear from the context that my \"the people that whine today\" was more a rhetoric device than an actual statement about all people having an opinion on Ada Lovelace contribution.\n\nI still stand by this point of view (not claiming it's an objective fact). I understand that some people are obsessive nitpickers (I tend to be as well, \"someone is wrong on the internet\" xkcd yadda yadda), but I still think that insisting on the controversy on Ada Lovelace contribution is, *in effect*, a way to endanger the success of this \"celebration of women in science\" by shifting the focus to something else.\n\nTo be painfully honest here, I didn't know (or cared) about this controversy until a few days ago, and initially accepted the nay-sayers claim that evidence clearly demonstrated Ada Lovelace didn't actually write those programs -- I thought that such nitpickers had they facts straight and solid. Now reading [this other claim](http://adainitiative.org/2012/10/ada-lovelace-day-countess-ada-lovelace/) linked in this thread made me reconsider : my current opinion is that the evidence against the importance of Ada Lovelace's contribution is actually mucher weaker than I thought (the claims of one of the interested parties, Babbage and later people infatuated with his work that may very well have had biased point of view). I'm sure I could come to a more definitive opinion on the matter by reading the actual correspondence, but I very much not want to be dragged into this controversy I initially didn't care about just to argue with some people I dislike on the internet.\n\nSo I withdraw all factual claims on *intent*, but still think that there is something to be explained about why some people (and it happens that one of them likes to discuss the fact that feminists monopolize the gender imbalance discussion and that we should care more about anti-men discrimination, which is a fine opinion in itself but helps me create a subjective picture of someone not really enamored with a day about contributions of women in science) choose to nitpick precisely about this, of all things. Apparent sexism in the way they discuss, that I have evidence against? Not at all (caring about factual accuracy and unbiased treatment of history is something I absolutely support). Underlying desire that we insisted *less* on the role of women in science ? That's my subjective feeling about the whole thing -- with no claim of factual accuracy. I do agree that claiming that Ada \"is not the first programmer\" is technically correct. On the other side, I think that claiming that Babbage wrote \"the first program\" is not technically correct (because I think you could go further and call programs the iterative algorithms that the mathematicians developed and understood; sure, they weren't executed on machines at the time, but that doesn't make them \"not programs\").\n\nI essentially don't care about what Ada Lovelace did. I like to have a day to celebrate women's contribution in science -- and in this day I would like to talk about women whose contribution matter to me, and I would probably pick someone else than Ada Lovelace.\n\nBut I can't help wondering why you seem to have this strange obsession with \"setting the record straight\" (while, as I say, everybody can understand that \"being the first programmer\" is so fuzzy a concept that it's not an objective statement about Ada Lovelace, or Babbage for that matter). I don't think that cringing to a claim in all circumstances makes the world a better place; you have all year to advance your personal obsession with the contribution on Ada Lovelace (by all means, do), why choose precisely this day if not to weaken the effect of the Ada Lovelace Day, if only by drawing the discussion to minutiae of the Lovelace/Babbage relation instead of actually speaking about advances of women in science?\n\nI'm genuinely interested in this question. What do you see as your motivation here? Ada Lovelace was an important women promoting science in a society that made any form of participation difficult for women. This controversy on whether or not she was \"the first programmer\" is ridiculous, this term being in fact impossible to define objectively. It would be inane to refuse to use Ada Lovelace's name just because some people exaggerated the claims about her actually writing computer programs.\n\nThere are numerous women that have made very important contributions to science in equally male-dominated times: Emmy Noether and Marie Curie just to cite a few, and their contributions are certainly much more important than those of Ada Lovelace.  \nBut don't be fooled: the people that whine today about Ada Lovelace \"not being the first programmer\" are just angry, for some reason, about efforts to increase involvement of women in scientific fields. Name it the Grace Hopper day and you'll have them claiming that the compiler she wrote was actually very primitive, that COBOL is a horrible programming language, and that she left her mathematics teaching position to work for the military. Ada Lovelace was an important women promoting science in a society that made any form of participation difficult for women. This controversy on whether or not she was \"the first programmer\" is ridiculous, this term being in fact impossible to define objectively. It would be inane to refuse to use Ada Lovelace's name just because some people exaggerated the claims about her actually writing computer programs.\n\nThere are numerous women that have made very important contributions to science in equally male-dominated times: Emmy Noether and Marie Curie just to cite a few, and their contributions are certainly much more important than those of Ada Lovelace.  \nBut don't be fooled: the people that whine today about Ada Lovelace \"not being the first programmer\" are just angry, for some reason, about efforts to increase involvement of women in scientific fields. Name it the Grace Hopper day and you'll have them claiming that the compiler she wrote was actually very primitive, that COBOL is a horrible programming language, and that she left her mathematics teaching position to work for the military. Ada Lovelace was an important women promoting science in a society that made any form of participation difficult for women. This controversy on whether or not she was \"the first programmer\" is ridiculous, this term being in fact impossible to define objectively. It would be inane to refuse to use Ada Lovelace's name just because some people exaggerated the claims about her actually writing computer programs.\n\nThere are numerous women that have made very important contributions to science in equally male-dominated times: Emmy Noether and Marie Curie just to cite a few, and their contributions are certainly much more important than those of Ada Lovelace.  \nBut don't be fooled: the people that whine today about Ada Lovelace \"not being the first programmer\" are just angry, for some reason, about efforts to increase involvement of women in scientific fields. Name it the Grace Hopper day and you'll have them claiming that the compiler she wrote was actually very primitive, that COBOL is a horrible programming language, and that she left her mathematics teaching position to work for the military. &gt;But don't be fooled: the people that whine today about Ada Lovelace \"not being the first programmer\" are just angry, for some reason, about efforts to increase involvement of women in scientific fields.\n\nOne could argue quite the opposite. By promoting someone whose achievements are less certain as a figurehead for women in science, it weakens the movement. People enthused by the story could research and discover the less impressive reality, and potentially consider it representative. Ada Lovelace was an important women promoting science in a society that made any form of participation difficult for women. This controversy on whether or not she was \"the first programmer\" is ridiculous, this term being in fact impossible to define objectively. It would be inane to refuse to use Ada Lovelace's name just because some people exaggerated the claims about her actually writing computer programs.\n\nThere are numerous women that have made very important contributions to science in equally male-dominated times: Emmy Noether and Marie Curie just to cite a few, and their contributions are certainly much more important than those of Ada Lovelace.  \nBut don't be fooled: the people that whine today about Ada Lovelace \"not being the first programmer\" are just angry, for some reason, about efforts to increase involvement of women in scientific fields. Name it the Grace Hopper day and you'll have them claiming that the compiler she wrote was actually very primitive, that COBOL is a horrible programming language, and that she left her mathematics teaching position to work for the military. &gt;But don't be fooled: the people that whine today about Ada Lovelace \"not being the first programmer\" are just angry, for some reason, about efforts to increase involvement of women in scientific fields.\n\nNot true, but I guess facts don't matter to some people like you. She [was not](http://en.wikipedia.org/wiki/Ada_Lovelace#Controversy_over_extent_of_contributions) the first programmer, btw. I think she [was](http://adainitiative.org/2012/10/ada-lovelace-day-countess-ada-lovelace/).  It's interesting to me, that in a post that supposed to be about celebrating women in tech and encouraging their participation, most of the comments are focused on something else.... debating the merits of one woman's contributions. -_-\n\nedit: typos It's interesting to me, that in a post that supposed to be about celebrating women in tech and encouraging their participation, most of the comments are focused on something else.... debating the merits of one woman's contributions. -_-\n\nedit: typos",
    "url" : "http://gallium.inria.fr/~scherer/gagallium/ada-lovelace-day-2012/index.html"
  }, {
    "id" : 19,
    "title" : "Physics, Topology, Logic and Computation: A Rosetta Stone [pdf]",
    "snippet" : "  I'm guessing most people won't open a pdf, so here's the tl;dr version: \n\n\n\n| Category Theory | Physics | Topology  | Logic          | Computation |\n|-----------------------|------------|--------------|----------------|------------------|\n| object                 | system  | manifold   | proposition  | data type     |\n| morphism           | process | cobordism | proof          | program      |             \n\n",
    "url" : "http://math.ucr.edu/home/baez/rosetta.pdf"
  }, {
    "id" : 20,
    "title" : "\"How Did Software Get So Reliable Without Proof?\"",
    "snippet" : "  \"Software developers have become adept at the difficult art of building reasonably reliable systems out of unreliable parts. The snag is that often we do not know exactly how we did it.\" \n\n  -- Bjarne Stroustrup, creator of C++ The parts usually are reliable - the integration is less so, in my opinion.  That's quite the opposite of what Stroustrup said. Food for thoughts. I'd agree with cratylus. You can depend on a computer to do *precisely* what you tell it to do. The message you are giving the computer however can be difficult to decipher at times. I recently solved a problem in Delphi, for example, that had me fairly frustrated. I must have typed 10 different combinations of code, all of which the compiler accepted and were readable and made sense to me as a human, but 9 of which did absolutely nothing when ran.  \n  \nEdit: and yes, I still use Delphi sometimes. I'm not disagreeing with cratylus, but I'd like to know what Stroustrup had in mind. I'm not disagreeing with cratylus, but I'd like to know what Stroustrup had in mind. I'm not disagreeing with cratylus, but I'd like to know what Stroustrup had in mind. I'd agree with cratylus. You can depend on a computer to do *precisely* what you tell it to do. The message you are giving the computer however can be difficult to decipher at times. I recently solved a problem in Delphi, for example, that had me fairly frustrated. I must have typed 10 different combinations of code, all of which the compiler accepted and were readable and made sense to me as a human, but 9 of which did absolutely nothing when ran.  \n  \nEdit: and yes, I still use Delphi sometimes. That's quite the opposite of what Stroustrup said. Food for thoughts.  Some part of this is also a side effect of Moore's Law. As computers got faster and more memory, the need for clever, potentially buggy algorithms was reduced.\n\nIf you look at a lot of old games, you'll see many glitches that ultimately come down to having limited memory or CPU. The Minus World in Super Mario Bros. and MissingNo in Pokemon both come down to having code and data structures that are programmed for compactness rather than being well-structured. Also, the Minus World glitch is exploited due to imperfect collision detection, which again comes down to speed and memory. (Technical explanations of each are linked below.) \n\nIt's not just limited to games, either; all software on older machines had to do these tricks sooner or later.\n\nOver time, Moore's Law made those tricks unnecessary. While some might regret that the younger generation of programmers don't know all these neat tricks, the fact is that those tricks were buggy and made the code unclear. Software really is better off without them.\n\nMinus world: http://www.romhacking.net/documents/343/  \nMissingNo: https://keithmaggio.wordpress.com/2010/04/06/game-glitch-analysis-missingno/ &gt; Some part of this is also a side effect of Moore's Law. As computers got faster and more memory, the need for clever, potentially buggy algorithms was reduced.\n\nYeah, Hoare in his [paper](http://dl.dropbox.com/u/85192141/1996-hoare.pdf) actually lists this as one of the main enablers:\n\n&gt; The first benefit of a superabundance of resource is to make possible a decision to avoid any kind of sophistication or optimisation in the design of algorithms or data structures. Common prohibitions are: no data packing, no optimal coding, no pointers, no sharing, no dynamic storage allocation. The maximum conceivably necessary size of record or array is allocated, and then some more. Similar prohibitions are often placed on program structures: no jumps, no interrupts, no multiprogramming, no global variables. Access to data in other modules is permitted only through carefully regulated remote procedure calls. In the past, these design rules were found to involve excessive loss of efficiency; up to a factor of a hundred has been recorded on first trials of a rigorously structured system. This factor had to be regained by relaxing the prohibitions, massaging the interfaces between modules, even to the extent of violating the structural integrity of the whole system. Apart from the obvious immediate dangers, this can lead to even greater risk and expense in subsequent updating and enhancing of the system. Fortunately, cheaper hardware reduces the concern for efficiency, and improved optimisation technology for higher level languages promises further assistance in reconciling a clear structure of the source code with high efficiency in the object code. Some part of this is also a side effect of Moore's Law. As computers got faster and more memory, the need for clever, potentially buggy algorithms was reduced.\n\nIf you look at a lot of old games, you'll see many glitches that ultimately come down to having limited memory or CPU. The Minus World in Super Mario Bros. and MissingNo in Pokemon both come down to having code and data structures that are programmed for compactness rather than being well-structured. Also, the Minus World glitch is exploited due to imperfect collision detection, which again comes down to speed and memory. (Technical explanations of each are linked below.) \n\nIt's not just limited to games, either; all software on older machines had to do these tricks sooner or later.\n\nOver time, Moore's Law made those tricks unnecessary. While some might regret that the younger generation of programmers don't know all these neat tricks, the fact is that those tricks were buggy and made the code unclear. Software really is better off without them.\n\nMinus world: http://www.romhacking.net/documents/343/  \nMissingNo: https://keithmaggio.wordpress.com/2010/04/06/game-glitch-analysis-missingno/ Some part of this is also a side effect of Moore's Law. As computers got faster and more memory, the need for clever, potentially buggy algorithms was reduced.\n\nIf you look at a lot of old games, you'll see many glitches that ultimately come down to having limited memory or CPU. The Minus World in Super Mario Bros. and MissingNo in Pokemon both come down to having code and data structures that are programmed for compactness rather than being well-structured. Also, the Minus World glitch is exploited due to imperfect collision detection, which again comes down to speed and memory. (Technical explanations of each are linked below.) \n\nIt's not just limited to games, either; all software on older machines had to do these tricks sooner or later.\n\nOver time, Moore's Law made those tricks unnecessary. While some might regret that the younger generation of programmers don't know all these neat tricks, the fact is that those tricks were buggy and made the code unclear. Software really is better off without them.\n\nMinus world: http://www.romhacking.net/documents/343/  \nMissingNo: https://keithmaggio.wordpress.com/2010/04/06/game-glitch-analysis-missingno/ Another nice perk of Moore's law law is test suites. All other things being equal (which of course they rarely are, so the effect is probably less pronounced than this), when your computing speed doubles you can run twice as many tests in the same amount of time, so as your hardware gets faster the amount you can plausibly test about a fixed program goes up.  That's a really good point which I think can be extended to the whole development loop of compiling, testing, debugging, writing code, etc.   It must be either reliable or unreliable even if you have proof for neither. It must be either reliable or unreliable even if you have proof for neither.   The same way bridges, aeroplanes and medicine happened.  What makes software so special?  Software is reliable?  Since when? Given all of the pieces of software that worked between you hitting keys on your keyboard and the polarizing filters flickering on and off on my screen so that I could read your sentences, I'd say that software is doing pretty well. Not to mention the software flying planes through the sky, and sitting between the accelerator pedal and engine of every modern car.  Those types of programs are often formally proven, however, so not really on topic. If you have a source or article about that I'd like to read it, I was under the impression that they have formal specifications but that doesn't necessarily equate to formally 'proven' programs, though I imagine they are tested and verified to a huge degree Given all of the pieces of software that worked between you hitting keys on your keyboard and the polarizing filters flickering on and off on my screen so that I could read your sentences, I'd say that software is doing pretty well. Exactly this, its incredible how far we have come in such a short time. Many people complaining about the state of technology are either too young or dont truly understand how we got here. Given all of the pieces of software that worked between you hitting keys on your keyboard and the polarizing filters flickering on and off on my screen so that I could read your sentences, I'd say that software is doing pretty well. Given all of the pieces of software that worked between you hitting keys on your keyboard and the polarizing filters flickering on and off on my screen so that I could read your sentences, I'd say that software is doing pretty well. That's selection bias. Of course his software is reliable enough to allow him to post that comment online. If it weren't, he wouldn't have been able to post the comment. The point regarding embedded software further up in this thread is definitely true though. How many plane/car/train, etc., accidents occur due to software bugs? And how many such vehicles that rely on software exist today?\n\nAlso, when you turn on a PC, nowadays, you expect a GUI, Internet access, etc. We no longer expect frequent BSoDs or graphical artifacts, or random crashing of applications. &gt; How many plane/car/train, etc., accidents occur due to software bugs? And how many such vehicles that rely on software exist today?\n\nAnd how many of them are not written using some formal method? That's selection bias. Of course his software is reliable enough to allow him to post that comment online. If it weren't, he wouldn't have been able to post the comment. My point was more illustrative.\n\nBut regarding the selection bias: I've never tried to post a comment on reddit that failed due to some software other than reddit. There's a huuuuuge chain of code between my keyboards and the reddit servers that has, time and time again, been reliable. Given all of the pieces of software that worked between you hitting keys on your keyboard and the polarizing filters flickering on and off on my screen so that I could read your sentences, I'd say that software is doing pretty well. Software is reliable?  Since when? Software is reliable?  Since when? Software is reliable?  Since when?  Ah good old Tony Hoarse. I think that is a name that a professor interested in \"formally verifying the absence of some kinds of bugs\" shouldn't be getting wrong.\n\nYou could maybe make an argument that some software is \"mostly reliable\" My windows machine doesn't turn its self off on its own much for example. A lot of that is because drivers are simple enough that they can be formally verified.\n\nEqually important is that windows has been \"tested\" by millions of users for years and years now. Most of the really nasty bugs have shown up enough times for developers to find and fix them. Windows obviously doesn't belong in a discussion about critical software though.\n\nWhat happens when testing your software puts a machine worth a few million dollars at risk? You can do a lot of simulations, but it can be hard to predict the response to every possible real world input. This is where a case for more formal verification can be made. The cost of funding an entire team to spend 5 years verifying software can pay for itself instantly if it finds a bug that might cause a single fighter jet to crash. From personal experience, such a team will generally find a few of these bugs, even in simple software. &gt; Equally important is that windows has been \"tested\" by millions of users for years and years now. \n\nYour argument is flawed. Driver models have been changed many times, as has substantial parts of the Windows kernel.\n\nOne of the big factors is that Microsoft very heavily tests code on Windows, including drivers. That includes software who's vendor has no affiliation to Microsoft in any way. MS has also made plenty of changes to driver models, such as the changes for graphics drivers in Vista, which makes them more reliable. For example my NVidia drivers have crashed plenty of times on Windows 7, but most of the time it just restarts. Under XP, you'd have been fucked.\n\nIt ultimately comes from the fact that 70% of blue screens are due to non-MS code, predominantly drivers, yet when your machine blue screens, users instinctively blame MS. Historically, blue screens were due to the slapped-on virtual memory implementation in the Win95-vintage versions. The memory manager would end up accessing the wrong bits of memory and bad things happened. That was very much Microsoft's fault.\n\nOnce WinXP came out, they were using the NT kernel, which was designed for VM from the beginning. After that, BSoDs do tend to be bad driver or application code. &gt; Equally important is that windows has been \"tested\" by millions of users for years and years now. \n\nYour argument is flawed. Driver models have been changed many times, as has substantial parts of the Windows kernel.\n\nOne of the big factors is that Microsoft very heavily tests code on Windows, including drivers. That includes software who's vendor has no affiliation to Microsoft in any way. MS has also made plenty of changes to driver models, such as the changes for graphics drivers in Vista, which makes them more reliable. For example my NVidia drivers have crashed plenty of times on Windows 7, but most of the time it just restarts. Under XP, you'd have been fucked.\n\nIt ultimately comes from the fact that 70% of blue screens are due to non-MS code, predominantly drivers, yet when your machine blue screens, users instinctively blame MS. ",
    "url" : "http://blog.regehr.org/archives/820"
  }, {
    "id" : 21,
    "title" : "Peeking inside LuaJIT ",
    "url" : "http://playingwithpointers.com/archives/1010"
  }, {
    "id" : 22,
    "title" : "Koka: Javascript-like syntax, but with a powerful side-effect tracking type system ",
    "url" : "http://www.rise4fun.com/koka/tutorial"
  }, {
    "id" : 23,
    "title" : "Fundamentals of Artificial Neural Networks",
    "snippet" : "  [deleted] [deleted]",
    "url" : "http://neuron.eng.wayne.edu/tarek/MITbook/t_contents.html"
  }, {
    "id" : 24,
    "title" : "Looking for papers on automatically identifying state/region maps.",
    "snippet" : "I would like to read up on automatically identifying a location given a map of the region, which is inherently a computer vision/machine learning problem. I am assuming the process is done by looking for state/region boundaries, then comparing it to a known database. I also assume that the coloring of the region could be useful as well.\n\nCan anyone point to towards some of the seminal works so I can get started on reading up? Searching for automatic map recognition leads to a lot of stuff related to AI or diffusion maps, so any hints at keywords would also be appreciated.  Are the maps you'll be looking all at a certain scale or might you have to deal with both a city map and a continent-scale map? \n\nI imagine the simplest thing to do would be to parse the landmark/city names, look up candidate coordinates for each one, and then filter the results (to discard outliers).    Do the maps have textual elements (i.e. town/city labels)? ",
    "url" : "http://www.reddit.com/r/compsci/comments/11n8fz/looking_for_papers_on_automatically_identifying/"
  }, {
    "id" : 25,
    "title" : "Cheap Supercomputer ",
    "snippet" : "  I'm struggling to figure out the point of this project. From what I can tell, this project basically pairs up a cheap CPU with their version of a GPU. There's nothing really new about this, and realistically one of the larger issues isn't really the hardware, but rather trying to figure out software that parallelizes well. Besides, cloud computing has already made great strides in making supercomputing services far more available than they have ever been in the past. The point is to put computing power in the hands of people to experiment. It's about exploring the unknown in the comfort of your home. It's about porting Manticore or something completely different.  It's about playing with map-reduce. I think it's awesome that you can have 100 core computing cluster in your living room.  Fun fact: You already can with the [GreenArray](http://www.greenarraychips.com/), which is cheaper per-chip.\n\nIt's also a Forth platform, which probably turns people off, but whatever. It's cheaper and it's already here. The point is to put computing power in the hands of people to experiment. It's about exploring the unknown in the comfort of your home. It's about porting Manticore or something completely different.  It's about playing with map-reduce. I think it's awesome that you can have 100 core computing cluster in your living room.  I mean, you can already get a off-the-shelf GPU and play with map-reduce in the 'comfort of your home' for a lot less money and be getting a lot more power. I could see some very niche uses for this thing, but it'd still be pretty slow compared to your cloud computing options.  I mean, you can already get a off-the-shelf GPU and play with map-reduce in the 'comfort of your home' for a lot less money and be getting a lot more power. I could see some very niche uses for this thing, but it'd still be pretty slow compared to your cloud computing options.   Gotta love their justification for calling this a \"supercomputer\":\n\n&gt; The current $99 board aren't considered supercomputers by 2012 standards, but a cluster of 10 Parallella boards would have been considered a supercomputer 10 years ago.\n\nAnd my desktop would have been considered a supercomputer ten years before that.  You can't really call something a supercomputer because it would have been considered a supercomputer at some point in history.\n\nThat aside, it looks like an interesting project even though I don't really see what it offers over GPGPU- and FPGA- based solutions.   I think my 8 core computer that I paid $400 is pretty cheap and pretty super, but that doesn't mean I am always able to write code that takes advantage of more than one core. I foresee the same thing here. \n Where did you buy it from?",
    "url" : "http://www.kickstarter.com/projects/adapteva/parallella-a-supercomputer-for-everyone"
  }, {
    "id" : 26,
    "title" : "Testing Scientific Programs",
    "url" : "http://www.computer.org/portal/web/computingnow/content?g=53319&amp;type=article&amp;urlTitle=testing-scientific-programs"
  }, {
    "id" : 27,
    "title" : "The Art of the Propagator",
    "snippet" : "  See also Alexey Radul's phd thesis: http://web.mit.edu/~axch/www/phd-thesis.pdf",
    "url" : "http://dspace.mit.edu/handle/1721.1/44215"
  }, {
    "id" : 28,
    "title" : "People who have a BS in Computer Science: how much of the degree plan involves actual programming?",
    "snippet" : "I am a sophomore currently, and I feel that after 2 years of computer science in high school, and taking only one class that deals with programming so far, I am so far inadequately trained in actual programming. Don't get me wrong, I am excited to learn about computer theory, but when it comes to making actual programs with languages that involve things that would solve real world applications, rather than small little programs involving single algorithms and such, or also learning a good amount of languages to get me prepared for life outside college, I am worried not enough focus will go into it. I realize I should simply spend more time on my own time programming to get better, but it would be nice to see that my degree puts more effort into helping me learn some programming languages very well to train me for after college.\n\nEDIT: Thank you all very much for the responses! A lot of very good information here that I will have to pass on to fellow students that wonder the same as well.  ....And you aren't writing your own projects? A university isn't a trade school, and a CS degree isn't software engineering.\n\nEither get an internship, or at least start a project on your own! Think of programming as being like art. You have different tools and different media you can work in. There are different techniques you can apply, but you need to learn when and how to apply them, and you need to practice, practice, practice.\n\nBut seriously, if someone told you to expect a CS degree to teach you how to program well, someone told you wrong. I had classmates that couldn't program their ways out of paper bags. Myself, I chose some programming heavy electives: a couple courses on C++, a computer networking class with a lot of programming projects, and some hands-on AI and robotics courses with plenty of projects. In all of those cases, I could've gone for more theoretical classes. Yes, this. It's a multi-faceted problem, really. \n\nOne problem is with the students, both current and prospective. They do not realize the difference between computer science and programming. Programming is a very important part of computer science, but computer science is much more than programming and software engineering. Any CS programme will involve programming, and at a good university they'll also make sure you have at least basic familiarity with some modern languages and technologies, but a CS programme will not consist mostly of programming. And the programming that will be there will mostly be to implement algorithms and understand theory, not to solve real-world problems. Students fail to understand that a university provides academic training with a lot of theory, and that vocational training is instead provided at trade schools, shorter workshops, etc.\n\nThe other problem is with society, which generally demands university degrees. Trade schools are looked down upon, and that is bad. Most programmers do not need a CS degree. They need to know how to use a specific language and technology, and they need a basic understanding of key algorithms and data structures. This is the sort of thing best learned in a trade school, but an employer will always prefer a candidate with a university degree. Even though a CS degree says nothing about the graduate's programming skills as they matter in the real world.\n\nIf a CS programme at some university consists mostly of programming, then it's a crap university that is actually a glorified trade school and fails to teach the theory of computer science. You can hardly call yourself a computer scientists if you've never studied formal languages, compiler construction, Turing machines or computational complexity. Also, good universities will manage to make the programming assignments still teach you real-world skills, while bad universities will not. At a good university, you'll be using a language that is used in reality (Java, C++, Python, something else), you'll do at least one larger project where you cooperate with people to make a more complicated piece of software, and you'll encounter some technologies that are used in the real world. A MySQL server, parallel computation with CUDA, something. At a bad university, you'll be stuck programming in Turbo Pascal 5.5, using an ancient environment that's hard to even run on a modern computer, and implementing your algorithms in a language that doesn't see serious use in the industry. Why? Because you'll have a professor that began teaching in Pascal back when it was the hot new thing, and hasn't learned a single thing in the last 20 years.\n\nIf you're concerned about becoming better at programming, there's only one thing to do that - do more programming. Join an existing project, or start your own. Make a game. Write a program where you can edit BMP files and save them in JPEG, with your own encoder (will put your theory to a test). Program something else. You don't even necessarily have to complete the project, but you have to work on it and to write code to get better. Of course, any completed projects or useful involvement with projects by others will be a significant boost to your CV. Yes, this. It's a multi-faceted problem, really. \n\nOne problem is with the students, both current and prospective. They do not realize the difference between computer science and programming. Programming is a very important part of computer science, but computer science is much more than programming and software engineering. Any CS programme will involve programming, and at a good university they'll also make sure you have at least basic familiarity with some modern languages and technologies, but a CS programme will not consist mostly of programming. And the programming that will be there will mostly be to implement algorithms and understand theory, not to solve real-world problems. Students fail to understand that a university provides academic training with a lot of theory, and that vocational training is instead provided at trade schools, shorter workshops, etc.\n\nThe other problem is with society, which generally demands university degrees. Trade schools are looked down upon, and that is bad. Most programmers do not need a CS degree. They need to know how to use a specific language and technology, and they need a basic understanding of key algorithms and data structures. This is the sort of thing best learned in a trade school, but an employer will always prefer a candidate with a university degree. Even though a CS degree says nothing about the graduate's programming skills as they matter in the real world.\n\nIf a CS programme at some university consists mostly of programming, then it's a crap university that is actually a glorified trade school and fails to teach the theory of computer science. You can hardly call yourself a computer scientists if you've never studied formal languages, compiler construction, Turing machines or computational complexity. Also, good universities will manage to make the programming assignments still teach you real-world skills, while bad universities will not. At a good university, you'll be using a language that is used in reality (Java, C++, Python, something else), you'll do at least one larger project where you cooperate with people to make a more complicated piece of software, and you'll encounter some technologies that are used in the real world. A MySQL server, parallel computation with CUDA, something. At a bad university, you'll be stuck programming in Turbo Pascal 5.5, using an ancient environment that's hard to even run on a modern computer, and implementing your algorithms in a language that doesn't see serious use in the industry. Why? Because you'll have a professor that began teaching in Pascal back when it was the hot new thing, and hasn't learned a single thing in the last 20 years.\n\nIf you're concerned about becoming better at programming, there's only one thing to do that - do more programming. Join an existing project, or start your own. Make a game. Write a program where you can edit BMP files and save them in JPEG, with your own encoder (will put your theory to a test). Program something else. You don't even necessarily have to complete the project, but you have to work on it and to write code to get better. Of course, any completed projects or useful involvement with projects by others will be a significant boost to your CV. Yes, this. It's a multi-faceted problem, really. \n\nOne problem is with the students, both current and prospective. They do not realize the difference between computer science and programming. Programming is a very important part of computer science, but computer science is much more than programming and software engineering. Any CS programme will involve programming, and at a good university they'll also make sure you have at least basic familiarity with some modern languages and technologies, but a CS programme will not consist mostly of programming. And the programming that will be there will mostly be to implement algorithms and understand theory, not to solve real-world problems. Students fail to understand that a university provides academic training with a lot of theory, and that vocational training is instead provided at trade schools, shorter workshops, etc.\n\nThe other problem is with society, which generally demands university degrees. Trade schools are looked down upon, and that is bad. Most programmers do not need a CS degree. They need to know how to use a specific language and technology, and they need a basic understanding of key algorithms and data structures. This is the sort of thing best learned in a trade school, but an employer will always prefer a candidate with a university degree. Even though a CS degree says nothing about the graduate's programming skills as they matter in the real world.\n\nIf a CS programme at some university consists mostly of programming, then it's a crap university that is actually a glorified trade school and fails to teach the theory of computer science. You can hardly call yourself a computer scientists if you've never studied formal languages, compiler construction, Turing machines or computational complexity. Also, good universities will manage to make the programming assignments still teach you real-world skills, while bad universities will not. At a good university, you'll be using a language that is used in reality (Java, C++, Python, something else), you'll do at least one larger project where you cooperate with people to make a more complicated piece of software, and you'll encounter some technologies that are used in the real world. A MySQL server, parallel computation with CUDA, something. At a bad university, you'll be stuck programming in Turbo Pascal 5.5, using an ancient environment that's hard to even run on a modern computer, and implementing your algorithms in a language that doesn't see serious use in the industry. Why? Because you'll have a professor that began teaching in Pascal back when it was the hot new thing, and hasn't learned a single thing in the last 20 years.\n\nIf you're concerned about becoming better at programming, there's only one thing to do that - do more programming. Join an existing project, or start your own. Make a game. Write a program where you can edit BMP files and save them in JPEG, with your own encoder (will put your theory to a test). Program something else. You don't even necessarily have to complete the project, but you have to work on it and to write code to get better. Of course, any completed projects or useful involvement with projects by others will be a significant boost to your CV. Now my question is if a University begins to focus on the programming, wouldn't work places come to favor that Universities degrees because of knowledge that the candidates bring. Then wouldn't that University be seen as having a \"great\" CS program?  ....And you aren't writing your own projects? A university isn't a trade school, and a CS degree isn't software engineering.\n\nEither get an internship, or at least start a project on your own! Think of programming as being like art. You have different tools and different media you can work in. There are different techniques you can apply, but you need to learn when and how to apply them, and you need to practice, practice, practice.\n\nBut seriously, if someone told you to expect a CS degree to teach you how to program well, someone told you wrong. I had classmates that couldn't program their ways out of paper bags. Myself, I chose some programming heavy electives: a couple courses on C++, a computer networking class with a lot of programming projects, and some hands-on AI and robotics courses with plenty of projects. In all of those cases, I could've gone for more theoretical classes. ....And you aren't writing your own projects? A university isn't a trade school, and a CS degree isn't software engineering.\n\nEither get an internship, or at least start a project on your own! Think of programming as being like art. You have different tools and different media you can work in. There are different techniques you can apply, but you need to learn when and how to apply them, and you need to practice, practice, practice.\n\nBut seriously, if someone told you to expect a CS degree to teach you how to program well, someone told you wrong. I had classmates that couldn't program their ways out of paper bags. Myself, I chose some programming heavy electives: a couple courses on C++, a computer networking class with a lot of programming projects, and some hands-on AI and robotics courses with plenty of projects. In all of those cases, I could've gone for more theoretical classes. ....And you aren't writing your own projects? A university isn't a trade school, and a CS degree isn't software engineering.\n\nEither get an internship, or at least start a project on your own! Think of programming as being like art. You have different tools and different media you can work in. There are different techniques you can apply, but you need to learn when and how to apply them, and you need to practice, practice, practice.\n\nBut seriously, if someone told you to expect a CS degree to teach you how to program well, someone told you wrong. I had classmates that couldn't program their ways out of paper bags. Myself, I chose some programming heavy electives: a couple courses on C++, a computer networking class with a lot of programming projects, and some hands-on AI and robotics courses with plenty of projects. In all of those cases, I could've gone for more theoretical classes. ....And you aren't writing your own projects? A university isn't a trade school, and a CS degree isn't software engineering.\n\nEither get an internship, or at least start a project on your own! Think of programming as being like art. You have different tools and different media you can work in. There are different techniques you can apply, but you need to learn when and how to apply them, and you need to practice, practice, practice.\n\nBut seriously, if someone told you to expect a CS degree to teach you how to program well, someone told you wrong. I had classmates that couldn't program their ways out of paper bags. Myself, I chose some programming heavy electives: a couple courses on C++, a computer networking class with a lot of programming projects, and some hands-on AI and robotics courses with plenty of projects. In all of those cases, I could've gone for more theoretical classes. Is an engineering program in a university a \"trade school\"? University is no longer just a place to become well-rounded, it's intended for some kind of job training. While CS is not totally about Software Engineering, it is par for the course. Not everyone studying CS as an undergrad can be a professor of CS so they *must* get some software engineering skills in school. To not teach the basics of that is irresponsible IMHO, especially in this age where people get degrees specifically to get out of poverty. I agree that it's reasonable for at least some universities to offer some degree of job training (mine did, partially based on the electives that I chose). I specifically chose a school known for hands-on projects. It sounds like OP chose a school with a much more academic program. IMO, that's also a valid path for a university to focus on, and OP should've done their research on the curriculum before choosing that uni.\n\nThere need to be schools that still focus on being \"just a place to become well-rounded\", and it's the prospective student's responsibility to find out what kind of school they're trying to attend before they start. I guess I should have said *just* well-rounded. I'm down for a well-rounded education and I think it's what most people want anyway, but university education is not purely recreational for most people. Some do get sold on the idea that they can do whatever they want in university but they are often disappointed if they do study drama or whatever. I'd like to see multiple degree paths (emphases,  whatever). Academic - focused, software engineering - focused,  etc. I think it would be a good way to structure things.  It sounds good in theory but people usually don't know what they like until they try, and again, not everyone can become a professor. That's why a balance is the best option. So, 3 paths. 2 for the people that are positive which way they want to go, and one more-balanced \"compromise\" path, so if someone can't make the jump into academia, they've got a good chance at going for a software engineering position in the private sector.  I wish my university focused more on theory and less on programming which to be fair, I could have picked up on my own (theory on the other hand I would have to know about to learn about which I don't).\n\nAlso, I am sure employers love when students have been taught all that design patterns, software development methodologies and so on dullness, but - can they pay for it if they want me to learn that? Does it really have to be a _core_ module? It has exactly nothing to do with computers or computation - it's sheer business processes. This.\n\nI graduated with a degree in Software engineering. My school focused on teaching us \"business\" programming (Java/.Net flavoured), design patterns, project management and all sorts of database-y things. \n\nWe totally overlooked proper teaching about algorithms, data structures, Operating systems or programming languages. (In all fairness, we did tackle these subjects, but way too lightly for my taste). After all, isn't classical OOP *a la* C++ the only valid programming paradigm? /sarcasm\n\nAs a result, I have the ideal resume, with shiny keywords. It took me very little effort to find a mindless job with highly repetitive dull tasks and virtually no challenges at all. Interesting positions are rare, but require skills I feel I've been cheated out of.\n\nI spent the past 2 years self-studying at nights after work (the recent availability of amazing course materials online is a bliss), learning all these subjects by myself. It helps me remember why I got into programming in the first place.\n\nThe way I look at it, it's foolish to expect that your school will teach you everything. Inevitably, you'll have to learn a lot on your own. I would prefer if I were taught theory with qualified professors and practiced programming on my own, rather than have it the other way around.\n\n**TL;DR:** University is the best time you have to study these theoretical/abstract topics. Programming will come naturally, don't worry. This.\n\nI graduated with a degree in Software engineering. My school focused on teaching us \"business\" programming (Java/.Net flavoured), design patterns, project management and all sorts of database-y things. \n\nWe totally overlooked proper teaching about algorithms, data structures, Operating systems or programming languages. (In all fairness, we did tackle these subjects, but way too lightly for my taste). After all, isn't classical OOP *a la* C++ the only valid programming paradigm? /sarcasm\n\nAs a result, I have the ideal resume, with shiny keywords. It took me very little effort to find a mindless job with highly repetitive dull tasks and virtually no challenges at all. Interesting positions are rare, but require skills I feel I've been cheated out of.\n\nI spent the past 2 years self-studying at nights after work (the recent availability of amazing course materials online is a bliss), learning all these subjects by myself. It helps me remember why I got into programming in the first place.\n\nThe way I look at it, it's foolish to expect that your school will teach you everything. Inevitably, you'll have to learn a lot on your own. I would prefer if I were taught theory with qualified professors and practiced programming on my own, rather than have it the other way around.\n\n**TL;DR:** University is the best time you have to study these theoretical/abstract topics. Programming will come naturally, don't worry. This.\n\nI graduated with a degree in Software engineering. My school focused on teaching us \"business\" programming (Java/.Net flavoured), design patterns, project management and all sorts of database-y things. \n\nWe totally overlooked proper teaching about algorithms, data structures, Operating systems or programming languages. (In all fairness, we did tackle these subjects, but way too lightly for my taste). After all, isn't classical OOP *a la* C++ the only valid programming paradigm? /sarcasm\n\nAs a result, I have the ideal resume, with shiny keywords. It took me very little effort to find a mindless job with highly repetitive dull tasks and virtually no challenges at all. Interesting positions are rare, but require skills I feel I've been cheated out of.\n\nI spent the past 2 years self-studying at nights after work (the recent availability of amazing course materials online is a bliss), learning all these subjects by myself. It helps me remember why I got into programming in the first place.\n\nThe way I look at it, it's foolish to expect that your school will teach you everything. Inevitably, you'll have to learn a lot on your own. I would prefer if I were taught theory with qualified professors and practiced programming on my own, rather than have it the other way around.\n\n**TL;DR:** University is the best time you have to study these theoretical/abstract topics. Programming will come naturally, don't worry. *It took me very little effort to find a mindless job with highly repetitive dull tasks and virtually no challenges at all. Interesting positions are rare, but require skills I feel I've been cheated out of.*\n\nSeems like you are missing one big challenge- eliminating those highly repetitive tasks. Maybe make these changes configuration options, think of a more flexible data model (IE store key/value pairs instead of adding a database field or new table for each new piece of data), etc. \n\n I wish my university focused more on theory and less on programming which to be fair, I could have picked up on my own (theory on the other hand I would have to know about to learn about which I don't).\n\nAlso, I am sure employers love when students have been taught all that design patterns, software development methodologies and so on dullness, but - can they pay for it if they want me to learn that? Does it really have to be a _core_ module? It has exactly nothing to do with computers or computation - it's sheer business processes. I dunno about you not considering patterns and development methodologies non-core.\n\nThe truth is that no matter how good of a programmer you are, you will never reach your full potential without a good understanding of both.\n\nI also dropped out of school because fuck it, it's bullshit. But in the industry these things are not bullshit, you need to know more than how to code if you want to be a effective computer engineer.|\n\nYou can make plenty of money if you just code your heart out until you prove yourself. Anything you build is worth 10x what that paper is worth. I make 6 figures and did it with minimal post-secondary schooling. Dropped out after 2 years of thinking it was bullshit. I don't care about \"the industry\" and i don't want to be a programmer necessarily (I am not saying I won't look into it, just that I am not treating it as a default). Call me a freak but I would actually like it if they taught us more math and logic. I wish my university focused more on theory and less on programming which to be fair, I could have picked up on my own (theory on the other hand I would have to know about to learn about which I don't).\n\nAlso, I am sure employers love when students have been taught all that design patterns, software development methodologies and so on dullness, but - can they pay for it if they want me to learn that? Does it really have to be a _core_ module? It has exactly nothing to do with computers or computation - it's sheer business processes. &gt; I am sure employers love when students have been taught all that design patterns, software development methodologies and so on dullness\n\nNope, it's the sign of a bad school, and usually a bad programmer who can only think at the surface level. I have to weed out a ton of Java pattern/framework gurus who can't write a recursive function to traverse a binary tree. &gt; I am sure employers love when students have been taught all that design patterns, software development methodologies and so on dullness\n\nNope, it's the sign of a bad school, and usually a bad programmer who can only think at the surface level. I have to weed out a ton of Java pattern/framework gurus who can't write a recursive function to traverse a binary tree. Call me crazy but isn't recursive tree functions part of Java development courses? I'm sort of confused how that could be left out of the curriculum. Is everything just done in while/for loops?   The big lie of computer science departments is getting you to think they train programmers. They train computer scientists. Computer scientists do a lot of programming but they also do a lot of abstract mathematics and writing. If you just want to be a programmer, you don't need a degree, just experience. If you really want a degree in programming, find a software engineering degree like at UVic. It's not so much a \"big lie\" as a distinction that most people don't realize. Some schools have Computer Science and Engineering programs, some schools have Computer Science programs, some have Computer Engineering programs, some have Software Engineering programs. All of those are completely different and the difference isn't explained very often.\n\nPersonally, I'm happy with my CS degree. As you say, it's a lot easier to learn to program than it is to learn to do the math behind it and the writing necessary to formalize and communicate that math. They rely on that being a distinction most people don't realize, because parents encourage their kids to take computer science thinking it will lead to their kids getting well paying programming jobs.\n\nIf you're in a 4 year CS BSc program, you can quit after two years and be a good programmer with a good career.\n\nI quit after 1.5 years, and worked for 10 years, and had a great career to that point. But at that point, I had the feeling that I was an expert at using an operating system, a web server, a programming language, but I didn't have the background to become an expert at creating an operating system, a web server or a programming language. So at 29 years old I returned and finished my degree. I'm glad I did it the way I did it. I got the necessary background to be a programmer, which is what I wanted at the time, and now I have the background necessary to do much more, and when I learned that background, I actually had an appreciation for why that background was important! Could you give me some advice? I'm 24 year old programmer (part time) working on a Comp Sci degree. Right now I'm doing ASP.NET work, but my dream is to get into performance critical software, scientific/complex system modeling, distributed processing, or disassmebly/reverse engineering (I've had some success with toy projects, but can't de-obfuscate to save my life). I'd also like to get some experience with driver writing, or just getting closer to the hardware, as well as GPU oriented programming. Operating system programming might be nice to know, but I can't envision how I'd utilize that knowledge at the moment. (All over the place much?) \n\nWhat should I focus on to get me to the next level?\n It's not so much a \"big lie\" as a distinction that most people don't realize. Some schools have Computer Science and Engineering programs, some schools have Computer Science programs, some have Computer Engineering programs, some have Software Engineering programs. All of those are completely different and the difference isn't explained very often.\n\nPersonally, I'm happy with my CS degree. As you say, it's a lot easier to learn to program than it is to learn to do the math behind it and the writing necessary to formalize and communicate that math. The big lie of computer science departments is getting you to think they train programmers. They train computer scientists. Computer scientists do a lot of programming but they also do a lot of abstract mathematics and writing. If you just want to be a programmer, you don't need a degree, just experience. If you really want a degree in programming, find a software engineering degree like at UVic. The big lie of computer science departments is getting you to think they train programmers. They train computer scientists. Computer scientists do a lot of programming but they also do a lot of abstract mathematics and writing. If you just want to be a programmer, you don't need a degree, just experience. If you really want a degree in programming, find a software engineering degree like at UVic. I have just learned that my third year is going to using java almost exclusively. My heart sank a little.\n\nI want mostly theory and some practical, but focus on a single language I do not feel to be the right way. The big lie of computer science departments is getting you to think they train programmers. They train computer scientists. Computer scientists do a lot of programming but they also do a lot of abstract mathematics and writing. If you just want to be a programmer, you don't need a degree, just experience. If you really want a degree in programming, find a software engineering degree like at UVic. &gt; The big lie of computer science departments is getting you to think they train programmers.\n\nExcept that's not really a lie. I went into my CS program not knowing how to program, and left knowing how to program. And, considering I didn't have any programming experience, I suspect the merit of my CS degree was also the main reason I was able to get a job doing software engineering. They teach you the basics of programming in 1st/2nd year so you can implement the algorithms they teach you in 2nd/3rd year and so you can do the projects in 4th year, but becoming a programmer is a side-effect of getting a CS degree, it's not the goal. Just like being really good at working with fractions is a side-effect of being a carpenter. It's not about making you a mathematician, but if you can't add and subtract fractions, your work is going to suck.   I transferred to a pretty good school from a community college for a major involving CS.  I'll be straight with you though.  I programmed a lot more in community college.  Though I hear a lot of hackathon events going on around campus so maybe that's something you'd like to get involved with. how was the transition from community college to university in terms of work load? It actually wasn't so bad I thought. I had already gotten used to spending a lot of time studying a lot. Physics mostly. (Took up to modern physics) so when it got to studying things like networking I think of it as studying laws of the universe vs rules some people made up relatively recently. In terms of programming projects it wasn't a big ramp up because my community college taught us how to use C and I had been teaching it as well for about the two years I was there. When I got to university it was surprising that a lot of people didn't have as much experience with programming as me because classes at university are more theoretical, like they want you know more as opposed to practical where they want you to know how to do stuff. You can see this in projects since the university ones are of much larger scale but the skeleton is already provided compared in community college where we had to start from scratch. My university doesn't use as much C too. Anyway the point is I feel like I'm ahead of a lot of people in raw programming experience so projects aren't too bad. It's just that now I'm taking 3 or 4 CS classes a semester lots of projects stack up. \n\nI think my community college prepped me good. how was the transition from community college to university in terms of work load?         Wait, so cypherx, this kind of post *is* relevant? Nope, I just hadn't looked at reddit yet :-) \n\nThat said, the OP is young, the answers given him are pretty good, and the frontpage is otherwise mostly about CS--- so maybe I shouldn't delete it? What do you think?  Nope, I just hadn't looked at reddit yet :-) \n\nThat said, the OP is young, the answers given him are pretty good, and the frontpage is otherwise mostly about CS--- so maybe I shouldn't delete it? What do you think?  I personally am fine with this post, but when this is the kind of stuff that makes the front page, it might not be a bad idea to reevaluate the rules for this subreddit. Well, pretty much anything posted here \"makes the front page\" since we're pretty low volume. But yeah, the crowd has gotten  more receptive to open-ended beginner questions like \"what class should I take\", \"how much programming is in CS\", etc... Or atleast I assume so since this stuff gets upvoted a lot. I don't think we're at the tipping point yet, but when the majority actually wants these kinds of questions to dominate the subreddit I can step down and someone else can moderate/change the rules.      Nearly all of my classes involved programming, but none of them taught it. Towards the end of my degree I picked some heavy programming electives such as Computer Vision, Autonomous Multi-Agent Systems(robotics), OOP, and Software Engineering.\n\nAbsolute best decision I could have made, I basically had the perfect skillset for the job I was looking for(high frequency trading). In depth knowledge of theory, confidence in several languages(C, C++, and Java), and completed projects that I could use as code samples. \n\nI had exactly one interview before I graduated and was working fulltime the day after my last final exam.  RGM?  If I may ask, do you get to design or implement the trading algorithms? What kind of CS theory is involved? No, we're a tiny shop(4 employees). We generally are split, working in teams of 2. One team is working on new algos, while the other is coding. I've done a bit of both design and implementation, but mostly implementation. \n\nAs far as theory goes, I've used knowledge from several classes.\n\n*  Programming Languages - grammars and parsers\n* Algorithms - Big-O and algorithm runtime optimizations\n* Computer Architecture/OS - Micro optimizations for our realtime systems(microseconds really matter)\n* Autonomous Multi-agent Systems - Game theory, auction theory, artificial intelligence\n\nObviously we do a LOT of math as well, I wish I had taken a statistics class, but I did take an advanced Linear Algebra class designed for scientific programming which taught me Matlab, which has proven to be very useful. Sounds cool. That linear algebra class wasn't SSC 329c was it? I'm looking for another math class and more linear algebra would certainly be useful. It was, except I had the pleasure of having Robert Van De Geijn as my instructor. The man is simply brilliant and was really great at getting you to understand the how's and why's the linear algebra was working. The class was not about solving problem after problem by hand, it was about the theory of linear algebra but taught through practical uses in Matlab.      I'm taking a first semester CS lower division course (of a series of three) and we have programming assignments, labs, and discussion sections every week along with four projects throughout the semester. We did go into \"theory\" stuff like higher order functions and implementation of objects using functions, but at the same time there is an emphasis on writing good, re-usable code. It's still not enough. Thankfully, the clubs host hackathons, workshops, and coding challenges to supplement our classroom experience with practical knowledge. Don't assume too much from your first semesters, I think some programs give you programming classes first so you can understand what a computer *is* before you try to understand on a theoretical level.                                           As a fourth year CSE major I can confirm I do a lot of substantial programming.  It's hard for me to really see someone call themselves a \"computer scientist\" if they lack the ability to program.  Otherwise they're just someone who can't prove they know how to implement an algorithm they read about on wikipedia or something.  [deleted]    ",
    "url" : "http://www.reddit.com/r/compsci/comments/11k2gd/people_who_have_a_bs_in_computer_science_how_much/"
  }, {
    "id" : 29,
    "title" : "Question on Turing machine homework.",
    "snippet" : "I have the following homework question: Explain why the following is not a legal Turing machine:\nM= \"The input is a polynomial over variables x1, x2, ..., xk.\n1. Try all possible settings of x1,...,xk to integer values.\n2. Evaluate p on all of these settings.\n3. If any of these settings evaluate to 0, accept. Otherwise, reject.\"\n\nMy answer is: The above machine needs all possible integers to be provided in the input to perform the computation. Because these have not been supplied, this is not a valid Turing machine.\n\nIs this correct?  The integers don't have to be given as input. You can generate them. The problem here is different: Finding a integer solution for a polynomial equation of arbitrary degree is turing-indecidable (i.e. no turing machine can be built that can decide if an arbitrary polynomial equation has a solution over the integer values).\n\nIn particular the given procedure will not know when to stop and reject. It is guaranteed to terminate if it there is a solution, but if no solution exists this procedure will run forever (and the turing machine never halts.)\n\nSee [Diophantine equation](http://en.wikipedia.org/wiki/Diophantine_equation) and [Hilbert's 10th problem](http://en.wikipedia.org/wiki/Hilbert%27s_tenth_problem) It is not decidable, but why the Turing machine is invalid? Is there a requirement that a Turing machine always runs to completion? A turing maching may or may not halt, thats fine. Here the turing maching is not \"valid\" because it does not exist (no turing machine can be build with that specification) In what sense? I can certainly write a python program to do this, no? Nope :)\n\nYou can write a program for this specification:\n\n\"The input is a polynomial over variables x1, x2, ..., xk. 1. Try all possible settings of x1,...,xk to integer values. 2. Evaluate p on all of these settings. 3. If any of these settings evaluate to 0, accept. ~~Otherwise, reject.~~\"\n\nThe problem is that your program can never know if there is indeed no solution or if it simply didn't reach a solution *yet*. You never know when to stop searching for a solution if there isn't one. True, and I thought therefore it is not decidable, but I can still make a Turing machine that does that even if it never completes.\n\nOr is the fact that it can never reject but specified so the reason why the specification is never achievable? This problem builds upon basic automata (DFA and NFA) theory as well as basic language theory.  It is important to know exactly how these things are described and how they relate.  In one sense, if a problem is undecidable, the TM calculating it cannot provide an answer and is consequently not valid.  It does not exist within the language space of grammars and TMs and so on that describe questions that can be answered in polynomial time.  Soon, OP will be building Turing machines consisting of Turing machines and showing that some bigger problems cannot be answered in polynomial time because some some part of it is an invalid TM.\n\ntl;dr - computer science should be called computability science. I remember the hard part remaining now is complexity. Still stuff to do in research. This problem builds upon basic automata (DFA and NFA) theory as well as basic language theory.  It is important to know exactly how these things are described and how they relate.  In one sense, if a problem is undecidable, the TM calculating it cannot provide an answer and is consequently not valid.  It does not exist within the language space of grammars and TMs and so on that describe questions that can be answered in polynomial time.  Soon, OP will be building Turing machines consisting of Turing machines and showing that some bigger problems cannot be answered in polynomial time because some some part of it is an invalid TM.\n\ntl;dr - computer science should be called computability science. In what sense? I can certainly write a python program to do this, no? No.  You can write a python program that will accept (return True or whatever) if the polynomial ever evaluates to 0, but how will you write the condition to reject / return False? I see what you mean. Damn I am much dumber than I thought... It is not decidable, but why the Turing machine is invalid? Is there a requirement that a Turing machine always runs to completion? \"Otherwise, reject.\" What does this mean? How would it be translated to code? This is the problem.  In the case that there is no acceptable answer, it will keep running forever and never produce an output.  Thus, in some cases, it can't reject. It's worse than that. Not only can it never reject, it's not even clear that it can be translated into sensible code. If I had to do it, I'd write an infinite loop with a reject line after the loop, but that's only one possible interpretation of this bad use of the high-level, informal construct \"Otherwise\". Whether or not it can be translated into sensible code is completely irrelevant to the matter.  It is about what can be expressed with a particular type of grammar that can be modeled by finite automata.  Replace my word \"sensible\" with \"corresponding\", then. My argument isn't that the code wouldn't look pretty, it's that the use of \"Otherwise\" is not an effectively computable step, and therefore cannot be translated into a body of code in an obvious way. By the Church-Turing thesis, if it can't be expressed as a program, it can't be a Turing machine.\n\nThe use of \"Otherwise\" does not necessarily imply that the steps are not effectively computable, but the burden would be on the algorithm designer to show how to implement it using a finite number of operations.\n\nThis is at the heart of the OP's problem. If \"Otherwise\" were a valid construct to use here, then a machine satisfying that high-level description would exist; nevermind whether it halts.\n The use of \"otherwise\" is indicative of an if-else relationship.  But as you noted, if you can't get through the \"if\" part in a finite number of steps, you'll never reach the \"else\" so the expression as a whole is not computable.\n\nAnd it is possible to construct a rudimentary version of the problem in code.  Instead of x1,...,xk, lets just use a single x.\n\n    for(x=1; ; x++)  {\n      if( f(x)==something) return true;\n    }\n    return false;\n\nAnd the for loop never breaks.  You could also set the middle part of the for to x&lt;LIMIT and show that since the problem space is over all integers, any value that LIMIT can be set to will limit your problem space and you will no longer have an equivalent \"machine.\"\n\nThe Church-Turing hypothesis isn't really the core of his problem, but it can be useful as another way to look at the problem.  The Church-Turing hypothesis basically says that since Turing machines and computers both operate on the type-0 (or recursively enumerable) language set, that the same problems should be solvable by both.  This is important to to learn, but for OPs problem it just shifts the problem into a different language.  The solution is still essentially the same either way. You gave the interpretation for the \"otherwise\" that I reluctantly suggested above. Under that interpretation, the Turing machine that the OP described *does* exist, contrary to the problem statement which asked why such a machine can't exist. My reasoning is that for the machine to not exist, its high-level description cannot be translatable to code.\n\nIndeed, if we look at your code, we can see that it never rejects; and yet there are cases where the purported Turing machine does reject, taking its description at its word. So the two entities described are not equivalent (and the latter doesn't even exist). The integers don't have to be given as input. You can generate them. The problem here is different: Finding a integer solution for a polynomial equation of arbitrary degree is turing-indecidable (i.e. no turing machine can be built that can decide if an arbitrary polynomial equation has a solution over the integer values).\n\nIn particular the given procedure will not know when to stop and reject. It is guaranteed to terminate if it there is a solution, but if no solution exists this procedure will run forever (and the turing machine never halts.)\n\nSee [Diophantine equation](http://en.wikipedia.org/wiki/Diophantine_equation) and [Hilbert's 10th problem](http://en.wikipedia.org/wiki/Hilbert%27s_tenth_problem) The integers don't have to be given as input. You can generate them. The problem here is different: Finding a integer solution for a polynomial equation of arbitrary degree is turing-indecidable (i.e. no turing machine can be built that can decide if an arbitrary polynomial equation has a solution over the integer values).\n\nIn particular the given procedure will not know when to stop and reject. It is guaranteed to terminate if it there is a solution, but if no solution exists this procedure will run forever (and the turing machine never halts.)\n\nSee [Diophantine equation](http://en.wikipedia.org/wiki/Diophantine_equation) and [Hilbert's 10th problem](http://en.wikipedia.org/wiki/Hilbert%27s_tenth_problem)   Are you a real person?",
    "url" : "http://www.reddit.com/r/compsci/comments/11lb0x/question_on_turing_machine_homework/"
  }, {
    "id" : 30,
    "title" : "Efficient algorithms for finding the square of a matrix that is its own transpose.",
    "snippet" : "Not sure if this is the right subreddit. it's a combination of maths, and programming so I guess it fits.  \nI need to square the adjacency matrix of a undirected graph as one of the steps for calculating the number of cycles of a certain length in the graph.  \nI know the naive method for calculating the product of 2 matrices which is O( n^3 )  \nI also know there are other methods for doing it that bring the exponent down a bit more.  \nAre there any methods of speeding this up given that:  \nI will be working with adjacency matrices of up to 6000x6000 so the big-O of the algorithm is probably going to be significant than scalar (is that the right term?) multipliers.  \nI will be working with undirected graph adjacency matrices so the matrix will be the same as its transpose.  \ni.e. A = A^t  \nAlso would since it is an adjacency matrix I will be dealing with 1s and 0s so instead of multiplying and adding it to a count would it be faster to increment a count iff both values are 1?  this would allow me to also to represent the initial matrix as with an array of booleans instead of ints so even faster because of that?\n  The naive method of calculating B = A^2 is:\n\nB\\_{ij} = \\sum\\_{k} A\\_{ik} A\\_{kj}\n\nSince A\\_{ab} == A\\_{ba} (as stated above) then:\n\nB\\_{ij} = \\sum\\_{k} A\\_{ki} A\\_{kj}\n\nor\n\nB\\_{ij} = a_i \\cdot a_j\n\nwhere a_k is the k-th column of A.\n\nIf A is a matrix composed of only ones and zeros then:\n\nB\\_{ij} = bitCount(a_i &amp; a_j)\n\nwhere bitCount(n) returns the number of bits set in the number n, &amp; is bitwise AND and we have represented the k-th column of A as a binary number, a_k.\n\nThis is still O(n^3) but suggests some optimisations:\n\n1. B\\_{ij} = B\\_{ji} and so one need only calculate half the output.\n2. Bitwise AND is fast on a CPU and so store the matrix A as a set of machine words per column. Calculate B\\_{ij} by by bitwise anding each corresponding word of the columns a\\_i and a\\_j and summing the bitCount-s.\n3. If the matrix is sparse one might be able to shortcut the bitCount by testing if either column or the bitwise AND is zero.\n4. The value of B\\_{ij} can never be greater than the number of rows in A, so if you have A being 6000 x 6000, a 16-bit integer will suffice.\n\nNone of this has affected the order of the algorithm but it has shown where some optimisation may be found which could reduce the constant of proportionality appropriately.\n\nSome back of envelope calculations:\n\n* Storage of A = 6000 * 6000 / 8 = 274MB. Big but not stupid. If you exploit the symmetry, it will be 137MB.\n* The algorithm above is embarrassingly parallel in B and does not modify A. Since modern GPUs have memory far &gt;137MB, I'd suggest implementing it on one. Hi. I implemented only the compact storage aspect of your post (that is, storing 32 bools as an int). I've hosted it at https://github.com/fedelebron/bit-packing-matrix . It squares a 4096x4096 matrix in about 1 second (sizes need to be multiple of 32 for this toy implementation, variable sizes would require a bit more care, but still be blazingly fast.) Hi. I implemented only the compact storage aspect of your post (that is, storing 32 bools as an int). I've hosted it at https://github.com/fedelebron/bit-packing-matrix . It squares a 4096x4096 matrix in about 1 second (sizes need to be multiple of 32 for this toy implementation, variable sizes would require a bit more care, but still be blazingly fast.) The naive method of calculating B = A^2 is:\n\nB\\_{ij} = \\sum\\_{k} A\\_{ik} A\\_{kj}\n\nSince A\\_{ab} == A\\_{ba} (as stated above) then:\n\nB\\_{ij} = \\sum\\_{k} A\\_{ki} A\\_{kj}\n\nor\n\nB\\_{ij} = a_i \\cdot a_j\n\nwhere a_k is the k-th column of A.\n\nIf A is a matrix composed of only ones and zeros then:\n\nB\\_{ij} = bitCount(a_i &amp; a_j)\n\nwhere bitCount(n) returns the number of bits set in the number n, &amp; is bitwise AND and we have represented the k-th column of A as a binary number, a_k.\n\nThis is still O(n^3) but suggests some optimisations:\n\n1. B\\_{ij} = B\\_{ji} and so one need only calculate half the output.\n2. Bitwise AND is fast on a CPU and so store the matrix A as a set of machine words per column. Calculate B\\_{ij} by by bitwise anding each corresponding word of the columns a\\_i and a\\_j and summing the bitCount-s.\n3. If the matrix is sparse one might be able to shortcut the bitCount by testing if either column or the bitwise AND is zero.\n4. The value of B\\_{ij} can never be greater than the number of rows in A, so if you have A being 6000 x 6000, a 16-bit integer will suffice.\n\nNone of this has affected the order of the algorithm but it has shown where some optimisation may be found which could reduce the constant of proportionality appropriately.\n\nSome back of envelope calculations:\n\n* Storage of A = 6000 * 6000 / 8 = 274MB. Big but not stupid. If you exploit the symmetry, it will be 137MB.\n* The algorithm above is embarrassingly parallel in B and does not modify A. Since modern GPUs have memory far &gt;137MB, I'd suggest implementing it on one. The naive method of calculating B = A^2 is:\n\nB\\_{ij} = \\sum\\_{k} A\\_{ik} A\\_{kj}\n\nSince A\\_{ab} == A\\_{ba} (as stated above) then:\n\nB\\_{ij} = \\sum\\_{k} A\\_{ki} A\\_{kj}\n\nor\n\nB\\_{ij} = a_i \\cdot a_j\n\nwhere a_k is the k-th column of A.\n\nIf A is a matrix composed of only ones and zeros then:\n\nB\\_{ij} = bitCount(a_i &amp; a_j)\n\nwhere bitCount(n) returns the number of bits set in the number n, &amp; is bitwise AND and we have represented the k-th column of A as a binary number, a_k.\n\nThis is still O(n^3) but suggests some optimisations:\n\n1. B\\_{ij} = B\\_{ji} and so one need only calculate half the output.\n2. Bitwise AND is fast on a CPU and so store the matrix A as a set of machine words per column. Calculate B\\_{ij} by by bitwise anding each corresponding word of the columns a\\_i and a\\_j and summing the bitCount-s.\n3. If the matrix is sparse one might be able to shortcut the bitCount by testing if either column or the bitwise AND is zero.\n4. The value of B\\_{ij} can never be greater than the number of rows in A, so if you have A being 6000 x 6000, a 16-bit integer will suffice.\n\nNone of this has affected the order of the algorithm but it has shown where some optimisation may be found which could reduce the constant of proportionality appropriately.\n\nSome back of envelope calculations:\n\n* Storage of A = 6000 * 6000 / 8 = 274MB. Big but not stupid. If you exploit the symmetry, it will be 137MB.\n* The algorithm above is embarrassingly parallel in B and does not modify A. Since modern GPUs have memory far &gt;137MB, I'd suggest implementing it on one. The naive method of calculating B = A^2 is:\n\nB\\_{ij} = \\sum\\_{k} A\\_{ik} A\\_{kj}\n\nSince A\\_{ab} == A\\_{ba} (as stated above) then:\n\nB\\_{ij} = \\sum\\_{k} A\\_{ki} A\\_{kj}\n\nor\n\nB\\_{ij} = a_i \\cdot a_j\n\nwhere a_k is the k-th column of A.\n\nIf A is a matrix composed of only ones and zeros then:\n\nB\\_{ij} = bitCount(a_i &amp; a_j)\n\nwhere bitCount(n) returns the number of bits set in the number n, &amp; is bitwise AND and we have represented the k-th column of A as a binary number, a_k.\n\nThis is still O(n^3) but suggests some optimisations:\n\n1. B\\_{ij} = B\\_{ji} and so one need only calculate half the output.\n2. Bitwise AND is fast on a CPU and so store the matrix A as a set of machine words per column. Calculate B\\_{ij} by by bitwise anding each corresponding word of the columns a\\_i and a\\_j and summing the bitCount-s.\n3. If the matrix is sparse one might be able to shortcut the bitCount by testing if either column or the bitwise AND is zero.\n4. The value of B\\_{ij} can never be greater than the number of rows in A, so if you have A being 6000 x 6000, a 16-bit integer will suffice.\n\nNone of this has affected the order of the algorithm but it has shown where some optimisation may be found which could reduce the constant of proportionality appropriately.\n\nSome back of envelope calculations:\n\n* Storage of A = 6000 * 6000 / 8 = 274MB. Big but not stupid. If you exploit the symmetry, it will be 137MB.\n* The algorithm above is embarrassingly parallel in B and does not modify A. Since modern GPUs have memory far &gt;137MB, I'd suggest implementing it on one.  Strassen's algorithm?",
    "url" : "http://www.reddit.com/r/compsci/comments/11k919/efficient_algorithms_for_finding_the_square_of_a/"
  }, {
    "id" : 31,
    "title" : "The Nobel Prize in Economics given to Roth and Shapley for there work in game theory (e.g. stable matching problem)",
    "snippet" : "  &gt;for *there* work Eye don't sea the problem their. *twitch* dont bee sew mien, im just being humerus &gt;for *there* work &gt;for *there* work  This is awesome news for CS as well: first, a lot of this research is actually hot and happening right now, not just 20-30 years ago; second, a lot of it is being done on the CS and algorithms side, looking for things like online stable matching, the effect of altruistic donors in kidney exchanges, dealing with couples in the med school residency matching program, and probably a lot more!\n\nThis is so close to CS that we could almost share it with Economics in my view (biased).  Why isn't there a nobel prize in computer science yet? Why isn't there a nobel prize in computer science yet? Why isn't there a nobel prize in computer science yet? Why isn't there a nobel prize in computer science yet? &gt;yet\n\nYou don't understand how the Nobel Prizes work. There is precedent; they added Economics in 1969. Why isn't there a nobel prize in computer science yet?",
    "url" : "http://www.nobelprize.org/nobel_prizes/economics/laureates/2012/#"
  }, {
    "id" : 32,
    "title" : "Question: What is best practice for storing very large data sets?",
    "snippet" : "I am writing some custom software for a client that will be gathering a large amount of data.  We are only sampling at 240 Hz/CH which is nice, however we have 400 channels of data, which put us sampling at 96 kHz over a 36 hour period.  For a grand total of 12,441,600,000,000 samples (double precision).  We are trying to figure out the best way to store this data and be able to easily access and review the data in future analysis with MatLab. So far, we have been thinking of:\n\n* csv files that are split every 5 minutes\n* mySQL database\n* TDMS\n\nI'm leaning toward the database, however We'll still have to be very clever when it comes to the db architecture. The only real restriction however, is that the laptop being used will not have any internet access and all data will need to be stored locally.\n\n[EDIT] I punched in the wrong value into my calculator of 96000 kHz instead of 96 kHz. After 36 hours we will have 12,441,600,000 samples.  You plan to fit 90 TB onto a single laptop?\n\nAnd that's just the raw bytes of 12,441,600,000,000  8 byte samples.\n\nThe string encoding needed to store those numbers in a CSV would make it closer to 100-140 TB depending on how precise you want them to be.  It will be far worse with XML (*shudder*).\n\nHave you checked out the [HDF](http://en.wikipedia.org/wiki/Hierarchical_Data_Format) format?  MATLAB [supports it](http://www.mathworks.com/help/matlab/ref/hdf5.html), and it is a format designed specifically for scientific data.  \n\nHDF is accessible from all of your standard languages (C/C++/Java/python) and you shouldn't have any trouble finding compatible libraries for your language of choice.\n\nBut seriously, &gt;90TB on a laptop?? So I had my initial calulcation wrong.  However, double precision is out. We will try I32 or I16, but we will have to verify the requirements. Maybe we can get away with an I8 but we do need the value signed. So I had my initial calulcation wrong.  However, double precision is out. We will try I32 or I16, but we will have to verify the requirements. Maybe we can get away with an I8 but we do need the value signed. Should be 93GB of raw data, if my calculations are correct:\n\n240 samples / s * 400 channels * 129600 s * 8B/sample = 92.70 GB\n\n\nIf you'd encode that as CSV, it would probably be closer to 300 GB.  Are you sure about that number? 12 trillion double precision floating point numbers (according to IEEE 75x^1) would be about 90 terabytes. I don't think the format used to store that amount of data has much impact on ease of retrieval or processing speed. Even if you actually stored only 1 byte no-precision characters, that would still be 11 terabytes or so. \n\n\n\n1: Don't remember the exact number of the standard. That number is not correct.  I just realized that I punched in 96000 kHz into my calculator. It was supposed to be 96 kHz. So 96kHz * 36 Hrs = 12,441,600,000 samples. Slightly more reasonable. Yeah, that's nothing.  I'd dump it all as either CSV or JSON, and then load it into a DB later if necessary for processing purposes.\n\nIt would also be easy to go from CSV or JSON to a format that implements any sort of clever compression (would storing deltas rather than raw readings and then doing Run Length Encoding or a simple huffman tree on them yield big or small benefits?).  During the experiment, I would expect dumping the data as quickly as possible would be of primary importance.  Storage is cheap.  Use it!  But later, compression might even help speed up access and processing... It's still 91GB. Storing 8 byte floating point numbers, which would then have `log(2^56) ~ 17` fraction digits (in base 10), `~3` exponent digits, and 1 sign character, as CSV would require 22 characters/bytes (including field delimiter) per 8 byte float, blowing up the 91GB to 273GB. It would probably be a lot worse with JSON. I don't know whether OP's laptop has enough storage for that. That number is not correct.  I just realized that I punched in 96000 kHz into my calculator. It was supposed to be 96 kHz. So 96kHz * 36 Hrs = 12,441,600,000 samples. Slightly more reasonable.    How will it be indexed, channel and time?   ",
    "url" : "http://www.reddit.com/r/compsci/comments/11imsg/question_what_is_best_practice_for_storing_very/"
  }, {
    "id" : 33,
    "title" : "A nice review article about advances in dataflow programming languages.",
    "snippet" : " ",
    "url" : "http://www.cs.ucf.edu/~dcm/Teaching/COT4810-Spring2011/Literature/DataFlowProgrammingLanguages.pdf"
  }, {
    "id" : 34,
    "title" : "Data Structures and Analysis Video Lectures?",
    "snippet" : "I'm taking a Data Structures and Analysis course online and would love some recommendations for online video lecture. Thanks in advanced.   [MIT OCW](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/)  [Coursea - Algorithms I](https://www.coursera.org/course/algs4partI)\n  \n[Coursea - Algorithms II](https://www.coursera.org/course/algs4partII)\n  \n[Coursea - Algorithms: Design and Analysis, Part 1](https://www.coursera.org/course/algo)\n  \n[Udacity - Algorithms (hands on)](http://www.udacity.com/overview/Course/cs215/CourseRev/1)",
    "url" : "http://www.reddit.com/r/compsci/comments/11h6oe/data_structures_and_analysis_video_lectures/"
  }, {
    "id" : 35,
    "title" : "How does the Detect Language part of Google translate work?",
    "snippet" : "For my background knowledge: I'm a third year undergrad studying math and computer science, if that helps. Thanks!  Not sure how they do it, but here's one approach that might work.\n\nFirst, narrow down the possible languages to a few by looking at the character set. Just build a list of all unique characters in the text, and compare to known lists of possible characters for each language. This will already give a fairly good subset of languages.\n\nThen look at letter and bigram frequencies, also easy to find and known in advance for languages. Where English has a lot of \"th\", German would have a lot of \"ch\", for example. This lets you assign higher probabilities to some languages.\n\nNext look through the text to see if it has words that are among the most common in some language. A real dictionary check is slow, so what I'd do would be just to have a list of some 20 most common words in each language, and search for them.\n\nI would guess that these three factors, probably with some weights added, would make for accurate detection a lot of the time. Definitely not the best method, better ones would be more sophisticated, but for a quick-and-often-accurate method, that should suffice. Not sure how they do it, but here's one approach that might work.\n\nFirst, narrow down the possible languages to a few by looking at the character set. Just build a list of all unique characters in the text, and compare to known lists of possible characters for each language. This will already give a fairly good subset of languages.\n\nThen look at letter and bigram frequencies, also easy to find and known in advance for languages. Where English has a lot of \"th\", German would have a lot of \"ch\", for example. This lets you assign higher probabilities to some languages.\n\nNext look through the text to see if it has words that are among the most common in some language. A real dictionary check is slow, so what I'd do would be just to have a list of some 20 most common words in each language, and search for them.\n\nI would guess that these three factors, probably with some weights added, would make for accurate detection a lot of the time. Definitely not the best method, better ones would be more sophisticated, but for a quick-and-often-accurate method, that should suffice.    ",
    "url" : "http://www.reddit.com/r/compsci/comments/11i3ev/how_does_the_detect_language_part_of_google/"
  }, {
    "id" : 36,
    "title" : "Differences and relations between randomized and nondeterministic algorithms",
    "snippet" : "  ",
    "url" : "http://cs.stackexchange.com/q/5008/98"
  }, {
    "id" : 37,
    "title" : "Formal Grammar of English",
    "snippet" : "  I've been researching for an upcoming attempt at a natural English (or a subset there-of) parser. If anyone has any other interesting or useful resources on the topic, I'd love to see them. Great place to start would be [here](http://nltk.org/); the parse tree they show as an example of on the main page is the same one in the paper you linked, too. Jurafsky and Martin, who wrote the linked paper, [are also the authors of a great textbook on NLP](http://www.cs.colorado.edu/~martin/slp.html).\n\nIf you're going to write a parser just for fun, I recommend starting with a very small subset of the language -- a complete one is probably impossible just due to the flexibility of English. Thanks for the resources! What you suggested is my goal, though. I'd like to start by building a parser for a small subset of simple English. I want it to be capable of parsing sentences at maximum complexity \"Tony has a red apple\". I've decided to begin by making the concession of permitting alien parentheses for a smaller amount of ~~unambiguity~~ ambiguity in the parse tree as well as permitting the writer to specify the part of speech. I'd then like to feed this into an interpreter for logical deduction. My goal is to have it solve Einstein's Puzzle from English clues. I'm not sure how much experience you have with this, but you've specified quite a complicated problem. Not only do you have the task of natural language processing, but also knowledge representation and logical deduction. [Here's another great textbook that](http://aima.cs.berkeley.edu/) that covers the latter tasks. \n\nIf you're concerned only about types of sentences that you encounter in Einstein's puzzle, then that simplifies a lot of things, since there's a small grammar to parse. Take, for example, \n\n&gt; The Dane drinks tea.\n\nYou could parse this as NP VP (not standard notation but you get the idea. The subject noun phrase would be broken down as Article Noun. So for a grammar to parse that, all you need is the following rules:\n\n* S -&gt; NP VP\n* NP -&gt; N | Art N\n* VP -&gt; V-trans NP\n* N -&gt; Dane | tea\n* Art -&gt; the\n* V-trans -&gt; drinks\n\nThen when it comes to the knowledge representation, you would have a binary predicate Drinks, where Drinks(Dane, tea) is true.\n\nI'm not sure what you mean by alien parentheses. I would say I have a decent foundation in the topics of Programming Languages, AI, and Formal Grammars. Most of this comes from my college classes (I graduated last year. Each class, aside from \"Grammar Class\" was pretty hands on, writing compilers for a simple, made-up programming language, and a Chess AI). I actually own the book you recommended; it was my AI textbook and I enjoyed it so much I decided not to sell it back. I understand that it will be challenging, but I feel limiting the domain of the problem to Einstein's puzzle (and puzzles like it) will help in making it a manageable project.\n\nTo answer your question about the \"alien parentheses\", I mean parentheses that wouldn't normally exist in an English sentence. So if I decide to attempt to make the grammar more complicated than the one you described (which is exactly what I thinking of starting out with), the user can use parentheses to group tokens. For instance, if I added the ability to have prepositional phrases, the user could write \"I (shot an elephant) in my pajamas\" to indicate the normal meaning. They could also write \"I shot (an elephant in my pajamas)\" to indicate that humorous reading. This is still one of the optional \"it'd be cool if I could figure out a way to do it\" ideas, though. I haven't thought it through. The purpose behind it is to reduce ambiguity. I'll also be having the user specify the part of speech as well for the same purpose. Ultimately, there should be 1 parse tree for every expression.\n\nIsn't there a language that deals with logical deduction, though? The statements are written in very much the same way you wrote \"Drinks(Dane,tea)\" and then you can query it? It's perfect for these kinds of puzzles and I'm sure I'll need to write an interpreter to draw conclusions from the represented knowledge.\n\nedit: I also intend to use a \"Parts of Speech\" word list to validate user input. &gt;  I enjoyed it so much I decided not to sell it back\n\nA man after my own heart.\n\n&gt; Isn't there a language that deals with logical deduction, though?\n\nMany. By far the most famous one is Prolog.\n\n&gt; I also intend to use a \"Parts of Speech\" word list to validate user input.\n\nAs with all aspects of dealing with natural language, this is 100% solvable for toy problems where you can specify the acceptable word list and grammar, and not otherwise with the current state of the art.\n\n&gt; My goal is to have it solve Einstein's Puzzle from English clues.\n\nLimiting the problem domain to syllogisms makes your goal achievable. Whether it's easy or still quite hard will depend on how many kinds of user input you decide to tackle.\n\n&gt; attempt at a natural English (or a subset there-of) parser.\n\nYou don't have any choice but to make it a subset, and a small subset, at that. Full natural language parsing is still beyond the state of the art, because it is a strong AI problem.\n\nBut syllogisms are a good small contained domain that doesn't require knowledge of the noun phrase semantics, at least.\n\nSo it could be roughly on par with, and somewhat easier than, [Winograd's 1970 SHRDLU](http://en.wikipedia.org/wiki/SHRDLU) natural language processing system, a truly landmark hack.\n &gt;You don't have any choice but to make it a subset, and a small subset, at that. Full natural language parsing is still beyond the state of the art, because it is a strong AI problem.\n\nThat was poorly worded on my part, I meant to write \"attempt at a natural English (er, that is to say, a subset there-of - it'll parse some English but not all English) parser\".\n\nAnd thank you! Prolog was the one I was thinking of. I'm tempted to make this thing piggy-back on prolog, but I'm inexperienced with the language and I enjoy the idea of creating a system for logical deduction. \n\nIt's good to hear your confidence, though. I'm confident in my ability to make this but now I feel like it's less likely naive confidence. haha.\n\nAnd thanks for the link to the SHRDLU (that acronym is too long to be unpronounceable). The excerpt was very impressive. Especially: \n&gt;Person: Is at least one of them narrower than the one which I told you to pick up?  \nComputer: Yes, the red cube.\n\nWhat strikes me is \"the one which I told you to pick up\". The program recognized \"the one\" as a pronoun for what they were just talking about, and then filtered for content after \"which\" (I wonder what it would say in response to \"which I will tell you to pick up\" =P). Perhaps if I get ambitious with this project, I might attempt to implement such abilities. Perhaps something involving the OpenCog project. &gt; I'm tempted to make this thing piggy-back on prolog\n\nSolving logic puzzles is one of the classic examples that appears in Prolog books (and in AI intro books focused on Prolog).\n\nTurns out that simple grammars are also a strength of Prolog, so you can find examples of parsing simplistic English, too -- but that's not to say that Prolog is necessarily the *best* way to do so.\n\nYou can do a quick hack in Prolog that illustrates your project goals in a weekend, no problem, but it sort of gets exponentially harder to make the quick hack more full-featured, so I consider Prolog to be powerful yet misleading, in that sense.\n\nIt's extremely common to prototype algorithms in Prolog, and then translate them into some other language for implementing a full system.\n\n&gt; Perhaps if I get ambitious with this project, I might attempt to implement such abilities.\n\nMind you, Shrdlu was a *hell* of a hack, and is not easy to duplicate even today, leading to the conclusion that Winograd was a hell of a hacker.\n\nAlso don't be mislead by the \"1970\" part of it; the enhanced dialect of Lisp that he wrote it in is reasonably advanced in many respects even by today's standards, and he put a tremendous amount of work into the project.\n\nHis book on Shrdlu (adaptation of his PhD thesis) is impressive and daunting.\n\nPretty much everyone agrees that the secret to successful strong-AI-related projects is to dial them back as much as possible. Shrdlu was very difficult and was not really replicated for the longest time, and even at that, is agreed to have succeeded only because he chose a very, very confined problem domain (blocks), and a tiny subset of English related to the chosen tiny problem domain.\n\nStart small. &gt;Solving logic puzzles is one of the classic examples that appears in Prolog books (and in AI intro books focused on Prolog).\n\nThat's the only prolog program I've written. It was strikingly simple to translate the parameters of the logic puzzle directly into prolog and then just query for desired information. I also enjoyed learning about how it actually works.\n\n&gt;Also don't be mislead by the \"1970\" part of it\n\nDon't be mislead by what what I mean by \"ambitious\". I'll be happy with just making a solver for logic puzzles that reads plain English clues (worded a relatively specific way). I mean to say that if I find myself obsessed with augmenting it (as I tend to do with these projects), I'll probably work in the direction of SHRDLU. I agree that the apparent capabilities of SHRDLU are very impressive, even by today's standards (even when you take into account that the user, in all likelihood, probably needed to know *just how* to phrase each statement). ",
    "url" : "http://www1.cs.columbia.edu/~julia/jmchapters/ch10.pdf"
  }, {
    "id" : 38,
    "title" : "Good documentaries for any specific topics in computer science?",
    "snippet" : "Pretty open to any suggestions really,but does anybody know of any documentaries that delve in to something specific about computer science or computer science history? I'd be more interested in more intermediate to advanced subject matter, but I guess for the sake of the general population, anything goes.  Here's a mixture of documentaries &amp; sources with some lecture material.    \n   \nIf you're after more advanced material then you should be searching for lectures rather than documentaries.\n\n[Inseparable From Magic: Manufacturing Modern Computer Chips](https://www.youtube.com/watch?v=NGFhc8R_uO4)    \n  \n[ArsDigita University - MIT 2000-2001](http://archive.org/details/arsdigita)   \n   \n[JP Dunning - Destroying Evidence Before Its Evidence](https://www.youtube.com/watch?v=lqBVAcxpwio)   \n   \n[Clifford Stoll - 1995 - Computers &amp; the future](http://video.google.com/videoplay?docid=-666540182028461233)   \n   \n[Adam Curtis' All Watched Over By Machines Of Loving Grace](http://www.bbc.co.uk/programmes/b011rbws)  \n   \n[BBC documentary: History of Computers](https://www.youtube.com/watch?v=NbhbssXWDAE)   \n   \n[CosmoLearning: Computer Science Documentaries](http://www.cosmolearning.com/computer-science/documentaries?sort=views)     \n   \n\n \n\n  \n Here's a mixture of documentaries &amp; sources with some lecture material.    \n   \nIf you're after more advanced material then you should be searching for lectures rather than documentaries.\n\n[Inseparable From Magic: Manufacturing Modern Computer Chips](https://www.youtube.com/watch?v=NGFhc8R_uO4)    \n  \n[ArsDigita University - MIT 2000-2001](http://archive.org/details/arsdigita)   \n   \n[JP Dunning - Destroying Evidence Before Its Evidence](https://www.youtube.com/watch?v=lqBVAcxpwio)   \n   \n[Clifford Stoll - 1995 - Computers &amp; the future](http://video.google.com/videoplay?docid=-666540182028461233)   \n   \n[Adam Curtis' All Watched Over By Machines Of Loving Grace](http://www.bbc.co.uk/programmes/b011rbws)  \n   \n[BBC documentary: History of Computers](https://www.youtube.com/watch?v=NbhbssXWDAE)   \n   \n[CosmoLearning: Computer Science Documentaries](http://www.cosmolearning.com/computer-science/documentaries?sort=views)     \n   \n\n \n\n  \n If you are going to watch something by Adam Curtis', you should watch \"The Power of Nightmares\".  \"All Watched Over By Machines of Loving Grace\" and \"The Trap\" are both beautiful and powerful, but most computer people won't appreciate his undertones of antiscience.  \n\nJust my 2 cents.  Really, you should watch the intros to each film if not the entire length.    This isn't exactly computer science, but it is a quite interesting documentary about the rise of the open source movement.\n\nhttp://www.revolution-os.com/ Also Pirates of Silicon Valley. Someone I work with just told me about this today. I plan on watching a little bit tonight.  This isn't exactly computer science, but it is a quite interesting documentary about the rise of the open source movement.\n\nhttp://www.revolution-os.com/   I can't remember what they were called, but the BBC have made several good documentaries about Bletchley Park, Alan Turing, cryptography etc.. I can't remember what they were called, but the BBC have made several good documentaries about Bletchley Park, Alan Turing, cryptography etc.. I can't remember what they were called, but the BBC have made several good documentaries about Bletchley Park, Alan Turing, cryptography etc.. I can't remember what they were called, but the BBC have made several good documentaries about Bletchley Park, Alan Turing, cryptography etc.. I can't remember what they were called, but the BBC have made several good documentaries about Bletchley Park, Alan Turing, cryptography etc.. I can't remember what they were called, but the BBC have made several good documentaries about Bletchley Park, Alan Turing, cryptography etc.. I can't remember what they were called, but the BBC have made several good documentaries about Bletchley Park, Alan Turing, cryptography etc..  i forget what the name was, but it was about origami and computational geometry. very interesting movie nonetheless i think it had the word fold in the name.   ",
    "url" : "http://www.reddit.com/r/compsci/comments/11fthb/good_documentaries_for_any_specific_topics_in/"
  }, {
    "id" : 39,
    "title" : "Areas of research in Operating Systems?",
    "snippet" : "Hi everyone I am a grad student and currently considering to carry out research in the field of OS. Can you guys please tell me some research areas in this field and some topics which are pretty hot these days in them(the areas)? Many thanks :)  ",
    "url" : "http://www.reddit.com/r/compsci/comments/11gfwy/areas_of_research_in_operating_systems/"
  }, {
    "id" : 40,
    "title" : "An interesting discussion on how the conventional wisdom about the impossibility of inferring causality was overturned by Judea Pearl et al.",
    "snippet" : "  I'm not sure I feel convinced. His argument seems to be that, given three variables, we can sometimes identify some incorrect three-variable causation graphs. If, by the process of elimination, only one three-variable graph is not incorrect, it must be correct. What leaves me unconvinced is that this only considers three-variable graphs.\n\nIf four-variable (or more-variable) graphs are allowed, can we still conclude that all correct graphs have causation edges in the same places the \"correct\" three-variable graph does? Yes. The great thing about Bayesian networks, is that if we have established that two events are not statistically independent then there must exist some path between them in our Bayesian network. Granted, the number of possible paths increases as you add vertices, but it should be unsurprising that you need more information to characterize larger systems. Importantly, the information you gathered to form your smaller system is still valid, and so you can gradually build larger Bayesian networks, and get a steady increase in the predictive power of your model.  &gt; if we have established that two events are not statistically independent then there must exist some path between them in our Bayesian network.\n\nOkay, I believe this 100%. Are you saying I should also believe that the path in the larger network definitely has the same directionality as the path in the small network (which was the original question anyway)? I'm not sure I feel convinced. His argument seems to be that, given three variables, we can sometimes identify some incorrect three-variable causation graphs. If, by the process of elimination, only one three-variable graph is not incorrect, it must be correct. What leaves me unconvinced is that this only considers three-variable graphs.\n\nIf four-variable (or more-variable) graphs are allowed, can we still conclude that all correct graphs have causation edges in the same places the \"correct\" three-variable graph does? I find this treatment a bit better:\n\nhttp://normaldeviate.wordpress.com/2012/06/18/48/\n\nThe difference being that one of the three variables is multidimensional variable that contains all possible variables. Which is, as Wasserman points out, very difficult to observe in a real life study leading to the need for randomized testing. I find this treatment a bit better:\n\nhttp://normaldeviate.wordpress.com/2012/06/18/48/\n\nThe difference being that one of the three variables is multidimensional variable that contains all possible variables. Which is, as Wasserman points out, very difficult to observe in a real life study leading to the need for randomized testing. I'm not sure I feel convinced. His argument seems to be that, given three variables, we can sometimes identify some incorrect three-variable causation graphs. If, by the process of elimination, only one three-variable graph is not incorrect, it must be correct. What leaves me unconvinced is that this only considers three-variable graphs.\n\nIf four-variable (or more-variable) graphs are allowed, can we still conclude that all correct graphs have causation edges in the same places the \"correct\" three-variable graph does? I'm not sure I feel convinced. His argument seems to be that, given three variables, we can sometimes identify some incorrect three-variable causation graphs. If, by the process of elimination, only one three-variable graph is not incorrect, it must be correct. What leaves me unconvinced is that this only considers three-variable graphs.\n\nIf four-variable (or more-variable) graphs are allowed, can we still conclude that all correct graphs have causation edges in the same places the \"correct\" three-variable graph does?  The comments there say it more eloquently than I could, but this isn't really \"overturning\" conventional wisdom in any sense.  As Yudkowsky puts it, this is \"completely bog-standard Bayesian networks, causal models, and causal diagrams\", most of which began appearing in rudimentary form at least as early as the 18th century.  Assumptions about causality go into the analysis from the start, so all that's really being done is inferring specific characteristics of causality from a more general framework.\n\nOf course, all of that is not to detract from the main points of the article, which are all sensible and useful and so on.  I just mean to say that the headline is a bit misleading.  But I guess that's the point of headlines. The comments there say it more eloquently than I could, but this isn't really \"overturning\" conventional wisdom in any sense.  As Yudkowsky puts it, this is \"completely bog-standard Bayesian networks, causal models, and causal diagrams\", most of which began appearing in rudimentary form at least as early as the 18th century.  Assumptions about causality go into the analysis from the start, so all that's really being done is inferring specific characteristics of causality from a more general framework.\n\nOf course, all of that is not to detract from the main points of the article, which are all sensible and useful and so on.  I just mean to say that the headline is a bit misleading.  But I guess that's the point of headlines. Please see this quote from the article:\n\n\"For a long time, the conventional wisdom in philosophy was that this was **impossible** unless you knew the direction of time and knew which event had happened first. Among some philosophers of science, there was a belief that the \"direction of causality\" was a meaningless question, and that in the universe itself there were only correlations - that \"cause and effect\" was something unobservable and undefinable, that only unsophisticated non-statisticians believed in due to their lack of formal training.\"\n\nAnd this one:\n\n\"All this discipline was invented and systematized by Judea Pearl, Peter Spirtes, Thomas Verma, and a number of other people in the 1980s and you should be quite impressed by their accomplishment, because before then, inferring causality from correlation was thought to be a **fundamentally unsolvable** problem.\" [deleted] Where is the intervention in the Burglar/Earthquake/Alarm example?\n\n*edit:* To elaborate, I think you've missed the point of the article, which is that you *can* infer causality from observing only - which is precisely what he does in the example in the article. Wouldn't an experiment be necessary to determine the probability of a burglar given the alarm?  Since the alarm could also be set off by kids hitting a baseball through the window, a tree falling on the house, a housefire, a microburst, a resident failing to deactivate the alarm, corrosion on the contacts breaking the circuit on the detector, and many, many other causes, you'd have to do an experiment somehow to figure out just how often an alarm going off and a burglar breaking into the house coincide. No you'd just figure it out with a survey. Presumably you can tell whether your house has been burgled. How is a survey not an experiment? I'm not familiar with the definitions as they're used in this context, so maybe a survey is an experiment, but it's definitely not an intervention. You seemed to be saying that an intervention was necessary to determine the probability of the burglar given the alarm. I would expect the intervention necessary for your probability to be correct would be the introduction of randomization.  As it is, an observation of an alarmed house in our culture is pretty obviously very far from random.  Both the poor and the rich would be unrepresented (the poor because they can't afford alarms, the rich because they have their own security staff and such).  Without intervention, the observation would be useless unless you specifically cited the statistic as 'chance of burglary given alarm in X socioeconomic group in Y culture in year Z' or the like.\n\nWithout setting any sort of control and not doing any kind of intervention, your statistic would reveal far less about burglars and alarms than it would reveal details of your specific culture.  I would expect the intervention necessary for your probability to be correct would be the introduction of randomization.  As it is, an observation of an alarmed house in our culture is pretty obviously very far from random.  Both the poor and the rich would be unrepresented (the poor because they can't afford alarms, the rich because they have their own security staff and such).  Without intervention, the observation would be useless unless you specifically cited the statistic as 'chance of burglary given alarm in X socioeconomic group in Y culture in year Z' or the like.\n\nWithout setting any sort of control and not doing any kind of intervention, your statistic would reveal far less about burglars and alarms than it would reveal details of your specific culture.  How is a survey not an experiment? Wouldn't an experiment be necessary to determine the probability of a burglar given the alarm?  Since the alarm could also be set off by kids hitting a baseball through the window, a tree falling on the house, a housefire, a microburst, a resident failing to deactivate the alarm, corrosion on the contacts breaking the circuit on the detector, and many, many other causes, you'd have to do an experiment somehow to figure out just how often an alarm going off and a burglar breaking into the house coincide. &gt; you'd have to do an experiment somehow to figure out just how often an alarm going off and a burglar breaking into the house coincide.\n\nNo, you can figure this out with the data he provides (ie. through observation), the probability of a Burglar given an Alarm is:\n\nP(burglar|alarm) = (.000162+.0151)/(.000162+.0151+.0078+.00097) ~= 0.635\n\n That 'observation'?  That's an experiment.  Unless I'm missing something? Wouldn't an experiment be necessary to determine the probability of a burglar given the alarm?  Since the alarm could also be set off by kids hitting a baseball through the window, a tree falling on the house, a housefire, a microburst, a resident failing to deactivate the alarm, corrosion on the contacts breaking the circuit on the detector, and many, many other causes, you'd have to do an experiment somehow to figure out just how often an alarm going off and a burglar breaking into the house coincide. [deleted]     And when you find out that the population of people who don't exercise in your culture just so happens to overlap almost perfectly with the people who eat something which radically increases fat retention, and you've passed laws manipulating the lives of people based on your belief that exercise is the key, and you realize 20 years on that you've ruined the lives of millions of actual human beings and vastly increased the suffering in the world by getting lazy and refusing to face the fact that an issue such that involves human biology is flat out too complex to be reliably amenable to such 'reasoning'... what then?\n\nWhat determines 'truth' is always an issue of human suffering.  We have enough history behind us of people ignoring the strictures of logic and trying to paste over the cracks in their argument with good intentions, and there are corpses in our wake.  When we form consensus and accept something as truth, policy gets made around it.  Policy is, at its base, leaving reason behind.  It is the point where we say, as a society, 'this is proven, and if reasonable arguments cannot convince you, we are confident enough in it to use force'.\n\nDue to the restrictions of practicality, and the fact that we often have to act without full information or proof, it is inevitable that we will make mistakes.  I can see no advantage to this inference of causality save for making people who should not be confident in their conclusions confident in them.  If you have strong statistical evidence, why is it not adequate to point that out and also point out that there are possibly confounding factors?  Such an approach would hopefully keep arguments which are not CERTAIN outside the realm of policy and in the realm of personal decision in order to hopefully reduce the overall suffering that can be laid at the door of science being presumptive.\n\nI think a good test of this means of understanding is to apply it to the past.  What conclusions would we have come to had we accepted this in 1900?  Would eugenics be science fact?  Would it have fueled colonialism?  Would it have taken the demographic and social trends of the time and fought against school integration and civil rights?  The standard of truth doesn't change often because its impact is nothing short of monumental.  It shapes whether our institutions jump to conclusions and sweeps aside all the messy 'details' and accepts things like widespread cultural flaws as fundamentally true simply because they are statistically ubiquitous, or whether they are parsimonious and conservative in what they dub 'truth'.  Given the extremely large and rapidly expanding population of people across the globe who engage in no critical thought whatsoever in their life and rely entirely on what is handed to them by those perceived to be in authority or what is simply floating around around them, it is becoming more important that we be as critical and conservative as possible in the intellectual realm.",
    "url" : "http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/"
  }, {
    "id" : 41,
    "title" : "What are some of the reasons you like computer science?",
    "snippet" : "  You can apply it to almost anything. Do you like physics, biology, chemistry, math, music, robots, or language? Well there's a sub-discipline of Computer Science for you! And many others, too!\n\nAnd until the people in these \"pure\" disciplines learn Computer Science for themselves, you'll be in high demand! Computer Science is broad enough to have purity of its own---that's another great thing: computability, formal languages, problem complexities... these things are as pure as any field of pure math. You can apply it to almost anything. Do you like physics, biology, chemistry, math, music, robots, or language? Well there's a sub-discipline of Computer Science for you! And many others, too!\n\nAnd until the people in these \"pure\" disciplines learn Computer Science for themselves, you'll be in high demand! You can apply it to almost anything. Do you like physics, biology, chemistry, math, music, robots, or language? Well there's a sub-discipline of Computer Science for you! And many others, too!\n\nAnd until the people in these \"pure\" disciplines learn Computer Science for themselves, you'll be in high demand! I like politics, law and social \"stuff\" - can't apply CompSci to that. Vice-versa it works though... *sigh*\n\n\n(Sorry for digging out the old thread)  Because it teaches me a new way to think. It helps me see that there are better ways to do almost everything. It's hard not to blur the line between coding and computer science for questions like this though, so specifics are hard for me. I really, really like the fact that I can take an everyday problem, throw some CS at it, and have an elegant solution. All by myself. It's just plain fun.\n\nPlus, it puts beer on the table. Yeah, I love that I can apply CS concepts to whatever problem I'm tackling, whether writing a script to automate something or using math to tease apart data or thinking of elegant ways to store things. It makes life sexy!\n\nThe whole beer on the table thing as an added perk makes it that much better. As yummy as beer is though, it'd taste a lot more sour if that was the only reason we loved CS. I'm really surprised how much learning CS has helped me with learning math, as opposed to the other way around. You're learning the wrong math if it's not helping you learn CS! I'm a lot more advanced in CS than I am in Math right now. I'm in a Data Structures and Algorithms course for CS and PreCalc for math. As much as I liked calc as a physics major, I've found stats/prob is a lot more useful for CS. The nice thing about the mathy/theoretical CS courses is that they have some (not much if it's Algorithms and a bit more if it's Machine Learning) application that gives the math a purpose to exist. Math for the sake of math can be cool but it's nice to have something that motivates it. I completely agree. I've never really been good at Math.. I've always wanted to love it because I know how awesome it can be.. but I've never been able to get a grasp on it, and I've never been taught anything purposeful. I really can't wait to get to Physics. I'll have to look into Stats/Prob courses too. I am starting to get better at it though, which is awesome. I don't know why but this past summer a lot of algebra stuff that I was getting stuck on just.. clicked. Between AP stats and Engineering stats and CS stats courses it all started to make more and more sense. Or maybe I got looper and looper...\n\nI loved physics cause all the math had to equal out and each term had a real world significance. Conservation of energy and mass and momentum!! Until I got to the shenanigans of quantum mechanics. And thus sealed my decision towards CS Masters.  \n\nWhat are you majoring in? I'm a lot more advanced in CS than I am in Math right now. I'm in a Data Structures and Algorithms course for CS and PreCalc for math. Ah, so you haven't actually gotten to math yet. It's unfortunate that they don't start teaching math until after calc, when most people stop with the curriculum. Your compsci courses are likely teaching you more math than your math courses right now. *blink blink* whut.. Algebra isn't math? I feel like my teacher's would argue otherwise :P\n\nBut yeah, no practical math. I'm really looking forward to getting to the point where the variables actually mean something and aren't just random letters.. And like I said, comp sci is helping me more with math, so far, than math has with comp sci :P\n\nEdit: I did take Discrete Math last semester, that was sorta useful.. No no, I'm not talking practical math, I mean *math*: deriving proofs from axioms and definitions, seeing patterns and formalizing them, freeing an idea from a specific domain into a more general one. At least everyone I know was shown very little actual math before college-level discrete math or real analysis or abstract algebra. \n\nHigh school \"math\" seemed far more about memorizing these magic rules and applying them like some human calculator, with no sense of where these rules came from, or how we know them to be true; no attempt to get us to see the art of constructing a counterexample or diving a proof.\n\nBut yeah, I guess practical stuff is good too... *blink blink* whut.. Algebra isn't math? I feel like my teacher's would argue otherwise :P\n\nBut yeah, no practical math. I'm really looking forward to getting to the point where the variables actually mean something and aren't just random letters.. And like I said, comp sci is helping me more with math, so far, than math has with comp sci :P\n\nEdit: I did take Discrete Math last semester, that was sorta useful.. haha dude try to stop drinking and/or sulking over the work for discrete math. It was very useful for me in applying logic to CS - just had to pay attention. I'm a lot more advanced in CS than I am in Math right now. I'm in a Data Structures and Algorithms course for CS and PreCalc for math. Pure math is especially helpful for being able to think about advanced CS, particularly in the theoretical side.  Subjects like number theory, combinatorics and abstract algebra, and really any class that requires you to prove theorems, will give you a very good intuition about solving problems that can only help you in computer science.   That's more like Discrete math than Calc...  Did my first bachelors in physics, had a bad and abortive gradschool experience, and now I'm doing a second BS/MS in CSE (my school is weird and treats it as engineering).\n\nI've had enough math and logic to choke a horse, what I'm enjoying is actually MAKING things for once. My school treated compsci as engineering as well. Believe or not, it's a benefit, and will give you an advantage in an engineering environment. Hate to say it, but there is a hefty bias in the engineering industry ('specially in aerospace) against computer science majors. If you can prove you aren't just a code monkey, it'll help fight that bias. Believe me, I know. I applied for plenty of engineering jobs that I was qualified for (say, simulation and the like), but companies look askance at a physics degree. Right, fine, I'll play that game (all the while thinking that engineering culture is kinda weird and insular).   I went back to school to get a compsci degree in the late 90s, right in the middle of the first .com bubble. Like a ton of other people, I did it because that was where the money was. Unlike a ton of other people, I actually had an aptitude for it.\nWhy do I love it?\nYou can take something ephemeral, logical thought, transform that thought into mathematics, and transform that math into bits that accomplish something in the real world. That's magic! It's telekinesis! The transformation of thought-&gt;math-&gt;code-&gt;results, and the challenges associated with that transformation is why I love compsci. I went back to school to get a compsci degree in the late 90s, right in the middle of the first .com bubble. Like a ton of other people, I did it because that was where the money was. Unlike a ton of other people, I actually had an aptitude for it.\nWhy do I love it?\nYou can take something ephemeral, logical thought, transform that thought into mathematics, and transform that math into bits that accomplish something in the real world. That's magic! It's telekinesis! The transformation of thought-&gt;math-&gt;code-&gt;results, and the challenges associated with that transformation is why I love compsci. [deleted] [deleted]    I like puzzles, or more over I like finding solutions.  Which is not the same as riddles, brainteasers, or numerical puzzles like sudoku. Those I hate.\n\nI love it for the same reasons I love math...without any of the reasons I hate math.  Although the occassional rote memorization tends to crop up in formal education, and a lot of techniques build on a lot of other techniques.\n\nRight now I'm studying image processing and algorithms and ever day is just \"Wow, if I can do this...then I can do this, this and that!\" occasional rote memorization?  Thats all that math is! \n\nI have never had a math teacher explain anything. \"just learn it.\" You've never been taught then.  Look at...um...Pythagorean and Trignometric Identities.  Now, the Pythagorean Identity was crystalized into my brain in by the time I was twelve.  I can't forget it.  The Trig identities are more numerous, and despite memorizing them repeatedly every few years, I forget them a lot.  But I can derive them all starting from the Pythagoren.  Granted, it's shitty for tests because I spend a lot of time doing that. I couldn't derive them...\n\nSounds about right. Well, you made me feel better about my day.  So there's that. I couldn't derive them...\n\nSounds about right.                  Computer science is doing God's work. God gave man such large brains, and the entire universe to mold into his tools. As a computer scientist, you are advancing our species technologically, intellectually, and spiritually. Knowledge is power. You contribute the platforms that empower people, that take down dictators and connect economies and everything in between. This is an important job. The future of our species will be guided by the technology you create today. Go forth. Be bold and be confident in yourself. You've discovered a piece of the meaning of life.\n\n**edit** Welp, I tried to be inspiring. Screw you nerds, I'm going home. Computer science is doing God's work. God gave man such large brains, and the entire universe to mold into his tools. As a computer scientist, you are advancing our species technologically, intellectually, and spiritually. Knowledge is power. You contribute the platforms that empower people, that take down dictators and connect economies and everything in between. This is an important job. The future of our species will be guided by the technology you create today. Go forth. Be bold and be confident in yourself. You've discovered a piece of the meaning of life.\n\n**edit** Welp, I tried to be inspiring. Screw you nerds, I'm going home.",
    "url" : "http://www.reddit.com/r/compsci/comments/11djsh/what_are_some_of_the_reasons_you_like_computer/"
  }, {
    "id" : 42,
    "title" : "Math Nerds vs. Code Monkeys: Should Computer Science Classes Be More Practical? A look at both sides of the issue.",
    "snippet" : "  I took CS and Math courses when I was 19 but loved pure programming, I glossed the math details to get my degree.  I didn't really appreciate the math at all.\n\nNow, I come back 10+ years later and love the math and hate the pure programming.  The math and algorithms are where the strong abstract concepts lie.   It is better to look at a problem abstractly than worry about irrelevant details.\n\nWith that said, I think people may absorb more of the practical stuff especially if they are young.  Sometimes you lose people if you just throw pure theory at them.  So I think it takes balance. I agree. There's something to be said about the addictiveness of coding to people just starting out with CS. The math and theory behind CS is certainly beautiful, but it takes time before you acquire the lens to truly appreciate it, and it tends to intimidate and repel those without a solid background.\n\nI've always been envious of people who had a flair for maths and theoretical CS in college. They glide through the algorithm-heavy courses so effortlessly, it was as if they had access to some secret vault of abstract reasoning tricks for every problem out there.  CS courses should teach theory.  It would be a complete and total waste for schools to start teaching kids how to write code.  The only way to pick up these skills is by actually going out and doing it.  You can't be lectured into writing good code.\n\nIMO, the ideal curriculum would be almost entirely open-ended projects.  Present the student with the theory and let them figure out how to apply it on their own.  They can get their practical experience that way. CS courses should teach theory.  It would be a complete and total waste for schools to start teaching kids how to write code.  The only way to pick up these skills is by actually going out and doing it.  You can't be lectured into writing good code.\n\nIMO, the ideal curriculum would be almost entirely open-ended projects.  Present the student with the theory and let them figure out how to apply it on their own.  They can get their practical experience that way. I agree a little bit.  But you aren't going to win the hearts of 19-20 year old students if you just throw a bunch linear algebra at them.  But they may appreciate linear algebra with applied machine learning with side programming projects.\n\nMost school curriculum disagree with me and are all about the fundamentals.  I guess that is why they only retain 20% of their students by graduation. CS courses should teach theory.  It would be a complete and total waste for schools to start teaching kids how to write code.  The only way to pick up these skills is by actually going out and doing it.  You can't be lectured into writing good code.\n\nIMO, the ideal curriculum would be almost entirely open-ended projects.  Present the student with the theory and let them figure out how to apply it on their own.  They can get their practical experience that way. I agree.\n\nAnd if you want a more practical education there are, other ways you can take such as software engineering. CS is an academic field and if that is not what you want then you simply shouldn't study it. I guess the problem is that people associate 'computer scientist' with 'programmer'.\n\nEdit: Another comment posted here implies that there is no such thing as a software engineering degree. I know there is in Denmark, but maybe there is not in the USA? If not, there should be.  Most universities in the US do not have separated CS and SE departments or degrees. In many cases, the CS department is considered an engineering discipline rather than a pure science. In my experience, most people in these programs are there to learn a mixture of software engineering and cs skills so that they can become more than code monkeys but still do software development. For example, my program required courses like Discrete Math, Theory, Data Structures, and Algorithms but also required at least one software engineering course and offered a half dozen more as electives. \n\nI'm not sold on the idea that there should be a distinction. Where would a class on compilers fit in? A lot of optimizations are based in engineering tricks to improve caching performance and such (engineering) but the principles of a compiler are much more theoretical (science). \n\nIn many ways, a good understanding of Data Structures or whatever is critical to becoming a good software engineer. Remember that software engineering is *way* more than being a code monkey, and this comes from somebody who is doing a Ph.D. in CS so if anything I'd be biased against SE.  I agree.\n\nAnd if you want a more practical education there are, other ways you can take such as software engineering. CS is an academic field and if that is not what you want then you simply shouldn't study it. I guess the problem is that people associate 'computer scientist' with 'programmer'.\n\nEdit: Another comment posted here implies that there is no such thing as a software engineering degree. I know there is in Denmark, but maybe there is not in the USA? If not, there should be. I agree.\n\nAnd if you want a more practical education there are, other ways you can take such as software engineering. CS is an academic field and if that is not what you want then you simply shouldn't study it. I guess the problem is that people associate 'computer scientist' with 'programmer'.\n\nEdit: Another comment posted here implies that there is no such thing as a software engineering degree. I know there is in Denmark, but maybe there is not in the USA? If not, there should be. I agree.\n\nAnd if you want a more practical education there are, other ways you can take such as software engineering. CS is an academic field and if that is not what you want then you simply shouldn't study it. I guess the problem is that people associate 'computer scientist' with 'programmer'.\n\nEdit: Another comment posted here implies that there is no such thing as a software engineering degree. I know there is in Denmark, but maybe there is not in the USA? If not, there should be. There is software engineers degrees all over the place.\n\nIt's not a protected engineering discipline, so anybody can call themselves a software engineer. I got compsci with some engineering classes and I call myself a software engineer.\n\nPersonally I would be upset if I got stuck doing software engineering as a computer scientist.\nHonestly coding is something you pick up from practice and is more than easy to learn on the job. My value is that I can build effective solutions for problems most people cannot wrap their heads around without large amounts I work. Albeit I'm more an engineer than a scientist in my job, I still get to pull on my compsci background from time to time and those are usually the times where I look good.\n\nAlso math is awesome when you get to apply it, some of the most interesting things I ever learnt we're in algorithm and math courses. Job and work aside computer science isn't just a job ticket it's teaching you a new way to think. When mathematicians, physicists and computer scientists talk it's awesome, maybe I'm bias but I feel like I can articulate logic 5000x better than when I went in. &gt; There is software engineers degrees all over the place.\n&gt; It's not a protected engineering discipline\n\nI assume you're talking about the US? In Canada, software engineer is a protected engineering discipline. I'm in Canada. Worked in BC and Alberta.\n\nOther than Texas, I think Ontario has a process to protect the term as does Quebec, but it's nothing even close to the scale of any other engineering discipline.\n\nI guess if you're working in those provinces you can't buy as I live in western Canada I isn't even realize they had anything until I googled it.\n\nFurthermore there are a VERY large body of people who had no interest in actually creating a p.Eng for software given the body of knowledge is so diverse, it seems almost silly to have a body of knowledge, unlike Civil or other Engineering disciplines which are very strictly defined and understood. My program is barely a decade old, so it's likely that softeng is just so new that there haven't been enough lawsuits to stop people from calling themselves an engineer. From [wikipedia](http://en.wikipedia.org/wiki/Software_engineer#Regulatory_classification_2) (and my education) it sounds like it *is* restricted, just not enforced a lot. People call themselves other kinds of engineers to, but civil engineers are probably more on the offensive. Are you Ontario?\n\nI just called up a friend who did Software Engineering at U of Calgary, graduated 2 years ago and he said he's never heard of any licensing or being able to get a P.Eng license. I was in UBC and we didn't have a Software Engineering problem, but we were told that because there was no need since there was no actually P.Eng Licensing for it. My school was in Ontario, but I'm not in Ontario and I also didn't get my PEng because I don't practice engineering. Few of my classmates do (and I think few of them are getting their designation).\n\nI guess we were taught the same thing as other engineers and I just assumed it was the same throughout Canada. I wonder if that's just because softeng is new or because the PEO is different from other provincial engineering organizations.\n\nU of Calgary's programs [are accredited](http://www.ucalgary.ca/pubs/calendar/current/sc-5-3.html) by the Canadian Information Processing Society, but not the Canadian Engineering Accreditation Board (who has accredited their other engineering programs). Could be an East vs West thing? I think Concordia and McGill had softeng before Waterloo. Quebec engineers won their lawsuit against Microsoft calling their developer software engineers [source](https://docs.google.com/viewer?a=v&amp;q=cache:m2igAQi9rccJ:www.peo.on.ca/enforcement/Quebec_MS_April2004.pdf+&amp;hl=en&amp;gl=ca&amp;pid=bl&amp;srcid=ADGEESjiu6xBAAFi-bZhloNs9mSw2JaMXAaCvQnBGrJo8C7BP_BiySNUvC7gTb7MPyqhgTNFanBrrvjyifT8qtL3YstRZBKRnALC__J6N0xen2HZEy67pXR66pLmGFcC4MbWWatujQRu&amp;sig=AHIEtbTplnhbxM4qC_7FdsIyeurRMy87DA), so maybe they just have enough SE PEngs to do something about it. Yea, there are accreditations floating around, but there is still a very large school of people who disagree with the idea of making Software Engineering a Professional Engineering discipline.\n\nI was shocked to find out they already have started in Ontario, I think the numbers I found was 525 licensed Software Engineers which struck me as very low, but I guess in reality nobody is really looking for it, so nobody really feels the need to get it other than for bragging rights. My program is barely a decade old, so it's likely that softeng is just so new that there haven't been enough lawsuits to stop people from calling themselves an engineer. From [wikipedia](http://en.wikipedia.org/wiki/Software_engineer#Regulatory_classification_2) (and my education) it sounds like it *is* restricted, just not enforced a lot. People call themselves other kinds of engineers to, but civil engineers are probably more on the offensive. I'm in Ontario and my job title is \"Design Engineer\". I do not have a P.Eng. nor do I have an engineering degree (I have a CS degree).\n\nMore specifically to \"Software Engineering\", Google in Ontario hires \"Software Engineers\" without reference to any professional designation. The title of \"Software Engineer\" is not currently regulated in Ontario.\n\n Please report yourself to the [PEO](http://www.peo.on.ca/). ;)\n\nBut to be serious: through the Professional Engineers Act, the PEO has a government-granted monopoly on the title engineer (I believe this is similar to the title physician).\n\n&gt; Individuals may not call themselves a professional engineer, an engineer, a P.Eng., or use any similar title that may lead to the belief that they are qualified to practise professional engineering, unless they are licensed by PEO.\n\nThey taught us scare stories about a guy who was successfully sued for $50k for using the term engineer without an official designation.\n\nMany US-based companies continue to use Software Engineer for their Canadian employees because there's not much similar protection in the US (I believe it's constrained to a few states).\n\nAlso, just because everyone's doing it doesn't mean it's correct, just that it's not enforced. I think it's perfectly correct to call what I do \"software engineering\" (much like one can be a \"railway engineer\");it's different from professional engineering. As is mentioned elsewhere, there is no accepted best practices in SE. Get 10 SEs into a room and you'll get 10 sets of best practices.\n\nLooking at the PEO website, it would seem that this matter is far from black-and-white:\n\nhttp://www.peo.on.ca/enforcement/Software_engineering_page.html\n\nIf they were completely in the legal right, the page above would be *much* more forceful.  CS courses should teach theory.  It would be a complete and total waste for schools to start teaching kids how to write code.  The only way to pick up these skills is by actually going out and doing it.  You can't be lectured into writing good code.\n\nIMO, the ideal curriculum would be almost entirely open-ended projects.  Present the student with the theory and let them figure out how to apply it on their own.  They can get their practical experience that way. One thing I learned from teaching people how to program is that most people do not have very good logic. You can try and teach logic, but keep in mind that there are many who are simply not capable of critical thought. I don't believe anyone isn't capable but it's not like teaching a subject matter, it's a way of thinking. It needs to be taught throughout the educational system starting early.  It's just so hard though! I had a digital circuits class earlier today, and I spend a good amount of time explaining to a guy how I got AB'+BC+BD from \"The function returns 1 when A = 1 provided B = 0 or when B = 1 provided either C or D = 1.\" He thought I was a wizard or something. To be fair, the more literal translation is AB'+B(C+D), so I can see being *momentarily* confused. While that is correct, in the class we were specifically looking at sums of products. I tried simplifying something similar to that and the professor told me a Chinese proverb about a guy drawing a snake. It's just so hard though! I had a digital circuits class earlier today, and I spend a good amount of time explaining to a guy how I got AB'+BC+BD from \"The function returns 1 when A = 1 provided B = 0 or when B = 1 provided either C or D = 1.\" He thought I was a wizard or something. Non-coder here (I got lost on the way to /r/math). I didn't get that until I realized that B' = complement of B.\n\nYou sure it wasn't a notational thing? It's just so hard though! I had a digital circuits class earlier today, and I spend a good amount of time explaining to a guy how I got AB'+BC+BD from \"The function returns 1 when A = 1 provided B = 0 or when B = 1 provided either C or D = 1.\" He thought I was a wizard or something. But that returns 2 when B=C=D=1 which contradicts the statement? Maybe this is something in circuits that I don't know...\n\nOr maybe \"either or\" is ambiguous. It's boolean logic, not decimal. + is OR, 0,1 is false,true. I don't believe anyone isn't capable but it's not like teaching a subject matter, it's a way of thinking. It needs to be taught throughout the educational system starting early.  You people and your double negative logic. You people and your double negative logic. I think some of us tend to think in NOTs, NANDs, and NORs :) One thing I learned from teaching people how to program is that most people do not have very good logic. You can try and teach logic, but keep in mind that there are many who are simply not capable of critical thought. One thing I learned from teaching people how to program is that most people do not have very good logic. You can try and teach logic, but keep in mind that there are many who are simply not capable of critical thought. Don't know what your teaching experience is but I teach high school ap computer science and if a kid has passed algebra or precalc (prerequisite for my course), over the course of a year I can definitely teach them enough logic and computational thinking to succeed in a college level intro programming class. Even the ones who aren't math people and are totally lost in the beginning can be taught to think like a good programmer. It's usually a lack of persistence and bad study skills that gets in the way, not some innate inability that cs people have that the larger population doesn't. CS courses should teach theory.  It would be a complete and total waste for schools to start teaching kids how to write code.  The only way to pick up these skills is by actually going out and doing it.  You can't be lectured into writing good code.\n\nIMO, the ideal curriculum would be almost entirely open-ended projects.  Present the student with the theory and let them figure out how to apply it on their own.  They can get their practical experience that way. CS courses should teach theory.  It would be a complete and total waste for schools to start teaching kids how to write code.  The only way to pick up these skills is by actually going out and doing it.  You can't be lectured into writing good code.\n\nIMO, the ideal curriculum would be almost entirely open-ended projects.  Present the student with the theory and let them figure out how to apply it on their own.  They can get their practical experience that way. CS courses should teach theory.  It would be a complete and total waste for schools to start teaching kids how to write code.  The only way to pick up these skills is by actually going out and doing it.  You can't be lectured into writing good code.\n\nIMO, the ideal curriculum would be almost entirely open-ended projects.  Present the student with the theory and let them figure out how to apply it on their own.  They can get their practical experience that way. CS courses should teach theory.  It would be a complete and total waste for schools to start teaching kids how to write code.  The only way to pick up these skills is by actually going out and doing it.  You can't be lectured into writing good code.\n\nIMO, the ideal curriculum would be almost entirely open-ended projects.  Present the student with the theory and let them figure out how to apply it on their own.  They can get their practical experience that way. What are some good books that you have seen on theory? * CLRS     [Vast tome of CS algo knowledge, but starting to show its age],\n* Sedgewick and Flajolet's intro to algo analysis book      [little bit old, but still very relevant for being able to analyze algorithms],\n* Vazirani's (or Williamson's) approximation algorithms book     [both are relatively new, but great books on the subject. all hail the primal-dual method],\n* Sipser's automata book    [classic text that covers classic material in an illuminating and straightforward fashion],\n* Barak and Arora's complexity book    [great update to Papadimitriou's CC book. Good sketches of proofs help in understanding the actual proofs]\n\nIf you want to work more on the theory side of things, those books will take you a looooong way.\n I don't advocate one book being enough, and I would also suggest:\n\n* Algorithm Design - Kleinberg Tardos\n* Algorithm Design - Goodrich &amp; Tamassia\n* The Nature of Computation - Moore &amp; Mertens I'm reading KlenbergTardos at the moment is good I'm reading KlenbergTardos at the moment is good * CLRS     [Vast tome of CS algo knowledge, but starting to show its age],\n* Sedgewick and Flajolet's intro to algo analysis book      [little bit old, but still very relevant for being able to analyze algorithms],\n* Vazirani's (or Williamson's) approximation algorithms book     [both are relatively new, but great books on the subject. all hail the primal-dual method],\n* Sipser's automata book    [classic text that covers classic material in an illuminating and straightforward fashion],\n* Barak and Arora's complexity book    [great update to Papadimitriou's CC book. Good sketches of proofs help in understanding the actual proofs]\n\nIf you want to work more on the theory side of things, those books will take you a looooong way.\n They came out with a new edition of CLRS - not sure if that was enough to stop it from showing its age, though, as I haven't gotten my mitts on it. What are some good books that you have seen on theory? What are some good books that you have seen on theory? CS courses should teach theory.  It would be a complete and total waste for schools to start teaching kids how to write code.  The only way to pick up these skills is by actually going out and doing it.  You can't be lectured into writing good code.\n\nIMO, the ideal curriculum would be almost entirely open-ended projects.  Present the student with the theory and let them figure out how to apply it on their own.  They can get their practical experience that way. CS courses should teach theory.  It would be a complete and total waste for schools to start teaching kids how to write code.  The only way to pick up these skills is by actually going out and doing it.  You can't be lectured into writing good code.\n\nIMO, the ideal curriculum would be almost entirely open-ended projects.  Present the student with the theory and let them figure out how to apply it on their own.  They can get their practical experience that way. CS courses should teach theory.  It would be a complete and total waste for schools to start teaching kids how to write code.  The only way to pick up these skills is by actually going out and doing it.  You can't be lectured into writing good code.\n\nIMO, the ideal curriculum would be almost entirely open-ended projects.  Present the student with the theory and let them figure out how to apply it on their own.  They can get their practical experience that way. It's funny in my class right now(java one), the teacher is horrible. Allthough she does make people work on projects from a book, the concepts are never properly explained. I ended up writing a program for a student cause he was completely lost in what to do(I know, shouldn't do that). Also it seems most computer science students in my area are getting dumber and dumber each year, since most students just try spending 2 minutes thinking about the problem, then give up cause their brain hurts...:( &gt; Also it seems most computer science students in my area are getting dumber and dumber each year\n\nPerhaps it's different elsewhere, but from what I've observed here the people with highly technical minds go into math, physics, or some kind of engineering. \n\nCS kind of gets the left-overs.  You do realize that you just insulted everyone in this subreddit, right? &gt; Also it seems most computer science students in my area are getting dumber and dumber each year\n\nPerhaps it's different elsewhere, but from what I've observed here the people with highly technical minds go into math, physics, or some kind of engineering. \n\nCS kind of gets the left-overs.  [deleted]  There should be a separate Software Engineering degree. There should be a separate Software Engineering degree. Not sure if you're being sarcastic, but in Canada, there is. There is in the US too. Not sure if you're being sarcastic, but in Canada, there is. There should be a separate Software Engineering degree. There should be a separate Software Engineering degree. Many schools already do this.\n\nHowever, I still see Software \"Engineering\" (what a joke!) as mostly a subfield of and outgrowth from CS theory.  If you don't know that theory, you're going to fall flat on your face in an SE degree. Universities apply this model already. Math Education degrees require a selection of courses from a typical math degree, as well courses on teaching.\n\nI feel like a pure software engineering course without computer science isn't a bad idea, but it seems more appropriate at a trade school than at a university. Many schools already do this.\n\nHowever, I still see Software \"Engineering\" (what a joke!) as mostly a subfield of and outgrowth from CS theory.  If you don't know that theory, you're going to fall flat on your face in an SE degree. There should be a separate Software Engineering degree. There should be a separate Software Engineering degree. I believe most good University systems offer a software engineering course.\n\nThe problem is that they are often seen, and sometimes run, as a CS-lite. A version for beginners, which concentrate on some more practical issues and practical technologies, whilst sacrificing a lot of the real theory students should be taught.\n\nWhether that is true or not, I don't know. However that is the perception out there. Is this in the US? My Canadian SE program was certainly not for beginners. But it was also an accredited engineering program.\n\nI'm curious if there's a large divide between the way software engineering is perceived in different countries. (Or if it's the typical SE vs CS rivalry nonsense where each one thinks their better than the other.) Which school? I believe most good University systems offer a software engineering course.\n\nThe problem is that they are often seen, and sometimes run, as a CS-lite. A version for beginners, which concentrate on some more practical issues and practical technologies, whilst sacrificing a lot of the real theory students should be taught.\n\nWhether that is true or not, I don't know. However that is the perception out there. There should be a separate Software Engineering degree. There should be a separate Software Engineering degree. Sorry, but coding is not engineering.  There may be a day when it is so, but coding is mostly about linguistics.  Semantics. CS is theoretical, by saying \"software engineering\" im referring to the practical side, the implementation side. It's a fairly standard term and most people know what it means. Semantics is part of linguistics.&lt;/troll&gt;\n\nI've never liked the term 'engineering' attached to software, there's an implication of determinism that is lacking and should not be even a little bit emphasized. \n\nDeveloping and Developer, or programming/programmer are my preferred designation.   Determinism? Everything a developer or programmer or whatever you want to call it does is inherently deterministic. I don't know if you think \"engineering\" is somehow mundane or something, but it's a perfectly acceptable description of what the day to day of designing and writing software entails.  Sorry, but coding is not engineering.  There may be a day when it is so, but coding is mostly about linguistics.   I think that CS programs should be 90% theory, with coding incorporated regularly, but only to prove that the students actually know how to implement what they're learning.\n\nA mathematical background is absolutely essential. Learning about different algorithms and various ways of thinking to solve many different types of problems is what makes us good at what we do.\n\nThe knowledge is sort of useless if we can't use it, though, and that's where programming comes in. I think that we should know how different algorithms work, and we should be able to solve these with code. Coding is not difficult; the *ideas* are the hard part, and that's what we should be focused on. I think what's missing from your breakdown is the skill of taking \"requirements and domain specific knowledge\" and creating a good architecture/design. I don't think that traditionally falls into the category of computer science, but you also don't get it from standard \"programming assignments\" in school (like implementing algorithms and stuff). \n\nI could be wrong, but I think most CS graduates who are in the \"can't program\" boat probably can program fine if they're given very specific requirements (although this is just a guess). eg. Like if you asked them to implement an algorithm/data structure, they'd be able to, but if you asked them to write a whole application (maybe a text editor) they would have much more trouble.\n\n &gt; I think what's missing from your breakdown is the skill of taking \"requirements and domain specific knowledge\" and creating a good architecture/design.\n\nThis is **not** CS.  I think what's missing from your breakdown is the skill of taking \"requirements and domain specific knowledge\" and creating a good architecture/design. I don't think that traditionally falls into the category of computer science, but you also don't get it from standard \"programming assignments\" in school (like implementing algorithms and stuff). \n\nI could be wrong, but I think most CS graduates who are in the \"can't program\" boat probably can program fine if they're given very specific requirements (although this is just a guess). eg. Like if you asked them to implement an algorithm/data structure, they'd be able to, but if you asked them to write a whole application (maybe a text editor) they would have much more trouble.\n\n I've heard plenty of stories about interviewers asking people to implement simple things like FizzBuzz and the fibonacci sequence and failing. I think that's pretty sad. That's why we absolutely need to tell people how to apply what they learn. Theory is good, but if you can't link it to its application, it's a huge waste of time. Precisely why I do not think coding should be cut out entirely. The ideal program, I think, would consist of class periods spent learning theoretical ideas, and projects/labs for homework that involve implementing those ideas in a programming language of the student's choice (this way, they care more about what they're learning, not forced to conform to any particular standard, and will probably learn it better).\n\nPlease note that I did not suggest completely cutting out programming; obviously it's important. Many people, though, can't even write up psuedocode for FizzBuzz and the fibonacci sequence, because they don't understand the ideas!\n\nThis is what I'm saying. I worked as a programmer before I took a single CS course, and that's how I learned my way around. But that wasn't very hard; knowing the basics about (most) programming languages is relatively simple.\n\nIf I were an interviewer for a job, I would not expect every applicant to be completely fluent in our language of choice -- and that is why I'm a strong advocate for theoretical knowledge as opposed to knowing the ins and outs of Java really well. Knowing Java won't help you. Knowing the ideas that are implementable by programming will.\n\nSo, as I said, say we're using C#, and I want some potential employee to show me that he understands how programming works by writing a simple factorial function, recursively and iteratively. He can't write it in C#, but he can write it in Java, or psuedocode; this is enough for me.\n\nThe problem lies in those people who actually spend so much time focusing on programming languages themselves that they actually never get to experience true problem solving. This has mildly been the case in my CS program so far. For me, this is frustrating, because writing code is *not* the hard part of programming. Coming up with a solution that works is the hard part of programming. And this is what I would like universities to focus on.  You know, writing code is easy. Solving problems, understanding algorithms, understanding data structures and how to apply them. These are the important things. You know, writing code is easy. Solving problems, understanding algorithms, understanding data structures and how to apply them. These are the important things.   Computer science is SCIENCE, or alternatively \"math nerds.\" Programming is a TRADE. What about software engineering, or those who want to apply their knowledge at work? Software engineering is to programming as mechanical engineering is to automotive mechanic.\n\nSure, mechanics and programmers have great practical knowledge. But, the odds are that they aren't going to be making *new* things, especially the former. They know how to make things work, and how to fix issues, but they're not heading big projects or doing serious reworkings of the architecture, because that's not what they're for. Physicists and Computer Scientists are different again, because rather than doing industrial work, they are writing papers on the degradation of high-carbon alloys at high-temperature, high-pressure environments, or on the theoretical backing of a practical Oracle Machine and its use as a metaphor for the human brain. \n\nIn essence, scientists do theory, engineers do application, and technicians do implementation. Every single one of them is important to the whole process, but unlike physicists, mechanical engineers, and automotive technicians, most colleges don't differentiate between computer scientists, software engineers, and programmers. That's the primary problem here, and the article completely ignores it.      There should either be two program tracks within the degree or two degrees. Whichever path is taken, two curricula make sense.\n\nMy university didn't officially have a split like that, but there were elective courses that could take you in one direction or the other. I decided that I wanted to work in C++ when I got out of school, so I took a lot of courses that were heavy on using that language. You can't just take arbitrary classes in the major and expect to be well-suited to a particular job; you've got to tailor your degree to achieve the desired results.\n\nAs a separate point, in the higher-level courses, we were given theory lectures, then we were asked to write an implementation of the theory and write reports on what we learned. This strikes me as a good approach. If you're an upperclassman that can't take a text description of a problem and some lecture notes on the math/logic involved and write a program to do it, you don't have any business passing the course.\n\n&gt; Which ~~begs~~ raises the question\n\nThat kind of hurts my opinion of the article. There should either be two program tracks within the degree or two degrees. Whichever path is taken, two curricula make sense.\n\nMy university didn't officially have a split like that, but there were elective courses that could take you in one direction or the other. I decided that I wanted to work in C++ when I got out of school, so I took a lot of courses that were heavy on using that language. You can't just take arbitrary classes in the major and expect to be well-suited to a particular job; you've got to tailor your degree to achieve the desired results.\n\nAs a separate point, in the higher-level courses, we were given theory lectures, then we were asked to write an implementation of the theory and write reports on what we learned. This strikes me as a good approach. If you're an upperclassman that can't take a text description of a problem and some lecture notes on the math/logic involved and write a program to do it, you don't have any business passing the course.\n\n&gt; Which ~~begs~~ raises the question\n\nThat kind of hurts my opinion of the article. If you're taking classes to get suited to a particular job, a university is the wrong place to be. What you want is a trade school. Universities provide an education. Trade schools provide skills.\n\nPre-med is a bio/chem/physiology-heavy education curriculum at a university. Medical School is a trade school (albeit a very prestigious sort). If I thought that a trade school would give me the same depth of education that a university would at a lower price and with the same probability of scoring a job when I got out, that would've been an option.\n\nBased on how things turned out for me, if a university was \"the wrong place to be\", then I don't want to be \"right\". If university got you a job, I am very happy for you. However, a trade school has always been a faster and more assured path to a job doing what you were educated to do. Doubly so with unemployment soaring well into the double digits. &gt;more assured path to a job\n\nI'm not convinced of that. Faster and cheaper, I'll believe. From what I saw when I was looking, a 4-year degree opens more doors.\n\nI'm not saying that a trade school is never the answer; what I was looking for was a larger education, a good name on the degree, and a path for future learning.\n\nIf I didn't have the resources to attend a 4-year university, if I needed to get out of school quicker, or if I was purely going to get a job (rather than gain a richer understanding of the world and branch out into other subjects), a university wouldn't've made sense. I can agree with that. I'm graduating from a technical school, then going to university (it works like this here), and the internships are completely different. They take university graduates for programming jobs, and tech school graduates for unpaid internships if there's room left. Good luck on the job hunt. Finding your first real job is generally a challenge, regardless of what level you're coming in at.   Any undergraduate worth his salt will learn to become a decent programmer (at least by graduate levels), as a side effect of their degree. That isn't to say it's all about programming, it's to say they become good at using the tools they are given.\n\nMany undergraduates are bad programmers simply because they don't program.\n\nHowever, I do feel that CS gets a little wrapped up in it's mental models, cognitive frameworks, and abstract theory used to represent anything.\n\nThere was an American engineer during the 60s, who used engineering models to represent nature, and show how feedback was present through out the natural science. If you burn down a forest, it grows back, due to feedback. This became the defacto view for many scientists at the time.\n\nResearch later, based on collecting vast amounts of minute data in the natural world, showed that to be entirely wrong. The problem is that the abstract models used were too simplistic, and too abstract; they didn't really represent the real world.\n\nI also hate the number of pieces of research I see, which has some really cool ideas, but the implementation sucks. Looks which have their place in Windows 95, and terrible user interfaces. I feel too many academics care too little about the details. Ok, that's not the point of the research, but it does have a real effect. About your last paragraph: researchers aren't paid to make user friendly work. Conference papers are money in CS, and a paper submission isn't helped much by a good user interface. \n\nIf a professor does decide they want to sell some of the software they make, the university often takes a large enough cut that it isn't worth the additional work of making the research software into commercial grade software. Well I never said about them selling it to make money, or even polishing it enough to be a full product.\n\nI just don't like the idea that academia has to also mean low quality, or not caring at all about the quality of the implementation. A little polish can go a long way in helping to engage people about new ideas and concepts.   [deleted]     I am really hesitant to post this, because there is inevitably going to be someone who believes the whole concept that I am about to lay out as silly.  There is also plenty of room for clarification.\n\nI am currently trying to get a degree in CS, and I feel as though I come at it from a different perspective compared to many CS majors.\n\nI have been messing with computer programming for many years, and while I am not an amazing coder, I am fairly competent.  My main problem is that I never had a strong math background, which is the main failure point in my studies.  It is not that I do not understand mathematical concepts, because computers are mathematical in nature.\n\nRather, I have a hard time with the language of math.  I know this because I have no problem with logic tables, and have a firm grasp of binary, octal and hexadecimal numbers.  Also, I have never had too much difficulty picking up most of the programming languages that I have learned so far.\n\nSo far, I have reached the level of calculus, and it has been a struggle the whole way through.  To me, math is like speaking Romaji (Japanese), where something like C\\C++ is like speaking my native English.  I understand many of the concepts, but find it difficult to translate; to me it seems they are using the same symbols, but their structures are not the same.\n\nI don't exactly know where I am going with this, I just want to point out that you can be a good programmer and not necessarily seem good at math (even if you are, albeit in a different way).  For example, I have long understood the idea of oh notation when it comes to how I choose to write code, in fact it is almost natural to me.  But I cannot for the life of me seem to translate that understanding into actual mathematical terms without a huge struggle.\n\nI think that is long enough, if someone wants to ask for me to elaborate on something, please point it out.\n\nTL;DR: The language of mathematics =/= computer programming languages. Sounds like you understand concrete concepts in programming, but not abstract concepts in math. For many people this is a hard barrier to cross, but when you become a senior programmer, you will need to handle abstraction to architect and understand large systems.\n\nYou could try [this alligator game](http://worrydream.com/#!/AlligatorEggs) and see if you understand lambda calculus better. Maybe your frustration is just a product of poor/ineffective teaching methods. Not everyone learns the same way, so if you're not everyone and not a good self-learner, you get left behind.\n\nYou might also try Khan Academy's math stuff. I haven't looked at much of it, but he did a better job than my university stats prof.\n\nAlso, out of curiosity: Have you worked much with other people's large code bases? I am capable of abstract thinking.  In fact I was the top of my class in electronics A-school for the Navy.  My issue lies with the actual syntax that math is presented in.  I do not struggle with understanding what is going on and why things happen in the underlying concepts, I struggle with remembering how to write it down because there is so little linguistic consistency, (I mean that how you write something that falls within one area of mathematics differs how you write something that falls in another).  If I wrote the sentence:\n  The Fox(brown) + quick --&gt; jumped/(lazy dog)\nyou might understand what I meant, but the way that it is written is horrible even if you can make sense of it.\n\nI don't think that I am the only one who has a problem that arises solely out of the linguistics.  Math itself is so amazing and precise, yet I feel the language that is used to present it is clunky and lacks ergonomics.  That's what happens when you have thousands of years of researchers, with each person having to create how to represent the new concepts as they are discovered.\n\nI have never seen the alligator game, so I am going over it now (although calculus itself is not that difficult, I have far more problems with what interfaces with it).\n\nI am actually a bit of a Khan Academy hipster, I watched his Algebra videos back when he only had a few thousand subscribers.  Thank you for that tip though.\n\nAs far as \"working with other people's large code bases,\" I don't exactly know what you are asking.  If you mean actually being a contributing member to a project, then no, I have never done that.  I don't know why, I've never tried, and I frankly don't know where to start.  I am also a bit afraid of screwing up someone else's code due to a lack of formal experience.\n\nIf by \"working with other people's code bases\" you mean digging through source code from open source projects, I have done plenty of that.  If you mean simply compiling and using libraries, I can do that as well.\n\nBut I have always been unsure if I should actually complete a CS degree, even though I enjoy the challenge of programming as a hobby.  Many of them seem to know so very little about computers, but they have no problem with math.  Honestly, I wrote another rant here, but deleted it because one rant is enough for a comment.  That, and I don't think it fits with the discussion at hand.\n\nTL;DR: I am unsure if I even answered your question.  I'm a few months away from my BS in CS and the big saving grace at my school is lab work. We have a small in house network for Ciscio classes, set up virtual machines for Windows Server and play with different distros in Linux If you are taking Cisco classes and setting up Windows VMs, what you are learning is not CS. If you are taking Cisco classes and setting up Windows VMs, what you are learning is not CS. I wondered why this subreddit was so confusing at times to me.\n\nI will now slowly slink away and try to pretend this didn't happen I wondered why this subreddit was so confusing at times to me.\n\nI will now slowly slink away and try to pretend this didn't happen  From the article, “It’s widely understood that computer science doesn’t necessarily make you a good programmer,” Kreger says. “It gives you theoretical knowledge, but because there are so many topics, you skim through everything quickly, and a lot of people come out not being very good programmers.”  This quote is arguable the most important part of the article.  I've not yet been to college or a CS program, but if it is anything like high school, I imagine that a lot of topics either get forgotten over time, or not learned well enough.  A good program should focus on both the theories and soft skills, and some of the practical side of things to make sure students are picking up on concepts and know how to apply them.",
    "url" : "http://blog.smartbear.com/software-quality/bid/230121/math-nerds-vs-code-monkeys-should-computer-science-classes-be-more-practical"
  }, {
    "id" : 43,
    "title" : "Know of any HCI articles on social media / Reddit and fighting trolls?",
    "snippet" : "I have been browsing through the ACM Digital Library and no luck.\n\nI am looking for any journal articles on preventing and fighting against trolls online.  I was wondering if you knew of any articles that study social media and fighting trolls such as on Reddit and voting.\n\nThanks    I believe that trolls turn to stone when exposed to sunlight.\n\n(Sorry couldn't resist) Not only is this off-topic, it's also not in line with the typical fantasy rules. Perhaps you're thinking of gargoyles? Not only is this off-topic, it's also not in line with the typical fantasy rules. Perhaps you're thinking of gargoyles?",
    "url" : "http://www.reddit.com/r/compsci/comments/11cd0u/know_of_any_hci_articles_on_social_media_reddit/"
  }, {
    "id" : 44,
    "title" : "What is the practical ramification of using Silverman's rule of thumb with a non-Gaussian kernel?",
    "snippet" : "[Silverman's rule of thumb](http://en.wikipedia.org/wiki/Kernel_density_estimation#Practical_estimation_of_the_bandwidth) is only valid for a Gaussian kernel.  This is unfortunate, because the Gaussian kernel isn't really the fastest kernel to calculate (compared to a triangular or quartic kernel, for instance).  \n\n\nI realize that there are other ways to estimate bandwidth for a Kernel Density Estimator, but:\n\n\n* The rule of thumb is easy and convenient.\n* KDE isn't really a very exacting technique to begin with.\n* KDE with large data sets can spend most of its computational time in the kernel algorithm, so there's sometimes good reason to prefer a simple kernel.\n\n\nSo what happens when you estimate bandwidth with Silverman's rule of thumb, using a different but not-unlike-Gaussian kernel?  Do you just get a somewhat poorer than optimal estimation, or is it possible for the estimation to come out terribly?  \n\nBetter yet, does anyone know of a pathological case where substitution of a Gaussian kernel for something else \"Gaussian like\" (i.e. a kernel that someone might actually use and having the approximate shape of a Gaussian as opposed to a kernel consisting of uniform random noise or a kernel with an offset peak) results in a bad estimation?\n\n\n\n  ",
    "url" : "http://www.reddit.com/r/compsci/comments/11bkq2/what_is_the_practical_ramification_of_using/"
  }, {
    "id" : 45,
    "title" : "Computers from the ground up, from transistors to assembly code. Any good books to learn how a computer actually, physically operates (for someone with a background in programming and math)",
    "snippet" : "I went to school for maths, and have since been focussing on programming and hope to pursue my career in this area.\n\nMy math training is a huge advantage when it comes to internalizing algorithms and abstract ideas like classes and boolean algebra and so on (not to mention exciting new concepts like hypergraph databases: just seeing those two words together gives me a massive mind-boner).\n\nBut that's all abstract stuff. **I've no idea what my computer -- that physical, tangible hunk of metal that sits on my desk -- is actually doing.**\n\nPretty much everything that happens up until we have an assembly language is magic to me. I'm a hardware dunce. \n\nHow can a physical object tell itself to manipulate \"data\"? How does me pressing a bunch of plastic buttons right now make that big glowing box in front of me assume patterns that resemble letters and words? Where and how are those patterns stored and transmitted in the physical world? *Is the internet actually a series of tubes??* \n\nIf anyone can recommend a few books that explain how a computer turns electrically charged matter into streaming videos of naked ladies and apps that make fart noises, I'd be eternally grateful. \n\nI don't want to stray from my studies of programming too much, I'm not looking for a career in electrical engineering; but I'd like to have some general understanding of how this  magic hunk of metal that I work with all day, every day, actually operates.   I really like the book [The Elements of Computing Systems](http://www.nand2tetris.org/book.php). Most of the chapters are free on the author's website, http://www1.idc.ac.il/tecs/plan.html\n\nThe book starts out talking about logic circuit and how to build the logic for a CPU. Then assembly language, programming language, and a basic operating system. I really like the book [The Elements of Computing Systems](http://www.nand2tetris.org/book.php). Most of the chapters are free on the author's website, http://www1.idc.ac.il/tecs/plan.html\n\nThe book starts out talking about logic circuit and how to build the logic for a CPU. Then assembly language, programming language, and a basic operating system.  [Code by Charles Petzold.](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?ie=UTF8&amp;qid=1349913644&amp;sr=8-1&amp;keywords=code+charles+petzold)\n\nI wish there was a better way for me to convey this book is *exactly* what you are looking for. After I read it I felt so empowered knowing that I finally understood how computers worked. [Code by Charles Petzold.](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?ie=UTF8&amp;qid=1349913644&amp;sr=8-1&amp;keywords=code+charles+petzold)\n\nI wish there was a better way for me to convey this book is *exactly* what you are looking for. After I read it I felt so empowered knowing that I finally understood how computers worked. Correct me if I'm wrong, I don't have the book but by looking at the Contents it seems to me that this book is maybe for a larger audience, and maybe not as detailed, precise and in-depth as The Elements of Computing Systems, which I have. Am I wrong ?  \nI mean, I'm willing to buy it if it can teach me more than The Elements of Computing Systems have, so I'm curious ;). Correct me if I'm wrong, I don't have the book but by looking at the Contents it seems to me that this book is maybe for a larger audience, and maybe not as detailed, precise and in-depth as The Elements of Computing Systems, which I have. Am I wrong ?  \nI mean, I'm willing to buy it if it can teach me more than The Elements of Computing Systems have, so I'm curious ;). You aren't wrong. Given that OP is comfortable with assembly, I think they're going to be a little disappointed with Code. It's a great intro for laymen.  Yes, but it does talk about the hardware aspects of it. You can be proficient at assembly without having any clue about how the processor might work internally. [Code by Charles Petzold.](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?ie=UTF8&amp;qid=1349913644&amp;sr=8-1&amp;keywords=code+charles+petzold)\n\nI wish there was a better way for me to convey this book is *exactly* what you are looking for. After I read it I felt so empowered knowing that I finally understood how computers worked. [Code by Charles Petzold.](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?ie=UTF8&amp;qid=1349913644&amp;sr=8-1&amp;keywords=code+charles+petzold)\n\nI wish there was a better way for me to convey this book is *exactly* what you are looking for. After I read it I felt so empowered knowing that I finally understood how computers worked. [Code by Charles Petzold.](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?ie=UTF8&amp;qid=1349913644&amp;sr=8-1&amp;keywords=code+charles+petzold)\n\nI wish there was a better way for me to convey this book is *exactly* what you are looking for. After I read it I felt so empowered knowing that I finally understood how computers worked. [Code by Charles Petzold.](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?ie=UTF8&amp;qid=1349913644&amp;sr=8-1&amp;keywords=code+charles+petzold)\n\nI wish there was a better way for me to convey this book is *exactly* what you are looking for. After I read it I felt so empowered knowing that I finally understood how computers worked. [Code by Charles Petzold.](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?ie=UTF8&amp;qid=1349913644&amp;sr=8-1&amp;keywords=code+charles+petzold)\n\nI wish there was a better way for me to convey this book is *exactly* what you are looking for. After I read it I felt so empowered knowing that I finally understood how computers worked.  The [Hennessy &amp; Patterson computer architecture book](http://www.amazon.com/Computer-Architecture-Quantitative-Approach-Edition/product-reviews/0123704901/ref=dp_top_cm_cr_acr_txt?ie=UTF8&amp;showViewpoints=1) was the standard when I was going to school for this kind of thing. If your local college offers a digital logic course, I'd recommend taking that as well.\n\nThe internet question would be better addressed by a book about network systems. This is the book currently used in my Intro to Computer Organization class at University of Michigan. The class discusses just about everything from tranisistors to Assembly (and a little C, mostly to simulate assembly code)\n\nRead the entire second chapter for free over at Inkling if you want to check it out:\n\nhttps://www.inkling.com/store/book/computer-architecture-hennessy-5th/ The [Hennessy &amp; Patterson computer architecture book](http://www.amazon.com/Computer-Architecture-Quantitative-Approach-Edition/product-reviews/0123704901/ref=dp_top_cm_cr_acr_txt?ie=UTF8&amp;showViewpoints=1) was the standard when I was going to school for this kind of thing. If your local college offers a digital logic course, I'd recommend taking that as well.\n\nThe internet question would be better addressed by a book about network systems. I actually came to this subreddit to find out about this book. My friend found a copy of the second edition for $0.29 and it looks interesting. Would there still be some relevant information in the second edition?  There's an open course called From [Nand2Tetris](http://www.nand2tetris.org/) that might interest you. It was posted over in /r/programming a little while back, I think. Comments there suggested it's quality, but of course YMMV. &gt; It was posted over in /r/programming a little while back, I think.\n\nYup, among other subreddits: http://www.reddit.com/r/compsci/search?q=Nand+Tetris&amp;sort=relevance Oh wow, that's a lot of reposts. Well, I hadn't heard of it 'til the most recent progit post, anyway.    Yale Patt has a great freshman level textbook- extremely readable, I went through it in a week, and decided to transfer to his school.\nhttp://www.amazon.com/Introduction-Computing-Systems-Gates-Beyond/dp/0072994657\n\nIf you want to go in depth into how computers work- every part, and a lot of the networking protocols, I highly recommend \"Upgrading and Repairing PCs\", by Scott Mueller. It's a long book, but absolutely worth every page.  Yale Patt has a great freshman level textbook- extremely readable, I went through it in a week, and decided to transfer to his school.\nhttp://www.amazon.com/Introduction-Computing-Systems-Gates-Beyond/dp/0072994657\n\nIf you want to go in depth into how computers work- every part, and a lot of the networking protocols, I highly recommend \"Upgrading and Repairing PCs\", by Scott Mueller. It's a long book, but absolutely worth every page.  Wow thanks! It looks like the two books would complement each other perfectly. Just looking at the content tables I got all giddy :)      With your math background check out the MiT OCW course on Electromagnetics. What does emag have to do with digital logic/computer architecture? My hydro-powered computer broke but it's okay because now I use my electrically powered one.       This may sound kind of silly, but if you are looking for a solid understanding on how everything works, I suggest installing minecraft and building your own computer with redstone.\n\nReading about it is one thing. Building your own is another. The software \"[logic.ly](http://logic.ly)\" would teach the same thing, but without the hassle of Minecraft.   CS, Software Engineering student here. So far the only textbook I have considered keeping is my Computer Architecture class.\n\n[Introduction to Computing Systems: From bits &amp; gates to C &amp; beyond](http://www.amazon.com/Introduction-Computing-Systems-gates-beyond/dp/0072467509/ref=sr_1_1?ie=UTF8&amp;qid=1349980957&amp;sr=8-1&amp;keywords=From+gates+to+C)\n\nIt's a little pricey, but if you look around you might find it cheaper. After reviewing all the suggestions, that's the one I ended up ordering (used for $80 CAD), along with \"Repairing and Upgrading PC's\". They both look real solid.      ",
    "url" : "http://www.reddit.com/r/compsci/comments/11a0cq/computers_from_the_ground_up_from_transistors_to/"
  }, {
    "id" : 46,
    "title" : "Any computer scientists here working in some form of biomedical engineering (or robotics) field; care to discuss their work?",
    "snippet" : "I'm interested to know how you leveraged a classic computer science degree to work in programs that normally accept electrical or mechanical engineers. I'm also interested in hearing what kind of work you are doing.\n\nId like to do biomedical research, specifically working with medical devices or robotics applied to the medical domain. \n\nImaging, AI, and computational geometry are obvious areas where a computer scientist can make contributions to these fields, but I worry that a lack of background in materials science or mechanical engineering could make this unrealistic. Although I plan to try pick up some of the skills in those areas that I am missing by working with Ardunio a bit as I finish up my degree.\n\nThanks.  ",
    "url" : "http://www.reddit.com/r/compsci/comments/11aqrt/any_computer_scientists_here_working_in_some_form/"
  }, {
    "id" : 47,
    "title" : "The \"World Of Programming\" Infographic",
    "snippet" : "  Where is Grace Hopper? Seriously. She wrote COBOL and the first compiler... that's beyond noteworthy. She invented the term “bug” in the context of software. I'd say that's also beyond noteworthy.    &gt; // Vim and Emacs are the two most popular editors amongst \"Real Programmers\"\n\nI'm an Emacs user, but fuck this stupid elitist bullshit.  Seems incomplete. No mention of Lisp.  What was the point or even the context of this graphic?  It seems very arbitrary to me.  Indeed, it seems self-congratulatory more than informative What was the point or even the context of this graphic?  It seems very arbitrary to me.  What was the point or even the context of this graphic?  It seems very arbitrary to me.     No Church? McCarthy? Hopper?\n\nVan Rossum? RMS? really? No Church? McCarthy? Hopper?\n\nVan Rossum? RMS? really? RMS I might disagree (though if one is very strict with regard to the programming topic you are correct), but really a van Rossum instead of a McCarthy and a Hopper? You got to be kidding me... Tell me again what was so novel about Python (apart from the crippled lambda due to ignorance). You misinterpreted my comment.\n\n&gt; No Church? McCarthy? Hopper?\n\nThis is ridiculous that they are missing.\n\n&gt; Van Rossum? RMS? really?\n\nThis is ridiculous that they are included. No, I was heavily agreeing with you though reading my comment again I see that it is quite ambiguous. Sorry about that.    Very neat! A little blunt, but well designed, and a consistent portrait style.\n\nOne thing though...\n\n...needs more [))))))))))))))))))))))))))))))](http://en.wikipedia.org/wiki/John_McCarthy_\\(computer_scientist\\)).       ",
    "url" : "http://media.smashingmagazine.com/wp-content/uploads/2010/06/aboutprogramming04.jpg"
  }, {
    "id" : 48,
    "title" : "Genetic Algorithms vs other Stochastic Algorithms",
    "snippet" : "Alright. I have implemented a GA, a SA, and a RR to solve the same problem. It's time to see who wins. But...how? Help me out:\n\nThe three stochastic algorithms are each run 30 times, each time with a different random seed. They are optimization algorithms and they each spit out the quality of the solution they found at the end of each run. So there are 3 sets of 30 numbers.\n\nNow don't fret over runtime complexity or any of that crap. They are all solving the same problem, which has a very very large solution space. None of them have any hope of finding the global optima. So they all terminate after the same number of evaluation operators. They run roughly the same time.\n\nHow do you compare these algorithms? Just compare the average of their solutions? What about statistical significance? How do you know which algorithm is the best? How do we rank em?  Just plot the 3 distribution curves together on the same chart (with different colors) and stare at that shit until you feel like getting a beer.  You earned it. You could save your eyes and use Wilcoxon tests for each pair then compare medians for each of the significant pairs. Assuming all are significant for whatever magic p-value you pick you can put them into an ordering.  GA - Genetic Algorithm,\nSA - Simulated Annealing,\nRR - ? GA - Genetic Algorithm,\nSA - Simulated Annealing,\nRR - ? GA - Genetic Algorithm,\nSA - Simulated Annealing,\nRR - ?    So, which algorithm won?",
    "url" : "http://www.reddit.com/r/compsci/comments/11aobc/genetic_algorithms_vs_other_stochastic_algorithms/"
  }, {
    "id" : 49,
    "title" : "SmoothLife - a continuous version of Conway's Game of Life",
    "snippet" : "  Here's the information from the [video page](http://www.youtube.com/watch?v=KJe9H6qS82I):\n\nSmoothLife is a family of rules created by Stephan Rafler. It was designed as a continuous version of Conway's Game of Life - using floating point values instead of integers. This rule is SmoothLifeL which supports many interesting phenomena such as gliders that can travel in any direction, rotating pairs of gliders, wickstretchers and the appearance of elastic tension in the 'cords' that join the blobs.\n\nhttp://sourceforge.net/projects/smoothlife/\n\nPaper describing SmoothLife and SmoothLifeL: http://arxiv.org/abs/1111.1567\n\nThis animation created in Ready: http://code.google.com/p/reaction-diffusion\n\nMore videos: https://www.youtube.com/playlist?list=PL69EDA11384365494&amp;feature=plcp\n\nDetails:\n\n* Outer radius: 20.0\n* Radius ratio: 3.0\n* Birth range: 0.257 - 0.336\n* Survival range: 0.365 - 0.549\n* Alpha_n (outer sigmoid width): 0.028\n* Alpha_m (inner sigmoid width): 0.147\n* Timestep: 0.05 (Euler integration)\n* Frames: 10819 (saving 1 image per 8 timesteps)\n* Total time units elapsed: 4327.6\n* World size: 512x512, with toroidal wrap-around.\n* Color representation: linear mapping of [0,1] onto [black,white]\n* OpenCL source code: http://reaction-diffusion.googlecode.com/svn/trunk/Ready/Patterns/smoothlife_smoothlifeL.vti\n* Computation time: 74 minutes on an nVidia GeForce GTX 460 (in Ready, Stephan's software at the sourceforge link is much faster)\n Here's the information from the [video page](http://www.youtube.com/watch?v=KJe9H6qS82I):\n\nSmoothLife is a family of rules created by Stephan Rafler. It was designed as a continuous version of Conway's Game of Life - using floating point values instead of integers. This rule is SmoothLifeL which supports many interesting phenomena such as gliders that can travel in any direction, rotating pairs of gliders, wickstretchers and the appearance of elastic tension in the 'cords' that join the blobs.\n\nhttp://sourceforge.net/projects/smoothlife/\n\nPaper describing SmoothLife and SmoothLifeL: http://arxiv.org/abs/1111.1567\n\nThis animation created in Ready: http://code.google.com/p/reaction-diffusion\n\nMore videos: https://www.youtube.com/playlist?list=PL69EDA11384365494&amp;feature=plcp\n\nDetails:\n\n* Outer radius: 20.0\n* Radius ratio: 3.0\n* Birth range: 0.257 - 0.336\n* Survival range: 0.365 - 0.549\n* Alpha_n (outer sigmoid width): 0.028\n* Alpha_m (inner sigmoid width): 0.147\n* Timestep: 0.05 (Euler integration)\n* Frames: 10819 (saving 1 image per 8 timesteps)\n* Total time units elapsed: 4327.6\n* World size: 512x512, with toroidal wrap-around.\n* Color representation: linear mapping of [0,1] onto [black,white]\n* OpenCL source code: http://reaction-diffusion.googlecode.com/svn/trunk/Ready/Patterns/smoothlife_smoothlifeL.vti\n* Computation time: 74 minutes on an nVidia GeForce GTX 460 (in Ready, Stephan's software at the sourceforge link is much faster)\n anyone else having trouble getting the program to run? It gives me errors in the log file:\n\n&gt;Vertex shader was successfully compiled to run on hardware.\n\n&gt;error in vertex shader!\n\n&gt;Fragment shader was successfully compiled to run on hardware.\n\n&gt;error in fragment shader!\n\n&gt;Vertex shader(s) linked, fragment shader(s) linked.\n\n&gt;shader program error! Those look like they may be user generated errors. What graphics card are you running? You might make sure it's compatible with whatever shader version they're running. Just because the shaders can be compiled doesn't mean they'll run on the hardware you have. I'll go play with the code when I have some time later and see if I notice anything that might be useful to you! Radeon HD 6850. Its not an old graphics card or anything.\n\nYou don't have to mess with the code or help me troubleshoot, I don't care *that* much, but if you want to I'd appreciate it. Radeon HD 6850. Its not an old graphics card or anything.\n\nYou don't have to mess with the code or help me troubleshoot, I don't care *that* much, but if you want to I'd appreciate it. I've just recently been trying to learn more about GPU coding so I'll definitely keep an eye out for anything that might be useful to you.\n\nThat's a pretty new graphics card so I doubt it's a shader compatibility issue. anyone else having trouble getting the program to run? It gives me errors in the log file:\n\n&gt;Vertex shader was successfully compiled to run on hardware.\n\n&gt;error in vertex shader!\n\n&gt;Fragment shader was successfully compiled to run on hardware.\n\n&gt;error in fragment shader!\n\n&gt;Vertex shader(s) linked, fragment shader(s) linked.\n\n&gt;shader program error! I'm sorry, I'm just an average programmer. It shouldn't say anything if it compiles correctly! At least my hardware doesn't. If you have the opportunity to change the source code and recompile it, you can edit the last line of the setShaders() routine to return 0; Let me know, if this helped, please. I'm sorry, I'm just an average programmer. It shouldn't say anything if it compiles correctly! At least my hardware doesn't. If you have the opportunity to change the source code and recompile it, you can edit the last line of the setShaders() routine to return 0; Let me know, if this helped, please. anyone else having trouble getting the program to run? It gives me errors in the log file:\n\n&gt;Vertex shader was successfully compiled to run on hardware.\n\n&gt;error in vertex shader!\n\n&gt;Fragment shader was successfully compiled to run on hardware.\n\n&gt;error in fragment shader!\n\n&gt;Vertex shader(s) linked, fragment shader(s) linked.\n\n&gt;shader program error! anyone else having trouble getting the program to run? It gives me errors in the log file:\n\n&gt;Vertex shader was successfully compiled to run on hardware.\n\n&gt;error in vertex shader!\n\n&gt;Fragment shader was successfully compiled to run on hardware.\n\n&gt;error in fragment shader!\n\n&gt;Vertex shader(s) linked, fragment shader(s) linked.\n\n&gt;shader program error! Yes, I have the same problem! Did you solve it? If not, what did you try that didn't work? Did you get advice from anywhere else? Nah, I gave up on it. Didn't seem worth figuring out, it has something to do with it being an ATI card I think. Here's the information from the [video page](http://www.youtube.com/watch?v=KJe9H6qS82I):\n\nSmoothLife is a family of rules created by Stephan Rafler. It was designed as a continuous version of Conway's Game of Life - using floating point values instead of integers. This rule is SmoothLifeL which supports many interesting phenomena such as gliders that can travel in any direction, rotating pairs of gliders, wickstretchers and the appearance of elastic tension in the 'cords' that join the blobs.\n\nhttp://sourceforge.net/projects/smoothlife/\n\nPaper describing SmoothLife and SmoothLifeL: http://arxiv.org/abs/1111.1567\n\nThis animation created in Ready: http://code.google.com/p/reaction-diffusion\n\nMore videos: https://www.youtube.com/playlist?list=PL69EDA11384365494&amp;feature=plcp\n\nDetails:\n\n* Outer radius: 20.0\n* Radius ratio: 3.0\n* Birth range: 0.257 - 0.336\n* Survival range: 0.365 - 0.549\n* Alpha_n (outer sigmoid width): 0.028\n* Alpha_m (inner sigmoid width): 0.147\n* Timestep: 0.05 (Euler integration)\n* Frames: 10819 (saving 1 image per 8 timesteps)\n* Total time units elapsed: 4327.6\n* World size: 512x512, with toroidal wrap-around.\n* Color representation: linear mapping of [0,1] onto [black,white]\n* OpenCL source code: http://reaction-diffusion.googlecode.com/svn/trunk/Ready/Patterns/smoothlife_smoothlifeL.vti\n* Computation time: 74 minutes on an nVidia GeForce GTX 460 (in Ready, Stephan's software at the sourceforge link is much faster)\n Thanks for the information.\n\nI didn't see any oscillators in the sim, do they exist?\n\nAlso - have you seen any emergent behaviour of the kinds seen in the GOL, such as large \"rocket ships\", bouncers, and glider generators? Here's the information from the [video page](http://www.youtube.com/watch?v=KJe9H6qS82I):\n\nSmoothLife is a family of rules created by Stephan Rafler. It was designed as a continuous version of Conway's Game of Life - using floating point values instead of integers. This rule is SmoothLifeL which supports many interesting phenomena such as gliders that can travel in any direction, rotating pairs of gliders, wickstretchers and the appearance of elastic tension in the 'cords' that join the blobs.\n\nhttp://sourceforge.net/projects/smoothlife/\n\nPaper describing SmoothLife and SmoothLifeL: http://arxiv.org/abs/1111.1567\n\nThis animation created in Ready: http://code.google.com/p/reaction-diffusion\n\nMore videos: https://www.youtube.com/playlist?list=PL69EDA11384365494&amp;feature=plcp\n\nDetails:\n\n* Outer radius: 20.0\n* Radius ratio: 3.0\n* Birth range: 0.257 - 0.336\n* Survival range: 0.365 - 0.549\n* Alpha_n (outer sigmoid width): 0.028\n* Alpha_m (inner sigmoid width): 0.147\n* Timestep: 0.05 (Euler integration)\n* Frames: 10819 (saving 1 image per 8 timesteps)\n* Total time units elapsed: 4327.6\n* World size: 512x512, with toroidal wrap-around.\n* Color representation: linear mapping of [0,1] onto [black,white]\n* OpenCL source code: http://reaction-diffusion.googlecode.com/svn/trunk/Ready/Patterns/smoothlife_smoothlifeL.vti\n* Computation time: 74 minutes on an nVidia GeForce GTX 460 (in Ready, Stephan's software at the sourceforge link is much faster)\n wow! much better.  I just went with the first decent google result for \"Stepha Rafler Smoothlite\", since you had neglected to link the video page as well ;-).  [Here's the paper](http://arxiv.org/abs/1111.1567) for people like me who are infuriated by pretty videos without even a voiceover hinting at what might be going on.\n\nAlso, the video's too long.  I was getting really bored after about a minute ... and I'm not sure what the highlight at 3:12 is supposed to be.\n\nCool project though.  Now I get to sit down and spend at least 15 minutes with the paper, being far more entertained! &gt; what the highlight at 3:12 is supposed to be.\n\nMaybe the two \"glider\"-like thingies colliding and spinning?  Huh.  They have membranes. That's the first thing I thought! Every time I see a really cool new CA I always wish I could jump into the future and see how close it compares to future computational cell models in biology. Like if these are incredibly simplifications/rough approximations of biological behavior - or if it's really just an interesting coincidence that they appear so similar. Either way I think the answer would be really thought provoking! It's more an approximation of fundamental particles than anything else, and that's *really cool*. Are there any papers or articles on that similarity that you know of? I can't think of or find a paper that directly compares the Standard Model to a quantum continuous-valued cellular automaton; the comparison exists only in my brain and in the brains of a few other people whom my father and I have managed to interact with. (However, there *are* papers on the universe being a discrete cellular automaton, which annoy me a bit. For example, [this](http://arxiv.org/pdf/physics/9907013.pdf).) My beliefs come from having looked at a fundamental description of the Standard Model long and hard, and decided, \"Yep, this looks exactly like a quantum continuous-valued cellular automaton.\" What I'd suggest is to read more about the Standard Model and decide for yourself whether I am right or wrong.\n\nEDIT: Oh shit. Obviously the thing to do here is to become a particle physicist and write a paper on this. I hope no one beats me to it!  I want to see smooth go (the game). my suggestion:\n\nWhat it would need making continuous is following:\n\n* the concept of a group\n* the concept of a liberty\n* the concept of a stone\n* the concept of a point\n\n\na group would be a blob, simply or multiply connected, with a 1d border.\n\na point would be \"anywhere\"\n\na liberty would be a piece of border\n\na stone would be a disk (?)\n\n\nThe rules in new terms:\n\nif a blob has no border left, it is removed\n\nyou can place a disk anywhere you like, but it doesn't delete anything under it.\n\ncould be played only on computer.\n\n\nAfter every turn, an image processing filter is applied that smoothes out any sharp features.\nSo e.g. when you place two disks adjacent to each other so that they form a \"blob\", i.e. \"group\",\nthe border of this blob has sharp features.\nThese would be smoothed out in the image processing step, so that it is easier for\nthe opponent to place his disk adjacent in such a way that a big part of the border is taken away without him wasting too much of his \"stone\".\n\nOf course, two consecutive applications of this filter should not change anything anymore.\nThere are such filters, I know it.\n\nnow we only need someone who's writing the program to check if it works...\n\nIt would use real time GPU processing of the filter, so that you can see it's effect while placing the stone. I want to see smooth go (the game). Now I can't stop thinking about how that would work. Each move is to add a [Gaussian](https://www.google.co.uk/search?num=10&amp;hl=en&amp;safe=active&amp;authuser=0&amp;site=imghp&amp;tbm=isch&amp;source=hp&amp;biw=1519&amp;bih=871&amp;q=gaussian&amp;oq=gaussian&amp;gs_l=img.3..0l10.4499.5609.0.6007.8.6.0.2.2.0.84.355.6.6.0...0.0...1ac.1.Mb1t5Qzle4k) of fixed width to some location on the board? A region with two or more local minima (eyes) cannot be eroded? I don't know.  The two-eye condition is an emergent property of the game rules rather than a rule itself.  You don't actually need two eyes to live (see seki), and what constitutes and eye is not a local criterion, by which I mean that a loop shaped group that contains two 'false' eyes in the right way can be alive. Ok, good point. We need to find some way for life and death to fall out naturally. If one player was +1 and the other was -1 we would (since Gaussians have infinite range) end up with something like [Voronoi Go](http://www.youtube.com/watch?v=h142bwXh-SI), where each location is colored depending on whether it is positive or negative. And then maybe the combination rule is such that overlapping your own pieces gives a higher value than just adding the two together, so that connected pieces are stronger? Just thinking out loud.  As someone who doesn't know very much about the Game of Life, what interesting differences exist between a continuous and a discrete version?\n\nThe video is very cool, but (again, as someone who knows very little about GoL) my initial response would be \"this looks like GoL with anti-aliasing\" or \"This looks like GoL, zoomed way, way out\". Is that a decent (albeit naive) summary, or is there a lot more to it than that? ",
    "url" : "http://plus.google.com/110214848059767137292/posts/WtPBhYJswAe"
  } ],
  "processing-time-source" : 103,
  "processing-result.title" : "compsci6_reddit.xml",
  "XmlDocumentSource.xml" : {
    "absolutePath" : "/Users/cole/code/cs467/r-topics/carrot2-cli-3.6.2/../data/compsci6_reddit.xml"
  }
}
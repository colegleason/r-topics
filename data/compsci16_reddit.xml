<searchresult><compsci /><document><title>The Finest Machine (Kickstarter) -- "I hope to explain, in one book, how quantum physics, materials science, and the theory of computation give rise to the modern microprocessor..."</title><url>http://www.kickstarter.com/projects/nickblack/the-finest-machine</url><snippet>  He seem to be a proficient programmer, but he has no background in quantum physics or materials science. I doubt if he could dig these topics and came up with decent technically correct book. He seem to be a proficient programmer, but he has no background in quantum physics or materials science. I doubt if he could dig these topics and came up with decent technically correct book. I agree, unfortunately. If he approached in this manner, and presented it from someone with his background and intimate knowledge of programming that is discovering how material science and quantum physics helped shape this (through rigorous research backed by interviews and, ideally, testimonials penned by experts in their respective fields, then it would be something that, while a very good idea (Robert Penrose penned an excellent, similar read with [The Emperor's New Mind](http://en.wikipedia.org/wiki/The_Emperor's_New_Mind)) would be very compelling. @plexxor: http://www.youtube.com/watch?v=SoJMLElMfu8 i made this lame video for the Kickstarter page last evening. perhaps it will reinforce your doubts; perhaps it will relieve them.

i assure you that where i cannot write with rigor, i shan't be writing at all. there's nothing that would make me more miserable. "whereof one cannot speak, thereof one must be silent," as mr. wittgenstein said.

i'd have to say that *The Emperor's New Mind* was rather the inverse of what you suggest (which is, incidently, very much what i expect this to become) -- he's certainly better versed in physics than computability! though that didn't prevent several major flaws (Max Tegmark's criticisms of *TENM* are quite devastating, though it remains a good conceptual book, and his *Road to Reality* is one of my all-time favorites).

either way, thanks for your interest! --nick black As far as the video is concerned, the world needs more gonzo computer science writing.  A brilliant and very well-read friend of mine created a Kickstarter project to write a highly intriguing book, The Finest Machine. 

From Kickstarter, about the book:

I hope to explain, in one book, how quantum physics, materials science, and the theory of computation give rise to the modern microprocessor. While some mathematical sophistication is required for a full treatment, the majority of the book will be accessible to any motivated reader with high school mathematical training. This is not a textbook, though it will involve significant portions of the undergraduate and graduate curricula in computer science.

After reading The Finest Machine, the sedulous reader will understand the physical limits of computational devices, how silicon and optics yield computational structures, and the conceptual boundaries of computational theory. We start with electrons, build up through ALUs and compilers, and end at the brain. Along the way, we'll learn things like why your cell phone has multiple cores, why we use bits instead of trits or quatrits, the ancient art of single-instruction set computing, and why people who quip that "the universe is a computer" seem never to have time nor space sufficient for their algorithms.

Initial sketch at a table of contents: http://dank.qemfd.net/the-finest-machine.pdf 

 I hope he gets the funding. I put down $50. I did the same.  Thanks!  :) This is good, but he needs some serious marketing. If he could get frontpaged somewhere (like slashdot) that would ensure the success. This book must be written. It is the companion to Code (the book) that we all need. thank you so very much for your kind words! --nick black Just remember, as good as you think you are 3 months is only going to get you a 1st draft. I really hope you find yourself a good editor who specializes in making technobabble not so babbly. Otherwise the book is going to go over the head of your target audience. Considering the subject matter, that is going to be a be a challenge. This is good, but he needs some serious marketing. If he could get frontpaged somewhere (like slashdot) that would ensure the success. This book must be written. It is the companion to Code (the book) that we all need. [Link](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?ie=UTF8&amp;amp;qid=1334959971&amp;amp;sr=8-1)  Sounds like it would be interesting... but would it contain anything not already contained in the Feynman Lectures on Computation?

  Why is it necessary to secure funding before writing the book?  Most books aren't written that way, unless you're an established author or a famous person.
 Why is it necessary to secure funding before writing the book?  Most books aren't written that way, unless you're an established author or a famous person.
 @zorkmids: think of it as an enzyme. this book's going to get written some day (unless my heart blows up beforehand), but bills have to be paid, and lifestyles kept up, and it's an issue of whether i'm going to bang out text and do research at night after work, or whether there'll be an intense three months of unmolested labors.

if i can get the latter, that's great. if not, it'll take longer, and there's less chance it'll be written at all. i'm pretty sure the world will keep on spinnin' either way.

it'd be nice to get a really solid interdisciplinary book in amongst all the Get-Rich-Writing-Facebook-Apps and kardasian bullshit, though, don't you think? "some work of noble note may yet be done," as mr. tennyson said. but your point is valid, if not entirely correct.

thanks! --nick black, author Why is it necessary to secure funding before writing the book?  Most books aren't written that way, unless you're an established author or a famous person.
  It'd be more interesting if you cover mechanical computers first and finish with the microprocessor. Really nearly all of the pieces were in place very early on, except the materials science part.  It'd be nice to see the chip as the singularity of all of these advances rather than the focus of the book.  Even going as far back as the antikythera mechanism and before - most of it's parts proved that the civilization that created was far more advanced than we give them credit. The primary thing we have now that they didn't - materials and fabrication technologies.
 [deleted] It'd be more interesting if you cover mechanical computers first and finish with the microprocessor. Really nearly all of the pieces were in place very early on, except the materials science part.  It'd be nice to see the chip as the singularity of all of these advances rather than the focus of the book.  Even going as far back as the antikythera mechanism and before - most of it's parts proved that the civilization that created was far more advanced than we give them credit. The primary thing we have now that they didn't - materials and fabrication technologies.
 @o0o: a totally valid point. i'd considered mechanical machines, and cutting the Difference Engine, the ballistic and codebreaking machines of the World Wars (Purple, Enigma machines, etc), and Konrad Zuse's contraptions prior to the Z3 was an agonizing decision. they had to go, however:

(a) this is not a history book. it is a synthesis of computation, physics, and computer architecture *as they stand today*. the development of computability theory -- Church, Turing, Kleene, Post, and all those other hoary logicians -- is highly relevant, because their results continue to form the backbone of computability theory. no architect at AMD or Intel need know the details of the Difference Engine. is it fascinating? yes, and if i had countably infinitely pages and countably infinitely many years, they'd have their place. but i do not.

(b) if i covered them, i'd have to talk about vacuum relay tubes and ILLIAC and MANIAC et al, and i have no idea how vacuum computers work, and they don't even have the steampunk credit of a Babbage-era device, and all this talk brings us dangerously close to the rocky shores of COBOL, which have no place whatsoever in my book.

(c) i don't know enough about them, and doubt superficial research would provide me insights worth writing down. a book about how to transform into a rhinoceros would also be a better book than this one, but i'm sadly incapable of writing it.

thanks for your interest! --nick black  This looks awesome!

I pledged $50 at first but then bumped it up to $64 when I realized he was shooting for an amount that was a power of 2.  Why we taking risk out of book writing? Author writes book, if reviews are good, people buy it. </snippet></document><document><title>Operating Systems class - C++ or Java?</title><url>http://www.reddit.com/r/compsci/comments/sj0qs/operating_systems_class_c_or_java/</url><snippet>Hello redditors, I'm currently attending college and I'm in the process of wrapping up the semester, and finishing a Data Structures course. I'm currently registered next semester for the oh so dreaded Operating Systems course and apparently you can do all the assignments in either C++ or Java.

I'm learning Data Structures right now in Java, but I have a tiny bit of background in C++ as my introductory and next level classes were taken at another school. So, here's my question -- do you guys think it would be easier for me to learn C++ for Operating Systems? I've heard somethings are much easier to accomplish in C++ as opposed to Java. What do you guys think?
Thanks!  How exactly would an OS class be taught in Java? If Java runs on a VM, would it really be feasible to write an OS in it? We had to do all of our assignments in C, not even C++. First year CS major here, I'm wondering the same thing  ...would love an answer to this question.  How can you learn about OS without learning how to manage memory and such?! Fourth year undergrad student here.  I'm taking a CS class on Operating Systems in Java and it is a total fucking joke.  All we do is a bunch of assignments about synchronization using Java Semaphores.  In lectures the professor gives us an extremely high-level overview of basic concepts like scheduling (he actually explained what first-come first-serve means) and caching.  The textbook is in its 8th edition and it tracks the lectures pretty well (Java semaphores and everything), so I think it's safe to assume there's a bunch of shitty OS courses like this in schools all over the nation.

And to top it all off, he actually said "You kids should really consider learning C before you graduate" once.
 

**tl,dr** At my university it's just a catch-all  for stuff they don't talk about in other classes. At my undergrad, the OS class used to be taught kind of like what you're describing.  When they brought in a few new faculty who actually had their PhDs in CS (as opposed to math with CS masters), the course switched up to a fairly in-depth study of how the OS works and actual implementations of stuff in NACHOS (Not Another Completely Heuristic Operating System, an OS that runs on an R10K emulator IIRC).  Well, okay, my year was the first year that did it and we only made it through the first two projects, implementing locks and condition variables and doing some synchronization examples, and then implementing loading applications into memory and multitasking (and a shell to interface with it).  The next project would've been virtual memory, which I think the next year's class got to, and maybe replacing the sample file system came after that.

Learning C before you graduate:  My PhD advisor was "appalled" to learn that people were graduating with a CS degree in an engineering school without knowing C and at least one scripting language, so I know she at least made her compilers class work in C and learn Python.  One of my roommates was a EE undergrad and I think she said the learn Python requirement even spilled over into her classes. I find it really shocking that you wouldn't be forced at some point to program in C. Although in one of my classes I was surprised that a bunch of grad students didn't know what the hell was going on in regards to LISP, but I guess that is far less common than I would like to think it is.  I find it really shocking that you wouldn't be forced at some point to program in C. Although in one of my classes I was surprised that a bunch of grad students didn't know what the hell was going on in regards to LISP, but I guess that is far less common than I would like to think it is.  I find it really shocking that you wouldn't be forced at some point to program in C. Although in one of my classes I was surprised that a bunch of grad students didn't know what the hell was going on in regards to LISP, but I guess that is far less common than I would like to think it is.  Fourth year undergrad student here.  I'm taking a CS class on Operating Systems in Java and it is a total fucking joke.  All we do is a bunch of assignments about synchronization using Java Semaphores.  In lectures the professor gives us an extremely high-level overview of basic concepts like scheduling (he actually explained what first-come first-serve means) and caching.  The textbook is in its 8th edition and it tracks the lectures pretty well (Java semaphores and everything), so I think it's safe to assume there's a bunch of shitty OS courses like this in schools all over the nation.

And to top it all off, he actually said "You kids should really consider learning C before you graduate" once.
 

**tl,dr** At my university it's just a catch-all  for stuff they don't talk about in other classes. We did it in C with *the* Tennanbaum OS book. Fourth year undergrad student here.  I'm taking a CS class on Operating Systems in Java and it is a total fucking joke.  All we do is a bunch of assignments about synchronization using Java Semaphores.  In lectures the professor gives us an extremely high-level overview of basic concepts like scheduling (he actually explained what first-come first-serve means) and caching.  The textbook is in its 8th edition and it tracks the lectures pretty well (Java semaphores and everything), so I think it's safe to assume there's a bunch of shitty OS courses like this in schools all over the nation.

And to top it all off, he actually said "You kids should really consider learning C before you graduate" once.
 

**tl,dr** At my university it's just a catch-all  for stuff they don't talk about in other classes. Fourth year undergrad student here.  I'm taking a CS class on Operating Systems in Java and it is a total fucking joke.  All we do is a bunch of assignments about synchronization using Java Semaphores.  In lectures the professor gives us an extremely high-level overview of basic concepts like scheduling (he actually explained what first-come first-serve means) and caching.  The textbook is in its 8th edition and it tracks the lectures pretty well (Java semaphores and everything), so I think it's safe to assume there's a bunch of shitty OS courses like this in schools all over the nation.

And to top it all off, he actually said "You kids should really consider learning C before you graduate" once.
 

**tl,dr** At my university it's just a catch-all  for stuff they don't talk about in other classes. Java semaphores?  I thought it just used the "synchronize" keyword, with support for condition variables.  Did you have to build the semaphores yourself or am I mistaken? Java has a bunch of stuff for concurrency in [java.util.concurrent](http://docs.oracle.com/javase/7/docs/api/index.html?java/util/concurrent/package-summary.html), including semaphores. Java has a bunch of stuff for concurrency in [java.util.concurrent](http://docs.oracle.com/javase/7/docs/api/index.html?java/util/concurrent/package-summary.html), including semaphores. Java semaphores?  I thought it just used the "synchronize" keyword, with support for condition variables.  Did you have to build the semaphores yourself or am I mistaken? The professor has some stupid multithreading library he made that he's trying to shove down our throats, so we used its Semaphore implementation. Java semaphores?  I thought it just used the "synchronize" keyword, with support for condition variables.  Did you have to build the semaphores yourself or am I mistaken? Fourth year undergrad student here.  I'm taking a CS class on Operating Systems in Java and it is a total fucking joke.  All we do is a bunch of assignments about synchronization using Java Semaphores.  In lectures the professor gives us an extremely high-level overview of basic concepts like scheduling (he actually explained what first-come first-serve means) and caching.  The textbook is in its 8th edition and it tracks the lectures pretty well (Java semaphores and everything), so I think it's safe to assume there's a bunch of shitty OS courses like this in schools all over the nation.

And to top it all off, he actually said "You kids should really consider learning C before you graduate" once.
 

**tl,dr** At my university it's just a catch-all  for stuff they don't talk about in other classes. In my school there are no requirements to get into the CS program.  This is because CS 124 requires students to learn both C *and* Assembly.

CS 124: ASM, C

CS 142, 235, 236: C++

CS 240: Java

Beyond that I'm not sure. What ASM do you learn? ASM is really the only way to form a concrete understanding of pointers, and I respect your school's decision to do so.

Our school did this in the second year, after C and pointer are introduced. It worked but not as well. How exactly would an OS class be taught in Java? If Java runs on a VM, would it really be feasible to write an OS in it? We had to do all of our assignments in C, not even C++. Short answer: Emulator.

Depending on who you ask, computer science degrees (especially at liberal arts colleges) are about the concepts and the how of solving problems/algorithms, and not the language specifics.  As a result, if you've been taught only in Java up to that point, it's arguable whether it's worth the detours to learn C++.  I know in my undergrad OS class, which was in C++ after almost exclusively doing Java in all the lower-level classes, plenty of people struggled with all the language things before they even got around to solving the actual problem at hand.

So anyway, in much the same way you can rewrite TCP over UDP packets as a learning exercise, there's no reason you couldn't fake a machine and OS.  Your main OS can just have a giant char array or vector that represents system memory (or maybe some sparse representation would make more sense), malloc calls essentially return an offset into the array, etc.  Especially if you just call memory one long contiguous array, the JVM will never automatically catch when some process within your OS has written out of its memory bounds, so you could study ways to detect that within your Java OS.

The more I think about answering your question, the more writing something like this sounds like a fun rainy day project. Short answer: Emulator.

Depending on who you ask, computer science degrees (especially at liberal arts colleges) are about the concepts and the how of solving problems/algorithms, and not the language specifics.  As a result, if you've been taught only in Java up to that point, it's arguable whether it's worth the detours to learn C++.  I know in my undergrad OS class, which was in C++ after almost exclusively doing Java in all the lower-level classes, plenty of people struggled with all the language things before they even got around to solving the actual problem at hand.

So anyway, in much the same way you can rewrite TCP over UDP packets as a learning exercise, there's no reason you couldn't fake a machine and OS.  Your main OS can just have a giant char array or vector that represents system memory (or maybe some sparse representation would make more sense), malloc calls essentially return an offset into the array, etc.  Especially if you just call memory one long contiguous array, the JVM will never automatically catch when some process within your OS has written out of its memory bounds, so you could study ways to detect that within your Java OS.

The more I think about answering your question, the more writing something like this sounds like a fun rainy day project. Short answer: Emulator.

Depending on who you ask, computer science degrees (especially at liberal arts colleges) are about the concepts and the how of solving problems/algorithms, and not the language specifics.  As a result, if you've been taught only in Java up to that point, it's arguable whether it's worth the detours to learn C++.  I know in my undergrad OS class, which was in C++ after almost exclusively doing Java in all the lower-level classes, plenty of people struggled with all the language things before they even got around to solving the actual problem at hand.

So anyway, in much the same way you can rewrite TCP over UDP packets as a learning exercise, there's no reason you couldn't fake a machine and OS.  Your main OS can just have a giant char array or vector that represents system memory (or maybe some sparse representation would make more sense), malloc calls essentially return an offset into the array, etc.  Especially if you just call memory one long contiguous array, the JVM will never automatically catch when some process within your OS has written out of its memory bounds, so you could study ways to detect that within your Java OS.

The more I think about answering your question, the more writing something like this sounds like a fun rainy day project. &amp;gt; if you've been taught only in Java up to that point, it's arguable whether it's worth the detours to learn C++

Therein lies the problem. If you have good grounding in Java, it should be trivial to pick up C++. CS programs that do most of their teaching in a single language are doing their students a disservice, because students *should* have the ability to quickly pick up new languages. Indeed, at my school, the first CS quarter is in Java, the second quarter is in C++, and the third (the first semi-systems-y class) is in C, and it works out really well.

Obviously, I'm not expecting someone coming off an intro class in Java to be able to pick up Haskell overnight. How exactly would an OS class be taught in Java? If Java runs on a VM, would it really be feasible to write an OS in it? We had to do all of our assignments in C, not even C++. &amp;gt; If Java runs on a VM, would it really be feasible to write an OS in it? 

[It's been done.](http://en.wikipedia.org/wiki/JavaOS) &amp;gt; If Java runs on a VM, would it really be feasible to write an OS in it? 

[It's been done.](http://en.wikipedia.org/wiki/JavaOS) &amp;gt; If Java runs on a VM, would it really be feasible to write an OS in it? 

[It's been done.](http://en.wikipedia.org/wiki/JavaOS) How exactly would an OS class be taught in Java? If Java runs on a VM, would it really be feasible to write an OS in it? We had to do all of our assignments in C, not even C++. How exactly would an OS class be taught in Java? If Java runs on a VM, would it really be feasible to write an OS in it? We had to do all of our assignments in C, not even C++. How exactly would an OS class be taught in Java? If Java runs on a VM, would it really be feasible to write an OS in it? We had to do all of our assignments in C, not even C++. How exactly would an OS class be taught in Java? If Java runs on a VM, would it really be feasible to write an OS in it? We had to do all of our assignments in C, not even C++. How exactly would an OS class be taught in Java? If Java runs on a VM, would it really be feasible to write an OS in it? We had to do all of our assignments in C, not even C++.  C++ gives you a lot more control over things like memory management, even better would be using C...

Java hides a lot of details which could cause you grief if your assignments require some intricate work at lower level hardware/software

Plus you will likely do things with threads, pipes, and memory management... So I'd go with C++. How would the learning curve be going from Java to C++ in that case?  Syntax is pretty simular; however, it can be a pretty steep learning curve coming from a managed language to an unmanaged one. That being said, I found learning C++ and C made my java code run a lot faster since it became clear what I was actually doing at a lower level.

Also, if they don't teach it (and probably won't), look into C++11 - which is the new standard. It's a good bandwagon to get on since it's just at the beginning. If there is a choice between Java or C++, chances are it will not do anything that C++ would excel at that Java wouldn't either. In my OS class we could do it in any language as long as it worked but all the code examples were in Java. If you took the more advanced routes C was required since that is what the Tananbaum book uses (not C++). Out of curiosity, what were your assignments like? Make a scheduler, shell, etc for homework. In class we would do quizzes and go over the concepts. If you turned in something that wasn't right for the edge cases it didn't count as being done. Syntax is pretty simular; however, it can be a pretty steep learning curve coming from a managed language to an unmanaged one. That being said, I found learning C++ and C made my java code run a lot faster since it became clear what I was actually doing at a lower level.

Also, if they don't teach it (and probably won't), look into C++11 - which is the new standard. It's a good bandwagon to get on since it's just at the beginning. C++ gives you a lot more control over things like memory management, even better would be using C...

Java hides a lot of details which could cause you grief if your assignments require some intricate work at lower level hardware/software

Plus you will likely do things with threads, pipes, and memory management... So I'd go with C++.  I'm in an OS course right now and we're doing all the programming assignments in C. I think it's the most natural language to toy around with a UNIX OS in. I'm in an OS course right now and we're doing all the programming assignments in C. I think it's the most natural language to toy around with a UNIX OS in.  You can take OS in Java? Transfer schools. I don't mean to be rude, but seriously, *you cannot modify the OS in any major way* with Java. Technically, I guess you can with the JNI - but that's an extremely convoluted wrapper around native C code.

I took OS and used C exclusively, and we re-wrote linux kernel modules, added drivers, and messed with the process run queue and stuff. It was a lot of fun... that said, 0% of that can be done with Java. More to the point, I think it would be difficult in any language with garbage collection. C++ can be bent enough to work, but... ew.

I've written C, C++, C#, Java, and x86 (if we're talking systems stuff). I've done embedded systems work. I've written wrappers with the JNI. I've ported [NanoVM](http://www.harbaum.org/till/nanovm/index.shtml) (a tiny JVM) to Zilog processors. You cannot do actual systems-level programming with Java, period. It requires C.

I really don't mean to be so negative, but that is really scary to me. Take a look at [http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/](http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/). It's an older version of the linux kernel. It's *all* C code (and still is). No C++, no C# (obviously), no Java. The kernel is truly the OS. Applications talk to it, and it talks to hardware for the applications. Windows *should* use all C in the kernel as well, but I have no idea.

If nothing else, learn C++. It could just be that the assignments are things like implementing scheduling algorithms or the like instead of tearing into any one particular OS. I suppose.... I don't think you can simulate cache thrashing in Java though, because of the GC. There are just so many things you simply can't do, so much behavior you can't see. Eh. You can take OS in Java? Transfer schools. I don't mean to be rude, but seriously, *you cannot modify the OS in any major way* with Java. Technically, I guess you can with the JNI - but that's an extremely convoluted wrapper around native C code.

I took OS and used C exclusively, and we re-wrote linux kernel modules, added drivers, and messed with the process run queue and stuff. It was a lot of fun... that said, 0% of that can be done with Java. More to the point, I think it would be difficult in any language with garbage collection. C++ can be bent enough to work, but... ew.

I've written C, C++, C#, Java, and x86 (if we're talking systems stuff). I've done embedded systems work. I've written wrappers with the JNI. I've ported [NanoVM](http://www.harbaum.org/till/nanovm/index.shtml) (a tiny JVM) to Zilog processors. You cannot do actual systems-level programming with Java, period. It requires C.

I really don't mean to be so negative, but that is really scary to me. Take a look at [http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/](http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/). It's an older version of the linux kernel. It's *all* C code (and still is). No C++, no C# (obviously), no Java. The kernel is truly the OS. Applications talk to it, and it talks to hardware for the applications. Windows *should* use all C in the kernel as well, but I have no idea.

If nothing else, learn C++. &amp;gt; You can take OS in Java? Transfer schools.

Couldn't agree more. I took my Operating Systems class in Java. They left it up to the instructor, and the quarter I took it I just got shitty luck.

[Worst. Textbook. Ever.](http://www.amazon.com/gp/product/0763735744/ref=wms_ohs_product)

The introduction basically says "no one *really* writes operating systems anymore, so we're not going to teach you about writing operating systems. Instead, we're going to teach you how to *model* operating systems...using Java!"

A typical chapter on say process scheduling would have a few pages about "here's how a process scheduler works", and then, I shit you not, "here's our UML diagram for how to model a process scheduler". Followed by a couple pages of code of "here's our Process class and our ProcessScheduler class".

Finally, and here's the best part, maybe half of each chapter would simply be console output from running their simulation. Picture pages upon pages of:

    process 1 acquired CPU
    process 1 released CPU after 42ms
    process 3 acquired CPU
    process 3 released CPU after 12ms
    process 2 acquired CPU
    process 2 released CPU after 33ms
    ...

I honestly think that the page count of the book came in way, way under what the publisher wanted, so like a high school student fiddling with font size and margins, they just churned out an extra 100 pages of simulation output and crammed it in.

To top it all off, most of my homework assignments in that class were "take the CD in the back of the book, run the horribly shitty simulation GUI that's included, and change some parameters around. What happens when you increase the number in the 'processes' textbox but don't increase the number in the 'CPUs' textbox?"

Fortunately, I was later able to take a graduate-level class in writing device drivers, where I got to write honest-to-god device drivers for both NetBSD and Windows, from a professor who was one of the original hackers of NetBSD. To this day, I think I learned more from that class, and had more fun, than the rest of them put together. Hahaha, you must be from Western. I burned that book in a bonfire, well worth the price. You also forgot that half of the time a java exception was thrown when you started the simulation. &amp;gt; You can take OS in Java? Transfer schools.

Couldn't agree more. I took my Operating Systems class in Java. They left it up to the instructor, and the quarter I took it I just got shitty luck.

[Worst. Textbook. Ever.](http://www.amazon.com/gp/product/0763735744/ref=wms_ohs_product)

The introduction basically says "no one *really* writes operating systems anymore, so we're not going to teach you about writing operating systems. Instead, we're going to teach you how to *model* operating systems...using Java!"

A typical chapter on say process scheduling would have a few pages about "here's how a process scheduler works", and then, I shit you not, "here's our UML diagram for how to model a process scheduler". Followed by a couple pages of code of "here's our Process class and our ProcessScheduler class".

Finally, and here's the best part, maybe half of each chapter would simply be console output from running their simulation. Picture pages upon pages of:

    process 1 acquired CPU
    process 1 released CPU after 42ms
    process 3 acquired CPU
    process 3 released CPU after 12ms
    process 2 acquired CPU
    process 2 released CPU after 33ms
    ...

I honestly think that the page count of the book came in way, way under what the publisher wanted, so like a high school student fiddling with font size and margins, they just churned out an extra 100 pages of simulation output and crammed it in.

To top it all off, most of my homework assignments in that class were "take the CD in the back of the book, run the horribly shitty simulation GUI that's included, and change some parameters around. What happens when you increase the number in the 'processes' textbox but don't increase the number in the 'CPUs' textbox?"

Fortunately, I was later able to take a graduate-level class in writing device drivers, where I got to write honest-to-god device drivers for both NetBSD and Windows, from a professor who was one of the original hackers of NetBSD. To this day, I think I learned more from that class, and had more fun, than the rest of them put together. I agree with you. True, the skill to experiment with and model a system is important. But it is really not supposed to be taught in an OS class, otherwise why it is called an OS class? At least for me an OS class should include an introduction to and a solid implementation of lower level coding, scheduling, interrupt, exception, memory management, syscalls and so on. OS is such a perfect place to demonstrate the complexity of modern software and the moto "the truth is in the code." Sure, but Java is an insane choice for a simulation language, too. Something like ML or Lisp, or Python, where you can see the concepts in your code and not just mountains of boilerplate, makes much more sense. Java the language for writing greatest common denominator industrial plumbing code --which I respect and I do for money-- but it has no place in a computer science education outside of a course on OOAD or enterprise software engineering.  Yes, but that would mean students have to *learn* ML or Lisp or Python... and that's *hard*.   Come on, at least lisp isn't hard to learn. &amp;gt; You can take OS in Java? Transfer schools.

Couldn't agree more. I took my Operating Systems class in Java. They left it up to the instructor, and the quarter I took it I just got shitty luck.

[Worst. Textbook. Ever.](http://www.amazon.com/gp/product/0763735744/ref=wms_ohs_product)

The introduction basically says "no one *really* writes operating systems anymore, so we're not going to teach you about writing operating systems. Instead, we're going to teach you how to *model* operating systems...using Java!"

A typical chapter on say process scheduling would have a few pages about "here's how a process scheduler works", and then, I shit you not, "here's our UML diagram for how to model a process scheduler". Followed by a couple pages of code of "here's our Process class and our ProcessScheduler class".

Finally, and here's the best part, maybe half of each chapter would simply be console output from running their simulation. Picture pages upon pages of:

    process 1 acquired CPU
    process 1 released CPU after 42ms
    process 3 acquired CPU
    process 3 released CPU after 12ms
    process 2 acquired CPU
    process 2 released CPU after 33ms
    ...

I honestly think that the page count of the book came in way, way under what the publisher wanted, so like a high school student fiddling with font size and margins, they just churned out an extra 100 pages of simulation output and crammed it in.

To top it all off, most of my homework assignments in that class were "take the CD in the back of the book, run the horribly shitty simulation GUI that's included, and change some parameters around. What happens when you increase the number in the 'processes' textbox but don't increase the number in the 'CPUs' textbox?"

Fortunately, I was later able to take a graduate-level class in writing device drivers, where I got to write honest-to-god device drivers for both NetBSD and Windows, from a professor who was one of the original hackers of NetBSD. To this day, I think I learned more from that class, and had more fun, than the rest of them put together. You can take OS in Java? Transfer schools. I don't mean to be rude, but seriously, *you cannot modify the OS in any major way* with Java. Technically, I guess you can with the JNI - but that's an extremely convoluted wrapper around native C code.

I took OS and used C exclusively, and we re-wrote linux kernel modules, added drivers, and messed with the process run queue and stuff. It was a lot of fun... that said, 0% of that can be done with Java. More to the point, I think it would be difficult in any language with garbage collection. C++ can be bent enough to work, but... ew.

I've written C, C++, C#, Java, and x86 (if we're talking systems stuff). I've done embedded systems work. I've written wrappers with the JNI. I've ported [NanoVM](http://www.harbaum.org/till/nanovm/index.shtml) (a tiny JVM) to Zilog processors. You cannot do actual systems-level programming with Java, period. It requires C.

I really don't mean to be so negative, but that is really scary to me. Take a look at [http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/](http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/). It's an older version of the linux kernel. It's *all* C code (and still is). No C++, no C# (obviously), no Java. The kernel is truly the OS. Applications talk to it, and it talks to hardware for the applications. Windows *should* use all C in the kernel as well, but I have no idea.

If nothing else, learn C++. Operating Systems courses are NOT about interacting with an OS. They are about scheduling algorithms, locking, file systems. All of which can reasonably be taught through Java. You can take OS in Java? Transfer schools. I don't mean to be rude, but seriously, *you cannot modify the OS in any major way* with Java. Technically, I guess you can with the JNI - but that's an extremely convoluted wrapper around native C code.

I took OS and used C exclusively, and we re-wrote linux kernel modules, added drivers, and messed with the process run queue and stuff. It was a lot of fun... that said, 0% of that can be done with Java. More to the point, I think it would be difficult in any language with garbage collection. C++ can be bent enough to work, but... ew.

I've written C, C++, C#, Java, and x86 (if we're talking systems stuff). I've done embedded systems work. I've written wrappers with the JNI. I've ported [NanoVM](http://www.harbaum.org/till/nanovm/index.shtml) (a tiny JVM) to Zilog processors. You cannot do actual systems-level programming with Java, period. It requires C.

I really don't mean to be so negative, but that is really scary to me. Take a look at [http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/](http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/). It's an older version of the linux kernel. It's *all* C code (and still is). No C++, no C# (obviously), no Java. The kernel is truly the OS. Applications talk to it, and it talks to hardware for the applications. Windows *should* use all C in the kernel as well, but I have no idea.

If nothing else, learn C++. Would you transfer schools from [Berkeley?](http://inst.eecs.berkeley.edu/~cs162/fa10/). We used to do OS in Java (I never took it myself). But then last year they tried to "reboot" the course and got [some of the worst ratings ever](http://ninjacourses.com/explore/1/course/COMPSCI/162/#ratings) so I don't really know if it's any good, but the course had very good ratings before the reboot. You can take OS in Java? Transfer schools. I don't mean to be rude, but seriously, *you cannot modify the OS in any major way* with Java. Technically, I guess you can with the JNI - but that's an extremely convoluted wrapper around native C code.

I took OS and used C exclusively, and we re-wrote linux kernel modules, added drivers, and messed with the process run queue and stuff. It was a lot of fun... that said, 0% of that can be done with Java. More to the point, I think it would be difficult in any language with garbage collection. C++ can be bent enough to work, but... ew.

I've written C, C++, C#, Java, and x86 (if we're talking systems stuff). I've done embedded systems work. I've written wrappers with the JNI. I've ported [NanoVM](http://www.harbaum.org/till/nanovm/index.shtml) (a tiny JVM) to Zilog processors. You cannot do actual systems-level programming with Java, period. It requires C.

I really don't mean to be so negative, but that is really scary to me. Take a look at [http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/](http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/). It's an older version of the linux kernel. It's *all* C code (and still is). No C++, no C# (obviously), no Java. The kernel is truly the OS. Applications talk to it, and it talks to hardware for the applications. Windows *should* use all C in the kernel as well, but I have no idea.

If nothing else, learn C++. If there's a school that actually teaches OSes in a more hands-on way, I'd be interested. As it is, the OS course I took was really weak. It wasn't *in* Java, but it also didn't involve a lot of coding at all.

Incidentally, why'd you use C to build an OS, rather than C++? I know Linus has strong feelings on the matter, but I never really looked into it. One thing that immediately comes to mind is exception handling. Why would you voluntarily choose error codes over that, even if you aren't using objects for anything? Exceptions are very hard to reason about, compared to error codes, most applications simply crash whenever any exception occurs, because if you do not catch it close enough to the source, you will a) leak memory and b) not be sure what state your global data structures are in. This isn't really an option for a piece of software that is meant to ensure stability.

I think the main problem with C++ is that all the niceties is really the programmer giving up control of some aspect of his code (and often performance), and when you are trying to really optimise things, that's really not good enough. &amp;gt; most applications simply crash whenever any exception occurs, because if you do not catch it close enough to the source, you will a) leak memory and b) not be sure what state your global data structures are in.

Right.

I'm fine with that.

When an exception happens and you don't handle it *properly,* that's fairly similar to not handling an error code properly. Need to wind up global data structures? (Wait, why do you have global data structures in the first place?) If you aren't doing it in the exception handling, you probably aren't doing it with the error code handling, either.

Here's why I like exceptions: When you *forget* to handle an exception, you fail *noisily.* When you forget to check an error code, *the error is silently ignored.* As far as I'm concerned, programming in C without exceptions is little better than programming in Visual Basic with "on error resume next" enabled.

Now, ideally, you want everything handled, and I get that error codes can make that a little easier to reason about. But if you miss something (and you *will*), there's no safety net of any kind.

Should we have safety nets in our kernels? Well, exceptions don't necessarily cost performance on a modern compiler, so I'd say yes, absolutely, when there isn't a cost. Where else is it more critical that code actually be correct?

&amp;gt; I think the main problem with C++ is that all the niceties is really the programmer giving up control of some aspect of his code (and often performance), and when you are trying to really optimise things, that's really not good enough.

I would tend to agree, but there are advantages to this, also. By telling the compiler more about what you're trying to do and why, it can optimize more. The worst place to be is at the wrong level of abstraction, halfway between high enough and low enough, and I think C hits that often.

Here's an example of what I mean by "wrong level of abstraction": In C, if you're looping over an array, that loop might be unrolled, because the C compiler knows about arrays. In C++, you'd usually loop over at least a vector, and you'd probably use iterators -- even if you don't, the C++ compiler doesn't necessarily know about vectors -- so you end up with structures like this:

    for(auto i = list.begin(); i!=list.end(); ++i) {
      // do something with *i
    }

You can get that loop unrolling back, by *adding* a layer of abstraction:

    for_each(list.begin(), list.end(), [](SomeClass &amp;amp;obj){
      // do something with obj
    });

Now, I'm not bringing lambdas in as an example of something I'd want in kernel code, though I certainly wouldn't mind. But the point is that C is probably not the right level of abstraction. It only really fits the PDP-11. This null-terminated-string bullshit is *not* something I want a modern kernel to even *think* about except maybe in its system call interface (for compatibility with POSIX). It's easier to get your compiler to warn you when return values are ignored by simply annotating error returning functions as __must_check.

And that would be my preference, since it is much easier to forget to deal with an exception than an explicit error, and __must_check will simply not let you compile bad code, therefore getting rid of the run time issues.

So what are the exact semantics of the for_each loop? Does the compiler need to call list.end() after processing every element? Probably, since solving the alias analysis problems required to ensure that none of the state the list.end() function relies on has been changed. And function calls are comparatively expensive. And all of a sudden you need to start trying to work around your abstraction instead of simply knowing you're not modifying the list and manually caching the end pointer.

And loop unrolling is unrelated to whether the compiler knows about vectors or not, it's a pretty trivial task for arbitrary loops, deciding whether it is the right thing to do is tricky, but the operation itself is trivial.

And if you start using classes you begin to have less control of the data format in memory, and if you try to use encapsulation and private fields you end up with everyone needing to constantly recompile all drivers, etc.

And while everyone talks about string being NUL terminated in C, this is only by convention, you could just as easily use pascal strings if you wanted to, there's just no real need to since the kernel's main job isn't string processing, so it isn't really a perf hit. (file paths and username/passwords are the only things I can think of off the top of my head, everything else has a length)

I'm just left with the feeling that every C++ feature is a nice easy way to shoot yourself in the foot. If there's a school that actually teaches OSes in a more hands-on way, I'd be interested. As it is, the OS course I took was really weak. It wasn't *in* Java, but it also didn't involve a lot of coding at all.

Incidentally, why'd you use C to build an OS, rather than C++? I know Linus has strong feelings on the matter, but I never really looked into it. One thing that immediately comes to mind is exception handling. Why would you voluntarily choose error codes over that, even if you aren't using objects for anything? There are a couple of reasons why I'd argue against exceptions in a kernel.  The first, and most important, is the exceptions rely on type lookups.  Type lookups are not safe across module boundaries.  If any exceptions enter or leave any kernel modules, you can't guarantee that the types are what they say they are, and the worst place to accidentally overwrite memory is in the kernel.  The worst thing that will happen with error codes is you'll misreport an error, which is far less worse than accidentally erasing half of the scheduler's data.

The second is that you can't guarantee exception performance, because the details are hidden from you.  Returning an error code has the same cost as returning a valid result, so you can guarantee that certain functions always complete within a specified amount of time.  You don't have any control over the exception handling code, and many compilers generate exception handling code that is slow.

The third is that unknown error codes are storable for later reference, because they're integers.  If you catch an exception of an unexpected type, you can't do anything with it because you know nothing about it.

I believe there are a lot of places where exceptions are a better choice than error codes, but in a kernel is not one of them.  If I had to point to C++'s strongest advantage in kernel code, it would be RAII.  Ensuring that data structures are never in an inconsistent state and always get deallocated properly is a great benefit in code where every little detail has to be correct. &amp;gt; The first, and most important, is the exceptions rely on type lookups. Type lookups are not safe across module boundaries. If any exceptions enter or leave any kernel modules, you can't guarantee that the types are what they say they are...

Could you explain a bit more about this, or where to look? What's a type lookup, how does it work, and why does it break across module boundaries?

&amp;gt; The worst thing that will happen with error codes is you'll misreport an error, which is far less worse than accidentally erasing half of the scheduler's data.

Even this would at least bring attention to the issue. I'm far more concerned with, say, an error being flat-out *ignored,* so half my filesystem is corrupted before I know anything's wrong.

&amp;gt; Returning an error code has the same cost as returning a valid result, so you can guarantee that certain functions always complete within a specified amount of time.

I don't know that C gives you quite that guarantee. It's better than an exception coming out of nowhere, but given the things the optimizer is allowed to do, if you *need* something clamped to a hard time limit, wouldn't it make sense to do it in assembly?

Also, we're talking about errors, which are generally for *exceptional* situations. If it really did blow up, when would I care about getting that result within a specified amount of time?

I suppose I can see preferring error codes in a limited scope, within a module, where you want to be sure exactly what everything's doing. But at the API boundary seems like exactly where you want exceptions. You might have the whole module in your head, but do you know what the library call is going to do? I suppose the correct approach here, with C, is to just ensure *every* function call checks for errors and handles them, so that when you see a naked function call, you get as nervous as when you see pointer arithmetic.

&amp;gt; The third is that unknown error codes are storable for later reference, because they're integers. If you catch an exception of an unexpected type, you can't do anything with it because you know nothing about it.

If that's actually the case, that sounds like a design flaw in C++ exceptions. In, say, Java or Ruby, I can catch *any* exception, without knowing the type, and store that just as easily as an int. There's also some standard exception superclasses which have the ability to give me a string back, sometimes a stacktrace.

&amp;gt; If I had to point to C++'s strongest advantage in kernel code, it would be RAII. Ensuring that data structures are never in an inconsistent state and always get deallocated properly is a great benefit in code where every little detail has to be correct.

My understanding is that one of the benefits of this is with exceptions. Worst case, you don't know what to do with the exception, but hopefully (and hey, you're a kernel, you can mandate this) you get an exception that's a child of some type that at least has an error code and string. So you log it somewhere, and if everything's using RAII appropriately, everything it was trying to do should be cleaned up properly now -- no leaked memory, no resources left open which should be closed, etc. A type lookup is exactly what it sounds like: it looks up what type a variable is.  Without looking up the type of the exception, the correct exception handler can't be called, since exception handlers only work on specific types.  When handling a local exception this can often be optimized out, but when handling an exception from another module the handling code has no way of knowing what type of exception is being thrown, so it needs to look it up.  This involves a string comparison of the name of the exception's type.

The problem arises when 2 modules define a type with the same name.  One module throws it, and the other catches it.  Now each module thinks it has a value of the type it defined, but it doesn't.  Here's some code:

	//Module 1
	struct except
	{
		string reason;
	};

	//Module 2
	struct except
	{
		string reason;
		vector&amp;lt;string&amp;gt; modules_passed_through;
	};

If module 2 gets an exception of type `except` from module 1, it'll think it's bigger than it really is.  If module 2 does something like `exception.modules_passed_through.push_back("module 2")` it'll be adding data to an uninitialized vector, which will write that string to an unspecified memory address.  Since the modules run in kernel mode they can access any processes memory, which means module 2 could be overwriting the memory of some random process or even important kernel memory (maybe it belongs to the filesystem driver, which will now silently corrupt your hard drive).

[This](http://gcc.gnu.org/wiki/Visibility#Problems_with_C.2B-.2B-_exceptions_.28please_read.21.29) has info about how it relates to visible symbols (which you'll need to deal with if you're making a kernel) and [this](http://www.cplusplus.com/reference/std/typeinfo/type_info/) has more info on C++ type info objects in general.  I haven't seen a lot written about this, but I have a bunch of experience from writing a program that dynamically runs modules in their own threads and passing them messages.  I solved this problem by making the thread swallow every exception right before it exits and just emit a generic error message, but that's not a viable solution if you actually want to report errors between modules and not just log stuff.

As for speed, remember that C is basically portable assembler.  As long as you're not doing anything too complicated you can guarantee performance for certain instructions by assuming the worst case and a compiler that will turn something like `if (errno==EINVAL)` into a load, a comparison, and a jump (which pretty much every compiler does).  And C has the very nice advantage of working everywhere, not just on that specific architecture.

And sometimes speed is important everywhere.  If you have a device driver that *needs* to be called sometime in the next 30-50 milliseconds and you know that a certain system call never takes more than 13 milliseconds, you can call that 2 or 3 times without missing the deadline.  If that function throws an exception and now takes 60 milliseconds you've missed the boat and your expensive piece of scientific equipment is broken.

And C++ exceptions are designed to be general.  In Java, you can only throw types deriving from `Throwable` (and IIRC Ruby has a similar restriction); in C++ you can throw any type.  If you restricted yourself to only catching exceptions deriving from `std::exception` that approach would work, but modules can execute arbitrary code, so you can't stop them from throwing objects of type `int` or `foo`.  In Java, you're always catching a pointer to a `Throwable` object, which why you can make those guarantees.  You can argue about whether C++'s generality is good or bad, but it's a design goal that I think adds more to the language than it takes away, and like you say, you can just ignore any exceptions not derived from your appointed exception class.

Yes, RAII works well with exceptions, but it also works well without them.  Even if you use error codes instead of exceptions, RAII will still help ensure that you haven't leaked any resources.  RAII is my favorite C++ "feature", and it works well in almost any situation. &amp;gt; The problem arises when 2 modules define a type with the same name. One module throws it, and the other catches it. Now each module thinks it has a value of the type it defined, but it doesn't.

Wouldn't namespacing solve this issue?

But I will have to go back and read your links when I have more time. Thanks!

&amp;gt; As for speed, remember that C is basically portable assembler.

[No, it isn't.](http://james-iry.blogspot.com/2010/09/moron-why-c-is-not-assembly.html)

&amp;gt; As long as you're not doing anything too complicated you can guarantee performance for certain instructions by assuming the worst case...

That's fair. C++ worst-case is a bit more hidden, and I can see where both of these could provide hard guarantees that, say, Java can't.

&amp;gt; And C++ exceptions are designed to be general. In Java, you can only throw types deriving from Throwable (and IIRC Ruby has a similar restriction);

That's surprising, actually. I knew about, and expected, the Java limitation, but Ruby usually lets you use any object for anything, so long as it implements the right methods. I'm not sure how it's decided what is and isn't an exception, but it's so far foiled my casual attempts to convince it that the integer 5 is an exception.

Edit: I suppose I should qualify this: I have successfully convinced Ruby that the integer 5 is both 'null' and 'false' before. Also that false == true. Exceptions to "an object is just the methods it responds to" are *rare.*

&amp;gt; modules can execute arbitrary code, so you can't stop them from throwing objects of type int or foo.

This is true. But you also can't stop something from calling exit(), or in the kernel, from halting the machine. You could certainly force everything to throw std::exception, or even kernel::exception (presumably a custom type), as a matter of policy. &amp;gt; The problem arises when 2 modules define a type with the same name. One module throws it, and the other catches it. Now each module thinks it has a value of the type it defined, but it doesn't.

Wouldn't namespacing solve this issue?

But I will have to go back and read your links when I have more time. Thanks!

&amp;gt; As for speed, remember that C is basically portable assembler.

[No, it isn't.](http://james-iry.blogspot.com/2010/09/moron-why-c-is-not-assembly.html)

&amp;gt; As long as you're not doing anything too complicated you can guarantee performance for certain instructions by assuming the worst case...

That's fair. C++ worst-case is a bit more hidden, and I can see where both of these could provide hard guarantees that, say, Java can't.

&amp;gt; And C++ exceptions are designed to be general. In Java, you can only throw types deriving from Throwable (and IIRC Ruby has a similar restriction);

That's surprising, actually. I knew about, and expected, the Java limitation, but Ruby usually lets you use any object for anything, so long as it implements the right methods. I'm not sure how it's decided what is and isn't an exception, but it's so far foiled my casual attempts to convince it that the integer 5 is an exception.

Edit: I suppose I should qualify this: I have successfully convinced Ruby that the integer 5 is both 'null' and 'false' before. Also that false == true. Exceptions to "an object is just the methods it responds to" are *rare.*

&amp;gt; modules can execute arbitrary code, so you can't stop them from throwing objects of type int or foo.

This is true. But you also can't stop something from calling exit(), or in the kernel, from halting the machine. You could certainly force everything to throw std::exception, or even kernel::exception (presumably a custom type), as a matter of policy. If there's a school that actually teaches OSes in a more hands-on way, I'd be interested. As it is, the OS course I took was really weak. It wasn't *in* Java, but it also didn't involve a lot of coding at all.

Incidentally, why'd you use C to build an OS, rather than C++? I know Linus has strong feelings on the matter, but I never really looked into it. One thing that immediately comes to mind is exception handling. Why would you voluntarily choose error codes over that, even if you aren't using objects for anything? If there's a school that actually teaches OSes in a more hands-on way, I'd be interested. As it is, the OS course I took was really weak. It wasn't *in* Java, but it also didn't involve a lot of coding at all.

Incidentally, why'd you use C to build an OS, rather than C++? I know Linus has strong feelings on the matter, but I never really looked into it. One thing that immediately comes to mind is exception handling. Why would you voluntarily choose error codes over that, even if you aren't using objects for anything? If there's a school that actually teaches OSes in a more hands-on way, I'd be interested. As it is, the OS course I took was really weak. It wasn't *in* Java, but it also didn't involve a lot of coding at all.

Incidentally, why'd you use C to build an OS, rather than C++? I know Linus has strong feelings on the matter, but I never really looked into it. One thing that immediately comes to mind is exception handling. Why would you voluntarily choose error codes over that, even if you aren't using objects for anything? Here a course description for Stanford's OS class:
&amp;gt;This class introduces the basic facilities provided in modern operating systems. The course divides into three major sections. The first part of the course discusses concurrency: how to manage multiple tasks executing at the same time and sharing resources. Topics in this section include processes and threads, context switching, synchronization, scheduling, and deadlock. The second part of the course addresses the problem of physical memory management; it will cover topics such as linking, dynamic memory allocation, dynamic address translation, virtual memory, and demand paging. The third major part of the course concerns file systems, including topics such as storage devices, disk management and scheduling, directories, protection, and crash recovery. After these three major topics, the class will conclude with a few smaller topics such as virtual machines.
**The class work consists of one problem set and a series of four programming projects based on the Pintos kernel**. You will learn a lot from these projects, but be prepared to spend a significant amount of time working on them.

And here's the University of Chicago's OS course description:

&amp;gt;This course provides an introduction to the basic concepts and techniques used to implement operating systems. Topics include processes and threads, interprocess communication and synchronization, memory management, segmentation, paging, linking and loading, scheduling, file systems, and input/output. **The course will revolve around the implementation of an x86 operating system kernel** If there's a school that actually teaches OSes in a more hands-on way, I'd be interested. As it is, the OS course I took was really weak. It wasn't *in* Java, but it also didn't involve a lot of coding at all.

Incidentally, why'd you use C to build an OS, rather than C++? I know Linus has strong feelings on the matter, but I never really looked into it. One thing that immediately comes to mind is exception handling. Why would you voluntarily choose error codes over that, even if you aren't using objects for anything? UWaterloo CS student here. We have a required third year level OS course that is based on OS/161, which requires us to implement thread management, VM, and some syscalls. Not really a lot of coding, but requires a lot of reading. There is also an optional 4th year RTOS course where only the bootloader is given. We were required to implement a microkernel and user space applications on the ARM11 board driving some sensors.

C vs C++... I guess C is the traditional choice which is probably the only reason to choose C instead of C++? If there's a school that actually teaches OSes in a more hands-on way, I'd be interested. As it is, the OS course I took was really weak. It wasn't *in* Java, but it also didn't involve a lot of coding at all.

Incidentally, why'd you use C to build an OS, rather than C++? I know Linus has strong feelings on the matter, but I never really looked into it. One thing that immediately comes to mind is exception handling. Why would you voluntarily choose error codes over that, even if you aren't using objects for anything? For kernel code, you can't use most of the new features that C++ adds without reimplementing them yourself because they require userspace libraries. Anything that requires you to import a header file that you didn't create yourself is pretty much a no go when writing kernel code. The one big feature that C++ gains you for kernel code is RAII but that one feature might not be enough to make it worth using C++ instead of C. &amp;gt; For kernel code, you can't use most of the new features that C++ adds without reimplementing them yourself because they require userspace libraries.

Well, or you port the userspace library. I realize this can't always work, but I'd be surprised if kernels had to completely reimplement string handling, for example.

&amp;gt; The one big feature that C++ gains you for kernel code is RAII but that one feature might not be enough to make it worth using C++ instead of C.

I mean, unless this feature also brings an associated cost, I don't see it as a problem. You don't need to use every feature of C++ in order to use RAII.

That said, I do get that it might not be worth convincing, say, all the current Linux Kernel developers to start using g++ instead of gcc, given that many of them *hate* C++. I'm sympathetic to that, actually, I just hate C also. It's not just the library. There are parts of the grammar which don't work at that lever.

Take memory allocation. There are two big problems:

* New is an operator which relies upon routines _being implemented at the same level._ This means that you can't use those routines to implement new, you have to find another way of doing it.

* The kernel memory allocation function, kmalloc(), takes two parameters: the size, and a special flags variable. The second one could be any of the following:

GFP_KERNEL for normal allocation. The problem with it is that it may go to sleep if the system is running low on memory, making it inappropriate for atomic code (i.e. interrupt handlers.)

GFP_ATOMIC for atomic allocation. This can't sleep, and has a higher chance of failing.

GFP_DMA for allocation DMA memory, for drivers dealing with DMA-capable cards incapable of addressing memory at addresses higher than 16MB.

etc.

So, not only do you end up with a meta-programming headache, you also end up with a new which behaves in quite a different way. In other words, it's not like the library at all.

Or, you could save yourself the headache and do it the C way. &amp;gt; New is an operator which relies upon routines being implemented at the same level. This means that you can't use those routines to implement new, you have to find another way of doing it.

So... you implement new in terms of malloc?

&amp;gt; The kernel memory allocation function, kmalloc(), takes two parameters: the size, and a special flags variable. The second one could be any of the following:

This is problematic, but manageable. For one, the kernel also uses a stack, so we have that. I'd imagine it'd be possible to set some per-thread flag that 'new' checks.

And then there's "placement new", so when you need it, kmalloc can be invoked directly. It's only moderately ugly, and definitely the sort of thing that could be abstracted away. Here's an example I found when looking for this:

    #include &amp;lt;iostream&amp;gt;
    #include &amp;lt;malloc.h&amp;gt;
    using namespace std;

    class Foo {
      public:
        int value;
        Foo();
    };

    Foo::Foo():
      value(1)
    {}

    int main() {
      void *mem = malloc(sizeof(Foo));
      Foo *val = new(mem) Foo();
      cout &amp;lt;&amp;lt; val-&amp;gt;value &amp;lt;&amp;lt; endl;
      val-&amp;gt;~Foo();
      free(mem);
    }

This is the sort of thing that could be abstracted away into a smart pointer class. Didn't feel like finishing the implementation of this one, but it seems simple enough:

    int main() {
      kmalloc_ptr&amp;lt;Foo&amp;gt; val(GFP_DMA);
      cout &amp;lt;&amp;lt; val-&amp;gt;value &amp;lt;&amp;lt; endl;
      // freed on destruction, as it falls out of scope here.
    }

Edit: Fixed a variable name in the above.

That's not a lot of meta-programming. It's certainly not more than the standard libraries already contain with auto_ptr. Meanwhile, the normal 'new' could default to GFP_KERNEL, which does pretty much what you expect. Still, it's kind of ugly, and abstraction and kernels don't really mix well for a variety of reasons. &amp;gt; it's kind of ugly,

Really? Uglier than C?

    int main() {
      Foo *val = kmalloc(sizeof(val), GFP_DMA);
      init_foo(val);
      printf("%d\n", val-&amp;gt;value);
      free(val);
    }

Maybe it's a matter of taste, but I already dislike this. And we haven't even added error checking:

    int main() {
      Foo *val = kmalloc(sizeof(val), GFP_DMA);
      if (val == NULL) return 1;
      if (init_foo(val) == SOME_ERROR) {
        free(val);
        return 2;
      } 
      if (printf("%d\n", val-&amp;gt;value) &amp;lt; 0) {
        free(val);
        return 3;
      } 
      free(val);
      return 0;
    }

I mean, beauty is in the eye of the beholder, maybe, but the above code is something only a mother could love.

The C++ version is shorter, and already handles errors -- if an exception happens anywhere in that, we can be sure 'val' will be properly freed.

&amp;gt; ...abstraction and kernels don't really mix well for a variety of reasons.

I know there are places where this is true, but actually, a lack of abstraction would be a serious problem for a modern kernel. Consider:

* Filesystems. So long as a filesystem provides the right interface, it can be implemented in any way you like.
* Network interfaces and protocols. NFS, CIFS, etc, are all going to need to talk to the network. They shouldn't each have to re-implement drivers for every network card, or their own IP stack, etc.
* Block devices. Unless it's NFS, at least your root filesystem probably sits on top of a block device. It'd suck for your filesystem to need to re-implement every drive controller. As it is, the "block device" concept could be implemented in any way you like -- "ramdisks" (though tmpfs is a better approach), NBD or iSCSI over a network, or local SATA, PATA, USB, FireWire, etc.

...and so on.

What you need is both abstraction and the ability to avoid or bypass that abstraction. The SATA driver needs to provide a block device so that a filesystem doesn't need to know the word "SATA", but it also needs to talk directly to the hardware.

When you have the wrong level of abstraction, you get weird hacks. For example, ramdisks have now almost entirely been replaced by tmpfs, but for a long time, you'd actually pre-allocate a chunk of memory and format an actual ext2 filesystem on it -- you'd have an actual initial ramdisk, not just an init tmpfs.

And while I do get the argument for C over C++, this particular mix of high-level and low-level abstractions sounds like a job tailor-made for C++. In fact, there seem to be places where structs full of function pointers are passed around as callbacks, and whoever calls those function pointers is likely to get a void pointer back to look after -- this seems like a problem tailor-made for a class.

At the same time, you need ridiculously low-level stuff, up to and including inline assembly, and C++ can do that, too -- while not *everything* that worked in C will work in C++, it's pretty close. So even if the filesystem API expected an object rather than a struct, the internal implementation of those filesystems could stay C, or mostly C. You don't need to kill kmalloc to allow a kmalloc-driven new or auto pointer.

There's a more practical argument that I'm never going to get the Linux kernel ported to C++, which makes sense. I mean, even if there was no other issue, Linus hates the language. But I do think that if I were starting a new kernel today, I'd at least try C++, if not something entirely crazy like [a fully-managed OS](http://cosmos.codeplex.com/). (Entirely crazy is not used sarcastically here; Cosmos actually working sounds crazier to me than convincing Linus that Linux should use C++.) I think the C version is prettier: at least it's clear at all times what you are doing exactly.

Instead of arguing over the merits of one language over the other, I'd like to recommend the following, because I'm really bad at speaking and arguing:

http://kerneltrap.org/node/2067

The LKML FAQ question to which the article linked:
http://www.tux.org/lkml/#s15-3

The top answer on this Stack Overflow question really nails it:
http://stackoverflow.com/questions/520068/why-is-the-linux-kernel-not-implemented-in-c
 If there's a school that actually teaches OSes in a more hands-on way, I'd be interested. As it is, the OS course I took was really weak. It wasn't *in* Java, but it also didn't involve a lot of coding at all.

Incidentally, why'd you use C to build an OS, rather than C++? I know Linus has strong feelings on the matter, but I never really looked into it. One thing that immediately comes to mind is exception handling. Why would you voluntarily choose error codes over that, even if you aren't using objects for anything? You can take OS in Java? Transfer schools. I don't mean to be rude, but seriously, *you cannot modify the OS in any major way* with Java. Technically, I guess you can with the JNI - but that's an extremely convoluted wrapper around native C code.

I took OS and used C exclusively, and we re-wrote linux kernel modules, added drivers, and messed with the process run queue and stuff. It was a lot of fun... that said, 0% of that can be done with Java. More to the point, I think it would be difficult in any language with garbage collection. C++ can be bent enough to work, but... ew.

I've written C, C++, C#, Java, and x86 (if we're talking systems stuff). I've done embedded systems work. I've written wrappers with the JNI. I've ported [NanoVM](http://www.harbaum.org/till/nanovm/index.shtml) (a tiny JVM) to Zilog processors. You cannot do actual systems-level programming with Java, period. It requires C.

I really don't mean to be so negative, but that is really scary to me. Take a look at [http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/](http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/). It's an older version of the linux kernel. It's *all* C code (and still is). No C++, no C# (obviously), no Java. The kernel is truly the OS. Applications talk to it, and it talks to hardware for the applications. Windows *should* use all C in the kernel as well, but I have no idea.

If nothing else, learn C++. Sad to say it, but many (most?) undergrad CS programs don't actually teach kids how computers work.  Even many CE programs (for schools that distinguish between the two) start people off in Lisp/Scheme/whatever and go directly to Java.  

It's terrifying.  Good job security, I guess...

FWIW, I think Windows is actually C++.  But you're right about the Linux Kernel, and kernels in general.  Nobody in their right mind would write a kernel at a higher level of abstraction than C.  (And no, that does not contradict my previous statement about Windows.) Windows is C, writing C++ drivers is possible, but unsupported. I was under the impression Windows NT is mostly C++ and Linux/Unix is only C . Windows NT is still the underlying tech in the windows kernel.... so... You can take OS in Java? Transfer schools. I don't mean to be rude, but seriously, *you cannot modify the OS in any major way* with Java. Technically, I guess you can with the JNI - but that's an extremely convoluted wrapper around native C code.

I took OS and used C exclusively, and we re-wrote linux kernel modules, added drivers, and messed with the process run queue and stuff. It was a lot of fun... that said, 0% of that can be done with Java. More to the point, I think it would be difficult in any language with garbage collection. C++ can be bent enough to work, but... ew.

I've written C, C++, C#, Java, and x86 (if we're talking systems stuff). I've done embedded systems work. I've written wrappers with the JNI. I've ported [NanoVM](http://www.harbaum.org/till/nanovm/index.shtml) (a tiny JVM) to Zilog processors. You cannot do actual systems-level programming with Java, period. It requires C.

I really don't mean to be so negative, but that is really scary to me. Take a look at [http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/](http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/). It's an older version of the linux kernel. It's *all* C code (and still is). No C++, no C# (obviously), no Java. The kernel is truly the OS. Applications talk to it, and it talks to hardware for the applications. Windows *should* use all C in the kernel as well, but I have no idea.

If nothing else, learn C++. You can take OS in Java? Transfer schools. I don't mean to be rude, but seriously, *you cannot modify the OS in any major way* with Java. Technically, I guess you can with the JNI - but that's an extremely convoluted wrapper around native C code.

I took OS and used C exclusively, and we re-wrote linux kernel modules, added drivers, and messed with the process run queue and stuff. It was a lot of fun... that said, 0% of that can be done with Java. More to the point, I think it would be difficult in any language with garbage collection. C++ can be bent enough to work, but... ew.

I've written C, C++, C#, Java, and x86 (if we're talking systems stuff). I've done embedded systems work. I've written wrappers with the JNI. I've ported [NanoVM](http://www.harbaum.org/till/nanovm/index.shtml) (a tiny JVM) to Zilog processors. You cannot do actual systems-level programming with Java, period. It requires C.

I really don't mean to be so negative, but that is really scary to me. Take a look at [http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/](http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/). It's an older version of the linux kernel. It's *all* C code (and still is). No C++, no C# (obviously), no Java. The kernel is truly the OS. Applications talk to it, and it talks to hardware for the applications. Windows *should* use all C in the kernel as well, but I have no idea.

If nothing else, learn C++. You can take OS in Java? Transfer schools. I don't mean to be rude, but seriously, *you cannot modify the OS in any major way* with Java. Technically, I guess you can with the JNI - but that's an extremely convoluted wrapper around native C code.

I took OS and used C exclusively, and we re-wrote linux kernel modules, added drivers, and messed with the process run queue and stuff. It was a lot of fun... that said, 0% of that can be done with Java. More to the point, I think it would be difficult in any language with garbage collection. C++ can be bent enough to work, but... ew.

I've written C, C++, C#, Java, and x86 (if we're talking systems stuff). I've done embedded systems work. I've written wrappers with the JNI. I've ported [NanoVM](http://www.harbaum.org/till/nanovm/index.shtml) (a tiny JVM) to Zilog processors. You cannot do actual systems-level programming with Java, period. It requires C.

I really don't mean to be so negative, but that is really scary to me. Take a look at [http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/](http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/). It's an older version of the linux kernel. It's *all* C code (and still is). No C++, no C# (obviously), no Java. The kernel is truly the OS. Applications talk to it, and it talks to hardware for the applications. Windows *should* use all C in the kernel as well, but I have no idea.

If nothing else, learn C++. My senior-level OS class only had assignments on:

1. Communicating between processes
2. Threads
3. Synchronization
4. Page Replacement algorithm analysis You can take OS in Java? Transfer schools. I don't mean to be rude, but seriously, *you cannot modify the OS in any major way* with Java. Technically, I guess you can with the JNI - but that's an extremely convoluted wrapper around native C code.

I took OS and used C exclusively, and we re-wrote linux kernel modules, added drivers, and messed with the process run queue and stuff. It was a lot of fun... that said, 0% of that can be done with Java. More to the point, I think it would be difficult in any language with garbage collection. C++ can be bent enough to work, but... ew.

I've written C, C++, C#, Java, and x86 (if we're talking systems stuff). I've done embedded systems work. I've written wrappers with the JNI. I've ported [NanoVM](http://www.harbaum.org/till/nanovm/index.shtml) (a tiny JVM) to Zilog processors. You cannot do actual systems-level programming with Java, period. It requires C.

I really don't mean to be so negative, but that is really scary to me. Take a look at [http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/](http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/). It's an older version of the linux kernel. It's *all* C code (and still is). No C++, no C# (obviously), no Java. The kernel is truly the OS. Applications talk to it, and it talks to hardware for the applications. Windows *should* use all C in the kernel as well, but I have no idea.

If nothing else, learn C++. You can take OS in Java? Transfer schools. I don't mean to be rude, but seriously, *you cannot modify the OS in any major way* with Java. Technically, I guess you can with the JNI - but that's an extremely convoluted wrapper around native C code.

I took OS and used C exclusively, and we re-wrote linux kernel modules, added drivers, and messed with the process run queue and stuff. It was a lot of fun... that said, 0% of that can be done with Java. More to the point, I think it would be difficult in any language with garbage collection. C++ can be bent enough to work, but... ew.

I've written C, C++, C#, Java, and x86 (if we're talking systems stuff). I've done embedded systems work. I've written wrappers with the JNI. I've ported [NanoVM](http://www.harbaum.org/till/nanovm/index.shtml) (a tiny JVM) to Zilog processors. You cannot do actual systems-level programming with Java, period. It requires C.

I really don't mean to be so negative, but that is really scary to me. Take a look at [http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/](http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/). It's an older version of the linux kernel. It's *all* C code (and still is). No C++, no C# (obviously), no Java. The kernel is truly the OS. Applications talk to it, and it talks to hardware for the applications. Windows *should* use all C in the kernel as well, but I have no idea.

If nothing else, learn C++. You can take OS in Java? Transfer schools. I don't mean to be rude, but seriously, *you cannot modify the OS in any major way* with Java. Technically, I guess you can with the JNI - but that's an extremely convoluted wrapper around native C code.

I took OS and used C exclusively, and we re-wrote linux kernel modules, added drivers, and messed with the process run queue and stuff. It was a lot of fun... that said, 0% of that can be done with Java. More to the point, I think it would be difficult in any language with garbage collection. C++ can be bent enough to work, but... ew.

I've written C, C++, C#, Java, and x86 (if we're talking systems stuff). I've done embedded systems work. I've written wrappers with the JNI. I've ported [NanoVM](http://www.harbaum.org/till/nanovm/index.shtml) (a tiny JVM) to Zilog processors. You cannot do actual systems-level programming with Java, period. It requires C.

I really don't mean to be so negative, but that is really scary to me. Take a look at [http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/](http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/). It's an older version of the linux kernel. It's *all* C code (and still is). No C++, no C# (obviously), no Java. The kernel is truly the OS. Applications talk to it, and it talks to hardware for the applications. Windows *should* use all C in the kernel as well, but I have no idea.

If nothing else, learn C++. You can take OS in Java? Transfer schools. I don't mean to be rude, but seriously, *you cannot modify the OS in any major way* with Java. Technically, I guess you can with the JNI - but that's an extremely convoluted wrapper around native C code.

I took OS and used C exclusively, and we re-wrote linux kernel modules, added drivers, and messed with the process run queue and stuff. It was a lot of fun... that said, 0% of that can be done with Java. More to the point, I think it would be difficult in any language with garbage collection. C++ can be bent enough to work, but... ew.

I've written C, C++, C#, Java, and x86 (if we're talking systems stuff). I've done embedded systems work. I've written wrappers with the JNI. I've ported [NanoVM](http://www.harbaum.org/till/nanovm/index.shtml) (a tiny JVM) to Zilog processors. You cannot do actual systems-level programming with Java, period. It requires C.

I really don't mean to be so negative, but that is really scary to me. Take a look at [http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/](http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/). It's an older version of the linux kernel. It's *all* C code (and still is). No C++, no C# (obviously), no Java. The kernel is truly the OS. Applications talk to it, and it talks to hardware for the applications. Windows *should* use all C in the kernel as well, but I have no idea.

If nothing else, learn C++. You can take OS in Java? Transfer schools. I don't mean to be rude, but seriously, *you cannot modify the OS in any major way* with Java. Technically, I guess you can with the JNI - but that's an extremely convoluted wrapper around native C code.

I took OS and used C exclusively, and we re-wrote linux kernel modules, added drivers, and messed with the process run queue and stuff. It was a lot of fun... that said, 0% of that can be done with Java. More to the point, I think it would be difficult in any language with garbage collection. C++ can be bent enough to work, but... ew.

I've written C, C++, C#, Java, and x86 (if we're talking systems stuff). I've done embedded systems work. I've written wrappers with the JNI. I've ported [NanoVM](http://www.harbaum.org/till/nanovm/index.shtml) (a tiny JVM) to Zilog processors. You cannot do actual systems-level programming with Java, period. It requires C.

I really don't mean to be so negative, but that is really scary to me. Take a look at [http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/](http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/). It's an older version of the linux kernel. It's *all* C code (and still is). No C++, no C# (obviously), no Java. The kernel is truly the OS. Applications talk to it, and it talks to hardware for the applications. Windows *should* use all C in the kernel as well, but I have no idea.

If nothing else, learn C++. You can take OS in Java? Transfer schools. I don't mean to be rude, but seriously, *you cannot modify the OS in any major way* with Java. Technically, I guess you can with the JNI - but that's an extremely convoluted wrapper around native C code.

I took OS and used C exclusively, and we re-wrote linux kernel modules, added drivers, and messed with the process run queue and stuff. It was a lot of fun... that said, 0% of that can be done with Java. More to the point, I think it would be difficult in any language with garbage collection. C++ can be bent enough to work, but... ew.

I've written C, C++, C#, Java, and x86 (if we're talking systems stuff). I've done embedded systems work. I've written wrappers with the JNI. I've ported [NanoVM](http://www.harbaum.org/till/nanovm/index.shtml) (a tiny JVM) to Zilog processors. You cannot do actual systems-level programming with Java, period. It requires C.

I really don't mean to be so negative, but that is really scary to me. Take a look at [http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/](http://lxr.linux.no/linux-bk+v2.6.11.5/kernel/). It's an older version of the linux kernel. It's *all* C code (and still is). No C++, no C# (obviously), no Java. The kernel is truly the OS. Applications talk to it, and it talks to hardware for the applications. Windows *should* use all C in the kernel as well, but I have no idea.

If nothing else, learn C++. Yeah, object orientation is pretty unnecessary for OS work. Object Oriented Design is used extensively in Linux, in C. Look at the VFS. It's all OO. OO has nothing to do with running on an abstract VM that hides all the details of the underlying computer from you, like Java does. I was really arguing against both C++ AND Java, in favor of C.  I admit I am not a kernel hacker though, so perhaps my opinion was a little uninformed. C++ straightforwardly desugars to C. You can do the kernel in C++, as long as all your programmers are C programmers. They can use C++ responsibly. Other people can't use C++ responsibly because they don't understand how it works. They should use Java instead.

Java requires a lot of runtime support to work and cannot work on bare hardware. The problem is not OO. OO is meaningless. Structs embedded in other structs and arrays of pointers to functions are not a problem. The problem is bare pointers and unchecked casts, those make it possible to work with hardware and make it impossible to give the guarantees that are required for Java to work. Object Oriented Design is used extensively in Linux, in C. Look at the VFS. It's all OO. OO has nothing to do with running on an abstract VM that hides all the details of the underlying computer from you, like Java does. Without inheritance and polymorphism you cannot call a language OO. C code can be written in OO style, but OO does have something to do with certain language features that C does not have and cannot be implemented without creating something equivalent to Objective C or C++. mercurycc - I absolutely disagree with your comment. I'm not going to toot my own 'success' stories writing C code that is both polymorphic and has the ability to support inheritance, but believe me it exists. 

That said, as someone who has written C prior to C++ being a standard - it's nice having C++. You're not wrong there, but it absolutely can be done. C is a language that is tried and true... it can be manipulated, spindled, and mutilated. 

tl;dr: C can be used as an OO language, albeit its not for the feint of heart. I totally agree that C is very flexible. I think it is not really much more limiting than assembly. If you do want to get hand crafted OO working in C then as you demonstrated, it can be done. I have to admit I have not really thought this through, but in my mind also there are ways to do almost all OO features I know in C.

I have to add though, without changing the compiler at least you don't get the type soundness, and the syntax won't be good. Changing the compiler get you those, but then you are not far from something like C++. What does type soundness have to do with OO design? Without inheritance and polymorphism you cannot call a language OO. C code can be written in OO style, but OO does have something to do with certain language features that C does not have and cannot be implemented without creating something equivalent to Objective C or C++.  If you want to really learn how to develop an OS, C++ would be the better option.  Java's API obfuscates so much behind the scenes, and it doesn't make you appreciate things as much.  If you're really dreading the class though and you don't think you'll have as much time to dedicate to it as you should, then I would suggest Java.  My school only offered OS in java, and it really wasn't too difficult.  It's still a lot of work, but working in C would definitely be more complicated   Operating Systems is to be done in C. Anyone telling you otherwise should be promptly smacked. [Holy shit, we need to get you on the phone with UC Berkeley!](http://www-inst.eecs.berkeley.edu/~cs162/sp12/)

Actually we'll probably just need to fly you out there, since you're going to have to do a lot of smacking.

Edit: Downvoting won't change the validity of this post. But it's cool, I know what haters do. They hate. [Well, not me, but maybe these guys instead](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-828-operating-system-engineering-fall-2006/). So UC Berkeley and MIT use different languages for the OS courses? Wow! It's almost like programming language isn't the most important factor in creating a successful OS course! Amazing! So you're saying that having a hands-on knowledge of what *real* OS development is doesn't matter?  &amp;gt; So you're saying that having a hands-on knowledge of what real OS development is doesn't matter?

Not sure where you got that, or what you mean by "real OS development." What I am saying is that OS courses are about the concepts involved in running an operating system. "Toy" operating systems that are stripped down and easy to mess around with, but not adequate for actual use, are good for these courses. The one MIT uses (in the link you provided), isn't a "real" operating system that anybody uses, it's a stripped down remake of a 1970's Unix kernel for students to mess around with. The one Berkeley uses is built in Java, but it's the same concept.

"Real" operating system development, like working on the modern Linux kernel, is obviously important, but it's not what an introductory OS course is about. Wading through the modern Linux kernel's 13.5+ million lines of code would be an incredibly inefficient way of learning OS concepts. It would be kinda like trying to teach somebody graphics by having them mess around with the Call of Duty source code. I'm interested in hearing what sorts of concepts you think are essential to know, especially for linux, in OS courses.The reason I ask is because I have chosen the self-taught route (with guidance from top uni programs) and didn't think these OS courses were anything more than learning how to install/get around a linux system.

So far, scripting, ruby, python, and coreutils are what I'm familiar with. Any input would be much appreciated!

   I am surprised that you are offered an Operating System course in Java!  My OS course was roughly 90% theory and 10% coding. Our coding assignments were in Java. The professor said that the only reason we used Java was because so many people knew it already and he wanted us to focus on the theory vs learning a new language (most people didn't know C/C++ for example).  Its surprising how a lot of colleges dont teach C/C++. How the hell do you guys learn about pointers? Its surprising how a lot of colleges dont teach C/C++. How the hell do you guys learn about pointers?     I would like to point out [this](http://www.dabeaz.com/coroutines/) presentation. Especially Part 7, which revolves in building the fundamentals of an OS in *Python*. I found it enlightening, and it shows very well that the underlying concepts are independent of programming languages.

Also &#8220;Computer science is no more about computers than astronomy is about telescopes&#8221;. (Dijkstra)  If you are not doing an OS course in C, you are doing it wrong. If you're not writing an **OS** in C, you're doing it wrong.

If you're learning how scheduling and memory management works, whatever language you know best is the right tool because the algorithms are hard enough without learning a new language while you're at it I would have to argue that if you are in an OS class without getting your hands dirty with a real OS, you are doing it wrong.  I am currently in an OS course.  I have had very minimal prior C experience.  Actually going into a real OS, figuring things out, and implementing new features has proven absolutely invaluable. But it's not vocational training, the point of the OS class is to get to know how an OS works, as a general concept. Implementing features may be fun &amp;amp; useful for knowing where to find stuff in ( Linux, BSD, etc ) but not at all useful for knowing how a scheduler works ( different scheduling algorithms ), how the memory management  works, etc. Sorry, you are right, I forgot that the job of a university isn't to prepare you for a career. Wait.. First of all, it isn't. Second of all, there aren't that many careers in which you'd need to know how to write a particular driver for Linux but can afford to gloss over memory management.  I've commented a few other places in the thread about how it would be *okay* to take OS in Java.  That said, I'm still going to fall on the side of take it in C++ -- sure, dealing with pointers is a little tricky, but honestly, I understood pointers in C/C++ infinitely better after learning Java.  (The trick is that all non-primitive types in Java are really pointers.  lvalue references and rvalue references in C++ are another can of worms, though.)  But if you can code in Java, you will be able to code in C++, plus or minus a few head scratchers of why you're getting garbled output and some time searching for the API you want, and gaining familarity with the C or C++ standard libraries will definitely be to your advantage when job search time comes around.  Do you know what pointers are? If you don't, then I fear that C++ or C would be a horrific pain. If you don't you need to learn, this is college, this is where you learn these things. I wouldn't go insofar as to say he/she needs to learn how to use them. He/she might be better off becoming an expert in Java, or C# (*gross*), or PHP (*get out*). Or, he/she might not want to be a software engineer at all. It all depends on what his/her goals are regarding Computer Science.

My opinion? Learn C++. Then master C and C++ at the same time by learning the innards of both worlds. Then learn more about agnostic object-oriented design, and then... go from there in whatever direction you want. You think it's ok to graduate with a degree in computer science without learning how *pointers* work? Are you serious? Yes, I do, if you're graduating with the intent of studying computational complexity, or the other areas of computer science that require no programming language whatsoever.

Or, if they are graduating for the sake of getting a job as a software developer of a language that doesn't expose pointers. In that case, they shouldn't be in college. They should be attending a trade institution. Do you know what pointers are? If you don't, then I fear that C++ or C would be a horrific pain. ... or a learning experience. I run into far more people who abuse pointers more than they use them. I am a tutor for a Data Structures class that introduces pointers as the primary tool for implementing simple structures in C++. Far too many people allow memory leaks to slide so long as it compiles and works.

I tell them I won't help them unless they're willing to eliminate those memory leaks. I run into far more people who abuse pointers more than they use them. I am a tutor for a Data Structures class that introduces pointers as the primary tool for implementing simple structures in C++. Far too many people allow memory leaks to slide so long as it compiles and works.

I tell them I won't help them unless they're willing to eliminate those memory leaks. You're complaining that students learning about pointers for the first time end up using them incorrectly? Then you refuse to help until they fix a problem they've likely never ran into before, a problem that affects a lot of software put out by programmers making full salaries with benefits?

Honestly, you sound like a complete asshole of an instructor. In another comment you say that students don't need to learn how pointers work for a *computer science* degree. I really hope your school comes to its senses and fires you soon so that you have less time to poison the upcoming generation of students.

I am fully aware of how rude this comment is and would like to add that I am being more respectful than I feel is warranted with people like you. You've assumed too much about my personality and tutoring skills, and I'd like to correct that assumption. Unfortunately, I fear the hive-mind has spoken and shunned my approach to this problem.

I'm not complaining. I'm pointing out an observation that I've seen many times. This is their first time learning about pointers, so I will help them to use them properly if they are abusing them. Some students just say "nah, it's okay, my code works" and move on. This is a sign of laziness that could get them fired later on in their career. I am willing to help other students who want to use pointers properly; for those who are willing to "new" a pointer, and not properly "delete" it, I correct them by telling them "nuh uh, you've made a mistake". If they reply "no, it's okay", I try harder. However, there's a time when I have to say "whatever" and move on to students who are willing to work hard.

If a student is studying Computer Science for Software Engineering, then an intimate familiarity with pointers is necessary. If they are studying Computer Science for Computational Complexity, or Language Theory, or some other field that requires no need for a programming language, then I am one to say that knowledge of pointers is unnecessary. They are a tool of an implementation of a theoretical system.

I received an award this year for Outstanding Service, and I am watched over by other graders who all directly report to the professor teaching this C++ class. However, apparently I poked the wasp nest, so I will be addressing this disagreement issue in a new self post because I feel your accusations are closed-minded. I run into far more people who abuse pointers more than they use them. I am a tutor for a Data Structures class that introduces pointers as the primary tool for implementing simple structures in C++. Far too many people allow memory leaks to slide so long as it compiles and works.

I tell them I won't help them unless they're willing to eliminate those memory leaks.    If there's no assembly you should quit your program immediately.     The anti-Java circlejerk this thread has become is an embarassment to r/compsi.

First and foremost, it is completely possible to have a good OS course in Java. Don't take my word for it, [UC Berkeley does just that](http://www-inst.eecs.berkeley.edu/~cs162/sp12/). The fact that the JVM is garbage collected, hides pointers from the programmer, abstracts many things away behind API's, etc, is completely irrelevant. People are failing to realize that an OS implemented in Java and the JVM itself are two different things. This is such a basic concept I can't believe it needs to be pointed out, but holy shit does it ever.

That being said, I can't really comment on your situation too much. I think learning C++ before you graduate is a great idea, but I have no idea if learning it through your schools OS course is going to be a good idea. If I were in your situation I would probably base my course selection on the ratings of the respective teachers, IMO that will have a much bigger impact on how much you learn than the specific programming language. part of a good CS education should include exposing students to the way things are actually done. honestly, if I'm with msoft looking to hire an intern for my systems team and i have the choice between somebody who knows / used C to build an OS vs. somebody who used Java for an OS you bet your ass I'll take the kid that knows C

i'm all in favor of a theoretical CS education *where applicable*. I just don't think something like systems engineering has as much leeway as say, algorithmic analysis

EDIT: also, don't you think the historical context you get from building it in the language that the first OSes were built in is important? not to mention experience with debugging when you don't have an entire OS built up already? &amp;gt; part of a good CS education should include exposing students to the way things are actually done.

I completely agree.

&amp;gt; honestly, if I'm with msoft looking to hire an intern for my systems team and i have the choice between somebody who knows / used C to build an OS vs. somebody who used Java for an OS you bet your ass I'll take the kid that knows C

Knowing C and an OS course being taught in C are not, in any way, the same thing. I don't even know how you got to that conclusion. A student can know C and still have taken their OS course in Java. I honestly don't even know how else to respond to this statement.

&amp;gt; i'm all in favor of a theoretical CS education where applicable. I just don't think something like systems engineering has as much leeway as say, algorithmic analysis

Memory paging and segmenting, caching, virtual memory, scheduling, threading, deadlocking, inter-process communication etc. are just a few of the core of OS concepts, and they are not, in any way, language dependent. I don't know what you mean by "theoretical CS education" but if you're saying students should actually be writing programs to explore these concept then I completely agree. If you're saying that writing programs in anything except C is purely theoretical, then I'll have to disagree.

&amp;gt; EDIT: also, don't you think the historical context you get from building it in the language that the first OSes were built in is important?

No. [And from what you're saying, neither do you.](http://en.wikipedia.org/wiki/Timeline_of_operating_systems). (hint, none of the "first OSes" were built with C)

&amp;gt; not to mention experience with debugging when you don't have an entire OS built up already?

I'm not sure what you're even saying here, or how it relates to any specific programming language. in reverse order

 - I'm saying debugging when you're starting an OS from scratch is kinda tough and time - consuming. Less - so when you are just simulating an OS. 

 - Don't be pedantic, it's the first OSes that were built with a language that's still used to build OSes today (or more generally, even still used today). 

 - All fair points, and you're right that I think these concepts could be expressed in any language. 

 - In terms of an undergrad curriculum, I'm not sure there's any course besides OS that really rapes you with the ins and outs of the C language to the same degree.  The anti-Java circlejerk this thread has become is an embarassment to r/compsi.

First and foremost, it is completely possible to have a good OS course in Java. Don't take my word for it, [UC Berkeley does just that](http://www-inst.eecs.berkeley.edu/~cs162/sp12/). The fact that the JVM is garbage collected, hides pointers from the programmer, abstracts many things away behind API's, etc, is completely irrelevant. People are failing to realize that an OS implemented in Java and the JVM itself are two different things. This is such a basic concept I can't believe it needs to be pointed out, but holy shit does it ever.

That being said, I can't really comment on your situation too much. I think learning C++ before you graduate is a great idea, but I have no idea if learning it through your schools OS course is going to be a good idea. If I were in your situation I would probably base my course selection on the ratings of the respective teachers, IMO that will have a much bigger impact on how much you learn than the specific programming language. You have become an embarrassment to /r/compsci       </snippet></document><document><title>Real time systems subreddit ?</title><url>http://www.reddit.com/r/compsci/comments/sjwhh/real_time_systems_subreddit/</url><snippet>Hi !

I just finished my degree in college, and I found an internship in a lab specialized in real time scheduling (not sure about the term in english), continuous time identification, feedback scheduling...

I'm entirely new to these topics. I learnt mostly to write low-level C routines for Linux, got some basic understanding about OS and processor architecture, but not much else.

This internship is to begin in 3 weeks or so, and I'd like to start reading on the subject before the real work start. I found several publications from the researchers who will lead my work, and I don't understand everything.

I'd like to know if there is some specialized subreddit about this subject ? [/r/realtime](/r/realtime) doesn't seems alive.

Thanks !</snippet></document><document><title> What are Some Lesser Known Areas of Computer Science </title><url>http://www.reddit.com/r/compsci/comments/sil23/what_are_some_lesser_known_areas_of_computer/</url><snippet>Im intrested in computer science and want to know sub-fields and discplines exit besides the more straight forward ones (i.e those listed in the wikipedia article). They could be in either the theoretical or applicative side of things.    [Computational Erotica](http://www.google.com/search?q=Computational Erotica)? [Computational Egyptology](http://www.google.com/search?q=Computational Egyptology)? [Computational Informatics](http://www.google.com/search?q=Computational Informatics)? [Webscalogy](http://www.google.com/search?q=Webscalogy)? Webscalogy's not on google   Personally, I'm interested in Computer Science Education and Educational Technology. IEEE CSEET is a good conference.  Also an interesting and under appreciated part of computer science. Personally, I'm interested in Computer Science Education and Educational Technology. [deleted] I have not! I'm still very new to the field. I was only able to see a few of her works after a google search; apparently she shares the name with a breaking bad character and it's clogging up my search haha. [deleted]     Computational geometry? Bioinformatics? Natural language processing? Hey! Natural Language Processing is well known! We have friends too! And feelings! Hey, I'm interested in NLP. How does one get a job in it? Hey, I'm interested in NLP. How does one get a job in it? 1) Learn basic principles of CS

2) Learn python / c++

3) Be interested in linguistics

4) Do some cool personal projects with NLP techniques

5) Show your skills to company

6) They will hire you
 Can you suggest some personal projects in NLP? I am just taking a course in NLP from coursea. thanks Hey, I'm interested in NLP. How does one get a job in it? Hey! Natural Language Processing is well known! We have friends too! And feelings! [Fuck Computational Linguists](http://xkcd.com/114/) Computational geometry? Bioinformatics? Natural language processing?   quantum computers?     Technically I study VLSI design optimization, but in reality it's Evolutionary Algorithms for solving NP-complete problems using parallel processing. Anything good for reading more on this?        human-computer interaction

computational learning theory

various branches of economics: computational social choice, algorithmic game theory, mechanism design, ....

lots of subfields within systems

lots of subfields within programming languages

lots of applications of machine learning

randomized algorithms and analysis

etc http://en.wikipedia.org/wiki/Computational_economics</snippet></document><document><title>Math undergrad looking for any recommendations of interesting or useful titles/textbooks/manuals in computer science out of general interest.</title><url>http://www.reddit.com/r/compsci/comments/si004/math_undergrad_looking_for_any_recommendations_of/</url><snippet>I'm a third-year mathematics undergrad who's always had a bit of a crush on computer science from across the room. I was wondering if anyone could recommend any good titles/textbooks/manuals for someone with a general interest in the field. I'm not looking for anything in particular; they could be anything from instructional manuals on programming languages, entry-level texts on theoretical comp sci, or works considered to be of historical significance within the field. Here's a little background information about my experience with the field to illustrate where I am.



I took a course on java that went from the absolute basics, through defining/using methods, ending, if I recall, with using PrintReader/PrintWriter to import/read an input file and write to an output file. Prior to that I took a similar course in C++, the exact scope of which I can't recall, though I remember the final project was to write a program where the user plays games of rock-paper-scissors "against the computer." I did well in both classes. I've also done some **very** basic reading on theoretical computer science; I have a vague knowledge of what computational complexity entails, I know what a Turning Machine is, etc. Also, I'm fascinated with cellular automata, like Conway's Life, though I know very little about them; I've mostly just *read about* them.



Anyway, this post is already sufficiently long-winded. Sorry if this was the wrong place to post this. If so, let me know where it ought to go.

Thanks in advance for any recommendations.  Sipser is a great introduction to computability and languages. Using a Sipser book in my Formal Language and Automata Theory course. Very, very interesting class and book.

I'd second the recommendation.  I'd third the recommendation. Third it so hard. By far one of the best textbooks I've had in all my Comp. Sci. classes. My local used book store has a copy of Sipser for $15 that I've been meaning to pick up. Considering the $143 price tag on [Amazon](http://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/0534950973/ref=sr_1_1?ie=UTF8&amp;amp;qid=1334876863&amp;amp;sr=8-1), it's a pretty good bargain. I just don't know whether it's 1st or 2nd edition. Anyone have any idea if there are major differences? Sipser is a great introduction to computability and languages. Sipser is a great introduction to computability and languages. I highly recommend looking at Hopcroft either instead or in tandem. I found it a bit more rigorous and complete. For the OP, the rigor/completeness may not be worth the tradeoff. Sipser is a very smooth read, which makes it very suitable to read on your own as opposed to for a class or to consult as a reference. Sipser is a great introduction to computability and languages. Sipser is a great introduction to computability and languages.  There's a pretty good list at:
http://www.codinghorror.com/blog/2004/02/recommended-reading-for-developers.html


Personally, I'd recommend the following:

* [Concrete Mathematics](http://www.amazon.com/Concrete-Mathematics-Foundation-Computer-Science/dp/0201558025/ref=sr_1_1?ie=UTF8&amp;amp;qid=1334863142&amp;amp;sr=8-1)
* [The C Programming Language](http://www.amazon.com/C-Programming-Language-2nd-Edition/dp/0131103628/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1334863184&amp;amp;sr=1-1)
* [The Art of Computer Programming](http://www.amazon.com/Computer-Programming-Volumes-1-4A-Boxed/dp/0321751043/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1334863203&amp;amp;sr=1-1)
* [Beautiful Code](http://www.amazon.com/Beautiful-Code-Leading-Programmers-Practice/dp/0596510047/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1334863222&amp;amp;sr=1-1)
* [Programming Pearls](http://www.amazon.com/Programming-Pearls-2nd-Edition-Bentley/dp/0201657880/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1334863239&amp;amp;sr=1-1)
* [The Best Software Writing](http://www.amazon.com/The-Best-Software-Writing-Introduced/dp/1590595009/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1334863254&amp;amp;sr=1-1)
* [Code Complete](http://www.amazon.com/Code-Complete-Practical-Handbook-Construction/dp/0735619670/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1334863268&amp;amp;sr=1-1)  I'm also a math major who turned into CS. There are already a lot of good recommendations here so I won't add much, but I suggest reading [Code: The Hidden Language of Computer Hardware and Software](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?ie=UTF8&amp;amp;qid=1334870917&amp;amp;sr=8-1) by Charles Petzold.
 
It's not very technical and it's not in-depth, but I think it's an amazing book. You probably won't learn anything you're actually going to use by reading it, but I think this book has a unique ability for expressing the underlying facts that make us all find computer science so fascinating. It's a very fun read and it will give you a very broad overview of how computers work and how software gets compiled and ultimately ends up moving electrons around to make the magic happen.  Look at *The Nature of Computation* by Moore and Mertens.
 Look at *The Nature of Computation* by Moore and Mertens.
   ["Structure and Interpretation of Computer Programs"(SICP](http://mitpress.mit.edu/sicp/full-text/book/book.html) is classic, wise, deep and useful. 
["The Haskell Road to Logic, Maths, and Programming"](http://homepages.cwi.nl/~jve/HR/) might be a good way to reinterpret the math you already know from a programming point of view.

  [The art of computer programming](http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming)

[Introduction to Algorithms](http://en.wikipedia.org/wiki/Introduction_to_Algorithms)

Also, look [here](http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books)

Edit: 

Also, [on implementing interpreters](http://www.cs.brown.edu/~sk/Publications/Books/ProgLangs/)

I'd also recommend just picking a language (Python is a good once to start) and trying to implement things that interest you, dive into the official documentation. I've been looking for a good book on interpreters for a while now. Thanks! I have my exam on that book in 5 hours =(
I warn you, its all *functional programming.* Well, no time like the present to learn it then! Rocked it. I've been looking for a good book on interpreters for a while now. Thanks! [The art of computer programming](http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming)

[Introduction to Algorithms](http://en.wikipedia.org/wiki/Introduction_to_Algorithms)

Also, look [here](http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books)

Edit: 

Also, [on implementing interpreters](http://www.cs.brown.edu/~sk/Publications/Books/ProgLangs/)

I'd also recommend just picking a language (Python is a good once to start) and trying to implement things that interest you, dive into the official documentation. Why do people always recommend books that are awful for beginners? Neither of those books is going to be useful for someone who's just learned the very basics of programming and CS.[1] They're incredible books, but not something to invest in when there are *plenty* of free books and resources online. 

[Udacity](http://www.udacity.com) has some interesting classes designed for beginners.

MIT's [Structure and Interpretation of Computer Programs](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-001-structure-and-interpretation-of-computer-programs-spring-2005/) has good lectures and a great book online.

Berkeley's lectures are online [here](http://webcast.berkeley.edu/series.html#c,d,Computer_Science). I'd recommend [CS61B](http://webcast.berkeley.edu/playlist#c,d,Computer_Science,4BBB74C7D2A1049C) which is data structures (in Java). Occasionally the mechanics of the class get in the way, but the professor is very good.

There are also a lot of good blogs to read through: [Nick Johnson](http://blog.notdot.net/), [Coding Horror](http://www.codinghorror.com/blog/), [Gustavo Duarte](http://duartes.org/gustavo/blog/best-of), among others.

And Wikipedia can be a very good resource for CS theory, algorithms, etc.

http://www.billthelizard.com/2008/12/books-programmers-dont-really-read.html [The art of computer programming](http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming)

[Introduction to Algorithms](http://en.wikipedia.org/wiki/Introduction_to_Algorithms)

Also, look [here](http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books)

Edit: 

Also, [on implementing interpreters](http://www.cs.brown.edu/~sk/Publications/Books/ProgLangs/)

I'd also recommend just picking a language (Python is a good once to start) and trying to implement things that interest you, dive into the official documentation. [The art of computer programming](http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming)

[Introduction to Algorithms](http://en.wikipedia.org/wiki/Introduction_to_Algorithms)

Also, look [here](http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books)

Edit: 

Also, [on implementing interpreters](http://www.cs.brown.edu/~sk/Publications/Books/ProgLangs/)

I'd also recommend just picking a language (Python is a good once to start) and trying to implement things that interest you, dive into the official documentation. Also, stay the fuck away from The Art of Computer Programming, it won't do you any good unless you really *need* a reference that isn't the original paper on the topic. [The art of computer programming](http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming)

[Introduction to Algorithms](http://en.wikipedia.org/wiki/Introduction_to_Algorithms)

Also, look [here](http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books)

Edit: 

Also, [on implementing interpreters](http://www.cs.brown.edu/~sk/Publications/Books/ProgLangs/)

I'd also recommend just picking a language (Python is a good once to start) and trying to implement things that interest you, dive into the official documentation. [The art of computer programming](http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming)

[Introduction to Algorithms](http://en.wikipedia.org/wiki/Introduction_to_Algorithms)

Also, look [here](http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books)

Edit: 

Also, [on implementing interpreters](http://www.cs.brown.edu/~sk/Publications/Books/ProgLangs/)

I'd also recommend just picking a language (Python is a good once to start) and trying to implement things that interest you, dive into the official documentation.              </snippet></document><document><title>Free Computer Science courses from Coursera</title><url>https://www.coursera.org/category/cs</url><snippet /></document><document><title>Corrupting the Youth</title><url>http://carlos.bueno.org/2010/07/corrupting-the-youth.html</url><snippet>   Thank you OP for a fascinating read.

I seem to be missing the point of the Dutch National Flag problem; speaking as a freshman in college, it doesn't seem like a challenge at all. Some things come more easily than others. Relational theory was terribly simple to me, and it mystifies me that others don't get it. Otoh I fail hard at anything to do with anagrams and so-so with pointers. 

That some people "get" some ideas better than others is highly interesting. It suggests that we can improve both the clarity and speed of compsci education by trying to understand how different people think.  Thank you OP for a fascinating read.

I seem to be missing the point of the Dutch National Flag problem; speaking as a freshman in college, it doesn't seem like a challenge at all.  &amp;gt;The kids would cover their eyes and cock their heads for a few seconds, thinking about how to walk in a circle while blind. Then they did it. They walked forward just a little, then turned a little, then walked forward again, and so on until their inner ear told them they'd done a full circuit. Not only did these children solve the problem immediately using visceral thinking, their solution was hundreds of times more efficient. Somehow they were intuiting differential equations.

How does this solve the robot problem?  This just sounds like ''walk in a circle''. They're not doing anything the robot can't do. Trivially encoding their actions in robot-lang, we get

1. Move forward a bit.
2. Rotate &#960;/4 right (or left)
3. Mark the floor
4. Move forward a little bit
5. Rotate a little bit
6. Go to 3

which lacks a stopping condition and without some computation doesn't guarantee the circle is centered around the starting point, but does draw a good approximation of a circle on the floor.

BTW, "intuiting differential equations" (or calculus, for that matter) isn't nearly as impressive as it sounds. What they're actually intuiting is *mechanical physics* (which is pretty much based on integral and differential calculus, and thus differential equations), which given the immediacy thereof as a model isn't particularly surprising.     I agree with the article. The way we are taught in high school forces us to think in terms of complex mathematics instead of visualizing it in the first place. After screwing your mind with differential equations, your intuition somehow gets attached to it. The problem is, once problems get even moderately complex, trying to hold the entire thing in your head to form an intuition is impossible. If people don't learn to use algebraic reasoning, they will not be able to progress beyond simple problems.  &amp;gt; Re&#173;memb&#173;er that com&#173;put&#173;er sci&#173;ence has noth&#173;ing to do with com&#173;put&#173;ers as such, any more than geomet&#173;ry is about sur&#173;vey&#173;ing land. 

Computer science has as much to do with computers as thermodynamics has to do with engines.

(i.e.: Without that application, it's purely abstract math, and not especially interesting abstract math at that.) I like the analogy, except for the part about Computer Science not being interesting in the abstract.  </snippet></document><document><title>solving undecidable problems</title><url>http://www.reddit.com/r/compsci/comments/silqh/solving_undecidable_problems/</url><snippet>Scenario: Take some random undecidable decision problem. Assume you have access to as many people as you want. One by one, you present inputs to the problem to the people you have (taking more if you want to), and they attempt to solve the problem.

If I say I want N solutions to the problem (where a solution is a particular input and its result), is it potentially possible that, given enough time and people, I can get those N solutions no matter how big N is? Or, does the undecidablity of the problem imply that there is some input which my people will not be able to solve?  [deleted] &amp;gt; That is correct.

How can we know for sure that some input of the problem that's unsolvable exists? If all of the infinite inputs to the problem were solvable, wouldn't this still not contradict the undecidablity of the problem, since this doesn't lead to a way to construct a *finite* turing machine that solves the problem? [deleted]  I've always been wondering how anyone takes halting problem too seriously. Any physically implementable computer has limited amount of memory and after enough time either return to a previously visited state or halt. I know that in theory halting problem is undecidable but, in the world we live in, one can solve the problem for all computer programs and inputs.

Now to think of it, does anyone know an undecidable problem that can actually be realized in the real world? I've always been wondering how anyone takes halting problem too seriously. Any physically implementable computer has limited amount of memory and after enough time either return to a previously visited state or halt. I know that in theory halting problem is undecidable but, in the world we live in, one can solve the problem for all computer programs and inputs.

Now to think of it, does anyone know an undecidable problem that can actually be realized in the real world? I've always been wondering how anyone takes halting problem too seriously. Any physically implementable computer has limited amount of memory and after enough time either return to a previously visited state or halt. I know that in theory halting problem is undecidable but, in the world we live in, one can solve the problem for all computer programs and inputs.

Now to think of it, does anyone know an undecidable problem that can actually be realized in the real world?</snippet></document><document><title>New ACSL subreddit for high schoolers in ACSL(American Computer Science League)</title><url>http://www.reddit.com/r/ACSL/</url><snippet> </snippet></document><document><title>hash function that you can implement in your head</title><url>http://www.reddit.com/r/compsci/comments/shgha/hash_function_that_you_can_implement_in_your_head/</url><snippet>is there a fast hash function you can do in your head for example turning an English sentence not too long into a five letter checksum? One requirement is that when Alice picks a word, let's say, rabbit, and Bob has obtained 50 input-output pairs of the function

h_rabbit: sentence --&amp;gt; h('rabbit'+sentence)

where h is the hash function Alice is doing in her head, it should be hard for Bob to figure out h_rabbit. This rules out h that just takes first letters of first five words in a sentence.

It is assumed that Bob does not know the hash function h.  You could construct any number of hash schemes that satisfy your requirement, but you didn't specify if you want this to be a cryptographic hash (e.g., preimage and second preimage resistant), and whether it matters that messages collide frequently.

One idea might be:

Construct an array of 5 elements initialized to 0, or perhaps a secret key. Repeat with i = 0 to len(sentence) - 1: If sentence[i] is a letter, then determine what score it would fetch in [Scrabble](http://en.wikipedia.org/wiki/Scrabble_letter_distributions). Add this score to element (i % 5) of the array (i.e., go circularly around the array).

When the end of the sentence is reached, take each element in the array modulo 25 (far easier than mod 26). The result maps to a letter A-Y. 

This presupposes:

1. You can keep a 5 element array straight in your head. For ease of calculation, you can perform the modulus step after each addition.

2. You remember the scores of tiles in Scrabble; any other key could work.

3. You can quickly index the alphabet (hint: A = 1, M = 13, Z = 26).

Note that this is basically a system of linear congruences, so it is possible to solve given enough input / output pairs. You could try making it non-linear. I expect it might require more than 50 pairs but I'm not certain.  I don't think such a hash function exists. (Wikipedia has a [list](http://en.wikipedia.org/wiki/List_of_hash_functions); e.g. Bernstein's hash may just be possible to execute in your head, but it's non-cryptographic.)

Taking a stab here: just use a password manager, or put a list of passwords in your wallet. If you're worried about your wallet getting stolen, you could use [visual cryptography](http://en.wikipedia.org/wiki/Visual_cryptography) to divide your list of passwords in two, keeping half in your wallet and half in another pocket. You need to include the http:// in URLs for reddit to link them. Fixed, thanks.</snippet></document><document><title>PAL, the Personal Assistant that Learns</title><url>https://pal.sri.com/Plone/framework</url><snippet>  </snippet></document><document><title>netcdf (flat file) vs sql (database) query performance?</title><url>http://www.reddit.com/r/compsci/comments/sghw1/netcdf_flat_file_vs_sql_database_query_performance/</url><snippet>I have a file system like (where =&amp;gt; represents folder)

2012 =&amp;gt; April =&amp;gt; 1 =&amp;gt; data_1.netcdf / data_2.netcdf / etc.

2012 =&amp;gt; April =&amp;gt; 2 =&amp;gt; data_1.netcdf / data_2.netcdf / etc.

5 folders [years] =&amp;gt; 12 folders [months] =&amp;gt; 28-31 folders [days] =&amp;gt; files

I have over 1 terabyte worth of data (years of worth of data), would it be better to store it into a relational database (i.e. PostgreSQL) or is flat file storage a faster solution or should I opt for MySQL?

My code currently loops through each folder and reads each file, then loops through the file to pull the data out. It takes a bit of time to read through.  Leave it on disk - a database is probably going to be more overhead on reading with your code. If you have terrabytes of memory then maybe a database could help (if you repeatedly open the same files, the database could cache the data), but you don't so it's just going to be slow! I'm just unsure if I could better use something like PostgreSQL and take advantage of the relational database queries to grab data more efficiently &amp;amp; easily?

Right now it's like this

* for loop through directories until find file

* use netcdf reader on matlab to read file

* for loop through file to grab relevant information from netcdf and load into a vector

* end for loop; end for loop;

It takes about 2 hours or so *just* to load the data.  What are you doing with the data? 

Hard to answer this question without knowing the access patterns.

Are you reading all the data for each operation? Do you need to filter out a subset of the information for each operation? Is that filter date based?

 I'm doing computational calculations on the data set; i.e. plotting, then smoothing out the data for prediction.

Sometimes I read all the data, and sometimes the data is filtered (i.e. I read through each data file and pull out certain parts of the data).

[http://pastebin.com/Q2cbxyET](http://pastebin.com/Q2cbxyET) &amp;lt;= here is an example of what the data looks like. I'm doing computational calculations on the data set; i.e. plotting, then smoothing out the data for prediction.

Sometimes I read all the data, and sometimes the data is filtered (i.e. I read through each data file and pull out certain parts of the data).

[http://pastebin.com/Q2cbxyET](http://pastebin.com/Q2cbxyET) &amp;lt;= here is an example of what the data looks like.</snippet></document><document><title>Published in an ACM journal? Use their Author-izer tool to let people download your papers for *free* from their digital library!</title><url>http://www.acm.org/publications/acm-author-izer-service</url><snippet>   &amp;gt;ACM Author-Izer is a unique service that enables ACM authors to generate and post links on either their home page or institutional repository for visitors to download the definitive version of their articles from the ACM Digital Library at no charge.
&amp;gt;
&amp;gt;Downloads from these sites are captured in official ACM statistics, improving the accuracy of usage and impact measurements.  Consistently linking to definitive version of ACM articles should reduce user confusion over article versioning.

Please can someone try this and let's test to make sure the papers are actually free? I think this would go a long way in increasing access to compsci knowledge, making it more accessible to industry developers. Haven't tried it, but I have high confidence that it's credible:

1. Papers on dl.acm.org are already ~~free to download~~ (see response); this just lets you link to them.
2. Authors are already allowed to post ACM papers on their own website (see the fine print at the bottom of the page); this just helps them track # of hits. &amp;gt; Papers on dl.acm.org are already free to download; this just lets you link to them.

Only if you subscribe to the ACM Digital Library. &amp;gt;ACM Author-Izer is a unique service that enables ACM authors to generate and post links on either their home page or institutional repository for visitors to download the definitive version of their articles from the ACM Digital Library at no charge.
&amp;gt;
&amp;gt;Downloads from these sites are captured in official ACM statistics, improving the accuracy of usage and impact measurements.  Consistently linking to definitive version of ACM articles should reduce user confusion over article versioning.

Please can someone try this and let's test to make sure the papers are actually free? I think this would go a long way in increasing access to compsci knowledge, making it more accessible to industry developers. &amp;gt;ACM Author-Izer is a unique service that enables ACM authors to generate and post links on either their home page or institutional repository for visitors to download the definitive version of their articles from the ACM Digital Library at no charge.
&amp;gt;
&amp;gt;Downloads from these sites are captured in official ACM statistics, improving the accuracy of usage and impact measurements.  Consistently linking to definitive version of ACM articles should reduce user confusion over article versioning.

Please can someone try this and let's test to make sure the papers are actually free? I think this would go a long way in increasing access to compsci knowledge, making it more accessible to industry developers. &amp;gt;ACM Author-Izer is a unique service that enables ACM authors to generate and post links on either their home page or institutional repository for visitors to download the definitive version of their articles from the ACM Digital Library at no charge.
&amp;gt;
&amp;gt;Downloads from these sites are captured in official ACM statistics, improving the accuracy of usage and impact measurements.  Consistently linking to definitive version of ACM articles should reduce user confusion over article versioning.

Please can someone try this and let's test to make sure the papers are actually free? I think this would go a long way in increasing access to compsci knowledge, making it more accessible to industry developers. I did set this up, but I'm not sure if it works fully free or not. I'll check it out when I get by a computer. 

**EDIT:** [This is my page](http://dl.acm.org/author_page.cfm?id=81490693441). Is it working for people?  Still sends me to a "Pay for article" page

This is unfortunate. I was hoping that this would mean I could be even lazier when it came to updating my website.

Any chance you didn't hit a checkbox allowing for free dl or something? Ah, I think I understand how it works now. I just saw a Author-izer button below the title of the paper. Clicking it gives this pop-up:

&amp;gt;The ACM Author-Izer Service allows you to place a link on a single web page of your choice which will allow visitors who click on it to obtain the definitive version of your article from the ACM Digital Library at no charge. Not only will this enable increased access to your article, but it will enable ACM's download statistics to more accurately reflect usage. In fact, the link that you paste into your web page will (optionally) retrieve current download and citation statistics for your article for display whenever your web page is opened.

There's a checkbox to enable it for the paper. Below it, a textbox for the single URL from which requests will be placed with the caption:
&amp;gt;requests for your article will only be honored if they are referred from this URL


And a button to authorize it. Then, code is generated that must be placed into the webpage.   Or researchers could just host pdfs of their papers (of draft versions thereof, etc etc.) on their homepage. *And* put them on arXiv as well.

The reason why ACM did this, beside the fact that they're not fundamentally bad people, is that it allows them to keep control on where these articles are hosted and accessed. They stay relevant as a central place to access the material. You have to realize that it's as much as "gift to the research community" (as some people here emphasizes) as a way to keep control over those papers.  To be clear, it's only free to download for people who followed the link from your homepage.

Alternatively, the ACM [policy](http://www.acm.org/publications/policies/copyright_policy#Retained) allows you to post a pdf directly on your homepage, as long as you include this statement both in the PDF and on the webpage:

&amp;gt;  &#169; ACM, YYYY. This is the author's version of the work. It is posted here by permission of ACM for your personal use. Not for redistribution. The definitive version was published in PUBLICATION, {VOL#, ISS#, (DATE)} http://doi.acm.org/10.1145/nnnnnn.nnnnnn

I think doing that is probably better for the world at large: it makes it easy for other people to get hold of the article even if they find it through search engines, the article can be cached by Google etc, and it will still be accessible even if the ACM later change their mind and revoke the Author-Izer access.  What about IEEE policy? Can you do that with an a paper published and showed in IEEE Xplore ?</snippet></document><document><title>Writing a report (final dissertation)</title><url>http://www.reddit.com/r/compsci/comments/sfy36/writing_a_report_final_dissertation/</url><snippet>I'm producing a final report on a real-time character recognition program I created I have hit a dead end on things to write about, I was wondering if anyone had any links to good report writing for a technical degree? things to include in a technical report(I have all the obvious headings). Or anything that you have used that has really helped to create a great report.

Thanks in advanced :D  I keep a blog of advice I give to my students about this stuff, with some posts about writing. Not sure how relevant it will be to your particular report, but hopefully there's something useful for you there: http://projectsuccess.posterous.com/top-10-writing-mistakes-made-by-project-stude  If you have run out of things to write about, your paper is done.  Revise and edit.

As far as links on how to write "reports", look at peer reviewed journal articles.</snippet></document><document><title>Turing machine format checking</title><url>http://www.reddit.com/r/compsci/comments/sf68j/turing_machine_format_checking/</url><snippet>When learning about turing machines we often construct machines that take as input the description of some other object, and the first step of the machine's operation is usually "make sure the input string is of proper format". I was wondering if this was ever not possible.

Ie, is there a set [; S ;] of some kind of object such that for all alphabets [; \Sigma ;] and injections `[; f\colon S \rightarrow \Sigma^* ;]` the language `[; \{w \in \Sigma^* \mid \exists s \in S \mbox{ st. } f(s) = w \} ;]` is unrecognizable or undecidable?

edit: One thought was "oh, well duh, a set of unrecognizable/undecidable languages works", but I don't think this really works for what I was thinking, since I don't think there's an injection between unrecognizable languages and strings (at least I couldn't think of a way to construct one). I'm thinking of some set of objects that we can always encode to finite length strings,  but somehow can't really check if some arbitrary string was generated from that type of object.

edit2: Hm, well I guess one example is let S be the set of all halting turing machines. Halting turing machines can all be encoded to finite strings (since they're a subset of regular turing machines), and checking whether a string represents a halting-turing machines or not is unrecognizable.

But that example doesn't quite meet the criteria I was thinking of, since I just took a random subset of a type of object that can be encoded, instead of looking at the whole "class" of objects, although I'm not sure how to formally describe this idea.  The problem here is the definition of "Format"

Obviously if you restrict acceptable formats to recursive languages, then you can always decide if a given string is of the proper format. 

If you allow us to pick any format, then just pick an undecidable property of strings to be your format (the example you gave, Halting Turing Machine/input pairs) then you cannot. 

Generally, when we gloss over some step in the running of a TM, it's because we already know that step to be computable, and since recursive languages are closed under intersection, if making a TM to check format is trivial, then we need not include that step in dealing with the formatted string.  I guess my issue was with the halting turing machine example is with my definition of "objects". Turing machines in general can be encoded and format-checked. I'm wondering if there's a class of objects that it's not possible for, thought I'm not sure how to formalize this idea... Here's a simple proof that anything can be serializable.

1.  The integers are serializable (see: binary)

2.  [Warning: assumption] Your objects (whatever they are) are countable, that is: you can assign a number (integer) to each of them uniquely.

3.  Your objects are serializable because you can serialize the integers which represent them

This property is recursive.  If something is composed of serializable objects, it is itself serializable.

Just remember that 2.) doesn't apply to a) real numbers or b) infinite sets



Actually, just read this link: 

http://en.wikipedia.org/wiki/Recursively_enumerable_set

(and maybe [this one](http://en.wikipedia.org/wiki/Recursively_enumerable_language)) But how do we know that inverse-serialization is a decidable problem for whatever class of objects we're dealing with? It sounds like you have an idea of how you would convert any object into binary. To formalize this: we say our property P is that the string X is a valid encoding of an object O. To do this however, we need to formalize the object O, into SOME alphabet A. You can use whatever alphabet you want be it first order logic, set theory, or even english words, but you need a finite alphabet that formally describes your set of objects. Otherwise your problem is just that you haven't formally described your system well enough. 

If there is a computable encoding of all objects O, then there is a total Turing machine M that decides property P. You can probably see how you would easily make an encoding of any alphabet to binary. Since this is a bijection, you can just reverse the bijection to make a Turing Machine that decodes. 

As for being computable in poly-time, having an alphabet where encoding is polytime but "decoding" is not is the statement of the existence of [one-way functions](http://en.wikipedia.org/wiki/One-way_function) which is equivalent to P=|=NP (which seems very likely).  &amp;gt;  there is a total Turing machine M that decides property P

&amp;gt;  you can just reverse the bijection to make a Turing Machine that decodes

Are all bijections decidable? Mmmm not exactly what I was saying, more that:

Given a finite control which checks for each character c on one tape if f(c) is on the other tape, you can make a finite control which checks for each f(c) if c is on the other tape (provided the alphabet into which you are encoding has at least 2 characters).  ... I think you lost me. What is f here? It maps between two alphabets? Technically between characters in an alphabet and strings. But yes. &amp;gt;  there is a total Turing machine M that decides property P

&amp;gt;  you can just reverse the bijection to make a Turing Machine that decodes

Are all bijections decidable? I guess my issue was with the halting turing machine example is with my definition of "objects". Turing machines in general can be encoded and format-checked. I'm wondering if there's a class of objects that it's not possible for, thought I'm not sure how to formalize this idea...  </snippet></document><document><title>[Q] Did any of you graduate without any internship or work experience? (AKA am I screwed?)</title><url>http://www.reddit.com/r/compsci/comments/sdygo/q_did_any_of_you_graduate_without_any_internship/</url><snippet>As the title says, I'm wondering if any of you graduated without any sort of internship experience and I'm curious about how that turned out for you. I'm a Junior at a University very well known for its CS program, and each day as summer draws closer and I still have yet to hear back from any prospective internship employers (wasn't excellent at applying, did poorly in some interviews due to nerves).  I have a 3.4 technical GPA (freshman year was bad for me, but this isn't absolutely awful considering the school average) and I'm wondering how difficult it is to find work after graduating given that you have no work experience whatsoever.  

Even if I do secure an internship this late (school ends in a month) I would be sacrificing a family vacation to various parts of Asia like Taiwan/Japan, which is an experience that I would love to have.

Essentially I'm asking for stories and advice: Assuming I do get an offer, would you say the internship experience is more valuable than a trip with my family to the other side of the world? Assuming I don't get an internship, would you recommend summer courses to remove a humanities requirement so I can focus more on my academia, trying to get Research with a professor (unlikely grad school, but always a possibility), or trying to find some sort of open source project which I hear is also good to do (though I wouldn't even have any idea where to begin or what to do)?

Much Love &#9829;
  Freelance work to build a portfolio and a nice Github profile will go a long way. Three android apps, 1 windows app and god know how many websites on the ticket. The only think I feel like I missed is the teamwork experience. Three android apps, 1 windows app and god know how many websites on the ticket. The only think I feel like I missed is the teamwork experience. Freelance work to build a portfolio and a nice Github profile will go a long way. This. I would absolutely not hire someone fresh out of school with no experience whatsoever. I would have no qualms hiring someone with a github profile that has a few OSS projects with real users. (And code that doesn't make my eyes bleed.)

With no internship, you have plenty of time in the next few months. Set an alarm clock and act as if you went to work each weekday morning, and spend 5+ hours each day writing some code. It doesn't really matter what you work on, so long as you actually finish projects instead of having 20 github repos, each with an awesome project that was swept aside the second you hit the tedious part. So figure out something awesome, something you want, or if you don't have any ideas, write a few simple games.

Also, since this *will* be your portfolio, make it look pretty. Comment things that need to be commented, have a proper README that actually describes the project, and a little bit so one can have a start on how to understand it. I would hire someone out of school with no experience outside of their degree or school work. I find that a conversation is a much easier way to evaluate a potential candidate. If they show the qualities you want in the interview hire them and give them a chance to prove themselves. If they don't work out or aren't what you wanted let them go. Why exclude any potential candidate because of something a piece of paper says? 

I think one of the most important things to getting a job is having a very attractive looking resume. This makes your resume stand out and gets people interested in your. Hell, I'd hire somebody with no formal education at all if they are good and have a bunch of hobby projects. This. I would absolutely not hire someone fresh out of school with no experience whatsoever. I would have no qualms hiring someone with a github profile that has a few OSS projects with real users. (And code that doesn't make my eyes bleed.)

With no internship, you have plenty of time in the next few months. Set an alarm clock and act as if you went to work each weekday morning, and spend 5+ hours each day writing some code. It doesn't really matter what you work on, so long as you actually finish projects instead of having 20 github repos, each with an awesome project that was swept aside the second you hit the tedious part. So figure out something awesome, something you want, or if you don't have any ideas, write a few simple games.

Also, since this *will* be your portfolio, make it look pretty. Comment things that need to be commented, have a proper README that actually describes the project, and a little bit so one can have a start on how to understand it. what is OSS ? Open source software Thanks. :)  It's your career, not ours. I didn't do any work experience - but then, I'm not now working at Google. If you want to work in a highly competitive field - well, you'd better start competing. If there are other things more important in your life, then, don't worry - you won't be starving because you haven't done an internship.

It's pretty hard for anyone to give advice without knowing what your goals are.  Travel before you have two kids and a mortgage.   Doing this myself as a graduating CS major.  Landed a great job but I don't start until the end of August so I'm taking a pile of airline miles I've amassed and heading to Berlin.  You dont need an internship, you just need to finish some projects.

A game, a website, a mobile app, a github contribution, anything. Just get your code out there. 

When I interview people, if they have never coded for fun, its an automatic SORRY. I wish I'd understood this. I was embarrassed how much time I'd spent on private projects and never really mentioned it on my CV, figuring it would count for nothing. Oops. I wish I'd understood this. I was embarrassed how much time I'd spent on private projects and never really mentioned it on my CV, figuring it would count for nothing. Oops. You dont need an internship, you just need to finish some projects.

A game, a website, a mobile app, a github contribution, anything. Just get your code out there. 

When I interview people, if they have never coded for fun, its an automatic SORRY. Eh, I wouldn't rule out someone who hasn't coded for fun automatically.  But ill admit I am strongly biased toward candidates who can demonstrate enthusiasm for a personal project. Eh, I wouldn't rule out someone who hasn't coded for fun automatically.  But ill admit I am strongly biased toward candidates who can demonstrate enthusiasm for a personal project. You dont need an internship, you just need to finish some projects.

A game, a website, a mobile app, a github contribution, anything. Just get your code out there. 

When I interview people, if they have never coded for fun, its an automatic SORRY.  I did, although you won't like how I got to where I'm at now.  

Graduated with a double major in CS and Philosophy in 2009. Ended up taking a very entry level job, because it was all I could get.  $10/hr working with copier techs at the company I'm still with.  

After I did that for a few months, they moved me into some financial analysis work for another department at work.  After about a year of that they moved me into their software side.  A few months in there, and they've got me managing the larger projects and working with the developers on trouble spots.  

I guess the idea I'm trying to convey is that a lack of internship won't hold you back.  As long as you're willing to put in hard work, and start wherever you have to, you'll make it work if you want it bad enough.   CS and Philosophy grad from 2007 here with no prior internship/CS work related work experience upon graduation. I guess the market was a little bit better then because I had multiple job offers.

I did have some general work experience though (Air National Guard) which large companies looked very favorably upon.

edit: clarified work experience (lack thereof) Weird that there are so many philosophy/cpsc double majors in this thread. I only had a minor in philosophy myself, but got a job easily enough with terrible grades and no internships. I had a fair amount of "free time" projects. I am just lucky (and charming). The area you are in plays a big part in getting a job, I think.  Why is that weird ? It's all linear thinking, problem solving, argument building ( coding is just building arguments in a formal language ) etc. If your mind thinks in "axioms -&amp;gt; argument -&amp;gt; solution" you'd be attracted to both, except one you can graduate with 6 figure salary, and the other you graduate with 6 figures including the cents.

source: yet another philosophy minor Well, I think weird is the wrong word. It makes perfect sense to be interested in both, I agree. I just am surprised because I didn't think it was so common. I also don't think people who hire programmers see philosophy as an attractive minor/major as something like math/physics/business/management. Not that they shouldn't, I just didn't think they did. 

I'm curious, how do you present this to possible employers on a resume. I mainly focus on the fact that it is useful for having "good communication skills" since I think that is actually a rare trait among engineers/programmers. It is the only evidence I really can come up with for that (other than actually speaking with them.) Well, I think weird is the wrong word. It makes perfect sense to be interested in both, I agree. I just am surprised because I didn't think it was so common. I also don't think people who hire programmers see philosophy as an attractive minor/major as something like math/physics/business/management. Not that they shouldn't, I just didn't think they did. 

I'm curious, how do you present this to possible employers on a resume. I mainly focus on the fact that it is useful for having "good communication skills" since I think that is actually a rare trait among engineers/programmers. It is the only evidence I really can come up with for that (other than actually speaking with them.) The way I present it to possible employers (the employer who hired me) was like this:

You get a lot of applications from people with CS Degrees.  All of them can probably program well enough for the position you're looking to fill.  My Philosophy degree helps me stand out from the rest of them.  The Philosophy classes and course load helped teach me a vital skill that I didn't learn in my CS classes, sometimes you should stop and think about all the ways things can be done, before deciding which to do it with. 

In CS, I'd develop what I thought was a logical way to attack the problem, and then get it done.  In Philosophy, I learned to look at all the ways it could be solved, then use the most efficient one.  

Also, I used it as a way to promote myself and possible management tactics.  Being able to see and address issues that others might not have the capacity for, understanding impartiality in business situations, and other fluff.  

Although, a lot of that stuff overlaps with my desire to go to law school.  A lot of studying for the LSAT and reading law books helps with the impartiality bit.  Despite that dream being gone, it has helped me quite a bit.  Well, I think weird is the wrong word. It makes perfect sense to be interested in both, I agree. I just am surprised because I didn't think it was so common. I also don't think people who hire programmers see philosophy as an attractive minor/major as something like math/physics/business/management. Not that they shouldn't, I just didn't think they did. 

I'm curious, how do you present this to possible employers on a resume. I mainly focus on the fact that it is useful for having "good communication skills" since I think that is actually a rare trait among engineers/programmers. It is the only evidence I really can come up with for that (other than actually speaking with them.) I don't really communicate it to employers directly (it's not on my resume, for instance) but I work in a pretty specialized field, I really don't need such things to get noticed.  Are you doing open source?

Why not? It's as good as work experience. I'd like to get into open source but I can't seem to find any projects that I feel I can contibute to or they seem way above my abilities. Any suggestions? I'd like to get into open source but I can't seem to find any projects that I feel I can contibute to or they seem way above my abilities. Any suggestions? I'd like to get into open source but I can't seem to find any projects that I feel I can contibute to or they seem way above my abilities. Any suggestions?  I am British and not studying computer science, I am studying Business and IT, but am putting forth that I am primarily a programmer. I put my curriculum vitae onto ONE job website, and ever since nearly every day I get recruitment companies and software companies calling me to offer me interviews about things. I like to think I have a good all around knowledge of the most used languages, and though my grades are not necessarily great, the fact that I can prove that I am worthy of a graduate role is a great asset to me.

TL;DR: You'll be fine, everyone needs developers. As a Brit in the middle of a Comp Science degree, out of interest, what job website was that? Jobsite.co.uk. Seriously, I put my CV on there and a bit of background, filled out all of the forms, and recruiters have been calling me CONSTANTLY, offering me a LOT of different positions. It's really quite amazing, as long as you point out that you know your shit people will love you. Good luck! Awesome, I'll bookmark it for future use, thanks! Good luck with it mate! Word of advice, start applying for jobs a couple of months before you finish university, it's the lull time where employers are looking for people but most students haven't started applying. Oh and project as much enthusiasm as possible for the subject, it can be a great replacement for the fact that you're trying to just start your career. Thanks for the advice, I'm only in the second year right now, and could be doing a masters so it'll be a while yet, but I'll keep that in mind! If you're doing a masters then you'll have NOOO problem getting a job. are you in process of doing a masters'? would you say it's worthwhile to get an internship over the summer while working on an advanced degree (I'm thinking it will be if only for the real-world coding experience?) I'm finishing off a Bachelor's, I really don't personally feel like doing a Master's would be good for me. I think it's most important to focus on your university work, and keep a portfolio of all of your side projects. Almost every employer I interviewed wanted some code examples. Jobsite.co.uk. Seriously, I put my CV on there and a bit of background, filled out all of the forms, and recruiters have been calling me CONSTANTLY, offering me a LOT of different positions. It's really quite amazing, as long as you point out that you know your shit people will love you. Good luck! to clarify - they're surely not offering *positions*, they're offering interviews, right?    I tend to think that this is the best time in your life to travel, so I wouldn't skip the Japan &amp;amp; Taiwan trip.  If/when you jump into a full-time job, you pretty much lose the ability to take trips like that (unless you have an awesome employer or go freelance).

I don't know where you are, but the job market for software people is really hot in a lot of places in the US.  A lack of internship might not hold you back at all.  (Of course, next year there could be a tech crash.)  The other thing you can do is start working on one or two kickass side projects, post them on Github, and develop some experience that way. I don't know about working full time impeding the ability to travel.  If you use vacation time wisely and take the time to plan carefully (using holidays too if possible), there is definitely enough time for travelling.

Employment opened the door to travel because I can now afford to buy plane tickets and train passes.  

With 20 days for vacation/year, I manage to average about three great international trips each year.  I tend to think that this is the best time in your life to travel, so I wouldn't skip the Japan &amp;amp; Taiwan trip.  If/when you jump into a full-time job, you pretty much lose the ability to take trips like that (unless you have an awesome employer or go freelance).

I don't know where you are, but the job market for software people is really hot in a lot of places in the US.  A lack of internship might not hold you back at all.  (Of course, next year there could be a tech crash.)  The other thing you can do is start working on one or two kickass side projects, post them on Github, and develop some experience that way. &amp;gt; (Of course, next year there could be a tech crash.)

Why do you possibly anticipate this next year?  Because of the way the US government is stifling free Internet? Because of a $1B acquisition of a sepia filter. Some of that was stock, but I wonder how much. Right now I think we have a lot of millionaires on paper, but we'll see if they ever actually get to convert theoretical money into real money. Because of a $1B acquisition of a sepia filter. &amp;gt; (Of course, next year there could be a tech crash.)

Why do you possibly anticipate this next year?  Because of the way the US government is stifling free Internet? I tend to think that this is the best time in your life to travel, so I wouldn't skip the Japan &amp;amp; Taiwan trip.  If/when you jump into a full-time job, you pretty much lose the ability to take trips like that (unless you have an awesome employer or go freelance).

I don't know where you are, but the job market for software people is really hot in a lot of places in the US.  A lack of internship might not hold you back at all.  (Of course, next year there could be a tech crash.)  The other thing you can do is start working on one or two kickass side projects, post them on Github, and develop some experience that way. I have to disagree with what you're saying. The SD market is probably really hot out there for the top %1 of the graduating class, but for somebody between that and average it's virtually non-existent and hottest opportunities out there for those are unpaid internships and low-paying QA work. Given what I was able to find myself after graduating near the top of the class and what my friends that did do internship years in their program, I'd saying taking an internship is greatly more important than family trips. Sure family trip is great now and all, but if he takes an internship it'll help his career right after graduating and he'll be able to afford his own trip a year after that. This just isn't true.  "Computer and mathematical occupations" grew 6.9% from 2006-2010, the third largest growth for a field listed in the Jan. 2012 Department of Labor review ([source](http://www.bls.gov/opub/mlr/2012/01/)).  Also see [this link](http://logos.cs.uic.edu/recruit/csstatistics.htm) for more information about the quality of CS jobs.  According to the Bureau of Labor Statistics, future job prospects for computer science graduates are higher than for any other science or engineering field.


I just don't think it adds up that only the top 1% of the graduating classes get good opportunities.  In fact, I'm at a mid-to-low tier CS program and I see the majority of the graduating class getting adequate employment.  Sure, they aren't going to chase you if you don't stand out but there are definitely jobs. Well I guess the main problem I had with his statement was the part with "job market for software people is really hot." This is exactly what I was told all throughout my program and when it came time to actually check out what this market had to offer it was scant. Without an internship, unless he's a budding Ken Thompson, he'll have a difficult time getting his foot in the door at companies for anything other than very entry-level, underpaid positions. Besides even if it is "hot", assuming the OP is in a 4 year program, he's still got 1 year to go and anything could happen to this "hot" market in that time.

Edit: Can you put that study on your resume or will it hire you out of school for a better pay and position? If you want to put faith in a study and just hope that you'll be ok a few years from now all the power to you. But I think it's much smarter to put faith into more concrete things like actual work experience and an actual company that you already worked with that otherwise would not have hired you off the street. I have to disagree with what you're saying. The SD market is probably really hot out there for the top %1 of the graduating class, but for somebody between that and average it's virtually non-existent and hottest opportunities out there for those are unpaid internships and low-paying QA work. Given what I was able to find myself after graduating near the top of the class and what my friends that did do internship years in their program, I'd saying taking an internship is greatly more important than family trips. Sure family trip is great now and all, but if he takes an internship it'll help his career right after graduating and he'll be able to afford his own trip a year after that. And as a counter example, I go to a shitty state school in the Austin area and, as we have approached the end of the semester, my school email has essentially been getting spammed with employers wanting to interview the graduates. Mind you, that UT has a top-5 CS program and is 30 miles north, so it's not like my school is as good as it gets.       I graduated with a B.A., no internship or work experience or even a portfolio and a sub 3 gpa (3.6 in CS) from a small private liberal arts school (7 cs majors in my graduating class) about 2 years ago and got a job at a small consulting company (similar to Accenture). I don't code as much as I would like but it pays alright (i started at ~50k and make ~70k now), it is easy work and it isn't tough to transition to a more traditional software engineering role a year or so later. You can build your profile (github or otherwise) in your free time and you should be good to go.

You definitely have options. A CS major shouldn't be out of work for too long. Whatever you decide, good luck. Just remember your first job isn't the end all be all. [deleted]   I didn't graduate.  My house is only 3300 square feet with only "one" front door, and I can only afford a 3-series BMW.  Very frustrating.


 I didn't graduate.  My house is only 3300 square feet with only "one" front door, and I can only afford a 3-series BMW.  Very frustrating.


     I had 2.

Yes they look nice, yes they helped me get a good job. No they are not required. I work with several people who did not get one and they got the same job as me right out of college.

Some of the people I graduated with did not get any and they had a hard time getting a job. Some of them ended up at a chop shop. They are going to put their 2 years in to build the resume and then find better jobs. 

You are not fucked, there are always things you can do; though they may not be as nice right away. Im curious what you mean by "chop shop".  Ive not heard that term used in relation to this industry before.              </snippet></document><document><title>Enjoy algorithms and number theory? Please review my first open source project.</title><url>http://www.reddit.com/r/compsci/comments/sf1k1/enjoy_algorithms_and_number_theory_please_review/</url><snippet>A while ago I ran across [Pollock's Conjecture](http://mathworld.wolfram.com/PollocksConjecture.html) in [Skiena's Algorithms book](http://www.amazon.com/Algorithm-Design-Manual-Steven-Skiena/dp/1848000693/ref=dp_ob_title_bk) and thought that it would be a fun problem to play around with. After some initial experiments in Python I moved to C for control and speed and created [this project](https://github.com/rheyns/pollock-conjecture-stats) which can calculate coverage for the first billion integers in 5 minutes on a Core 2-based laptop. (About 30000x faster than on Skienna's machine.)

I'm currently trying to build up some personal projects to facilitate a career change in the near future and I'd like your help in critiquing my first project's code, documentation and anything else you can think of. If this is the wrong subreddit for this I'll be happy to move it elsewhere as well.

Thanks!  Looks pretty cool. Big data analysis seems to be the next big thing, this seems like a great excuse to learn MapReduce.  Thanks, I'll definitely look into MapReduce. I've thought about using it before but I just wanted to get something done as quickly as possible initially. As it stands this problem is embarassingly parallel so throwing more EC2 instances at it is pretty easy too. 

Administering all those instances is a pain though, know of anything to help with that?  MapReduce tends to be useful for I/O bounded problems. ~Petabyte datasets. You could certainly distribute it and have individual jobs batch processing independent sets of integers.</snippet></document><document><title>Code Club: an after school project to teach kids to program and not jusst use computers</title><url>http://codeclub.org.uk/</url><snippet>  </snippet></document><document><title>I'd like to hear r/compsci about Model-Driven Architecture/Development</title><url>http://www.reddit.com/r/compsci/comments/sebis/id_like_to_hear_rcompsci_about_modeldriven/</url><snippet>Hi,

My question is not really technical/in-depth but more about the general philosophy of things. I am currently working as a research intern on a big project that aims at providing industrial actors with an environment for developing critical embedded systems. 

Typically, the company can input its own domain-specific modeling languages and generate a whole lot of tools like code editor, graphical editor, simulator, code generator, etc... from their metamodels.

For those of you who are not familiar with MDA/MDD, i'll use the definitions from [modeling-languages.com](http://modeling-languages.com)
&amp;gt;MBE (model-based engineering) process is a process in which software models play an important role although they are not necessarily the key artifacts of the development (i.e. they do NOT &#8220;drive&#8221; the process as in MDD)

&amp;gt;MDD is a development paradigm that uses models as the primary artifact of the development process.

&amp;gt;MDA is the OMG&#8217;s particular vision of MDD and thus relies on the use of OMG standards. Therefore, MDA can be regarded as a subset of MDD

I would like to know from anyone involved in the computer science field / software engineering what are your thoughts on MDA/MDD ? Do you think it will become more popular as tools develop, or will it only stick to critical embedded software ? What are your experiences with it?

Also, it is hard to talk about MDA/MDD without mentioning UML or a subset of UML which may often be one of the concerned domain-specific modeling languages. Specifically, what do you think of UML's idea of trying to do everything ? I'm talking about the possibility of execution of UML activity diagrams and/or of UML state machine diagrams.

Thanks for your input !  Just signed up to answer. 

From the standpoint of a Software Engineer, I'll say that modeling tools which focus purely on the graphical definition and manipulation of the model have pretty much reached their limits of application.  

Disclaimer: my experiences are purely with "enterprise information systems".  My comments DO NOT apply to applications such as in electrical engineering or chip design.  Purely EIS.

Over the years I've seen a number of attempts, and they all have the same limitations: 

a) They're hard to version control.

DIFF is a nightmare.  The tools I've used either persist to their own binary format or XML.  XML is diffable, but few tools take that into account when serializing - so even tiny changes lead to big changes in the XML.

Apart from that it's hard to limit changes to one thing.  Say you want to change a model name.  That sounds simple.  However, one wrong click and you've nudged another object, often setting off a chain of visual updates.  Sure, that can often be solved by editing XML: but then you need to master not just the tools but also the serialization format.

b) Hard to Unit Test

Again, my experience has shown that the logic in models is pretty much impossible to unit test.  The best I could come up with was to generate simple 'test implementations' and test those. 

However, that's not something that the average tool user is capable of.

c) Exponential growth in complexity.

This is the killer for me: they start out simple, but get very complex very quickly.  You'd usually solve this by abstraction - but that's something which graphical tools aren't good at.  Or at least I've not encountered one yet.

d) Lower barriers of entry = less capable people using the tools.

This is pretty much my number 1 issue with graphical modelling tools.  They have a horrible tendancy in attracting "developers" or "modellers" who have little knowledge of systems, and no desire to learn.

Bad people == bad

Textual modelling tools on the other hand are where all of the interesting stuff is happening.  Using a DSL (domain-specific-language) you gain all of the advantages of modelling whilst being able to leverage all of the software engineering tools and techniques we've developed over the last 30 years.
</snippet></document><document><title>Why are lock-free algorithms called lock-free?</title><url>http://www.reddit.com/r/compsci/comments/sdc2z/why_are_lockfree_algorithms_called_lockfree/</url><snippet>I might be being overly pedantic about this, but almost every single "lock-free" algorithm uses some sort of compare and exchange or interlocked statement.  These are still implemented with something like 'lock cmpxchg8b ...' which still causes a bus lock, cache invalidation, etc.  Am I mistaken?  Are there any truely lockless algorithms or should I start a personal crusade to rename these to semaphore-less algorithms?   Lock-free algorithms aren't called "lock-free" because they are implemented without using any hardware/software lock instructions, they are called lock-free because their structure guarantees that they will never deadlock. You could write a lock-free algorithm that contains explicit semaphores in it, so long as you can guarantee that forward progress will always be possible. Lock-free algorithms aren't called "lock-free" because they are implemented without using any hardware/software lock instructions, they are called lock-free because their structure guarantees that they will never deadlock. You could write a lock-free algorithm that contains explicit semaphores in it, so long as you can guarantee that forward progress will always be possible. Can you please clarify the meaning of "never deadlock" a bit more?  There are many programs using mutexes, spinlocks, etc., which will not deadlock. </snippet></document><document><title>NEED HELP: Performing benchmarks for federated query engines (Semantic Web)</title><url>http://www.reddit.com/r/compsci/comments/sdf0e/need_help_performing_benchmarks_for_federated/</url><snippet>I have read the paper http://www.mpi-inf.mpg.de/~khose/publications/ISWC2011.pdf . My mentor suggested me to re-perform the tests on a local machine.

**What I wish to accomplish:**

Perform the benchmarks on various frameworks like FedX, DARQ, Alibaba.
Try changing the queries and see how it affects the overall benchmarks.

**Issues I am struggling with:-**

Where can I find the relevant documentation so that I can run this fedbench across various benchmarks?

I am unable to download the various data sources from the website. I have the RDF dump of only 1 datasource so it's not really possible for us to perform the benchmarks.

We have checkedout (svn) out the latest code from the code hosted at http://code.google.com/p/fbench/ and it differs a bit from the versions available in the downloads section. The difference mainly being the addition of DARQ, SPLENDID, FedX libraries and bridge jars (What are these, anyways?)

Most of the SPARQL endpoints I tried over HTTP were not working. Can you suggest me something in this regard.

I would really appreciate a reply which could be helpful to me in any way.</snippet></document><document><title>Raibert on writing - Advice for writing academic papers</title><url>http://www.cs.cmu.edu/~pausch/Randy/Randy/raibert.htm</url><snippet>  Also, a more playful (yet excellent) take by Simon Peyton-Jones [videos and slides index page](https://research.microsoft.com/en-us/um/people/simonpj/papers/giving-a-talk/giving-a-talk.htm), and direct link to [Slides PDF](https://research.microsoft.com/en-us/um/people/simonpj/papers/giving-a-talk/writing-a-paper-slides.pdf)

  </snippet></document><document><title>Peg: a lazy non-deterministic concatenative programming language inspired by Haskell, Joy, and Prolog.</title><url>https://github.com/HackerFoo/peg</url><snippet>  </snippet></document><document><title>How to write a simple interpreter in JavaScript</title><url>http://www.codeproject.com/Articles/345888/How-to-write-a-simple-interpreter-in-JavaScript</url><snippet /></document><document><title>Computer modeled using swarms of soldier crabs</title><url>http://arstechnica.com/science/news/2012/04/computer-built-using-swarms-of-soldier-crabs.ars</url><snippet> </snippet></document><document><title>Academic publishing: Open sesame</title><url>http://www.economist.com/node/21552574</url><snippet>  [PhD Comics on publishing](http://www.phdcomics.com/comics/archive.php?comicid=1200)

Unfortunately I don't see this changing any time soon. This mentality is engrained into the academic community which passes it on to the next generation. To get a job you need to make a name for yourself and the "easiest" way to do that is to publish in top journals (in compsci we prefer conferences, but it's the same idea). You have a point, but I do think that it will be changing sometime soon.

Consider something like [arXiv](http://arxiv.org/).  Although it's not peer-reviewed, all of these pre-prints of articles are released online for free.  Pre-prints that are solely published on arXiv still have a significant amount of influence (the most notable of which is clearly [Grigori Perelman's solution to the Poincare conjecture](http://arxiv.org/abs/math.DG/0211159)).  

Mathematics and computer science are leading the way on arXiv, and I think our fields are going to be the first ones to really get rid of publishing like that soon.  Even though we still have some time to go to get rid of that mentality, I think this is a good first step towards open access for academic works. </snippet></document><document><title>Potential of Artificial Neural Networks?</title><url>http://www.reddit.com/r/compsci/comments/s8rw1/potential_of_artificial_neural_networks/</url><snippet>I've been interested in ANN for a while. Do you guys think in the near future we will be seeing 'creative' machines (hardware or software) that process information in this way? Is there something better on the horizon? Or do you think that this pursuit of building a machine with an intelligence much like our own is a lost cause?  Several state of the art methods in machine learning use neural networks such as convolutional neural networks or restricted boltzmann machines.  Look up deep learning, Hinton is a big name there.  But I wouldn't necessarily call these "creative" machines, although there has been work on a formal theory of fun and creativity by Schmidhuber( http://www.idsia.ch/~juergen/creativity.html ).

Though I think your phrasing is a bit naive (no offense), I do not think it is a lost cause. Also, I hope the irony of "a formal theory of fun and creativity" is not lost on everyone, haha. Several state of the art methods in machine learning use neural networks such as convolutional neural networks or restricted boltzmann machines.  Look up deep learning, Hinton is a big name there.  But I wouldn't necessarily call these "creative" machines, although there has been work on a formal theory of fun and creativity by Schmidhuber( http://www.idsia.ch/~juergen/creativity.html ).

Though I think your phrasing is a bit naive (no offense), I do not think it is a lost cause. Well you could argue that generative DBNs are creative in a way. Not in the general sense of the word, but they do create new patterns at least (i.e. Hinton's models of biological motion). That's gotta be worth something, eh?  [deleted] Several state of the art methods in machine learning use neural networks such as convolutional neural networks or restricted boltzmann machines.  Look up deep learning, Hinton is a big name there.  But I wouldn't necessarily call these "creative" machines, although there has been work on a formal theory of fun and creativity by Schmidhuber( http://www.idsia.ch/~juergen/creativity.html ).

Though I think your phrasing is a bit naive (no offense), I do not think it is a lost cause.   [deleted] [deleted]  I'm always skeptical of anyone who makes specific predictions about the field of AI. We have a long way to go in understanding how our own brains work, let alone implementing an artificial version. That said I do not think it is a lost cause, and I have not seen a convincing argument that it is not at least theoretically possible. Still, my money would be on "later" rather than "sooner".  I don't think it is necessary to study how the brain works to create artificial intelligence.  I actually think trying to do this would be the wrong approach. Instead we figure out algorithms that learn. 

Artificial Neural networks are not significantly similar to the brain (Although they were originally inspired by primitive notions about the brain).  Attempts to make them more like the brain have not been particularly successful, not really because they don't learn better, but mainly because they're slower and not that much better. 
 I think there are two issues that need to be addressed:

1) What we consider intelligence is actually a small part of what the brain does. Reasoning and creativity are highly dependent on instincts and biological imperatives.
2) Human brains are massively parallel and I think much more nuanced in function than binary-based computers, which means that implementing a human-like intelligence is a huge project. I don't think it is necessary to study how the brain works to create artificial intelligence.  I actually think trying to do this would be the wrong approach. Instead we figure out algorithms that learn. 

Artificial Neural networks are not significantly similar to the brain (Although they were originally inspired by primitive notions about the brain).  Attempts to make them more like the brain have not been particularly successful, not really because they don't learn better, but mainly because they're slower and not that much better. 
 &amp;gt; I actually think trying to do this would be the wrong approach.

In regards to creating intelligence? I think you're right. We evolved this way, so it works for us. That doesn't mean it's going to work for an artificial species. 

However, it's probably a good step to understand how we compute information and how that process becomes our conciousness to learn, at least one, method of doing this before we seriously make inroads into making artificial conciousness.  I don't think it is necessary to study how the brain works to create artificial intelligence.  I actually think trying to do this would be the wrong approach. Instead we figure out algorithms that learn. 

Artificial Neural networks are not significantly similar to the brain (Although they were originally inspired by primitive notions about the brain).  Attempts to make them more like the brain have not been particularly successful, not really because they don't learn better, but mainly because they're slower and not that much better. 
 I'm always skeptical of anyone who makes specific predictions about the field of AI. We have a long way to go in understanding how our own brains work, let alone implementing an artificial version. That said I do not think it is a lost cause, and I have not seen a convincing argument that it is not at least theoretically possible. Still, my money would be on "later" rather than "sooner".     The term ANN is a very broad topic.  ANNs are great in that they represent a mathematical "black box" which can learn without manual adjustment of the coefficients.  However, at the basic level they don't completely model the way the brain works.  We don't fully understand how the brain works.  I personally believe that time is another input into most neurons, and there's probably chemical inputs that affect a neuron's output that we don't understand.  So while I think we can and will eventually create machines that equal and exceed the capability of the human brain, it won't happen until we have a better understanding of the brain. Yeah, though isn't there some proof that a 3 layered neural network can be made to solve any problem, and that the issue is more just being able to find the right topology/weights? Yeah, though isn't there some proof that a 3 layered neural network can be made to solve any problem, and that the issue is more just being able to find the right topology/weights? When people say "solve any problem", they refer to learning any mathematical function where the inputs can be represented as an series of numbers. Getting from this to general intelligence is still pretty tricky. 

If you enjoy the slightly less technical and high level discussion of AI, I can really recommend Hofstadter's "I am a strange loop "  As someone who did a MS on ANNs, I'm not impressed with them any more. The inherent degree of randomness makes their results irreproducible, and the training algorithms are typically slow. I've grown to prefer SVMs which have reproducible, provably optimal results and much faster training rates. 

If you're interested in NNs, convolutional are the way to go imo, but that requires a good amount of domain knowledge to set up the network structure properly.  We can't even define what an intelligence like our own is, how are we supposed to build something that is like it? 

Hell, we don't even really understand how, or why, our brain really works in regards to computational representation of creative intelligence.

Baring those minor details, the core problem always comes down to: why would we build human like intelligence in something that isn't human? Generalist tools have never really served humanity that well, having a range of highly specialist tools has always served us better in long run. 

As such, we are probably much better off building self-learning machines that are designed around one solving one specialist set of problems. There might be a novel interface that makes it gives it a natural feel of communication, but it's actual creative intelligence would be based around the problem it is intended to solve. 

Which is exactly the way that most practical AI has been implemented in Industry. Automated Vehicle  Drivers aren't intended to preform Colon surgery as well (because that would be weird, right?) they are intended to drive some vehicle around. 

---

If the question was simply about possibility. 

Sure, on a long enough time span hard AI is plausible.   I believe a recurrent neural network would be capable of this. A project I am working on is creating a massive neural network over a number of systems to see how complex I can get it to become. Due to the increased power of machines and their continuing increase, I think neural networks will become very useful in the near future. What's the learning algorithm you will use? Backpropagation no longer works in such cases. Genetic algorithms, it's a really good approach to it in my opinion. It's generally the approach for "I can't be arsed to work out an actually efficient optimization algorithm", actually.

How are you going to work out the optimal mutation rate and population sizes?

Are you sure that a genetic approach will be more efficient than Simulated Annealing? That would assume you got a cross-over operator in mind that really makes sense for a multilayer or free-form ANN?

If not, go for Simulated Annealing, it's less memory intensive because it's basically a GA with a population of one, but there's much better theoretical work done for selecting the proper learning rates and such.

And getting those learning rates correct is going to get you several orders of magnitude quicker to the optimal solution. Not really something to gloss over.

Another bit of advice: Work out the math for a bit. Are there layers still, or is it completely free form? If it's got layers you can model all sorts of things with matrix multiplications and speed up the optimization process perhaps with some partial backpropagation type of thing. 

If it's free form, remember that there are really efficient ways to solve systems of equations (Gaussian elimination), for example to find the coefficients for a linear combination of certain neurons to least-squares approximate a certain signal. Look up [Echo State Networks](http://en.wikipedia.org/wiki/Echo_state_network) ([PDF](http://www.faculty.iu-bremen.de/hjaeger/pubs/ESNScience04.pdf)) for an interesting example.

I'm just warning you and pointing you a bit into the right direction, because what you describe (to throw a GA at a random free-form ANN) seemed a good idea to me as well, several times even, and, well, the convergence of a Genetic Algorithm in such cases is almost always quite disappointing: It's slow and you really can't tell if the (generally shitty) result you're seeing is the best your parameters can do, or maybe it's a global minimum, or maybe you should just let it run for the whole night and it'll find something better (as for the latter, unless you got a solid reason to expect it to, it probably won't.)   I'm pretty sure there are beter ways than ANN, but ANN with some augmented functionalities of other kinds (like database searching using standard algorithms) will just bring us result faster. Purly ANN strong AI (including creativity and real art, but this depends on sensory data it will have, as well interaction with environment, other machines, people and our culture and physical world) in software/hardware should be possible around 2045. I really belive so, but will see.

ANN by itself have multple usage. Mostly clasification. Problem with most ANN used currently is separate lerning phase. We need adaptative ANN, which actually learns itself when using. So it must be more feedback and actuall learning must be done automatic. It will create more autonomous and inteligence to the system, like generalization, inference and deduction.

As of creativity, currently some evolutionary algorithms are better suited for this purpose, with good results. You can easly use ANN to interpret an outcom of evolutionary algorithm to drive its direction into needed result. There is many possibilities to mix things. Oh really? What other structures are better than ANN? I just say that "I'm pretty sure there are better ways", not nacasarly known yet to humanity...
</snippet></document><document><title>The Hardness of the Lemmings Game, or
Oh no, more NP-Completeness Proofs (pdf)</title><url>http://dimacs.rutgers.edu/~graham/pubs/papers/cormodelemmings.pdf</url><snippet>  The Lemmings level encoding the 3-SAT problem is one of the greatest things I've ever seen.  For anyone that didn't read the entire paper, you have to see it:

[http://i.imgur.com/13chk.png](http://i.imgur.com/13chk.png) So the idea is the boxes on the top left are the constraint gadgets that encodes the clauses while the boxes on the bottom right encodes the variables. Each clause can spawn a single lemming with a single dig power, and it has to choose one of three blocks to dig and fall through with each block corresponding to satisfying a single term in that clause. Furthermore, the lemmings in the variable gadgets must build bridges to avoid a lethal fall, and can choose to build that bridge either on the left hole (representing setting variable x_i to true) or on the right hole (representing setting x_i to false). Each of the terms in the clauses are connected with a single chute that drops the lemmings into the room with the variable and hole/bridge corresponding to that term. If a bridge is built under where the chute drops the lemming, then that lemming survives, otherwise it dies. 

For example, if one of my clause is (x1 V ~x2 V x3), and the lemming associated with that clause chose to dig through the ~x2 block, but the lemming in the x2 variable room decides to build the bridge on the left hole (corresponding to setting x2 to true), then the clause lemming will eventually find its way into the chute dropping it off into the x2 room. But since there is no bridge covering the ~x2 hole, the lemming falls through the hole and dies.

From here, the other input variables are easy: We give each clause lemming a single dig power, and each variable lemming a bridge power, and we ask whether its possible in our reduction instance for all lemmings to survive.

Hence, for the 3SAT satisfiable -&amp;gt; Lemming satisfiable side, we can intuitively see that in order for all variable lemmings to survive, each can only build a single bridge, or else at least one will not have a bridge and will fall to his death. Furthermore, if in two clauses we assign contradictory truth values to the same variable, then one of the lemmings will leap to his death in the corresponding variable room and Lemming will also return no.

This is one of the most creative 3SAT reductions I've seen, I love it This is a fantastic explanation.  I was going to explain it myself, but I don't think I could do any better. :D </snippet></document><document><title>Thinking about taking computability class</title><url>http://www.reddit.com/r/compsci/comments/s8uyl/thinking_about_taking_computability_class/</url><snippet>I'm going to be a senior in CS next year and I need two CS electives. This is one that I can take, but I was wondering if anyone else has taken a similar class, and if so, what it was like. It's a special topics math class, so I'm not really sure about taking it as my schedule is already looking pretty difficult.  Could you post the course description? I took a class called "Fundamentals of Computing Theory". We covered languages (regular, context free, recursive, recursively enumerable), turing machines, regular expressions, and some complexity theory. It was probably one of the best classes I've taken. It wasn't an easy class, but it was very do-able. </snippet></document><document><title>Question about minor in Math</title><url>http://www.reddit.com/r/compsci/comments/s805h/question_about_minor_in_math/</url><snippet>I'm 1 semester away from getting my BS in Computer Science. However I've been tempted to get a math minor as it would only extend my schooling for 2 additional semesters. I'm just wondering if this additional education is a factor in getting a job to potential employers?

edit1: Thanks everyone for the replies the information has been very useful    I have a math minor and a CS major, and I don't think any of my employers have particularly cared about the minor (of course I can't say that it didn't help distinguish my resume from others competing for my positions either). That said, the math minor was very worthwhile personally, and has provided me access to knowledge that has certainly been useful in problem solving.

For what it's worth, my extra courses were cal 3, diff eq, combinatorics and graph theory, and linear algebra. Of those combinatorics/graph theory was probably the most useful, but if you're doing anything relating to graphics linear algebra will likely be a better fit. I have a math minor and a CS major, and I don't think any of my employers have particularly cared about the minor (of course I can't say that it didn't help distinguish my resume from others competing for my positions either). That said, the math minor was very worthwhile personally, and has provided me access to knowledge that has certainly been useful in problem solving.

For what it's worth, my extra courses were cal 3, diff eq, combinatorics and graph theory, and linear algebra. Of those combinatorics/graph theory was probably the most useful, but if you're doing anything relating to graphics linear algebra will likely be a better fit. I have a math minor and a CS major, and I don't think any of my employers have particularly cared about the minor (of course I can't say that it didn't help distinguish my resume from others competing for my positions either). That said, the math minor was very worthwhile personally, and has provided me access to knowledge that has certainly been useful in problem solving.

For what it's worth, my extra courses were cal 3, diff eq, combinatorics and graph theory, and linear algebra. Of those combinatorics/graph theory was probably the most useful, but if you're doing anything relating to graphics linear algebra will likely be a better fit. Wow, those weren't required anyways? My major requires Calc 3, Combinatorics and Linear Algebra. Graph Theory can be taken as an elective for my major as well, but isn't required, nor is Diff Eq.  I have a math minor and a CS major, and I don't think any of my employers have particularly cared about the minor (of course I can't say that it didn't help distinguish my resume from others competing for my positions either). That said, the math minor was very worthwhile personally, and has provided me access to knowledge that has certainly been useful in problem solving.

For what it's worth, my extra courses were cal 3, diff eq, combinatorics and graph theory, and linear algebra. Of those combinatorics/graph theory was probably the most useful, but if you're doing anything relating to graphics linear algebra will likely be a better fit. I am definitely interested in graph theory specifically with network loads and I loved linear algebra when I took it so I may take the second level of that. I'm not a huge fan of combinatorics though so I'm going to try to avoid math that involves too much of that. Thank you for your input! As I will be looking for a job soon after maybe it will be worth it to distinguish me from my competition because living in Vancouver, IT jobs are actually quite competitive. Graph theory and combinatorics are extremely intertwined.[ Ramsey's theorem](http://en.wikipedia.org/wiki/Ramsey%27s_theorem) is kind of the archetypical example here, but it's by no way unique in this regard. I'd be hesitant to suggest graph theory if you're not comfortable with combinatorics.  Nope nope nope. You have your degree, get out there and learn in the field. If you find yourself needing more math, then look into learning it in your free time or maybe a specific class. If you just like math and want to learn it then there is even more reason to do it in your free time.   I double minored in Math and Econ.  Nobody batted an eyelash at the Math minor, but some interviewers were impressed with the Econ minor.  I guess you haven't seen [this]( http://i.imgur.com/MmVe7.jpg) picture. I guess you haven't seen [this]( http://i.imgur.com/MmVe7.jpg) picture. O_O I assume this is more for mathematicians doing very complex research though.. I guess you haven't seen [this]( http://i.imgur.com/MmVe7.jpg) picture.  That's kind of a lot, if you're borrowing you probably shouldn't do it. At my school I double majored in less time.     </snippet></document><document><title>I need advice on getting a second BS or MS.</title><url>http://www.reddit.com/r/compsci/comments/s7oy1/i_need_advice_on_getting_a_second_bs_or_ms/</url><snippet>After a lot of thought I have decided to go back to school.  I'm in my early 30's, which makes me feel too old at this point.  But better to start doing what I like now, right?  Anyways, I'm taking a C++ course while deciding if I want to go for a BS or MS.  I'm a molecular bio major, which has completely nothing to do with CS.  Anyways, I have a few questions that needs answers in order for me to decide which route I want to take.
1. Does school matter?  I've read somewhere that the drop off is so significant between a school like MIT and Madison that unless this is what I have to choose between, it doesn't matter.  I would love to hear your opinions as it would help a lot with my decision.
2.  MS or BS?  I still need to take a handful of basic computer courses that are required for entry into a Master's program.  I probably have a lot of the gen-eds completed from my previous degree that would also fulfill CS BS requirements .  These required classes would also fulfill a BS requirement as well.  However, for a BS,  I would probably have to take Calc based physics and math beyond Calc 2.  I'm approximating about 2 years for either degree.  I guess my biggest concern is skipping undergrad and all its foundation courses to plunge into a graduate program.
3.  Is there anything else I can do to get a head start?  Time is definitely not on my side so any suggestions, even "internship, ASAP" would be wonderful.  I really have no idea where to start except enrolling community college courses.  I had a conversation with someone from work and he suggested looking for collaborative "open source projects".  And I found out about project-Eula on reddit, which I will check out after this.  Any other suggestions?

Thanks in advance for your input!

   1.) Realistically no. For MIT promo material, yes. There are only a few big name schools but a TON that aren't and yet people who have a BS, MS or PhD from the "no name school" still gets a job. The thing that MIT, Harvard , Stanford and the like give you are a strong network. But that doesn't mean you can't get that on your own with effort.
  
  2.) Masters all the way. There is no reason to get a new BS. It may cost more, take you longer and the position in the job market that you'll come out with isn't that great (you'll compete against other people with just a BS). A MS will give you more indepth knowledge, open up some jobs that weren't available to you before and give you an edge in the market.
  
  &amp;gt; I guess my biggest concern is skipping undergrad and all its foundation courses to plunge into a graduate program
  
  That won't happen. If admitted to a MSCS program, that school will most likely give you a conditional admittance, where if you complete the necessary pre-reqs and maintain a certain GPA doing so, you will be admitted into the grad program. I had to take 5 or 6 pre-reqs (depending on your programming skill you could take a Java class that combined Java I and Java II which were also pre-reqs). I was able to knock those out in 2 quarters (super stressful though as I work full time) and started on my grad work after that.
  
  
  3.) Enrolling in the community college courses is a bad idea (IMHO). It probably won't help towards the BS or the MS. The best it could do is give you the knowledge to test out of pre-reqs in the MS program (if the school you go to allows that). Otherwise you are just sinking your money into a program. Just start programming and learning as much as you can. Become a sponge. The more basics you know going into the program the better off you'll be. Thank you for commenting.  Regarding community college, most of the universities that I have looked up require that a handful of certain courses be taken and passed before applying to the MS program.  From what I have seen, it's usually programming 1 and 2 (JAVA or C ++ and JAVA or C++ data structures), discreet math, and 1-2 other courses depending on the institution. Are you sure though that the community college classes will transfer? Not all CC classes do transfer. I know in Illinois there is a list of (IAI, I believe) of CC classes that will for sure transfer to the State schools. Unfortunately, these courses do not even count towards my degree.  These are necessary requiremements so that I  possess basic knowledge of CS.  In other words, they won't even look at my application without these 5 or so classes completed, which kind of sucks.  But I guess it is understandable they want applicants without a CS degree to have taken these courses.  I'm assuming it helps both the school and the applicant gauge how much the applicant will understand?
Edit: spelling/grammar What @jhartwell means may be, your CC classes might not even count towards those requirements. The best way to handle this is to go to a academic counselor at the school you'll be attending with the course catalog of the CC you'll be attending for these prereqs. Get each course approved as fulfilling the requirements, preferably in writing. Unfortunately, the classes I need to take doesn't count towards the MS.  Those are just requirements to apply to the school for those who do not possess a CS background.  But the silver lining in taking the course is that it has affirmed my feeling towards CS, I know I'm going to enjoy it :)

Thank you for your help!   [deleted] Would you mind elaborating on what the key differences are between the 3? [deleted]  have you considered an associates instead of another BS or MA?  i have a BA and am actually going to go back and get an associates.  along the way, i will pick up 2-3 certificates (like java programming, for instance) from the same school that count toward the AS.  PLUS, they are going to give me some credits from the BA toward the AS, so it will take even less time.  not to mention the cost savings are enormous.

as an IT recruiter, i can tell you the school you went to means almost nothing to me or to employers.  it's what you can do that matters.  yes, some require a degree, but it could be *any degree,* not even necessarily in IT or CS.  of course, they want to see experience (or certs, if applicable, like a CCNA).  i'm working on getting a friend a dev job right now - he's been a systems admin for ten years but wants to switch to dev.  after school, he kept on programming as a hobby and now has the skills necessary to do it for a living.

with your bio background, you should have a good shot at getting in w/ a pharma co. if you're interested.  biotech is still growing, at least in my neck of the woods.

keep doing your research and do what's best for you - financially, etc.  PM me if you want to chat more and good luck.  :)  I don't think an MS is a very good idea. Higher education is getting unwound for being too expensive. Degrees don't matter after the first job. People care about what you've done and you can easily do something cool and learn a lot on your own. You might need to find a mentor of some kind for those times when you get stuck and don't know what questions to ask. Or find a cohort of people at your same skill level so you can help each other learn.

As far as learning the style of thinking, you're better off learning Python and using that to explore CS concepts in real-life projects that interest you. Make a game or an iPad app or some interesting data visualization or data analysis with Processing (or Nodebox). Make one cool thing and it will circulate around the social networks. 

To get a job, make a portfolio and post it online.

I have degrees from 2 of the top 4 CS schools but dropped out of the PhD.

MS degrees from a top school like MIT says, "This person couldn't finish the PhD for whatever reason. They probably know something but no guarantees." It's actually more of a layoff notice from the PhD program.

The really hot shit skills are machine learning (more mathematical and probability). There's an overlap in applying machine learning methods to bioinformatics. 

There's a book called Collective Intelligence that I've been reading and it walks you through some handy machine learning tricks for those of us who suck at math. There are a couple of intro bioinformatics books for biology people. Might be a more rewarding in road for you.

Drop the C++ class. It's not even worth the gas money. You'll memorize a lot of bizarre rules and then hate programming afterwards. Ditto with Java and Perl. (but not Javascript)

If you can integrate your bio background, you'll have a very unique skill set.   Edit: Accidently hit save on tablet screen ( am I doing this right?).
Thanks everyone for the input.  So far, it seems MS is the better choice.  I just received a response to an email I sent regarding the courses necessary to enter UIUC.  The person pretty much shot me down stating only 10% of all applicants to the MS program get in, and there were 1650 applicants which all either came from a CS background or have a strong understanding and experience in CS.  She opened up the response with "it's hard to advise you a successful path getting into our MS program if you do not have a CS background". Ouch.  I'll try other schools then! What is your job situation?  I work at a University, and while I'm paid a little less than I would get in "industry," the perks are great.  Free school is one of them, and working a block away from where your classes will also help a ton.  If you are open to it, apply for a job at a few places while you are at it...it is a pretty comfy situation.

H2P </snippet></document><document><title>Could use help on understanding sparse matrices (or at least a pointer to a good read on them).</title><url>http://www.reddit.com/r/compsci/comments/s78ln/could_use_help_on_understanding_sparse_matrices/</url><snippet>A few quick things up front. 1) I had posted this to /r/Matlab yesterday but it was recommended that I seek out more general math/CS subreddits than language-specific subreddits for this. 2) My background is physics, not math or compsci. I can hold my own in these areas (as long as we're not talking proof-bath math e.g. real analysis) but with math/CS I've found I deal better with concrete examples than "mathy" presentations (best way I know how to put it) and also do best with relating the math to something it could physically represent. 3) It's been pointed out to me that Matlab's sparse matrix implementation lets you know just about nothing about sparse matrices and still make good use of them, but I prefer to actually understand the tools I'm using whenever I can, and I do feel that I'm ever so close to understanding this with just one major obstacle left. With that said...

I've got a project which I'm currently deadlocked on because I cannot for the life of me get my head all the way around sparse matrices. I haven't even gotten up to manipulations yet because I'm stuck on figuring out the [IA array (as per the Wikipedia entry on the Yale format)](http://en.wikipedia.org/wiki/Sparse_matrix#Yale_format). I get that A is the array which simply stores all the non-zero elements, and I get that JA stores which column each entry in A came from, but the way row information is dealt with in IA is just breaking my head...every time I think I'm starting to get close to an understanding of it, I think just a bit deeper about it and then get utterly confused again. (I think I'd be happier if I just didn't understand a damn thing about this TBH--so frustrating to be so close yet so far!)

When I sat down and thought about it I was able to conceive of some simpler implementations based on what seems to be the fundamental point of sparse matrices--throwing out everything but the non-zero elements so that your 10,000x10,000 matrix doesn't break your computer due to the huge memory requirements--but I'm considering a point charge moving over a large conducting sheet, and got stuck on figuring out how to actually manipulate the matrix as the charge moves around (so I can figure out how to set up the initial configuration but coding in the adding/removal of entries as the script runs seems trickier, and keeping track of where to insert the added entries seems even trickier).

So then I went digging around online and the best I could find were a couple of papers that were written in a very heavy math/CS style--beyond what I feel comfortable handling due to having a very "physics" mind towards these topics (I'm definitely one of those physics people who is more than happy to deal with "this is a comp. Like I said, my background is physics and I find I learn this kind of stuff better with concrete examples than through proofs and formalisms and what-not; probably doesn't help that I never had time for even abstract algebra. Anyhow, when I tried to soldier on and read these papers since they were all I had, I felt like they were not actually addressing the IA array in any more detail than anything else I'd seen, and were thus not really advancing my understanding.

So any help and/or good reads on this topic would be greatly appreciated. :)    Can I ask whether you actually NEED to understand the implementation details of sparse matrices for your project, or just think it will help you in dealing with them? If the project in any way involves writing an implementation of them yourself, STOP and reconsider.  </snippet></document><document><title>His bicycle 'had only to find two numbers which when multiplied together would give 5,283,065,753,709,209'</title><url>http://ed-thelen.org/comp-hist/Lehmer-NS03.html</url><snippet>  This is awesome.  Fantastic writing.  As for how times have changed...

    $ time factor 5283065753709209
    5283065753709209: 59957 88114244437

    real     0m0.001s
    user     0m0.000s
    sys      0m0.000s

Edit: the whole thing reminds me of how Turing's bicycle chain would come off in Cryptonomicon.
  I really hope I wasn't the only person who said 5,283,065,753,709,209 * 1... He never said one of the factors had to be greater than 1.  I was right there with you. Always pick the low-hanging fruit first, I say... I really hope I wasn't the only person who said 5,283,065,753,709,209 * 1... He never said one of the factors had to be greater than 1.  You always have those factors. That doesn't mean it isn't prime. They are trying to find it's *prime* factors.  Relevant:

[Lehmer sieve](http://en.wikipedia.org/wiki/Lehmer_sieve)

The author was writing about his own device.   I was extremely interested in reading of this.  I'd read a vague mention of the machine in a book recently about computational matters surrounding prime numbers, and it piqued my curiosity.

Then I actually read it.  Holy flipping shit.  The author has a tremendous way with words.  I've never wanted to hunt down a writer and spend so much time cutting his face.  Seriously, enduring his eloqution should be rewarded with a medal and a merciful bullet.  I imagine it's just because the article was penned in 1932.  I expected some cultural difference, but his writing really got under my skin.  I imagine the author is long dead, but if he's actually not and you know of Derrick N. Lehmer, please let me know where is at.  I would love to help him discover what it feels like to have his eyes pulled from their sockets and his optic nerve tied in a bow. I actually *liked* his writing, very much. But then again, I like reading Thoreau and Emerson, and the author's tone reminded me of them. Style aside though, I thought he did a fine job of choosing similes. Hmmm. The whole thing read like a John McPhee piece. I actually *liked* his writing, very much. But then again, I like reading Thoreau and Emerson, and the author's tone reminded me of them. Style aside though, I thought he did a fine job of choosing similes. Hmmm. The whole thing read like a John McPhee piece. I was extremely interested in reading of this.  I'd read a vague mention of the machine in a book recently about computational matters surrounding prime numbers, and it piqued my curiosity.

Then I actually read it.  Holy flipping shit.  The author has a tremendous way with words.  I've never wanted to hunt down a writer and spend so much time cutting his face.  Seriously, enduring his eloqution should be rewarded with a medal and a merciful bullet.  I imagine it's just because the article was penned in 1932.  I expected some cultural difference, but his writing really got under my skin.  I imagine the author is long dead, but if he's actually not and you know of Derrick N. Lehmer, please let me know where is at.  I would love to help him discover what it feels like to have his eyes pulled from their sockets and his optic nerve tied in a bow. </snippet></document><document><title>How do you keep up with current research?</title><url>http://www.reddit.com/r/compsci/comments/s5vh6/how_do_you_keep_up_with_current_research/</url><snippet>I read a few blogs but in general I have no idea what is currently going on in my field of interest (formal methods, static analysis, verification in general). I would like to change that but I have no idea where to start.

How do you decide which papers to read and where do you get them from?  Formal methods is an exiting field! However, it is also a fantastically large one so keeping abreast of all the recent devellopments is a daunting, if not impossible, task. As lsd503 sugested, you can try reading proceedings from relevant conferences, [here](http://www.handshake.de/user/kroening/conferences.html) is a small list of some of the largest relevant meetings.

Again, each conference produces 20-30 very technical papers, so you have to really restrict yourself to your specific interests and try and pick up a bibliography of relevant names and papers to read (introductions and conclusions of papers are a good source of relevant source articles).

Wether or not to dive into the papers directly or start by reading less recent but more general and accesible source books really depends on your current level of expertise. Formal methods is an exiting field! However, it is also a fantastically large one so keeping abreast of all the recent devellopments is a daunting, if not impossible, task. As lsd503 sugested, you can try reading proceedings from relevant conferences, [here](http://www.handshake.de/user/kroening/conferences.html) is a small list of some of the largest relevant meetings.

Again, each conference produces 20-30 very technical papers, so you have to really restrict yourself to your specific interests and try and pick up a bibliography of relevant names and papers to read (introductions and conclusions of papers are a good source of relevant source articles).

Wether or not to dive into the papers directly or start by reading less recent but more general and accesible source books really depends on your current level of expertise. Thanks, I was hoping there would be an easier method than browsing through conference papers ;) Thanks, I was hoping there would be an easier method than browsing through conference papers ;)  portal.acm.org and http://ieeexplore.ieee.org/Xplore/ are where all of the biggest conference and journal papers are stored after publication. You will need a membership to access any of these databases but if you are at a school they will probably have a membership (try looking on the libraries website). If not the databases will normally let you see titles and abstracts to their papers and then use google scholar to see if a copy of the pdf has been put up somewhere (oftentimes authors will put copies up on their websites). Other than that you would need to become an IEEE or ACM member which is pretty expensive. Unfortunately I am no longer at a university. Is an ACM membership worth the money?  Unfortunately I am no longer at a university. Is an ACM membership worth the money?  Unfortunately I am no longer at a university. Is an ACM membership worth the money?        </snippet></document><document><title>Computer Science May Transition From Elective to Requisite </title><url>http://www.usnews.com/education/best-colleges/articles/2012/04/03/computer-science-transitions-from-elective-to-requirement-computer-science-transitions-from-elective-to-requirement</url><snippet> </snippet></document><document><title>What is usually taught in an undergrad Data Structures and Algorithms class?</title><url>http://www.reddit.com/r/compsci/comments/s6izw/what_is_usually_taught_in_an_undergrad_data/</url><snippet>I am currently taking an informal Data Structures and Algorithms class in high school, but I'll have to take it again next year as a freshman due to schedule conflicts. So far, we have covered sorting algorithms, array lists, linked lists, stacks, queues and trees. What I mean by cover is that we wrote our own versions. We just slightly touched on the Big O Notation, so I still have some issues deciphering between n log n and just log n.   Oh man, *highschool?*, you're going to be way ahead of the game. Your class sounds very much like what a normal undergrad data structures class will be like.

The undergrad class will likely be more mathematical (depending on how your professor teaches it) and will likely go over some proofs, will look at a few more sorting techniques and a few more data structures.

It will also likely look at things like [dynamic programming](http://en.wikipedia.org/wiki/Dynamic_programming). Depending on if you are required to take an introductory discrete mathematics course as a prerequisite a useful skill to know may be [induction](http://en.wikipedia.org/wiki/Mathematical_induction).

Honestly though, you are likely going to be much more prepared than everyone else.

edit: My algorithms class involved no implementation projects, every assignment was a list of proof questions - 90% of which I used induction to solve (life saver). I do know previous professors for the same class at my school did have a few implementation projects, but overall programming is not the main focus.

I should add that it is worth putting a lot of time and effort into this class. You'll encounter many of the topics again in upper level courses, they are also really good things to know going into a potential internship interview. Especially make sure you come out of it knowing the time complexity of several sorting algorithms, of major methods on the common data structures (Queues, stacks, lists, arrays, trees). 

You don't really need to know how to implement a lot of these things, just understanding the costs/benefits/downfalls of using various algorithms. Well, I've heard from an alumni that my teacher just goes overboard teaching the materials. I mean he treats it like a college class; the teacher has a PHD in EE. He didn't become a professor because his wife did not get a position in the same school, so they went on to work at a government institution as a systems engineer and then he started teaching at my high school 12 years ago. He just fell in love and quit his old job after trying to balance both for 5 years. He's a great teacher.


   Huh, that's a pretty impressive array of stuff for just high school. Agreeing with other folks: you're going to be way ahead of the game.

I've been a TA for Carnegie Mellon's intro data structures and algorithms class for 5 or 6 semesters now, and this is pretty much what we cover: arrays (static and dynamic), linked lists, stacks, queues, trees (binary search trees, heaps), collections (i.e. lists, sets, and maps), some OOP stuff (polymorphism, inheritance), hashing, recursion, searching and sorting algorithms, and big O. no graphs, max flow, or dynamic programming?  obligatory CLRS link: https://secure.wikimedia.org/wikipedia/en/wiki/CLRS

If you really want to learn algorithms, I highly highly recommend reading through CLRS and doing whatever exercises look fun. One readthrough of CLRS will make you better at algorithms than most CS students anywhere. Thanks, I already have the PDF on my Kindle. I hope to use it in conjunction with MIT's Algorithm class. I don't have time for it right now due to AP Tests.   From my experience it's what you described but all of it was in C++ Function Pointers? Unions? Friend classes/functions? Any of the other obscure topics usually not covered in traditional C++ classes?   </snippet></document><document><title>How many different kinds of computability are there that are weaker than Turing Complete?</title><url>http://www.reddit.com/r/compsci/comments/s4r46/how_many_different_kinds_of_computability_are/</url><snippet>I was studying some of the Turing Oracle systems that are stronger than Turing Complete and it got me thinking about the different ways a language can be weaker than Turning complete. Surely one can imagine all kinds of arbitrary limitations (like limiting yourself to N branching statements) but presumably all the limitations would fall into general classes of weaker systems. Is there a bestiary for these systems the way there is for computational complexity?  I think [this wiki article](http://en.wikipedia.org/wiki/Automata_theory) is an overview of what you're looking for.  It's been a few too many years for me to be able to be able to go into too much depth, but I did love the subject when it came up in school.  If you're familiar with Automata Theory and I've misunderstood what you're asking, you'll find the rest of this less than helpful.

The Linear Bounded Automata (LBA) machines (and thus languages written for them) are only able to solve a subset of the programs Turing Complete languages are due to limits on the amount of storage (but can use their storage in the same way).  Push Down Automata (PDA) are then a subset of that, because they have the same sort of storage as a LBA, but can only use their storage as a stack instead of an open storage tape.  Finally you come to the different automata, which are completely stateless due to having no storage.  Due to the hierarchy, a Turing machine can emulate a LBA, which can emulate a PDA, which can emulate an automata.

For example, I did an interpreter for a language called [Velato](http://esolangs.org/wiki/Velato) for one of my projects.  It uses intervals between notes in a midi file to determine instructions and variables.  However, due to the limit on the number of intervals (different octaves are not counted, only base note intervals), there's a limited number of variables, effectively reducing the language to a LBA.

I recommend starting from the bottom and working your way up to Turing complete, the hierarchy was built bottom up, so I think it makes a bit more sense that way.  I still have the book we used for it, I'll try to remember to post a link to it when I get home, but I suspect you'll have no trouble finding resources online discussing the topic. Perfect. This is exactly what I was thinking about. 

One further question - if you're up for it. The classes you've described have a strict hierarchy. Do there exist two automata classes where neither can emulate the other (and which both can be emulated by a Turing Complete machine)? A surprisingly hard question. Due to the space and time hierarchy, classes based on these two things will subsume lower classes. Most other situations which would fit your criteria are unproven (SC2/UL, UP/P, RP/coRP), and are tending to equality or consumption.

Perhaps the best example is NP and coNP. Both are believed, *very strongly* to be not equal, and if either is a subset of the other - then they are equal. Hence, neither should be able to emulate the other, and both can be emulated by a UTM.

I will comment again if I can think of a proven example of two classes which aren't subsets of the other. Perhaps UnambLinCFG and DetCFG .. grumble grumble.

[Edit : The NP, coNP example - if correct - generalizes to an infinite number of classes in the PH.]   Wait, are we talking about computational complexity or about computability? Or do the two map to each other in some standard way?

Thanks for entertaining my questions! Wait, are we talking about computational complexity or about computability? Or do the two map to each other in some standard way?

Thanks for entertaining my questions! The two aren't particularly different, really. You can think of P as things that are computable by a TM that can only run for a polynomial number of steps, for example. The proof techniques used for the two are generally the same, and you're unlikely to find anyone who works on one but not the other. Yeah but what does it means to restrict a language to "a number of steps which grows polynomialy with respect to a programs input". How could you even construct such a language?

Update: ussually when you go to prove to languages equivalent all you have to do is prove one can emulate the other. If we bring this weird class of polynomial languages into being you might that one automata can emulate another, but too slowly to fulfill its polynomial time requirement. Hmmm. I think this equivalence between Automata and complexity classes might be less clear than I thought. Yeah but what does it means to restrict a language to "a number of steps which grows polynomialy with respect to a programs input". How could you even construct such a language?

Update: ussually when you go to prove to languages equivalent all you have to do is prove one can emulate the other. If we bring this weird class of polynomial languages into being you might that one automata can emulate another, but too slowly to fulfill its polynomial time requirement. Hmmm. I think this equivalence between Automata and complexity classes might be less clear than I thought. Perfect. This is exactly what I was thinking about. 

One further question - if you're up for it. The classes you've described have a strict hierarchy. Do there exist two automata classes where neither can emulate the other (and which both can be emulated by a Turing Complete machine)?  In mathematical logic, there's a notion of a ["primitive recursive function"](http://en.wikipedia.org/wiki/Primitive_recursive_function), which (to oversimplify a lot) is a function that takes integer inputs and returns an integer that you can compute with a programming language that has for loops but no while loops.

The main effect of having no while loops is that there are no infinite loops -- every time you start a loop, you know how many times you'll go through the loop.  So every input produces an output -- there's no such thing as the halting problem because the function halts on all inputs.  You can actually show that there are functions that are so large that they can't be computed without some version of a while loop. Heck, while we're at it, you can look at systems like [coq](http://coq.inria.fr/) that implements a programming/proof language that. because it must be terminating, can not possibly be turing complete. In mathematical logic, there's a notion of a ["primitive recursive function"](http://en.wikipedia.org/wiki/Primitive_recursive_function), which (to oversimplify a lot) is a function that takes integer inputs and returns an integer that you can compute with a programming language that has for loops but no while loops.

The main effect of having no while loops is that there are no infinite loops -- every time you start a loop, you know how many times you'll go through the loop.  So every input produces an output -- there's no such thing as the halting problem because the function halts on all inputs.  You can actually show that there are functions that are so large that they can't be computed without some version of a while loop. Isn't a while loop just a special case of a for loop?

e.g.

    while (&amp;lt;condition&amp;gt;) &amp;lt;=&amp;gt; for (; &amp;lt;condition&amp;gt; ;) In C there's no real difference, but in Perl, Python, BASIC and a lot of other languages, you can only use the to loop over arrays ("for i in list") or sequences of numbers ("for i from 1 to n"), rather than arbitrary conditions. Isn't a while loop just a special case of a for loop?

e.g.

    while (&amp;lt;condition&amp;gt;) &amp;lt;=&amp;gt; for (; &amp;lt;condition&amp;gt; ;) I always thought a for loop was a special case of a while loop. They are turing equivelant. The only difference is speed for certain operations. What makes you say that; do you have a source? I would imagine it would vary greatly from language to language (and compiler/interpreter) first of all, and second I can't imagine that (for C-style for loops and C-style while loops) there would ever be a difference of more than one or two machine cycles for the entire loop, i.e. certainly not a significant and/or noticeable difference. Honestly, there shouldn't even be a difference at *all* as any compiler should effectively replace for loops with while loops and (vice versa) where performance could theoretically be gained by doing so. Imagine no more.  For loop:  
for(initializer; condition; incrementer){ looped stuff;}

Equivalent while loop:  
initializers;  
while(condition) {  
looped stuff;  
incrementer;  
}
   This is going to be telling, but I've never heard of a Turing Oracle system (or anything more powerful than Turing Complete). Is this just a different term for non-deterministic Turing Machines or something?

(Thanks, sorry for not getting at your main point though :/ )

Edit: I guess I can make a shot at your major question. I think these machines are classified best by what class of languages they can recognize. [The wikipedia page on Formal Languages](http://en.wikipedia.org/wiki/Formal_language) is probably a good starting point for finding a good classification of them. I'm no expert in this field though. Good luck with your research! :)   To answer your question pedantically, "countably infinite".</snippet></document><document><title>Clever Algorithms: Statistical Machine Learning Recipes</title><url>http://www.cleveralgorithms.com/machinelearning/index.html</url><snippet /></document><document><title>How can I get started with data mining on my own?</title><url>http://www.reddit.com/r/compsci/comments/s2u6k/how_can_i_get_started_with_data_mining_on_my_own/</url><snippet>I know there is a ton of data on the internet and I want to figure out and to harvest at least some it. Is there some set of tools where I can pull information off a particular website. For example is there some way I can pull how long each of my friends spends in facebook chat? Or, perhaps more simply, can I search the front page of reddit to see how often the word "cat" is mentioned?

Does anyone have any projects going on? What do you use?  In general, if you are a python fan the package Beautiful Soup; it's good for extracting information from html. 

Personally, I'm doing some research work that crawls twitter in real time, twitter has an easy use api. May be useful if you just want easily accessible social data to play around with. The drawback is that you cannot mine historical data.  The Twitter API doesn't allow it. In general, if you are a python fan the package Beautiful Soup; it's good for extracting information from html. 

Personally, I'm doing some research work that crawls twitter in real time, twitter has an easy use api. May be useful if you just want easily accessible social data to play around with. There's also [lxml](http://lxml.de/). In general, if you are a python fan the package Beautiful Soup; it's good for extracting information from html. 

Personally, I'm doing some research work that crawls twitter in real time, twitter has an easy use api. May be useful if you just want easily accessible social data to play around with. how can u get live data from twitter?  Reddit has a pretty simple API that you can immediately use, in JSON format.

[http://www.reddit.com/r/all.json](http://www.reddit.com/r/all.json)

Just add '.json' to the end of any link on Reddit to get it in machine readable form.  Lots of languages have built-in JSON parsers now, such as Python's json module so you can just do data analysis from there.    I found the [The WEKA toolkit](http://www.cs.waikato.ac.nz/ml/weka/) to be a nice centralised resource when it came to learning about the multitude of techniques and parameters used out there. There's a [book](http://www.amazon.com/Data-Mining-Practical-Techniques-Management/dp/0123748569/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1334080999&amp;amp;sr=1-1) too which is a very informative read if a little dry in places. 

This was used in my Language Identification project for speech signals and it worked quite nicely. I'm getting a PhD in data mining, and we use Weka all the time. The GUI is great for playing around with stuff, and you can do command-line calls when you want real power. I just got a paper accepted on cross-domain classification of documents with sparse metadata. I used Weka for all of the heavy lifting. I just got a paper accepted on cross-domain classification of documents with sparse metadata. I used Weka for all of the heavy lifting. which conference ?                 </snippet></document><document><title>Programming language idea?</title><url>http://www.reddit.com/r/compsci/comments/s3ktt/programming_language_idea/</url><snippet>Pure functional programming languages do not allow mutable data, but some computations are more naturally/intuitively expressed in an imperative way -- or an imperative version of an algorithm may be more efficient. I am aware that most functional languages are not pure, and let you assign/reassign variables and do imperative things but generally discourage it.

My question is, why not allow local state to be manipulated in local variables, but require that functions can only access their own locals and global constants (or just constants defined in an outer scope)? That way, all functions maintain referential transparency (they always give the same return value given the same arguments), but within a function, a computation can be expressed in imperative terms (like, say, a while loop).

IO and such could still be accomplished in the normal functional ways - through monads or passing around a "world" or "universe" token.  &amp;gt; My question is, why not allow local state to be manipulated in local variables, but require that functions can only access their own locals and global constants (or just constants defined in an outer scope)?

This kind of feature is sometimes called "effect masking" and was tried out in the FX programming language developed at MIT in the late 80s. You can find one of their reports on the language [here](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.534&amp;amp;rep=rep1&amp;amp;type=ps). They mainly used it to ensure code could be scheduled concurrently, even if it used mutation.

I think the issue is that tracking this information and enforcing it is actually quite hard. For example, you can return a closure that has a reference to a local or global variable that you export. Calling this closure can now affect your state from outside. Figuring out if a closure is actually closing over your state is difficult, since you need to track all the aliases, for example. D has something like it too.  In their pure functions, you can only manipulate your local scope, and nothing outside of it.    </snippet></document><document><title>Submit to TinyToCS: The CS Research Conference for Papers of 140 Characters or Less!!</title><url>http://tinytocs.org</url><snippet>  A serious version of this with  say, a one page limit, might actually be useful.  P is not NP. I have discovered a truly marvelous proof of this, which this margin is too narrow to contain. P is not NP. I have discovered a truly marvelous proof of this, which this margin is too narrow to contain. P is not NP. I have discovered a truly marvelous proof of this, which this margin is too narrow to contain.       [deleted]</snippet></document><document><title>What are some not as well known but interesting CS jobs?</title><url>http://www.reddit.com/r/compsci/comments/s15pf/what_are_some_not_as_well_known_but_interesting/</url><snippet>  Embedded systems programming!  As an embedded system designer you get build a system (often from scratch) that is directly deployed to unique hardware of some kind.  While programming a dish washer probably isn't terribly exciting, the field encompasses computing in robotics, satellites, cars, manufacturing systems, communications (its a long list).  NASA employs them, Intel employs them, BMW employs them. Embedded systems programming!  As an embedded system designer you get build a system (often from scratch) that is directly deployed to unique hardware of some kind.  While programming a dish washer probably isn't terribly exciting, the field encompasses computing in robotics, satellites, cars, manufacturing systems, communications (its a long list).  NASA employs them, Intel employs them, BMW employs them. Yup. In fact, Intel just announced they were partnering with Nissan to put Atom chips in their in-dash entertainment system. Not sure what involvement we have beyond providing CPUs, though.

There's also phones and tablets, of course, which is always fun. Embedded systems programming!  As an embedded system designer you get build a system (often from scratch) that is directly deployed to unique hardware of some kind.  While programming a dish washer probably isn't terribly exciting, the field encompasses computing in robotics, satellites, cars, manufacturing systems, communications (its a long list).  NASA employs them, Intel employs them, BMW employs them. Let me also mention that it is extremely difficult. I personally categorise it as 'mind crippling'. That's why it's fun!  That's what every sharp, young, intelligent graduate thinks but things change when you get older. Not for everyone grandpa!  I'm 33 and I appreciate intellectual challenge more every single day. Let me also mention that it is extremely difficult. I personally categorise it as 'mind crippling'. It's more difficult than most programming but i wouldn't go as far as 'extremely'. Have you ever seen an FPGA with 200 signals manipulating a state machine?  Its like a symphony accept when the tambourine comes in on the rising edge instead of the falling edge it cascades into a race condition that the hardware manufacturer has to debug.

Anyways that's my experience with it. Oh yeah it's tough and full of challenges just like the one you mention but, lets be honest, if you have a race condition you have multiple processes which for some embedded systems would be a dream come true. You have to enjoy these things to enjoy embedded systems otherwise it's going to frustrate you. If you enjoy it, it's easy. Maybe it's just me but embedded systems grab me and I can't put them down. Embedded systems programming!  As an embedded system designer you get build a system (often from scratch) that is directly deployed to unique hardware of some kind.  While programming a dish washer probably isn't terribly exciting, the field encompasses computing in robotics, satellites, cars, manufacturing systems, communications (its a long list).  NASA employs them, Intel employs them, BMW employs them.  I'm a hacker for a company that specializes in secure systems design and implementation. I find that many folks don't realize this is an actual career option. It's insanely interesting, includes lots of variety, you get to meet and work with a ton of bright people the world over, and you're a rare talent in a very hungry market. Do highly recommend. I've been interested in this for a while now. Sadly, my University doesn't teach any of this good stuff. Do you have any ressources to get myself started?  The old-school advice was to "learn C and learn Unix." There is much truth in this, but it's a bit blithe, so:

Universities haven't yet figured out how to do a good security curriculum. I know how it is; I used to teach at one. Anyway, most of us got started more or less on our own, then were fortunate enough to have a good mentor somewhere along the way.

If this is an area you're interested in, the best way to get started is to come out to [Defcon](http://defcon.org). It's an annual con in Vegas. Registration is cheap and is cash-only, at the door. Several thousand people come each year -- it's insane. You really should move heaven and earth to make it possible to attend. Bring friends to share a hotel room, or whatever.

There you'll see cool shit, get inspired, and figure out where to go next.

There are also books and stuff. None are amazing, but there's a few which are good enough to get started. Searching "hacking" on Amazon will find you some books.

It's a huge discipline. Anything that you can interact with is a target for hacking. Of course, there are some areas which are more common to focus on. Myself, I do a lot of research in vulnerability discovery in PC software.

Most importantly, learn C! Not all universities teach it anymore. So much of hacking history (the 90's!) was all about vulns in programs written in C. If you can't read C, this enormous corpus of knowledge will be out of reach.

You can PM me if you run into more questions. Could you recommend a book? I'm not sure what I'd look for.  Could you recommend a book? I'm not sure what I'd look for.  I'm a hacker for a company that specializes in secure systems design and implementation. I find that many folks don't realize this is an actual career option. It's insanely interesting, includes lots of variety, you get to meet and work with a ton of bright people the world over, and you're a rare talent in a very hungry market. Do highly recommend. When I look into hardcore security stuff... I get the sense that the practical world and the theoretical world is so vastly far apart, and will be kept there by committees and corporate business motives, that working in real security would be a bit disappointing.  I mean, in terms of penetration testing, systems design like you mention, etc.  The techniques you're going to be allowed to pursue by the company that wants it to have Turing-complete input processing at every turn and simply does not care that this will destroy any security it might have had just seems like it would be disappointing.  Most of the security systems I see torn apart from talks presented at Defcon, BlackHat, Chaos Communication Congress, etc are just horribly and predictably bad.  Am I off the mark on this?

Security researching seems like it would be endlessly fascinating, but something like design of secure systems for sale just seems like frustration waiting to happen.  I imagine going to a meeting where they say Customer X needs Profoundly Unwise Insecure Feature Z and I'd be told to shut up and do it and make it secure... somehow.  Most security is very black and white.  Either it is effective, or it is not.  Businesses mostly are incapable of dealing well with situations like that.  You tell them 'no, there is no way to do that securely' and their response is always 'find a way, I don't care how'. It's not a quarter as bad as you're suggesting. Is the alternative that most engineers creating systems meant to be secure are simply incompetent then?     Try reposting to [/r/cscareerquestions/](http://www.reddit.com/r/cscareerquestions/).   </snippet></document><document><title>Help with calculating time complexity</title><url>http://www.reddit.com/r/compsci/comments/s1tps/help_with_calculating_time_complexity/</url><snippet>Recurrence relation: T(n)=T(c*n)+T( (1-c) n ) + 1 , 0&amp;lt;c&amp;lt;1 .
I've tried solving this problem by : 
let c=1/t let 1-c=1/m 
and without loss of generality: 1/t&amp;lt;1/m.
and if we look the at the recursion tree, the slow branch will end when: n/(m^k) = 1 -&amp;gt; log_{m} n =k

the fast branch will end when : n/(t^k) = 1 - &amp;gt; log_{t} n =k 
so we have log n levels in the tree
and because each level in the tree is O(1), we get : T(n)=log n .
But I get the feeling I did something wrong here, can anyone point me to the right direction ?  According to the [general master theorem (case 3)](http://en.wikipedia.org/wiki/Master_theorem#Case_3), we get T(n) = Theta(n^x ), with x satisfying c^x +(1-c)^x =1, and therefore x=1. So your function is in O(n).

Unfortunately the (english) wikipedia entry on the master theorem is not very good.

Your algorithm to this formula could be the following:
Take an array A of length n, choose k to be floor(c*n), so 1&amp;lt;=k&amp;lt;=n, and proceed with A[1..k-1] and A[k..n] recursively. If n==1, then do something with this element and return. This algorithm is clearly in O(n) since it touches every element one time. Doesn't the master theorem talks about recurrence relation that only look like this : T(n)=a*T(n/b) + f(n)

and my problem looks like this: T(n)=T(c*n) + T ((1-c) n ) +1
I'm not sure you can ( or how ) you can use the master theorem here. http://en.wikipedia.org/wiki/Akra%E2%80%93Bazzi_method
 First time I've heard about this method.
Just to make sure I'm right:
Given: T(n)=T(cn)+T( ( 1-c)n) + 1 , 0&amp;lt;c&amp;lt;1 .
c^p + (1-c)^p =1 --&amp;gt; p=1
T(n) = \theta ( n(1 +integral_{from 1 to n } 1/u^2 du ) ) =
=\theta (n(1 -1/n+1 )=2n-1) =\theta ( n )

Image of the latex code : http://imgur.com/I7Xs6

is that right? Doesn't the master theorem talks about recurrence relation that only look like this : T(n)=a*T(n/b) + f(n)

and my problem looks like this: T(n)=T(c*n) + T ((1-c) n ) +1
I'm not sure you can ( or how ) you can use the master theorem here. This is no problem. Our form is T(N)=T(n/b_1)+T(n/b_2)+f(n), where b_1=1/c and b_2=1/(1-c). f(n)=O(n^0 ), so b_1^0 + b_2^0 =2 &amp;gt;1, so third case. I've never seen this argument : 
&amp;gt; so b_1^0 + b_2^0 =2 &amp;gt;1, so third case.

Can you explain more why it gives the third case? Oh sorry if this was not really helpful for you. The master theorem we teach has a completely different form.

You can read the last paragraph ("[Allgemeinere Form](http://de.wikipedia.org/wiki/Master-Theorem#Allgemeinere_Form)") in the German wikipedia. It should be understandable without speaking German, and it is much more intuitive (IMHO) than the one on the English wikipedia.

You can also search for the following on google scholar.

U Sch&#246;ning. Mastering the master theorem. Bulletin of the EATCS 71, 165-166 (2000). Ok, that makes more sense, just to make sure I got it, given :
T(n)=T(a*n)+T(b*n)+n, a,b&amp;gt;0, a+b &amp;lt;1 .
f(n)=\THETA ( n^k) - &amp;gt; k=1 .
a^1 + b^1 &amp;lt; 1
which means : T(n)=\THETA ( n^k ) = THETA (n) .

Is that right?  Exactly, this is case 1. Don't you think the MT is easier this way?</snippet></document><document><title>I'm having fun learning CS through solving simple games. Any suggestions what games are fun to solve?</title><url>http://www.reddit.com/r/compsci/comments/s100g/im_having_fun_learning_cs_through_solving_simple/</url><snippet>Short story: I first wrote a program that created unique mazes and then solved it with DFS, aftwerwards I wrote a program that can solve any rush hour puzzle with BFS or A* algorithm. It has been so much fun writing those two and I have learned so much, that i'd like to do some more, but all I can think of are quite similar to those two or are maybe a bit too complex (e.g. rubiks cube -&amp;gt; 3d object &amp;amp;&amp;amp; 3d graphics).

TL;DR So I'd like to ask reddit, what are fun games to solve that aren't too similar to Rush Hour and solving mazes?  Try http://projecteuler.net/. That should satisfy any puzzle solving hunger you may have. The [Python Challenge](http://www.pythonchallenge.com/), despite being designed with python in mind, can be done in any language. Pretty fun, slightly more mysterious than PE. He might be in a bit of a pickle at a certain step since it's quite Python specific. Try http://projecteuler.net/. That should satisfy any puzzle solving hunger you may have. These look like fun challenges to solve in ~10-30 minutes. Bookmarked!
But what I was actually looking for  is, something that takes a few hours/evenings to solve and program. These look like fun challenges to solve in ~10-30 minutes. Bookmarked!
But what I was actually looking for  is, something that takes a few hours/evenings to solve and program. &amp;gt;fun challenges to solve in ~10-30 minutes.

Only the first 75 or so, unfortunately ;)

Could also take a look at [codeforces](http://codeforces.com/) or [topcoder](http://www.topcoder.com/tc) if you fancy, though they're less in line with intuitive puzzles.  Soooo, I tried #379, figuring out how to do it was easy, but figuring out how to do it in a realistic time, A LITTLE HARDER. 

Here is [my code](https://gist.github.com/2352403), anyone tips on making it faster? I think I'm still iterating over junk variables that I can filter out. Heh, just a little.

Since your code takes ~n**2 "operations" to calculate g(n), No amount of 'filtering out junk variables' is going to make it run in anything less that a couple million years. 

If you stare at [wikipedia](https://en.wikipedia.org/wiki/Least_common_multiple#Fundamental_theorem_of_arithmetic) for a while you'll find there's a novel method to calculate f(n) in omega(n)~=log(log(n)) time, but even with that calculating g(n) is a couple hours too slow.

Pretty much none past #100 or so are 'bruteforceable'. You'll find most of the top solvers have like, PhDs in number theory :P I guess I got pretty close with g(10^7 ) in 72 mins then. (g(10^6 ) in ~3m40s)
[Updated code](https://gist.github.com/2352403) 'close' by 6 orders of magnitude :P

(runtime of project euler solutions are intended to be &amp;lt;1min on a low-spec computer, btw) These look like fun challenges to solve in ~10-30 minutes. Bookmarked!
But what I was actually looking for  is, something that takes a few hours/evenings to solve and program.   Boggle/Scramble is pretty fun to solve programatically. This looks like a fun idea, see if I can find some easy to implement dictionary of English words. If you're on Ubuntu (and presumably other Unix based OSes), there's a dictionary included in the file system. If you aren't, you could maybe copy it from someone who is. I'm on Ubuntu, but I can't find it, DDG/Google not helping either. Where is it? /usr/dict/words or /usr/share/dict/words. More info here: http://en.wikipedia.org/wiki/Words_(Unix)  How about Sudoku (and not using any brute force searches)? 

You can do rubiks cube without 3d graphics. Just display all 6 faces and show the changes happening to them.  As far as i know, soduko is np-complete, so solving it without using brute-force, might be a bit too complex. How about Sudoku (and not using any brute force searches)? 

You can do rubiks cube without 3d graphics. Just display all 6 faces and show the changes happening to them.  True, but after 1.5 day of pondering on how to nicely implement the rotation, I sort of gave up :( I couldn't find out how to rotate along without having to explicitly state which edge went where for each face and then that 2 times (clockwise/ccw).

I'm going to put Sudoku on my list! Sudoku is a great problem. While developing Sudoku, you may want to check out:
http://en.wikipedia.org/wiki/Boolean_satisfiability_problem

http://en.wikipedia.org/wiki/SAT_solver#Algorithms_for_solving_SAT Currently reading this! This sparked my interest as I have a test on Boolean Algebra somewhere this week. True, but after 1.5 day of pondering on how to nicely implement the rotation, I sort of gave up :( I couldn't find out how to rotate along without having to explicitly state which edge went where for each face and then that 2 times (clockwise/ccw).

I'm going to put Sudoku on my list! True, but after 1.5 day of pondering on how to nicely implement the rotation, I sort of gave up :( I couldn't find out how to rotate along without having to explicitly state which edge went where for each face and then that 2 times (clockwise/ccw).

I'm going to put Sudoku on my list! True, but after 1.5 day of pondering on how to nicely implement the rotation, I sort of gave up :( I couldn't find out how to rotate along without having to explicitly state which edge went where for each face and then that 2 times (clockwise/ccw).

I'm going to put Sudoku on my list! How about Sudoku (and not using any brute force searches)? 

You can do rubiks cube without 3d graphics. Just display all 6 faces and show the changes happening to them.  Is this typically solved with dynamic programming?  [SpaceChem](http://www.spacechemthegame.com/) is an incredibly fun and addictive puzzle game.  It's basically programming, the video game.  It's a great way to teach yourself to explore the limits of a programming language and architecture, especially dealing with architectures with limited space and processing capabilities.    http://pleasingfungus.com/ - Manufactoria  try to make a program which will read in a text file in a new language you make up, and then execute the program based on what's in the file. An interpreter for your own language!

Then, try to automatically detect when certain things go wrong in the program and raise errors. For example, if a variable is used before it is initialized, report an error. Some more advanced thing which would be very useful if you could detect every time would be if there is a loop which never exits (the program keeps looping forever). &amp;gt;if there is a loop which never exits (the program keeps looping forever)

This is technically impossible to determine- it's the [Halting Problem](http://en.wikipedia.org/wiki/Halting_problem). &amp;gt;if there is a loop which never exits (the program keeps looping forever)

This is technically impossible to determine- it's the [Halting Problem](http://en.wikipedia.org/wiki/Halting_problem). &amp;gt;if there is a loop which never exits (the program keeps looping forever)

This is technically impossible to determine- it's the [Halting Problem](http://en.wikipedia.org/wiki/Halting_problem).     Solve P = NP Man, isn't that formally undecidable yet? 

I leave academia for a decade, and you guys make no progress AT ALL.  Some fun puzzles:

1. Given 3, 5, and 8 liter containers, distribute 8 liters of water initially in the 8 liter container into 4 liters and 4 liters in the 5 and 8 liter containers, respectively. Your only operations are emptying one container into another container, stopping only when one container empties completely or another container fills up completely. This can be further generalized into other problems: given x, y, and z containers, is it possible to distribute z liters of water into z/2 liters in the other containers (given z/2 &amp;lt; x or y)? If so, what is the minimum number of moves?

2. Come up with some currencies for fake countries (or look up real currencies!). Design an algorithm that answers this question: starting at a given currency, it is possible to convert your money to various other currencies in such a way that you end up at the original currency with more money than you started?

3. Here's a question that will really challenge you and help you learn more about CS and computability: Given a boolean formula with n variables and m clauses (variables can be assigned values of true and false) of the form (x1 v x2 v !x3) ^ (!x1 v x2) ... etc. There are at most 3 "literals" that are OR'd together in one "clause" (i.e. you can only have at most 3 variables in each clause), and each clause is and'd together in order to form the entire formula.

Can you write an algorithm that finds values for x1, x2, ... , xn such that the formula is true quickly? The answer is no, not simply. This question (3SAT) is NP-complete. An interesting question though, is this: can you write an algorithm that can do this in non-polynomial time?  "can an you write an algorithm that can do this in non-polynomial time"

By the very definition of NP, if a problem A is in NP there exists a two place predicate R(x, y), computable in polynomial time, where if x is an instance of problem A then there is a polynomial size certificate y (a solution) such that R(x, y) is true.

Then all you would need to do to solve the problem in "non-polynomial" is try the the verifier machine which computes R(x, y) on every possible valid certificate y.

This leads us to the equivalent definition of NP. If a problem is in NP it can be decided by a non deterministic Turing machine in **nondeterministic polynomial time**. This is because a NDTM can simply run the verifier on all possible certificates simultaneously. To show the equivalence in the other direction, assume NDTMs can decide NP problems in nondeterministic polynomial time, since this means there will be one polynomial size path of state transitions that leads to an accept or reject state, we can use this "computation" as the certificate, then there exists a two place predicate, R(x, y), since we can simply use the polynomial size "computation" certificate for a given problem instance x and verify that the computation is valid.

My point is this isn't actually an interesting question. Of course you can solve an NP problem in non-polynomial time. You can simply perform a brute force search (exponential time) by the very definition of NP. As stated, this is done by simply trying the verifier on every potential solution.        </snippet></document><document><title>Is there a way to discover a pattern from two strings?</title><url>http://www.reddit.com/r/compsci/comments/s1dm9/is_there_a_way_to_discover_a_pattern_from_two/</url><snippet>Hi!
Is there any algorithm for pattern discovery in TWO strings?
E.g.,

input:
"some string"

and
"test-string-2"

Output:
"#e#string#"    //replace # with * - they are treated by reddit as formatting

I have been searching algorithms/papers/articles on the subject, but there are a lot of papers on pattern extraction, some papers on repeated pattern discovery in a single string, but nothing at all on the subject.  This looks like the [longest common subsequence problem](http://en.wikipedia.org/wiki/Longest_common_subsequence_problem)

Algorithm explained better [here](http://wordaligned.org/articles/longest-common-subsequence)  Maybe look into [sequence alignment](http://en.wikipedia.org/wiki/Sequence_alignment) algorithms?  Just a note on using \*... If you escape it with \\, it'll use the literal character instead of treating it as formatting.

So, "\\\*e\\\*string\\\*" will display as "\*e\*string\*"</snippet></document><document><title>Thoughts on a greedy algorithm to find a string difference?</title><url>http://www.reddit.com/r/compsci/comments/rzxzb/thoughts_on_a_greedy_algorithm_to_find_a_string/</url><snippet>I'm looking for a way to find a longest match of two strings, or better say list of words considering the order. Here is an example:  

First string:  

          The cat in the hat  

Second string:  

          gibberish the cat the cat in gibberish the hat  

The "best" match would be:  

          gibberish the cat (the cat in) gibberish (the hat)  

I came up with following algo, which seem terribly inefficient to me:  

1. Take all possible word permutations (because we may have same words with different indices). E.g.:  
 
         First string: the (1) cat (2) in (3) the (4) hat (5)
         Second string: gibberish the (1,4) cat (2) gibberish in (3) the (1,4) hat (5)

  As you can see this would take O(n!), where n is the number of repeated words in first string.  

2. Then for each index permutation I group increasing sequences (in O(k)) and take all possible combinations, which can be done in O(2^k), where k is the length of indices list. E.g.:

         [(1, 2), (1, 2, 3), (5), (4, 5)]
         Would have these combinations:
         [[(1,2)], [(1,2),(5)], [(1,2), (4,5)], ..., [(1,2,3),(4,5)], [(5)], [(4,5)]]
3. Then I look for combination with maximum length and this is the answer.

The resulting algorithm works in O(n! * (k + 2^k ) ), the average test case may have 10 repeated words, so using it is not an option. 
Any ideas on how to solve it efficiently?  I think you might be able to do it with some sort of [LCS](https://en.wikipedia.org/wiki/Longest_common_subsequence_problem) approach, unless I'm reading your question wrongly. I think you might be able to do it with some sort of [LCS](https://en.wikipedia.org/wiki/Longest_common_subsequence_problem) approach, unless I'm reading your question wrongly.   As you've said - just use a greedy algorithm:

Scan the second string while maintaining an index into the first, and every time you have a match with the current word in the first string, use it and advance the index by one.

The invariant is that at every stage we have the longest possible match. It's not hard to prove this by induction.

The reasoning is that if I already used up "the cat" when reaching the 3rd word in the second string, any optimal solution for &amp;gt;3, could also use this matching, so it's okay to pick the words greedily.
Not that for your example above, the algorithm would be this matching:

    gibberish (the cat) the cat (in) gibberish (the hat)

(which also demonstrates the point above)   </snippet></document><document><title>Can anyone explain how to decide if a sting is a palindrome using O(1) time as stated in this post?</title><url>http://rjlipton.wordpress.com/2011/01/12/stringology-the-real-string-theory/</url><snippet>   I was completely shocked for a second, but linear time is O(n) and not O(1).  :P

Anyway, this question comes up fairly often in job interviews but usually as a longest palindromic substring problem.  I'd write the code out, but it's explained and detailed a lot better here with Python code:

[http://www.akalin.cx/longest-palindrome-linear-time](http://www.akalin.cx/longest-palindrome-linear-time)

To just change it to check if a specific string is a palindrome in O(n) time, just compare the result of longest palindromic substring to your initial string.  If the two are the same, then your string is a palindrome.  Comparing two strings also takes at most O(n) time, so it's really the same algorithm.

**EDIT:** I misunderstood the problem, see scatters and repsilat's comments about online algorithms. That's linear, but it's not online constant as worst-case on new input you can be filling in approximately n entries in the lengths array (where n is the size of input received so far). Hm, how would the online constant algorithm work then?  Don't you need the entire word before you can determine whether it's a palindrome or not? Hm, how would the online constant algorithm work then?  Don't you need the entire word before you can determine whether it's a palindrome or not? An online constant algorithm should, for each successive letter of input, tell in constant time whether the input received so far is palindromic.  I have to admit I can't see how such an algorithm would work, as on partial input "ab^m" (for increasing m) it would have to maintain state without rereading the entire substring "b^m"; but then I haven't read the paper. An online constant algorithm should, for each successive letter of input, tell in constant time whether the input received so far is palindromic.  I have to admit I can't see how such an algorithm would work, as on partial input "ab^m" (for increasing m) it would have to maintain state without rereading the entire substring "b^m"; but then I haven't read the paper. Wouldn't it be very easy to do with a stack? Remember, constant time. When I finally hand you a letter that finishes a palindrome, you don't have time to traverse a stack, since that's not a constant-time operation. I'm thinking about adding (if the letter doesn't match the top of the stack) or popping off (if the letter matches the one on the top of the stack) as each letter is given.  If you get to the end and the stack is either empty or has only one letter (the middle letter that isn't repeated in a palindrome with an odd number of letters), then it was a palindrome.

Should be O(n), I think.

Edit: Nevermind, it'll get stuck on that middle letter that isn't repeated and add new letters when it shouldn't.  Can't think of a constant time way to fix it at the moment. I don't think it's that easy. If I hand you "bb", your stack as you described it here will be empty. Then I can hand you any number of even 'a's and you'll say it's a palindrome, but "bbaaaaaaaa" is clearly not a palindrome.

You really need a non-deterministic PDA which, at each letter, guesses both that this is the middle of the word (and hence should start trying to pop things off the stack) and that this is not yet the middle of the word (and hence should continue trying to push things onto the stack). But then simulating that with a deterministic Turing machine in the obvious way isn't O(n) any more, so I think the original claim of real-time is still quite surprising! Excellent point, that's an error case I didn't see!

I suppose you could do it if it were permissible to ask the array with the palindrome in it how long it is.  That's an O(1) since it would be precalculated by storing the letters in the array in any practical system.  We seem to be dealing with a theoretical system where the letters aren't all available first, though.     Good god, that link makes a bold, provocative claim and just drags on forever instead of getting to the meat. I find it very frustrating that they wouldn't just straight up state the problem and their assumptions, but rather tease you with technical filler when you already know the claim is impossible under normal interpretations. I'm going to disprove the claim first, and then read to find out what the heck they're talking about.

Deciding whether an input string s of length n is a palindrome takes &#969;(1) time in the worst case.

Proof: Information-theoretic argument: You need to read an unbounded amount of input to give the correct answer.

Assume that the palindrome problem can be decided in O(1) time. Then there exist positive constants k and n0 such that, for all strings of length n &amp;gt; n0, T(n) &amp;lt; k, where T(n) is the time taken to solve the problem. In any constant amount of time, at most a constant number of characters can be read. Let m be the maximum number of characters readable in k time.

Now let s1 be a palindromic string with length max(m+2, n0+1). The algorithm does not read at least two characters in s1. Since at least one of these characters is not the midpoint (if it exists), a change in these characters can produce another string s2 that is not a palindrome. But since the algorithm never read these characters, it cannot distinguish between the two strings, and therefore must give the same answer for both inputs s1 and s2, so it is wrong for at least one input.


Edit: It looks like I skimmed too casually and missed their definition of "real-time" before getting into the thick of it. They *don't* claim O(1) time, they claim O(n) with a restriction on the work not being amortized over large inputs.   I wonder what the space complexity for the algorithm would be?  It has to be O(n). It needs at least O(n) to tell if it's a palindrome, and since it takes a constant number of steps per input character, there's no way for it to use more than O(n).
 It has to be O(n). It needs at least O(n) to tell if it's a palindrome, and since it takes a constant number of steps per input character, there's no way for it to use more than O(n).
 It has to be O(n). It needs at least O(n) to tell if it's a palindrome, and since it takes a constant number of steps per input character, there's no way for it to use more than O(n).
   Can you please post the problem statement in a couple of phrases since I do not want to read that post? :) Determine if a string is a palindrome in linear time. Also, do it in online streaming, so that you can determine if any prefix is a palindrome also, as soon as the prefix is input, answering the question for all n prefixes in O(n) time, amortizing to O(1) per incremental character.  </snippet></document><document><title>What is LLVM and what are good resources to get started with it?</title><url>http://www.reddit.com/r/compsci/comments/rz3dp/what_is_llvm_and_what_are_good_resources_to_get/</url><snippet>I've been trying to get hold of an introductory material on LLVMs. Please share if you some. Thanks!  You hopefully already looked at [the official documentation page](http://llvm.org/docs/)? I think that [chapter 11](http://www.aosabook.org/en/llvm.html) from *The Architecture of
Open Source Applications* answers your question about what LLVM is (as you might already be aware of, as there is a link from the documentation page to this chapter). Maybe also [this could be of some help](http://www.reddit.com/r/Compilers/comments/ms6op/llvm_is_hard_to_use_help/) (maybe also [the relevant SO tag](http://stackoverflow.com/questions/tagged/llvm)).

But, you have to be more specific about what you are going to do. E.g., building a new language using LLVM as the back-end, writing some LLVM pass transforming/optimizing code or doing some analysis, modifying some existing part of LLVM, or something else? Thanks. I might have looked at the official documentation I dont remember.

I dont have anything specific in my mind. First step, as I reckon, is to understand what this LLVM business is all about. I hear this term being thrown around a lot and have no clue what it is all about. A presenation was what I was looking at - because most of them give an introduction and tries to justify the hype (if any) around it. Also they'd give why I should be interested in something like that.

EDIT: Clarification on "I dont remember" - I definitely checked the official documentation but literature over there wasnt novice friendly. Thanks! I can't possible explain LLVM in a post here.   But a few quick point might help.  

LLVM stands for "Low Level Virtual Machine", which I sometimes think is a bit dated as the "PROJECT" is much more than that.   LLVM from a user or developers point of view is used in conjunction with a compiler of some sort.   CLang being one commonly associated with the project.   CLang is a compiler for the "C" languages but there are a massive number of other projects, delivering compilers built on the LLVM infrastructure.  

The LLVM infrastructure however isn't there just to support compilers.   Again a limit to my ability to post here demands that I be brief but the LLVM project supports virtual machines and other programmer tools. It is better to look at the project as a very broad take on various ways to execute code on a platform.  

So I can only recommend reading the LLVM and CLang pages to start to get an idea to scope here.   As you are doing so research some of the projects that are linked to on these pages.    Some are very real attempts at delivering novel improvements to the state of the art while others might be considered research projects.    In any event a bit of study at these sights should help you understand LLVMs scope, it's very modern approach and the grow respect it is earning in the community.   I can't possible explain LLVM in a post here.   But a few quick point might help.  

LLVM stands for "Low Level Virtual Machine", which I sometimes think is a bit dated as the "PROJECT" is much more than that.   LLVM from a user or developers point of view is used in conjunction with a compiler of some sort.   CLang being one commonly associated with the project.   CLang is a compiler for the "C" languages but there are a massive number of other projects, delivering compilers built on the LLVM infrastructure.  

The LLVM infrastructure however isn't there just to support compilers.   Again a limit to my ability to post here demands that I be brief but the LLVM project supports virtual machines and other programmer tools. It is better to look at the project as a very broad take on various ways to execute code on a platform.  

So I can only recommend reading the LLVM and CLang pages to start to get an idea to scope here.   As you are doing so research some of the projects that are linked to on these pages.    Some are very real attempts at delivering novel improvements to the state of the art while others might be considered research projects.    In any event a bit of study at these sights should help you understand LLVMs scope, it's very modern approach and the grow respect it is earning in the community.   Thanks, I didn't even know what it stood for.

I haven't really kept up with it.  I'm guessing that this is because Java really brought back virtual machines, and then Microsoft did CLR (honestly it had to be just a copy).  So with VM's the rage and the technology seeing so much action, the linux and c/c++ people jumped in.  That's just a guess, I really don't know for sure. Thanks. I might have looked at the official documentation I dont remember.

I dont have anything specific in my mind. First step, as I reckon, is to understand what this LLVM business is all about. I hear this term being thrown around a lot and have no clue what it is all about. A presenation was what I was looking at - because most of them give an introduction and tries to justify the hype (if any) around it. Also they'd give why I should be interested in something like that.

EDIT: Clarification on "I dont remember" - I definitely checked the official documentation but literature over there wasnt novice friendly. Thanks! &amp;gt; I might have looked at the official documentation I dont remember.

Why anyone upvoted you, I have no idea. Read the fucking manual, and if you need to sober up to retain some of the information, do that too. I might have googled some relevant links for you, but I don't remember. I'm shocked too.  This seems like either an obvious troll post or some random non-technical manager type who didn't spend even the minimum amount of time researching his own question before bothering to post it to reddit. &amp;gt; I might have looked at the official documentation I dont remember.

Why anyone upvoted you, I have no idea. Read the fucking manual, and if you need to sober up to retain some of the information, do that too. I might have googled some relevant links for you, but I don't remember. </snippet></document><document><title>CS undergrad here, feeling like I'm "slow" to grasp the material, is this going to become a problem as I progress?</title><url>http://www.reddit.com/r/compsci/comments/ryfit/cs_undergrad_here_feeling_like_im_slow_to_grasp/</url><snippet>I'm a returning student...graduated and attempting a CS major after freaking out 5 years ago. Haven't done any math since otherwise had think in a "cs way".

I'm an A-student so far but I'm probably a bit slower in getting my work done than others, and I realize that the assignments I have right now are extremely easy. 

It's usually just a matter of me having to get my logic unscrambled, so sometimes I can finish a project pretty quickly, but other times my brain decides it's not going to play along and I'm having to slow way down, trace simple things out, etc.

I'm hoping it's just an issue of practice, but I'd appreciate to know honest opinions - if you're not somewhat lightning fast like some of these students coming into it fresh out of AP computer science, would upper-level courses be exponentially more difficult to manage?  

Is this something I can adapt to with more experience or may I not be mentally fast enough in the sense of living up to the standards of a software development career?

Edit: wow thanks so much to everyone for the very insightful responses, it's a lot to absorb and I really appreciate both the encouragement and reminders to stay realistic. Just...wow, much appreciation for everyone taking the time to contribute, thank you.  The number one thing I tell my students: the people around you aren't as smart as you think they are.  This happens in every discipline, but is especially true in computer science since people walk into the major with huge variations of prior skill and expertise.  The ironic thing is that this prior knowledge doesn't necessarily translate into success down the road, but it sure puts off other students who don't have that knowledge.

Build a network of friends in your department that you can trust.  And you'll see that as you work together that you'll all struggle on the same concepts and ideas.  Computer science is a large enough field that everyone has their strengths and weaknesses.  The best programmers may be horrible at proofs.  The best theoreticians may not be able to code their way out of a wet paper bag, etc. &amp;gt; Build a network of friends in your department that you can trust.

This. If your school has an [ACM student chapter](http://www.acm.org/chapters/maps/) or similar organization, consider meeting and spending more time with them. Often, these are centers of collaboration (commiseration?) and support for CS students. They can also be pretty fun, and are generally accepting of just about anyone from any background. The ACM regulars at my university include quite a few late-20- and early-30-somethings (and more), in addition to the expected 18-22 crowd. My ACM chapter was just founded last semester and is the single greatest thing that's happened to my coding. We had practice for programming competitions, which helped. The biggest thing was that CS major isn't exactly full of totally sociable people, and the ACM gives you a place to meet people who are good at the stuff you aren't and who need help with the stuff you are good at. I wish I did this in College because I had a college that would obsess about cheating for the first two days of class. It was so bad I couldn't even ask for which java library had something I needed in it from friends because they feared getting caught for academic dishonesty.

Answering the OP: The material almost always builds on itself, if you don't understand it immediately don't worry you'll get plenty of repetition to grasp it. If it does start affecting your grades talk to friends, TA's, and Profs to try to get some clarification. Relate material to things outside comp. sci. . I had an algorithms course that I struggled with until the very end, but realized I was over-complicating the subject.  &amp;gt; Build a network of friends in your department that you can trust.

This. If your school has an [ACM student chapter](http://www.acm.org/chapters/maps/) or similar organization, consider meeting and spending more time with them. Often, these are centers of collaboration (commiseration?) and support for CS students. They can also be pretty fun, and are generally accepting of just about anyone from any background. The ACM regulars at my university include quite a few late-20- and early-30-somethings (and more), in addition to the expected 18-22 crowd. &amp;gt; Build a network of friends in your department that you can trust.

This. If your school has an [ACM student chapter](http://www.acm.org/chapters/maps/) or similar organization, consider meeting and spending more time with them. Often, these are centers of collaboration (commiseration?) and support for CS students. They can also be pretty fun, and are generally accepting of just about anyone from any background. The ACM regulars at my university include quite a few late-20- and early-30-somethings (and more), in addition to the expected 18-22 crowd. The number one thing I tell my students: the people around you aren't as smart as you think they are.  This happens in every discipline, but is especially true in computer science since people walk into the major with huge variations of prior skill and expertise.  The ironic thing is that this prior knowledge doesn't necessarily translate into success down the road, but it sure puts off other students who don't have that knowledge.

Build a network of friends in your department that you can trust.  And you'll see that as you work together that you'll all struggle on the same concepts and ideas.  Computer science is a large enough field that everyone has their strengths and weaknesses.  The best programmers may be horrible at proofs.  The best theoreticians may not be able to code their way out of a wet paper bag, etc. This is so true!

I absolutely hate the math (especially discrete math which I am in my second class and about to write my final on) and I find a lot of the concepts hard but the programming itself its super easy because I have a lot of prior knowledge. I can pick up languages easily but the concepts in math are harder for me to grasp as quickly.

Having someone else in my class who can help me grasp some concepts I am not entirely clear on helps me a lot. The number one thing I tell my students: the people around you aren't as smart as you think they are.  This happens in every discipline, but is especially true in computer science since people walk into the major with huge variations of prior skill and expertise.  The ironic thing is that this prior knowledge doesn't necessarily translate into success down the road, but it sure puts off other students who don't have that knowledge.

Build a network of friends in your department that you can trust.  And you'll see that as you work together that you'll all struggle on the same concepts and ideas.  Computer science is a large enough field that everyone has their strengths and weaknesses.  The best programmers may be horrible at proofs.  The best theoreticians may not be able to code their way out of a wet paper bag, etc.  Computer Science involves a particular way-of-thinking that you should adapt to in time.  
  
For example, if you're using iterative or object-oriented languages like C and Java, then you need to be able to think deeper than just the functional level (i.e this is my input, and this is what my output should look like). You need to get used to thinking at the algorithm level (i.e. this is the first step in the problem, this is the next step in the problem, etc.)  
  
If you break down enough problems in this manner, it will get easier. Try practicing using [Project Euler](http://www.projecteuler.net). I think I started learning how to think in terms of algorithms just this last quarter. Well, the class I took was called "DATA STRUCTURES AND ALGORITHMS," so one would hope he learns how to think that way. :P

I can't even imagine trying think in terms of "this is my input, what's my output?" any more. I think I started learning how to think in terms of algorithms just this last quarter. Well, the class I took was called "DATA STRUCTURES AND ALGORITHMS," so one would hope he learns how to think that way. :P

I can't even imagine trying think in terms of "this is my input, what's my output?" any more.  I'm going to let you in on a secret... everyone who seems to fly through the material easily either:

A) Just learned it now at the same time as you you but the logic clicked just enough to do the work quickly and easily but doesn't truly "get it" on a deeper level.

B) Saw/Learned the material before (in a previous class, on their own, in the book the night before, etc), or,

C) Happens to think in a certain way that is conducive to what they're learning right now (so they just intuitively "get it") but has a weakness elsewhere in the field where you don't.

Computer science (along with every science, math, and engineering field) is **hard**, and don't let anyone tell you otherwise. With the exception of the VERY FEW true geniuses in the subject matter (and even at the best schools in the world only a couple of students are these kinds of people), every single person needs to sit down and drill through dense concepts that don't come easily to them. They need to spend time trying to wrap their head around the material and practice with the concepts more than once.

Our society tells us that the smartest people are the ones who seem to succeed with little to no work put in. This is only true at the most BASIC level of concepts, and even then you still have to DO the work to get the grades. Don't buy into that mentality. You think other people understand things more easily than you, but you don't see the hours THEY spent learning the material or similar concepts.

Every concept gets easier with time and practice. It'll come. [relevant](http://www.vanadac.com/~dajhorn/novelties/ESR%20-%20Curse%20Of%20The%20Gifted.html) I'm going to let you in on a secret... everyone who seems to fly through the material easily either:

A) Just learned it now at the same time as you you but the logic clicked just enough to do the work quickly and easily but doesn't truly "get it" on a deeper level.

B) Saw/Learned the material before (in a previous class, on their own, in the book the night before, etc), or,

C) Happens to think in a certain way that is conducive to what they're learning right now (so they just intuitively "get it") but has a weakness elsewhere in the field where you don't.

Computer science (along with every science, math, and engineering field) is **hard**, and don't let anyone tell you otherwise. With the exception of the VERY FEW true geniuses in the subject matter (and even at the best schools in the world only a couple of students are these kinds of people), every single person needs to sit down and drill through dense concepts that don't come easily to them. They need to spend time trying to wrap their head around the material and practice with the concepts more than once.

Our society tells us that the smartest people are the ones who seem to succeed with little to no work put in. This is only true at the most BASIC level of concepts, and even then you still have to DO the work to get the grades. Don't buy into that mentality. You think other people understand things more easily than you, but you don't see the hours THEY spent learning the material or similar concepts.

Every concept gets easier with time and practice. It'll come. Everybody thinks I'm flying through the material easily, but I actually spend 8+ hours on homework every day. That would probably be item D on your list. That's B. Everybody thinks I'm flying through the material easily, but I actually spend 8+ hours on homework every day. That would probably be item D on your list. I'm going to let you in on a secret... everyone who seems to fly through the material easily either:

A) Just learned it now at the same time as you you but the logic clicked just enough to do the work quickly and easily but doesn't truly "get it" on a deeper level.

B) Saw/Learned the material before (in a previous class, on their own, in the book the night before, etc), or,

C) Happens to think in a certain way that is conducive to what they're learning right now (so they just intuitively "get it") but has a weakness elsewhere in the field where you don't.

Computer science (along with every science, math, and engineering field) is **hard**, and don't let anyone tell you otherwise. With the exception of the VERY FEW true geniuses in the subject matter (and even at the best schools in the world only a couple of students are these kinds of people), every single person needs to sit down and drill through dense concepts that don't come easily to them. They need to spend time trying to wrap their head around the material and practice with the concepts more than once.

Our society tells us that the smartest people are the ones who seem to succeed with little to no work put in. This is only true at the most BASIC level of concepts, and even then you still have to DO the work to get the grades. Don't buy into that mentality. You think other people understand things more easily than you, but you don't see the hours THEY spent learning the material or similar concepts.

Every concept gets easier with time and practice. It'll come. Also, many people won't admit that they study a lot... 

You hear things like "When I took that course I never even opened the 
book", etc.. Very discouraging.  Thinking about programming problems is a skill that you will develop over time. Everyone starts out slowly. If the other students are doing their work faster it's probably because they have programmed more than you recently. I wouldn't worry about it.

If you want to improve then just program stuff as a hobby in your free time. You'll get way ahead of your classmates that way (or at least I did). Thinking about programming problems is a skill that you will develop over time. Everyone starts out slowly. If the other students are doing their work faster it's probably because they have programmed more than you recently. I wouldn't worry about it.

If you want to improve then just program stuff as a hobby in your free time. You'll get way ahead of your classmates that way (or at least I did).   Everybody else has good advice. Essentially, you'll likely need to just get used to the way of thinking, and it gets easier. As long as you are enjoying it, and know that you are able to do well in some sort of logical field (rather than, say, art or creative writing), you should be fine.

If that isn't enough, there's always the very simple fact that even if you were a C student who didn't really care much, you could walk into a job out of college with a $60k+ salary. It's so in demand that even if you aren't amazing, you can get a nice job. So if you enjoy it, I'd say to stick with it. It will probably get better, and if not, then its still not a bad option. &amp;gt;If that isn't enough, there's always the very simple fact that even if you were a C student who didn't really care much, you could walk into a job out of college with a $60k+ salary.

No they won't.  Unless they're a 4.0 student from Carnegie Mellon, Stanford, MIT, etc.  Try 40k.  That's assuming they're able to pass any coding interview if they scrape through with C's. &amp;gt;If that isn't enough, there's always the very simple fact that even if you were a C student who didn't really care much, you could walk into a job out of college with a $60k+ salary.

No they won't.  Unless they're a 4.0 student from Carnegie Mellon, Stanford, MIT, etc.  Try 40k.  That's assuming they're able to pass any coding interview if they scrape through with C's. Considering that many of the top companies are paying around $70k/year to their interns, getting a $60k full time position at any other company hardly seems unreasonable. 

Oh, and those $70k/year salaries go to people from non-elite schools too. I've never really understood the elitism about schools in a field that is as easy to teach yourself as CS. Google does give $70k internships, but you have to be the best of the best (of the best) to get a position with them.  Like I said there's exceptions for the exceptional.  But if we're talking about an average programmer, they'll be lucky to make that after a decade worth of experience.  

As an example, Quicken Loans which is considered one of the best IT companies to work for especially in Michigan where I'm from, their average pay for a new Software Engineer is around $45k.  This is if you're lucky enough to get a job with them.  For internships they pay $11/hour.

You're taking an extreme case and acting like that's normal.  Most internships aren't going to be paying more than $15/hour, if you're lucky enough to find one that pays.  Many internships I saw wanted YOU to pay THEM for the experience. Google does give $70k internships, but you have to be the best of the best (of the best) to get a position with them.  Like I said there's exceptions for the exceptional.  But if we're talking about an average programmer, they'll be lucky to make that after a decade worth of experience.  

As an example, Quicken Loans which is considered one of the best IT companies to work for especially in Michigan where I'm from, their average pay for a new Software Engineer is around $45k.  This is if you're lucky enough to get a job with them.  For internships they pay $11/hour.

You're taking an extreme case and acting like that's normal.  Most internships aren't going to be paying more than $15/hour, if you're lucky enough to find one that pays.  Many internships I saw wanted YOU to pay THEM for the experience. &amp;gt;If that isn't enough, there's always the very simple fact that even if you were a C student who didn't really care much, you could walk into a job out of college with a $60k+ salary.

No they won't.  Unless they're a 4.0 student from Carnegie Mellon, Stanford, MIT, etc.  Try 40k.  That's assuming they're able to pass any coding interview if they scrape through with C's. Or you get a job in a big city.

Starting wage for even the most minuscule jobs is 65k in San Francisco. And if you were a super genius at a top tier school you can fully expect to start at 80-100k   Comp. Sci. Grad here.  I too felt this way first year, things will come to you more clearly as you progress.  Don't give up and try to figure out hard problems yourself first.  Sometimes the way you imagine the problem will be much different than others and their explanation of solving it will seem confusing. I feel like the culture is very competitive at times, and it can be discouraging since many of my peers are braggarts (but I'm assuming this is also partially just how males tend to function, forgive me if this seems shallow).  Is this jockeying for the appearance of being the alpha programmer pretty universal? it dies off when they join a company for the first time and get schooled.  Dont let others bring you down or "seem" smarter.  The fact is that things will change rapidly in the real world and the knowledge you have in a certain area maybe knowledge they dont have.   Try to keep up with tech on your own time and be passionate about what you do :)       CSE undergrad here, like Kambingx said, your peers aren't the geniuses that you make them out to be, they probably just have more experience.  You'll get there.  Programming is tough, you have to develop a completely different, procedural mindset, and it's easy to get lost/confused.  There will be an amazing, lightbulb above your head, epiphany moment where everything will click and you will begin to enjoy and love programming.  I almost failed my first couple programming courses but after tons of practice, I had my "light bulb" moment and now i'm thriving in my courses.  Here are some things that helped me tremendously:  start out at www.codingbat.com to develop the basic skills of a language, then try out www.codeacadamy.com or www.projecteuler.com.  ProjectEuler is really tough but it will build both your programming and problem solving skills.     i finished my schools cs program. I found the ones who finished faster werent as bright because they were always helping each other finish assignments an they werent doing them by themselves. Later in the program you will see most of them fail. i finished my schools cs program. I found the ones who finished faster werent as bright because they were always helping each other finish assignments an they werent doing them by themselves. Later in the program you will see most of them fail. In my experience it was quite the opposite.  It was the people who grouped together who graduated and the people who didn't want to join a group who failed because they just couldn't complete the work. 

Being invited into a CS study group was one of the best things that ever happened to me in school.  When I needed help they were there to help and when somebody needed help I was there to help.  All of this helped define the material.  No better was of learning it that teaching it.

We didn't give each other answers, we helped each other.  Sadly some of my friends who decided to do things by themselves quickly got overwhelmed and once we hit junior year they droppped out or changed majors.    *Computer programmers are some of the dumbest people on earth.*

Dumb as masonry.

Yeah, I said it, and I'm one of them. If you have to devote 80% of your brainpower to unscrewing your toothpaste cap each morning, you might be a great computer programmer. Programming a computer is all about that intimate connection with the rudimentary steps involved in making something happen. The steps non-retarded people gleefully ignore. The longer it took you to learn how to tie your shoes (maybe you were 10, 16 or 28 when it finally clicked), the better you'll now be at explaining some painfully simple process to an idiot machine with no concept of the painfully simple. 

The quick among your classmates will go on to write functioning, disposable enterprise software with excellent test coverage. You, a slow, probably borderline retarded person, have a chance at making something great. 

Understand that computers as they exist today were not created by CS grads, they were created by weirdos, the mentally handicapped, and seriously autistic people. If you are slow, congratulations. You too have a learning disability. That's why you are a great candidate.

        If you are truly interested in CS (which it sounds like you are because you want to do it despite it not always being easy for you) then I think you'll find the time to learn it. Of course you'll be able to adapt and think more like a programmer as you go on. I can't promise that the upper-level classes will be easy, but if you love what you're doing and you're curious about it then you're a lot better off than many CS majors.  I struggle with this too. My goal is grad school, though, so not learning material and just scraping by doesn't cut it for me. So the fact that I have been struggling to keep up with the pace is jeopardizing my plans in a very concrete way. 

This struggle is new to me. I did fabulously even in CS classes up until now, and then I just hit a brick wall in understanding. I can't explain it, but I just can't wrap my head around what we're doing now.

I've been seriously considering switching out. So, you're not alone. All I can offer is knowing sympathy, not reassurance.  Isn't this what [/r/askcomputerscience](http://www.reddit.com/r/askcomputerscience) is for? Isn't this what [/r/askcomputerscience](http://www.reddit.com/r/askcomputerscience) is for?</snippet></document><document><title>Accessing wikipedia dumps : a question.</title><url>http://www.reddit.com/r/compsci/comments/rxw36/accessing_wikipedia_dumps_a_question/</url><snippet>I'm working on a project that requires me to interact with the wikipedia dump file (basically, to check wikipedia pages for certain words).

Now, the problem is the following : the dump is gigantic (as one would expect), and I'm not sure how is it possible (and what would the best way be) to access it, if possible as a database, from a program.

The language I'm using is Java, and I'm looking for answers actively right now, but I hoped that I might find someone with first-hand experience here :)

Cheers

edit : found wikixmlj ;; going to poke around with it, feedback would still be helpful  The dump file is in XML format, and you can use SAX parser to process the dump one wiki-article at a time. Each wiki article is inside a "page" tag. If you are just looking for words that exist on a wikipedia page, you can extract contents of just the "text" tag which exists inside a "page" tag. 

Once you have the text for each article, you can do all string searches that you need.        </snippet></document></searchresult>
<searchresult><compsci /><document><title>Polytope model for loop nest optimization</title><url>https://en.wikipedia.org/wiki/Polytope_model</url><snippet /></document><document><title>Does anyone have a suggestion for a good "actor oriented" language?</title><url>http://www.reddit.com/r/compsci/comments/r8h87/does_anyone_have_a_suggestion_for_a_good_actor/</url><snippet>I need to do an assignment in an "actor oriented" language such as [SALSA](http://wcl.cs.rpi.edu/salsa/), and i am leaning away from using SALSA because of things i've heard from others who have taken the class and used the language.  Does anyone have a suggestion for a good language which supports the "actor oriented" or "actor model" paradigm?

wikipedia's page on the actor model lists languages like Scala, Erlang and Scratch (3 that i'm considering), but neither Scala's or Scratch's page even mentions the actor model, or even concurrent programming, so i'm not so sure what actually uses this.  this also isn't particularly helped by the fact that "actor model" doesn't appear in wikipedia's list of paradigms.

**tl;dr** what's a good concurrent language which uses the actor model?  I've been researching Erlang, and the Actor model in general, pretty aggressively over the past couple of months. Erlang basically IS the Actor model, with everything you need in the language, and the supporting libraries, to do it right. It is also very mature and there is an active community.

If "the actor model" is your focus, I seriously doubt there is a better option today than Erlang. i honestly don't do enough with concurrency to need "the actor model," but it's required for the class, and SALSA just...isn't complete or particularly good...i really don't want to say mean things about it.  erlang also looks like a good option because i'm sometimes a little grumpy with java syntax, but i'm not too comfortable with declarative...i dunno, thanks!  i at least have a few like, actually used and maintained languages to look at.  Just fyi: [here is a tutorial using Scala and actors](http://www.scala-lang.org/node/242) if you want to use Scala. Just fyi: [here is a tutorial using Scala and actors](http://www.scala-lang.org/node/242) if you want to use Scala.  [Shakespeare](http://en.wikipedia.org/wiki/Shakespeare_%28programming_language%29). Har-de-har. i checked.  doesn't support concurrency.

on the topic of esoteric languages, i'd have to say my favorites are [brainfuck](http://en.wikipedia.org/wiki/Brainfuck_%28programming_language%29) and [piet](http://en.wikipedia.org/wiki/Piet_%28programming_language%29).  Shakespeare looks decent as well though...too bad i don't have any english requirements ever again.  fun trolling.  oh yea, also [whitespace](http://en.wikipedia.org/wiki/Whitespace_%28programming_language%29) for fun embedding of things within things.  As an RPI Comp Sci graduate, I came here specifically to tell you NOT to use SALSA. I see you are way ahead of me.

Sorry for not actually having a recommendation. As an RPI Comp Sci graduate, I came here specifically to tell you NOT to use SALSA. I see you are way ahead of me.

Sorry for not actually having a recommendation.  Just here to drop this of:

[http://learnyousomeerlang.com/content](http://learnyousomeerlang.com/content)

I don't know of any other languages, sorry =( Just here to drop this of:

[http://learnyousomeerlang.com/content](http://learnyousomeerlang.com/content)

I don't know of any other languages, sorry =(  Scala provides language support for actors, but [akka](http://akka.io/) provides general JVM support.  For Java it provides an OO interface to the actor model.    [deleted]  You could look for a concurrency-optimized  Scheme. Scheme was designed to test the Actor Model.

[Here](http://lambda-the-ultimate.org/node/2577) is a nice page on it
[Also, for Racket, a Scheme dialect](http://docs.racket-lang.org/reference/concurrency.html) whaaaaaat.  scheme AND actor model?  i must look into this.      [UnrealScript](http://udn.epicgames.com/Three/UnrealScriptReference.html)  [deleted]</snippet></document><document><title>Checking whether a bipartite graph G=(V,E) contains a subgraph H = (V,E'), |E'| = k, that is connected</title><url>http://www.reddit.com/r/compsci/comments/r8vjt/checking_whether_a_bipartite_graph_gve_contains_a/</url><snippet>I'm looking to implement a method that, given an adjacency matrix for an undirected bipartite graph, checks whether the graph contains a subgraph consisting of all nodes V, and a subset of edges E', where |E'| has specified cardinality k, such that (V,E') is connected. I'm having trouble finding what I want because I'm not sure of the right terminology.

I'm not concerned with worst-case complexity. I want ease of implementation. I can do a naive method of starting from some node, traversing the edges and checking whether I find a path of length k through all nodes (I believe this is equivalent to what I'm looking for). I'm hoping there is a quicker, more clever method someone can point me to.

This is not homework - although I'm not sure how I would verify that. Also, feel free to tell me that I should post this in programming or another subreddit instead. Thanks!  I am not exactly sure what you are looking for. On the one hand, it sounds like you're looking for a spanning tree. If the edges have costs associated with them, you'd want a minimum spanning tree. 

In the case that the edges have no costs, the worst case complexity is O(V) (so is the best case, actually) -- just perform a breadth first search through the graph, keeping track of nodes you've already seen.

However, you mention a path of length k. A spanning tree is not a path, so if you want a path through the nodes, that would require a different algorithm, because you have the constraint that you may only enter and leave a node a single time (except for the starting node and ending node of the path). If the graph is completely connected this is trivial, however if it is not completely connected you could end up getting stuck and will have to backtrack. Maybe someone else has a good algorithm for this, I'm not coming up with anything that doesn't sound like a TSP at the moment... You're right. The responses made me realize that I defined my question wrong.  I'm not sure if I should just submit another post with a more well-defined question, or edit this one to ask the correct one. I am not exactly sure what you are looking for. On the one hand, it sounds like you're looking for a spanning tree. If the edges have costs associated with them, you'd want a minimum spanning tree. 

In the case that the edges have no costs, the worst case complexity is O(V) (so is the best case, actually) -- just perform a breadth first search through the graph, keeping track of nodes you've already seen.

However, you mention a path of length k. A spanning tree is not a path, so if you want a path through the nodes, that would require a different algorithm, because you have the constraint that you may only enter and leave a node a single time (except for the starting node and ending node of the path). If the graph is completely connected this is trivial, however if it is not completely connected you could end up getting stuck and will have to backtrack. Maybe someone else has a good algorithm for this, I'm not coming up with anything that doesn't sound like a TSP at the moment...   </snippet></document><document><title>Retrospective: An Axiomatic Basis for Computer Programming</title><url>http://cacm.acm.org/magazines/2009/10/42360-retrospective-an-axiomatic-basis-for-computer-programming/fulltext?lot=49</url><snippet /></document><document><title>Help with a CS art project?</title><url>http://www.reddit.com/r/compsci/comments/r946t/help_with_a_cs_art_project/</url><snippet>I'm trying to create a wall poster with a theoretical CS flavor. The idea is this: take a set of [Wang Tiles](http://en.wikipedia.org/wiki/Wang_tile) and use them to visualize the computation of a Turing machine doing something interesting (there's a natural construction to do this mapping used in the proof that the tiling problem is undecidable; see [this paper](http://www.staff.science.uu.nl/~taati001/articles/tilings06.pdf) for a nice survey).

The only thing I'm stuck on now is what computation to show. Ideally it would have the following properties:

* Run for a number of steps that's some nice ratio of the amount of tape used (so that it ends up poster-shaped, rather than gigantic scroll-shaped).
* Not have a huge number of states (since the construction needs distinct colors for each &#931; &#8746; Q &#8746; (Q x &#931;)).
* Preferably be fairly "dynamic" in its use of the input tape -- so any given part of the tape should change somewhat often, since otherwise you're left with big swaths of simple two-color checkerboard patterns.

Note that it's not a requirement that this machine halt, as long as it's in the process of computing something interesting. A picture of the computation of the first 5 terms of the Fibonacci sequence would cool, for example (though I suspect it would take too many states).

Anyway, thanks for reading. Here's a tiling I generated today using [Karel Culik II's 13-tile aperiodic set](http://masterinfo.univ-mrs.fr/secret-m2if/articles-2009-2010/E._Jeandel.pdf):

* [10 x 15 tiles](http://i.imgur.com/q6TNf.png)

PS: The idea for this came from [Greg Egan's *Diaspora*](http://www.gregegan.net/DIASPORA/DIASPORA.html), in which he describes the possibility of life on a planet arising as simulations in a [biological set of Wang tiles](http://www.cs.duke.edu/courses/current/cps296.6/papers/WLWS98.pdf). It's a very cool book :)  4-state, 2-symbol [Busy Beaver](http://en.wikipedia.org/wiki/Busy_beaver) is the first one that springs to mind. Don't know if that's useful, but it's a small number of states (4) and a lot of activity (107 steps). Anything larger would be impossibly huge (5-state is 47 million steps). 4-state, 2-symbol [Busy Beaver](http://en.wikipedia.org/wiki/Busy_beaver) is the first one that springs to mind. Don't know if that's useful, but it's a small number of states (4) and a lot of activity (107 steps). Anything larger would be impossibly huge (5-state is 47 million steps). Here's Busy Beaver (4,2): [small](http://i.imgur.com/a0riq.png) and [large](http://i.imgur.com/4veau.png) and [large, annotated](http://i.imgur.com/axU6G.png). Had to add in support for halting and bidirectional tapes to the encoding. That's really, really lovely. Very much like some of the Op artists of the '60's. Can't put my finger on which one. Vasarely is close, but not exactly the one I'm thinking of.  I'm glad you like them :) My girlfriend suggested relaxing the constraint that there be only one tape head (i.e., seed the first row with two start tiles), and I thought the result was fun:

* [Head crash](http://i.imgur.com/hEoEu.png)
* [Head crash, annotated](http://i.imgur.com/wQouI.png)

The bits at the bottom right are the recursive backtracker trying to make the best of an impossible situation.  </snippet></document><document><title>Choosing a book to teach AP Computer Science with?</title><url>http://www.reddit.com/r/compsci/comments/r92t4/choosing_a_book_to_teach_ap_computer_science_with/</url><snippet>I'm teaching an AP Computer Science class next year, and the school wants us to use a textbook. Do any of you have any favorites you were taught with/used yourself?

Here are some of our options, but any book can be considered;





Bruce, Kim B., Andrea Pohoreckyj Danyluk, and Thomas P. Murtagh. Java:
An Eventful Approach. Upper Saddle River, NJ: Prentice Hall, 2005. 

Cahoon, James P., and Jack W. Davidson. Java Program Design 5.0.
McGraw-Hill, 2004. 

Cook, Charles E. Blue Pelican Java. Refugio, TX: Charles E. Cook, 2010. 

Dann, Wanda, Stephen Cooper, and Barbara Ericson. Exploring Wonderland:
Java Programming Using Alice and Media Computation. Upper Saddle River,
NJ: Prentice Hall, 2010. 

Deitel, H. M., and P. J. Deitel. Small Java: How to Program. 6th ed.
Upper Saddle River, NJ: Prentice Hall, 2005. 

Headington, Mark, Nell Dale, and Chip Weems. Programming and Problem
Solving with Java. Sudbury, MA: Jones and Bartlett, 2003. 

Horstmann, Cay. Big Java. 4th ed. Hoboken, NJ: Wiley, 2010. 

Horstmann, Cay. Java Concepts. 6th ed. Hoboken, N.J.: Wiley, 2010. 

Kolling, Michael, and David Barnes. Objects First with Java - A
Practical Introduction Using BlueJ Upper Saddle River, NJ: Prentice
Hall, 2002. 

Lambert, Ken, and Martin Osborne. Fundamentals of Java: AP Computer
Science Essentials for the A and AB Exams. 3rd ed. Boston: Thomson
Course Technology, 2007. 

Lewis, John, and William Loftus. Java Software Solutions: Foundations of
Program Design. 5th ed. Upper Saddle River, NJ: Prentice Hall, 2007. 

Lewis, John, William Loftus, and Cara Cocking. Java Software Solutions
for AP Computer Science. 2nd ed. Upper Saddle River, NJ: Prentice Hall,
2007.

Litvin, Maria, and Gary Litvin. Java Methods A &amp;amp; AB: Object-Oriented
Programming and Data Structures, AP Edition. Andover, MA.: Skylight
Publishing, 2006. 
  Big Java is a good 'learn to program in java' book. I'd really suggest getting it AND one of the AP prep books. The AP book will have more test-oriented knowledge to make sure the students do well.

Good luck with this! It's one of my small dreams to help out at a high school doing something like this one day. If you can get 1 kid (1 kid!) to have an inkling of what the church-turing thesis meant, or to understand why Godel's incompleteness theorem is important, you've pretty much changed a life. (Over dramatic, I know, but I really feel this way!)    </snippet></document><document><title>Plan 9: The Way the Future Was</title><url>http://www.faqs.org/docs/artu/plan9.html</url><snippet>  It seems new technology has often had the chance to adopt some of Plan 9's methods, and decided not to. For example, (from what I understand,) GVFS handles mounting remote filesystems locally completely differently. Why did they choose a different path? Is GVFS's way technically superior? I don't think there's a lot of conscious thought going on in those decisions. The idea of exposing functionality through the filesystem is pretty alien to most programmers these days. Most of them don't know what a fifo is, and `ioctl` is so awkward in UNIX that people assume working with the filesystem is fundamentally ugly.

Look at [this article](http://www.storytotell.org/essays/74-response.html) for how exposing functionality through the filesystem comes in handy, though - some quick examples given:

- Open `/dev/keyboard` to get the stream of characters you're writing now,

- Mount `/dev/mouse` from another computer and that mouse now works on your screen.

You want to play your music through the speakers on the other computer? Mount its `/dev/audio`.

It'd be incredible if we could do things like that even with regular applications. If a text editor window exposes its current contents to the filesystem, all you need to do to save a file is `cp` it somewhere. You could do a find and replace with just

    sed s/hello/goodbye/ &amp;lt; ./editor/file &amp;gt; ./editor/file

If `sed` isn't powerful enough you could use `awk` or `perl` or *anything*. How many times have you thought, "If only the regex find/replace in my text editor didn't suck so much..."?

It's the UNIX way - we have powerful tools that are appropriate to a lot of problems we still have, but the new culture around the linux GUI doesn't stop to think how to expose sane interfaces to do it. Instead they insist on rolling all the functionality into the binary and giving us shitty dbus interfaces. &amp;gt; You want to play your music through the speakers on the other computer? Mount its /dev/audio.

The hoops I have jumped through to get that working.

Of course even in the magic far off land where this works, you still need to agree on some sort of encoding scheme. Lossy or lossless? How about once network bandwidth goes to heck, do you degrade to lossless? 

 I'm tempted to say "Just use OSS". It has an almost-sensible UNIX-like API and it's available on BSD systems as well. Unfortunately, I think that ship has sailed - The *only* thing the Linux sound community seems to agree on is that they prefer ALSA to OSS.

The encoding basically needs to say three things:

- Sampling rate,

- Bits/sample, and

- Number of channels.

Compression and decompression can (and should) be done outside the device. I don't know enough about sound to know where you'd do mixing and volume control. At a guess, each app would probably get a device and the driver would mix all of them (with per-application volume controls). Apps that need to play more than one sound at a time would need to get them mixed on the outside. See I would want to take things a step further and instead of just having a /dev/audio I would want something like what Windows has with per process volume control and have each process that is using the sound card be exposed.

That'd be awesome. :-D

&amp;gt; Compression and decompression can (and should) be done outside the device.

Not sure what you mean here. I sure as hell want it compressed before it hits my network card.  See I would want to take things a step further and instead of just having a /dev/audio I would want something like what Windows has with per process volume control and have each process that is using the sound card be exposed.

That'd be awesome. :-D

&amp;gt; Compression and decompression can (and should) be done outside the device.

Not sure what you mean here. I sure as hell want it compressed before it hits my network card.  See I would want to take things a step further and instead of just having a /dev/audio I would want something like what Windows has with per process volume control and have each process that is using the sound card be exposed.

That'd be awesome. :-D

&amp;gt; Compression and decompression can (and should) be done outside the device.

Not sure what you mean here. I sure as hell want it compressed before it hits my network card.  I don't think there's a lot of conscious thought going on in those decisions. The idea of exposing functionality through the filesystem is pretty alien to most programmers these days. Most of them don't know what a fifo is, and `ioctl` is so awkward in UNIX that people assume working with the filesystem is fundamentally ugly.

Look at [this article](http://www.storytotell.org/essays/74-response.html) for how exposing functionality through the filesystem comes in handy, though - some quick examples given:

- Open `/dev/keyboard` to get the stream of characters you're writing now,

- Mount `/dev/mouse` from another computer and that mouse now works on your screen.

You want to play your music through the speakers on the other computer? Mount its `/dev/audio`.

It'd be incredible if we could do things like that even with regular applications. If a text editor window exposes its current contents to the filesystem, all you need to do to save a file is `cp` it somewhere. You could do a find and replace with just

    sed s/hello/goodbye/ &amp;lt; ./editor/file &amp;gt; ./editor/file

If `sed` isn't powerful enough you could use `awk` or `perl` or *anything*. How many times have you thought, "If only the regex find/replace in my text editor didn't suck so much..."?

It's the UNIX way - we have powerful tools that are appropriate to a lot of problems we still have, but the new culture around the linux GUI doesn't stop to think how to expose sane interfaces to do it. Instead they insist on rolling all the functionality into the binary and giving us shitty dbus interfaces. I don't think there's a lot of conscious thought going on in those decisions. The idea of exposing functionality through the filesystem is pretty alien to most programmers these days. Most of them don't know what a fifo is, and `ioctl` is so awkward in UNIX that people assume working with the filesystem is fundamentally ugly.

Look at [this article](http://www.storytotell.org/essays/74-response.html) for how exposing functionality through the filesystem comes in handy, though - some quick examples given:

- Open `/dev/keyboard` to get the stream of characters you're writing now,

- Mount `/dev/mouse` from another computer and that mouse now works on your screen.

You want to play your music through the speakers on the other computer? Mount its `/dev/audio`.

It'd be incredible if we could do things like that even with regular applications. If a text editor window exposes its current contents to the filesystem, all you need to do to save a file is `cp` it somewhere. You could do a find and replace with just

    sed s/hello/goodbye/ &amp;lt; ./editor/file &amp;gt; ./editor/file

If `sed` isn't powerful enough you could use `awk` or `perl` or *anything*. How many times have you thought, "If only the regex find/replace in my text editor didn't suck so much..."?

It's the UNIX way - we have powerful tools that are appropriate to a lot of problems we still have, but the new culture around the linux GUI doesn't stop to think how to expose sane interfaces to do it. Instead they insist on rolling all the functionality into the binary and giving us shitty dbus interfaces. The regexp an editor's buffer using fs interface can be much less efficient. The editor might be holding the data in a clever data structure like a rope. It would have to serialize the whole thing into a flat string, and then replace its internal representation with the data it got back from perl.

It's like piping stuff into a gzip process instead of using zlib.

This architecture is good backup plugin system, for applications you didn't think of beforehand. But for use cases you planned for, you should use in-process library calls. A regexp in an editor is a planned feature, it should be efficient.    Using Plan 9 (actually 9front) semi-regularly here.

Once you get used to the difference in mouse behavior between rio and X11, it's pretty nice, and it's much better documented and generally easier to do things from a programmers perspective than Linux. Once I learned how to use the mouse (with a thinkpad pointing stick) I absolutely love rio/plan9. When I use acme i always think "This is how computers should be used!"  from outer space?</snippet></document><document><title>I created a cellular automata subreddit!</title><url>http://www.reddit.com/r/cellular_automata/</url><snippet /></document><document><title>Church encodings in JavaScript</title><url>http://matt.might.net/articles/js-church/</url><snippet /></document><document><title>New Kurt G&#246;del video up today. The Class of all sets and more...</title><url>http://www.youtube.com/watch?v=B2DY8WvSOLU&amp;amp;uio=5</url><snippet /></document><document><title>Ian Wienand's Computer Science from the Bottom Up. Online book covering many operating system topics.</title><url>http://www.bottomupcs.com/</url><snippet>  </snippet></document><document><title>Invited to a "lifeboat" debate where they pit CS against Poly Sci, Psych, and Business. Opinions wanted.</title><url>http://www.reddit.com/r/compsci/comments/r5zae/invited_to_a_lifeboat_debate_where_they_pit_cs/</url><snippet>I was recently invited to attend a "lifeboat" debate where several faculty members are invited to debate that they (i.e., someone from their discipline) should receive the last spot on a lifeboat after a nuclear war. I have a PhD in CS, and I'll be up against PhD's in Political Science, Psychology, and Business. I'll have 5 minutes to present my case and a 5 minute rebuttal period. I'll also have to deal with a "Devil's Advocate" from Chemistry who will try and convince the audience that they shouldn't take any of four profs and just leave the seat empty.

My question to you, compsci, is what types of arguments should I be presenting, and what types of arguments should I be looking out for? My current plan:

- argue that we should have a source of power, so bringing a computer is not a bad idea
- argue that we should be out of the range of EMP, so the computer will still work
- argue that everything depends on computers, so a CS prof will be useful in helping out everyone in the group when things begin to get back to normal (i.e., get computer systems working for factories, get communications systems back working, etc). 
- I've downloaded a copy of Wikipedia, Project Gutenberg, Ubuntu/Gentoo/Debian/Chrome OS 

I expect that I'll take some flak about CS people just playing video games, and that computers won't work, etc, etc. Just thought that I would throw out the idea here and see what ideas others in r/compsci would have. (The idea for the reddit post comes from one of my students who knows that I read reddit. Hi, Tyler!)


  Why are you relying on a computer?

Your advantage in a post-apocalyptic world is that your skills in algorithms and efficiency are needed both in the short term and long term. The business prof is a very poor choice for the short term, (And I say would say overall, but that's not the point here), since most of what the business student learns is only valuable for large systems. Very large systems. They study the allocation of resources, not the most efficient way to use them. Which is great in a macro system when waste is at a high level because resources can't get to where they need to be, but they'll be good for nothing in a small situation where ultra-efficiency is called for. Likewise, the poly sci prof will have been valuable once you have a large system setup, but if you can get to that level, you're already golden. Your first concern would be surviving the small step, as a small group of survivors.  The psych prof, I can see them being an asset to the group, helping to keep them sane and giving them coping methods in a world where everything they ever knew has been destroyed. But again, your first concern is survival, and the psych prof isn't any good unless you already have the other necessary personal to survive.


So if space is so limited that there's only one spot left in the lifeboat, we need someone who can best determine, for arbitrary problems, the best or at least near-optimal allocation of time and resources for a given set of priorities, and that's an algorithmic problem. 



As for rebutting the chem prof; At the very worst, the extra person can be killed and eaten as an emergency food resource, so there is absolutely no point to leaving the seat empty.  That rebuttal is bulletproof. To play Devil's advocate, that extra seat will consume additional resources, but the promise of rewards just over the next hill will always make it convincing to keep him alive, just in case abundant resources are found. So the person may never be consumed and never serve a useful function.


As well, if he is eaten, by the time the person is eaten, he may have consumed more resources than he was worth; Is the food aboard the lifeboat perishable? If so, then this may preserve resources for a longer time, but otherwise, it's a careful balance.


As well, even if there is a gain, the rest of the team would take some additional sanity loss, (and they're already probably near a breaking point), by eating another human being. Does the sanity loss outweigh the gain in food?  So long as clean water, vitamin supplements, and medical supervision is plentiful, you can keep someone with lots of fat alive for a rather long time on a water-only diet. So long as clean water, vitamin supplements, and medical supervision is plentiful, you can keep someone with lots of fat alive for a rather long time on a water-only diet. To play Devil's advocate, that extra seat will consume additional resources, but the promise of rewards just over the next hill will always make it convincing to keep him alive, just in case abundant resources are found. So the person may never be consumed and never serve a useful function.


As well, if he is eaten, by the time the person is eaten, he may have consumed more resources than he was worth; Is the food aboard the lifeboat perishable? If so, then this may preserve resources for a longer time, but otherwise, it's a careful balance.


As well, even if there is a gain, the rest of the team would take some additional sanity loss, (and they're already probably near a breaking point), by eating another human being. Does the sanity loss outweigh the gain in food?  To play Devil's advocate, that extra seat will consume additional resources, but the promise of rewards just over the next hill will always make it convincing to keep him alive, just in case abundant resources are found. So the person may never be consumed and never serve a useful function.


As well, if he is eaten, by the time the person is eaten, he may have consumed more resources than he was worth; Is the food aboard the lifeboat perishable? If so, then this may preserve resources for a longer time, but otherwise, it's a careful balance.


As well, even if there is a gain, the rest of the team would take some additional sanity loss, (and they're already probably near a breaking point), by eating another human being. Does the sanity loss outweigh the gain in food?  To play Devil's advocate, that extra seat will consume additional resources, but the promise of rewards just over the next hill will always make it convincing to keep him alive, just in case abundant resources are found. So the person may never be consumed and never serve a useful function.


As well, if he is eaten, by the time the person is eaten, he may have consumed more resources than he was worth; Is the food aboard the lifeboat perishable? If so, then this may preserve resources for a longer time, but otherwise, it's a careful balance.


As well, even if there is a gain, the rest of the team would take some additional sanity loss, (and they're already probably near a breaking point), by eating another human being. Does the sanity loss outweigh the gain in food?  Why should a 5th person incur additional sanity loss? Most people are pretty well geared to living with more than 3 other people. In fact, we're pretty bad at living with fewer. I think you need to finish reading the sentence there, boss.  Why are you relying on a computer?

Your advantage in a post-apocalyptic world is that your skills in algorithms and efficiency are needed both in the short term and long term. The business prof is a very poor choice for the short term, (And I say would say overall, but that's not the point here), since most of what the business student learns is only valuable for large systems. Very large systems. They study the allocation of resources, not the most efficient way to use them. Which is great in a macro system when waste is at a high level because resources can't get to where they need to be, but they'll be good for nothing in a small situation where ultra-efficiency is called for. Likewise, the poly sci prof will have been valuable once you have a large system setup, but if you can get to that level, you're already golden. Your first concern would be surviving the small step, as a small group of survivors.  The psych prof, I can see them being an asset to the group, helping to keep them sane and giving them coping methods in a world where everything they ever knew has been destroyed. But again, your first concern is survival, and the psych prof isn't any good unless you already have the other necessary personal to survive.


So if space is so limited that there's only one spot left in the lifeboat, we need someone who can best determine, for arbitrary problems, the best or at least near-optimal allocation of time and resources for a given set of priorities, and that's an algorithmic problem. 



As for rebutting the chem prof; At the very worst, the extra person can be killed and eaten as an emergency food resource, so there is absolutely no point to leaving the seat empty.  Why are you relying on a computer?

Your advantage in a post-apocalyptic world is that your skills in algorithms and efficiency are needed both in the short term and long term. The business prof is a very poor choice for the short term, (And I say would say overall, but that's not the point here), since most of what the business student learns is only valuable for large systems. Very large systems. They study the allocation of resources, not the most efficient way to use them. Which is great in a macro system when waste is at a high level because resources can't get to where they need to be, but they'll be good for nothing in a small situation where ultra-efficiency is called for. Likewise, the poly sci prof will have been valuable once you have a large system setup, but if you can get to that level, you're already golden. Your first concern would be surviving the small step, as a small group of survivors.  The psych prof, I can see them being an asset to the group, helping to keep them sane and giving them coping methods in a world where everything they ever knew has been destroyed. But again, your first concern is survival, and the psych prof isn't any good unless you already have the other necessary personal to survive.


So if space is so limited that there's only one spot left in the lifeboat, we need someone who can best determine, for arbitrary problems, the best or at least near-optimal allocation of time and resources for a given set of priorities, and that's an algorithmic problem. 



As for rebutting the chem prof; At the very worst, the extra person can be killed and eaten as an emergency food resource, so there is absolutely no point to leaving the seat empty.  In many ways I would argue that the computer is one of man's greatest inventions...ever. It rivals the printing press for the knowledge it helps spread (if you can count the internet, which any CS PhD should have a decent working knowledge of) and the innumerable advancements it has helped spurn in only ~60 years. If we want any chance of surviving and finding success as a species, we would need someone who can document and help rebuild the systems that have made the last century the most productive in human history. Are we willing to lose all of that just so a Business major can make sure our boat hierarchy is a little more organized? The problem with that idea is that you want a computer engineer for that job, maybe a programmer later, not so much a CS guy.

As well, that's a long-term plan for large populations; All the resources needed to make a computer are pretty rare. 


As a long-term plan though, it is a nice side to the poli sci prof's being able to setup a better political system, and the business prof setting up good supply chains. You would need a CS guy to set the foundations of computing before you had an engineer build it. It's good that the chemist isn't one of the people vying for a seat. Because he would definitely be more useful than any of the other members.  The problem with that idea is that you want a computer engineer for that job, maybe a programmer later, not so much a CS guy.

As well, that's a long-term plan for large populations; All the resources needed to make a computer are pretty rare. 


As a long-term plan though, it is a nice side to the poli sci prof's being able to setup a better political system, and the business prof setting up good supply chains. &amp;gt;All the resources needed to make a computer are pretty rare.

Well, those resources needed to make a computer of the kind we are used to, sure. You could make a general computer along the lines of Babbage's Analytical Engine with more common material. Now we have to talk about 'rare'. We don't consider iron or identical gears to be rare, but a post-apocalyptic world would have to consider the gears at least, to be rare. 

If we have someone finding the coal, and smelting down the iron into usable forms, that probably means we already have an engineer, who would be better suited for building a calculating machine anyways.  If there are people floating around in a boat, it's not like the crust of the earth has been vaporized. There will be all the gears and machine bits you could ever want just lying about the place. In the sizes that you want? It's not like there's a standard gear size; They range all over the place. As well, we don't use a terrible amount of gears anymore to begin with. Can you even name one thing in your house with gears in it? But it's not like you have a fully formed machine missing a critical part; you're building it from scratch, which gives you a lot more flexibility in working with what you have. Also, there are still quite a few machines with actual gears in them. You're not limited to your house, there will be all sorts of abandoned cars, industrial machines, factories, toy cars, etc., etc. Why are you relying on a computer?

Your advantage in a post-apocalyptic world is that your skills in algorithms and efficiency are needed both in the short term and long term. The business prof is a very poor choice for the short term, (And I say would say overall, but that's not the point here), since most of what the business student learns is only valuable for large systems. Very large systems. They study the allocation of resources, not the most efficient way to use them. Which is great in a macro system when waste is at a high level because resources can't get to where they need to be, but they'll be good for nothing in a small situation where ultra-efficiency is called for. Likewise, the poly sci prof will have been valuable once you have a large system setup, but if you can get to that level, you're already golden. Your first concern would be surviving the small step, as a small group of survivors.  The psych prof, I can see them being an asset to the group, helping to keep them sane and giving them coping methods in a world where everything they ever knew has been destroyed. But again, your first concern is survival, and the psych prof isn't any good unless you already have the other necessary personal to survive.


So if space is so limited that there's only one spot left in the lifeboat, we need someone who can best determine, for arbitrary problems, the best or at least near-optimal allocation of time and resources for a given set of priorities, and that's an algorithmic problem. 



As for rebutting the chem prof; At the very worst, the extra person can be killed and eaten as an emergency food resource, so there is absolutely no point to leaving the seat empty.  A less heinous rebuttal would simply be that a young healthy (well-educated) human being would be required to diversify the gene pool as the group beings to reproduce.

giggity. &amp;gt;young healthy (well-educated) human being

But he's a CS professor (and I say that as a post-doc in the discipline).  &amp;gt;young healthy (well-educated) human being

But he's a CS professor (and I say that as a post-doc in the discipline).  I am a CS prof, but I am also fairly young (&amp;lt;40) and I can run a 5k.  When they finally try to eat you, at least you'll be able to outrun them. :) Outrunning the pack on a lifeboat does pose its own formidable challenges. A less heinous rebuttal would simply be that a young healthy (well-educated) human being would be required to diversify the gene pool as the group beings to reproduce.

giggity. Why are you relying on a computer?

Your advantage in a post-apocalyptic world is that your skills in algorithms and efficiency are needed both in the short term and long term. The business prof is a very poor choice for the short term, (And I say would say overall, but that's not the point here), since most of what the business student learns is only valuable for large systems. Very large systems. They study the allocation of resources, not the most efficient way to use them. Which is great in a macro system when waste is at a high level because resources can't get to where they need to be, but they'll be good for nothing in a small situation where ultra-efficiency is called for. Likewise, the poly sci prof will have been valuable once you have a large system setup, but if you can get to that level, you're already golden. Your first concern would be surviving the small step, as a small group of survivors.  The psych prof, I can see them being an asset to the group, helping to keep them sane and giving them coping methods in a world where everything they ever knew has been destroyed. But again, your first concern is survival, and the psych prof isn't any good unless you already have the other necessary personal to survive.


So if space is so limited that there's only one spot left in the lifeboat, we need someone who can best determine, for arbitrary problems, the best or at least near-optimal allocation of time and resources for a given set of priorities, and that's an algorithmic problem. 



As for rebutting the chem prof; At the very worst, the extra person can be killed and eaten as an emergency food resource, so there is absolutely no point to leaving the seat empty.  [deleted] The need for leadership amongst PhD's would be rather different. Who says a council of some sort couldn't be formed? Or that leadership is already in place? The lifeboat seems like it would have been a concerted effort, most likely already directed by someone. Remember, it's the last seat being filled.

The environment is also much different. Success is dictated by entirely different parameters. You couldn't run a lifeboat like you would a 9-to-5 business. People don't just 'go home' at the end of the day.

Your last point is somewhat irrelevant. The education of CS is much different than Business and you couldn't answer for the knowledge in between bachelor and PhD. I haven't downvoted you because you are still contributing to the discussion, even though I disagree with your points. I'm noting this because you had been in the negative with no comments as to why. [deleted]  That you're trained in mathematical / algorithmic thinking and know awesome fast ways to get things done.  Just sit in the corner muttering the whole time then near the end jump up and say "It's ok guys, I've developed a packing algorithm that's 3x more efficient than the one you're using; we can all go!"

Really though, It's a good idea to try and cover several scenarios; one where you have the computer and it works, one where you may find or develop a computer, and most importantly all the ways you'd be useful without any computer at all. I'm tempted to do that. I will have to address the issue of "what happens when the computer fails". I'm tempted to do that. I will have to address the issue of "what happens when the computer fails". Why do you keep wittering on about a computer? Computer science is not about computers. It's about computation.

If your big idea is to bring a laptop and a copy of wikipedia, why do we need you? Why can't the business major bring the laptop with him? Part of the idea is to bring props. What else should I bring as a prop? I know that CS is about computation and problem solving. But by bringing a laptop and some information, I can show to the students that I can care not only about my area of knowledge, but for all areas of knowledge, and that I can be useful not only in my own area, but every other area as well. If there were an economics guy along, he'd focus on his own areas of comparitive advantage. If anybody can do it, it's not that interesting. 

Poli-sci, psychology, and business can't do much; but they could still justify bringing a laptop with wikipedia on it.

I'd go for the hail mary: You know algorithms.  You know a lot of them, and you know how to design more. Machines are the physical manifestations of algorithms. The only one of the four professors who could plan and design the machines you'll need for immediate survival is you; business-guy might, in a stretch, remember reading about Archimedes' screw, but you'll be creating [bamboo technology](http://tvtropes.org/pmwiki/pmwiki.php/Main/BambooTechnology) by the time they're leaving the paleolithic.  You say you have a PhD in 'Computer Science' but all you can think of is to perform the functions for a workstation tech?

What about...

* Knowledge of optimization algorithms (resource allocation)
* Knowledge of fair voting algorithms (government)
* Others? You say you have a PhD in 'Computer Science' but all you can think of is to perform the functions for a workstation tech?

What about...

* Knowledge of optimization algorithms (resource allocation)
* Knowledge of fair voting algorithms (government)
* Others?  I may also try to argue that as the only technical person in the group, I should go, as we have seen in years past what has happened when a civilization has lost their technical knowledge. The loss of the mathematical and technical knowledge of the egyptians and others led to the downfall of their civilizations or periods like the Dark Ages.   [deleted] Indeed. He doesn't realise he's contending for a spot on the B Ark. [deleted]  How can I watch/listen? The student radio station may be recording it for rebroadcast later. I'm not sure. Have a CS student bring a laptop or a digital/tape recorder.  That way it can be posted here.  I am very interested in how this turns out.

(CS student here) Have a CS student bring a laptop or a digital/tape recorder.  That way it can be posted here.  I am very interested in how this turns out.

(CS student here) (I'm the person mentioned in the post, yes I just made an account, yes I've been a Reddit creeper for over four years)

Just had a conversation with the lad running the equipment and questioned whether this will be live broadcasted... he said no. However, I'll have first dibs on a MP3 recording on the debate. Stay tuned!  Six figure salary. Right at graduation.

/debate Six figure salary. Right at graduation.

/debate      Project planning will be vital to the survival of the group. Wasting non-replenish-able resources could easily lead to the death of everyone. Project planning is all about identifying dependencies between tasks, resources required for tasks, and monitoring that task are being accomplished. You're the only person with a copy of MS-Project and you're the only one who can project manage the survival of the species. 

Oh... and longer term there's a need for ensuring reasonable genetic mixing. Too many generations of cousins marrying will lead to extinction. Only you can identify and codify the initial relationships between all the survivors because, again, you're the only one with a copy of MS-Access and you're the only one who can database manage the survival of the species.

And lastly... you're the only one who thought to bring 10TB of pornography. As someone who did a PhD in Autonomic Database Management Systems, I am offended that you think MS Access is a real DBMS. :)

I do like the project planning aspect, though. And I've decided that an offline copy of wikipedia is more important than porn.     [deleted]    Argue for taking the chemist.  You need to think of not just what value you in the long term(i.e. computerized factories,etc) but also your value in the short term, immediately after the collapse of society. Why do you get to live and 4 other people die? What about your potential contribution outweighs their potential contribution?


Along your current line of thinking: Access to information that preserves the knowledge that is foundational to our current civilization is important. 

Also mention this: [http://www.cd3wd.com/cd3wd_40/cd3wd/index.htm](http://www.cd3wd.com/cd3wd_40/cd3wd/index.htm). 

This is directly relevant to the situation that you would find yourself in after a nuclear war.

  Here are some quick off the top of my head challenge questions to your plan...

Argument 1: "Why then, would we not use our power source (is it limited?) for other life-sustaining things, like boiling water, or powering lights? Surely using it on a computer would be wasteful."

Argument 2: "Wouldn't the EMP pulse have damaged a lot of the electrical components in the power grid, thus making a reliable power source hard to come by?"

I would also try to argue that teaching people the concepts of computer science is much more beneficial to getting society up and running again; not only because you would be enabling people to fix the plethora of computer technology out there, but you'd be enabling a method of succinct thinking and problem solving. In the given situation, this could be the difference between carrying the future of our race on, or not.

Let us know how you do :D  You carry with you the greatest potential for creating tools that will for sure be useful in the distant future and, depending on resources available on the lifeboat, in the near future as well. Despite there being a nuclear war there should still be plenty of equipment that can be repaired/used and what with all the equipment still available you would be the best candidate to serve as operator of these things.

Of course your use will vary depending on what other types are already on the lifeboat. Most people would assume engineers of a related field would be more suited to applied tasks but a CS would have farther reaching knowledge.

To add to your argument about having access to a computer; if the lifeboat was a planned event, there would have been preparations taken to shield resources from the effects of the nuclear bombs. It would almost be a certainty that you would have access to a computer. Also, to add to bringing a copy of Wikipedia etc....others could say that it doesn't take a CS to download information libraries and an operating system. You could rebut that you would be the only one capable of making changes to digital infrastructures and make the information contained in the library's more useful then just as a portable book. The reason that I downloaded a copy of Wikipedia is because I assume that most students outside of CS don't know that it can be done, and this may impress them. I also want to show to the group that my knowledge is not limited to just CS, and that I am doing my best to make sure that the entire group survives. And mainly because it is a prop, which is encouraged.    You'll be able to battle and disable the inevitable murderous robots and possibly use those parts to become lord of the wastes. I'll tell them that I have experience will Fallout 3 and Resident Evil 4, which will be much more useful than The Sims and Roller Coaster Tycoon. But in all seriousness you're the only one with the computers skill so getting access to locked terminals is one thing you can do that they can't. True. Once we get back to a place where there are locked terminals. And where is the life boat taking you anyway? this seems relevant to the debate. They don't tell us, which makes things a bit more difficult. I can argue that given the size and the limitations, it could be a spaceship, where I would be very useful. Otherwise, if earthbound, I can still argue that I would be away from EMP and that computers would still work.    The study of Business is about management. That sounds like a free loader to me. (And any business strategy is going to work until things get big enough in a few generations. People figured out business pretty well on their own long before any studied it.)

The study of Political Science is about politicians. That sounds like a free loader to me. (And any political system is going to work well enough until there are a few generations of people. People figured out governing pretty well on their own long before any studied it.)

Psychology? Well, honestly I'd pick this person. But the study is mostly dependent on modern civilization, and you're dealing with problems that the study hasn't really considered. Find a copy of "Feeling Good" and have everyone talk to each other and not let anyone be lonely. The rest is details for a few generations.

CS? 1. You're the closest off to being able to read how to get power back working. BS in CS often requires the most physics and math that will get you to power. Power means you can have labor-saving advances quicker. You are closest to emulating an engineer.

Now if there's already an engineer or physicist on the boat, you're fucked. Take the Psychologist. :) - but if you still want an out, mention that you've spent the most time making things work, an invaluable skill. It's hard to have enough people with that skill. In the absence of other scientists, you have the most skill in recording what does and doesn't work, and developing algorithms for things that work. You are also the closest there is to being able to duplicate other scientists, having had the most math and science classes. (More important than the bus number is the pneumonia number.) The study of Business is about management. That sounds like a free loader to me. (And any business strategy is going to work until things get big enough in a few generations. People figured out business pretty well on their own long before any studied it.)

The study of Political Science is about politicians. That sounds like a free loader to me. (And any political system is going to work well enough until there are a few generations of people. People figured out governing pretty well on their own long before any studied it.)

Psychology? Well, honestly I'd pick this person. But the study is mostly dependent on modern civilization, and you're dealing with problems that the study hasn't really considered. Find a copy of "Feeling Good" and have everyone talk to each other and not let anyone be lonely. The rest is details for a few generations.

CS? 1. You're the closest off to being able to read how to get power back working. BS in CS often requires the most physics and math that will get you to power. Power means you can have labor-saving advances quicker. You are closest to emulating an engineer.

Now if there's already an engineer or physicist on the boat, you're fucked. Take the Psychologist. :) - but if you still want an out, mention that you've spent the most time making things work, an invaluable skill. It's hard to have enough people with that skill. In the absence of other scientists, you have the most skill in recording what does and doesn't work, and developing algorithms for things that work. You are also the closest there is to being able to duplicate other scientists, having had the most math and science classes. (More important than the bus number is the pneumonia number.)  I'd take the business guy over the others, and I majored in CS. Business is inherently about organization and production, which is exactly the sort of expertise needed to rebuild a society.

If all you can think about is bringing a PC and a copy of Wikipedia, you're damn near useless.
 &amp;gt;Business is inherently about organization and production, which is exactly the sort of expertise needed to rebuild a society.

But any scientist who's actually run a lab will have hands on practical experience of making a team get shit done fast with limited resources.

I'm on the chemist's side, if this all the CS guy is  bringing to the table.

Take the damn laptop and throw them all overboard. &amp;gt;Business is inherently about organization and production, which is exactly the sort of expertise needed to rebuild a society.

But any scientist who's actually run a lab will have hands on practical experience of making a team get shit done fast with limited resources.

I'm on the chemist's side, if this all the CS guy is  bringing to the table.

Take the damn laptop and throw them all overboard.  I don't understand. Why are you on a lifeboat after a nuclear war? Did you happen to be on a boat when the nuclear war happened? Also, how did a nuclear bomb destroy your boat but not kill you?    CompSci is a stones throw away from electronics and robotics too.  I don't mean the kind that rise up in the wake of a nuclear disaster and obliterate the enemy... but the kind that fly over enemy territory to gain intel or the kind that go out into a battlefield and pick up wounded soldiers, heck even the ones that will keep a food supply safe for the hundreds (you bet a CS PhD candidate has studied some Thermodynamics!)... 

Even if the above represents more of an EE, I know for certain the more technically inclined brains of EE/CS majors bring a completely different intellectual aspect to problem solving. And in the hypothetical situation you mentioned, there will be a hell of a lot of problems to solve.  Why would anyone get a PhD in business ( and I didn't know they were offered)?  Is this the same debate that was profiled by "This American Life" a couple years ago? [Link to episode (act two)](http://www.thisamericanlife.org/radio-archives/episode/402/save-the-day) </snippet></document><document><title>How to hang a picture with n nails such that removing any k nails causes the picture to fall.</title><url>http://arxiv.org/abs/1203.3602v1</url><snippet>  You've probably all heard about the most trivial case where n and k equal one, but already at n=2 and k=1, the fact that it's even possible is quite surprising if you're not thinking outside the box. Yeah, the "puzzle" is to correctly identify the universe of allowable solutions, and then you are left with a math problem.

I recently realized that this is the essence of the Microsoft/Google brainteaser controversy. A curtly-posed puzzle can be broken down into three parts:

* The *"aha" brainteaser* part is in defining a math/CS problem that properly models the ill-specified puzzle. This is widely reviled. It is (poorly) defended as an attempt to test for "dealing with messy real world specifications".

* The *puzzle* part is in coming up with an informal solution to the well-specified problem. This is controversial. Proponents say it models real-world problem solving. Detractors say that problem solving is a domain-specific skill, not a general ability.

* The *programming* part is in carefully implementing the informal solution in quality code or pseudocode. This is generally accepted.

 FYI: Google (and so far as I know, Microsoft) [*don't use* "brainteaser questions"](https://plus.google.com/104272082042063211465/posts/J6qG89t1er2).  For those who don't know enough about carpentry to understand what the math/CS problem is, here is my translation into semi-formal math, with the caveats that (a) it's hard to write well using language that is simultaneously meaningful for puzzlers, computer scientists, and mathematicians), and (b) I'm not sure if the technical constraints I defined are precisely correct, especially in the higher n and k cases.

(The paper uses a formal algebraic model of the problem, and does not give a formal geometric model, but the real-world human experience of picture-hanging would (in my opinion) be best described with a geometric model.)

* Definition of an n-hanging: A "string" embedded in a "punctured 3-D space", such that that there is no way to "pull" the string into a configuration where the string is clearly separated from the nails.
* A "string" is a closed, possibly twisted 1-D loop (continuously deformed circle).
* "The nails" are a set of parallel lines, all perpendicular to a "wall".
* "pull" means to smoothly deform (or bend and shift), only in the directions parallel to "the wall".
* "punctured 3-D space" is regular 3-D space representing "the room", minus n infinite parallel puncture lines representing the nails)
*  One way to defined "clearly separated" is that you can cover the string with a ball (or other simply-connected shape) that does not cover or cross any nail line. 


You can also think of the loop as being embedded in 2-D space (the wall), and the nails as points, but then you have to do some mathematical gymnastics to define the allowed twisting of the string (the loop is allowed to cross itself in 2-D space.)

The paper's two main contributions are (a) a general solution to the problem, and (b) polynomial bounds on the number of twistings in many cases.

* Definition of an (n,k)-hanging: A n-hanging, such that removing any (k-1) nails yields an (n-k+1)-hanging, but removing any k nails fails to yield an (n-k)-hanging.

* Case: (1,0)-hanging. Ill-formed. This degenerate case is a single nail that does not hang a picture at all (removing 0 nails causes the picture to fall, but also "not removing 0 nails" fails to hang the picture). An example of this case is to, make the string a circle and the place the nail anywhere not inside or on the circle. 

* Case family: (n,0)-hanging. Obvious generalization from the (1,0 case).

* Case: (1,1)-hanging: Trivial: Place a point inside a circle. Removing the point reduces to the (0,0) case.

* Case family: (n,n)-hanging. Trivial: Place n points inside a circle. Simple indicutive analysis reduces to the (1,1) case.

* Case: (2,1)-hanging. This is the first interesting "puzzle" case.  A solution is given in the paper. Try it for yourself!

* Case universe: (n,k)-hanging. Read the paper!

 Where is this paper? It's linked at from the title of the Reddit post: http://arxiv.org/abs/1203.3602v1
  :O I saw demaine give this talk! </snippet></document><document><title>Smarter Than You Think: A Colony Of Viruses </title><url>http://thecreatorsproject.com/blog/smarter-than-you-think-a-colony-of-viruses</url><snippet /></document><document><title>From what I understand, the syntax of Lambda Calculus is defined by a context-free grammar. Correct me if I'm wrong, but if a CFG isn't Turing complete, then how can Lambda Calculus, a language that is generated by a CFG but interpreted differently, be Turing complete?</title><url>http://www.reddit.com/r/compsci/comments/r4xnp/from_what_i_understand_the_syntax_of_lambda/</url><snippet>This same question applies to all programming languages too. Their syntax is defined using context free grammars but even then they are considered Turing complete. How can a language produced by a CFG be Turing complete?    The difficulty of parsing an encoding of a turing machine and performing the behavior of that encoded machine are not equal.  I think you're misapplying the word "parse" to mean moore than it does. &amp;gt; I think you're misapplying the word "parse" to mean moore than it does.

Not sure how Moore's Law applies here... :)  </snippet></document><document><title>Hockey Sabermetrics and Comp Sci</title><url>http://www.reddit.com/r/compsci/comments/r51hj/hockey_sabermetrics_and_comp_sci/</url><snippet>I will be starting my MSc this fall but I am not sure what exactly what I want to study.  Fortunately the first two semesters are just classes so I can take stuff that interests me to help narrow down my field (or figure out what I don't like).  

In my personal life I have developed an interest in Hockey Sabermetrics.  Basically advanced statistics that tell how a player is performing.  Sometimes they may appear to be playing horrible but you can tell with these stats how offensively/defensively they are being used, how they control the play, how they move the puck and how lucky they are.  I don't believe there is a lot of research in this area since it is mostly used in the blogosphere and only just starting to catch on by the main stream media.  There is even huge arguements to how older stats like +/- are bad.

With that giant explanation over, I am interested in that field but I am not sure if or how I could even link it to Computer Science.     I'm not sure how CS would apply to something which is largely based on statistical modelling. If you want more information on this sort of thing, you should look at the roots of sabermetrics: baseball. There's a great book called Baseball: By The Numbers that discusses much of the history with some great examples.

However, one place CS can be very useful is in fantasy sports. Choosing players during a draft is a complex optimization problem that has to take into account not only the projected stats, but positional scarcity, keepers, playing time, non-linear cost functions (in rotisserie leagues), and many other complications. There is already drafting software available, but there are as many strategies and algorithms as there are players. Could be a worthwhile area to try improving.</snippet></document><document><title>Looking for info about building a GPU cluster</title><url>http://www.reddit.com/r/compsci/comments/r3mp6/looking_for_info_about_building_a_gpu_cluster/</url><snippet>
I recently found out that GPUs are cheaper and more powerful than regular CPUs. I am wondering about the Pros and Cons of building my own GPU cluster. If any one has built their own, I would also really appreciate some advice on how to get started.   I do some research in high-performance scientific computing on GPUs. I can't really advise you on building one, but I can caution you to really be sure you're going to get the performance gains you're looking for in your application. 

The idea that GPUs will instantly give you 100x or even 1000x performance boost is largely propagated by nVidia and friends, but only weakly backed up by research. Some applications (ie. dense matrix multiplications) give great performance, but in many many applications you'll only see a 1x to 5x speedup (see paper below), and at the cost of rewriting a bunch of code. 

Not saying it's definitely a bad idea, but don't let hype stop you from doing your research. And remember, always compare highly-optimized GPU routines to highly-optimized CPU routines, don't trust papers that compare optimized routines to naive ones.

Citing sources: PDF: [Debunking the 100X GPU vs. CPU Myth:
An Evaluation of Throughput Computing on CPU and GPU](http://sbel.wisc.edu/Courses/ME964/2011/Literature/LeeDebunkGPU2010.pdf) I do some research in high-performance scientific computing on GPUs. I can't really advise you on building one, but I can caution you to really be sure you're going to get the performance gains you're looking for in your application. 

The idea that GPUs will instantly give you 100x or even 1000x performance boost is largely propagated by nVidia and friends, but only weakly backed up by research. Some applications (ie. dense matrix multiplications) give great performance, but in many many applications you'll only see a 1x to 5x speedup (see paper below), and at the cost of rewriting a bunch of code. 

Not saying it's definitely a bad idea, but don't let hype stop you from doing your research. And remember, always compare highly-optimized GPU routines to highly-optimized CPU routines, don't trust papers that compare optimized routines to naive ones.

Citing sources: PDF: [Debunking the 100X GPU vs. CPU Myth:
An Evaluation of Throughput Computing on CPU and GPU](http://sbel.wisc.edu/Courses/ME964/2011/Literature/LeeDebunkGPU2010.pdf) I do some research in high-performance scientific computing on GPUs. I can't really advise you on building one, but I can caution you to really be sure you're going to get the performance gains you're looking for in your application. 

The idea that GPUs will instantly give you 100x or even 1000x performance boost is largely propagated by nVidia and friends, but only weakly backed up by research. Some applications (ie. dense matrix multiplications) give great performance, but in many many applications you'll only see a 1x to 5x speedup (see paper below), and at the cost of rewriting a bunch of code. 

Not saying it's definitely a bad idea, but don't let hype stop you from doing your research. And remember, always compare highly-optimized GPU routines to highly-optimized CPU routines, don't trust papers that compare optimized routines to naive ones.

Citing sources: PDF: [Debunking the 100X GPU vs. CPU Myth:
An Evaluation of Throughput Computing on CPU and GPU](http://sbel.wisc.edu/Courses/ME964/2011/Literature/LeeDebunkGPU2010.pdf)  Why don't you try hiring some of Amazon's Cluster GPU nodes first? Ahh I had no idea amazon had GPU - great tip! [deleted]   Basically, before you do this, you have to make sure you have some application in mind that is both coded in a GPU language like CUDA and also runnable on a cluster.  I can think of only two applications of the top of my head that fit that description, and those would be bitcoin and luxrender.

So, basically, unless you are already a master GPU programmer and cluster architect to write your own software, or have a lot of bitcoin mining or raytracing work to do, then I'd say its not worth the time and effort. I've heard mathematica can also use CUDA. Might be interesting to check that out Basically, before you do this, you have to make sure you have some application in mind that is both coded in a GPU language like CUDA and also runnable on a cluster.  I can think of only two applications of the top of my head that fit that description, and those would be bitcoin and luxrender.

So, basically, unless you are already a master GPU programmer and cluster architect to write your own software, or have a lot of bitcoin mining or raytracing work to do, then I'd say its not worth the time and effort. My main interest is in scientific computing and I was thinking that within a few months it would be cost effective to simply own a cluster.  Sure.  Do you plan on writing the code yourself in OpenCL and MPI?  If not, then I don't know how you intend to utilize your cluster.  There aren't any packages I'm aware of that simultaneously work over a network and on GPUs in that network.  You might be able to jury-rig Matlab to do it maybe, by using nework aware parfors alongside something like Jacket, but I imagine that no matter HOW you do it you'll have massive bandwidth issues. Yes, I think I am a little naive about the whole thing. I saw that GPUs are good for vector calculations and I know that matlab does everything in vectors so I assumed that I could just use it for anything. Do you think I would have the same bandwidth problems with just one GPU? Yes, I think I am a little naive about the whole thing. I saw that GPUs are good for vector calculations and I know that matlab does everything in vectors so I assumed that I could just use it for anything. Do you think I would have the same bandwidth problems with just one GPU? Yes, I think I am a little naive about the whole thing. I saw that GPUs are good for vector calculations and I know that matlab does everything in vectors so I assumed that I could just use it for anything. Do you think I would have the same bandwidth problems with just one GPU?   Everything is domain dependent here. What kinds of problems are you planning to tackle inform what kind of approaches you can take, which informs whether GPUs, let along GPUs in a cluster, is going to be useful for you.

What kind of applications are we talking here? various optimization techniques (linear and non-linear) for machine learning  GPUs, at least at the moment, suffer from slow branching. GPUs are only faster in some cases, not all.    Suggestion: Grab yourself a cheap GTX450 - 570 (you're looking between $50 and $350) and stick it in your desktop. Play around on that. Once you have code running (and solving a real problem) think about purchasing a mobo that supports quad-sli and sticking 4 gtx470s in them.

It takes a lot of work to take advantage of just a single GPU. Make sure you can squeeze the most performance out of that first, then move on to multiple GPUs.

I do GPU-accelerated monte carlo research at school - we have 2 4-Tesla clusters, and a single 4-gtx450 cluster. Most of my dev time has been spent on the 450 cluster (it was the first thing we bought, nice and cheap - basically a desktop that we ordered from NCIX) and I've only gone to the Tesla clusters for deployment tests. Don't spend a lot of money if you don't yet have a need.

Note that if you require double-precision floating point accuracy (which you probably do for most scientific computing) then you're going to need Tesla cards (they offer the best double-floating point bandwidth, and are the only GPUs on the market that provide ECC memory) which cost around $2500 each. You'll also need a way to cool these cards, as they draw a LOT of power and get very hot. One of our 4-Tesla clusters is a 2U rack mount at the school's datacenter, and the other is a customized desktop enclosure. The desktop one sounds like a jet engine, has 2 power supplies, and isn't very much fun to be around.

So, cost-wide, grabbing some cheap gtx450's is the way to go when you're first starting out. 4 of them cost you approx $200 + tax, but really, you'll only need 2 in the near future to get the hang of using multiple GPUs. I think our 450 cluster was about $2k, including a monitor, case, couple of 1TB drives, i7, 16gb ram (not ecc). More details about your research would be nice!    Specifically what sort of speed ups are you seeing over optimized CPU code.  

As to the noise, that sucks big time.  That is one reason I suggested considering the soon to arrive alternative in APU like processors.  Feasibility here depends upon how well you can map to more traditional clusters (separate boxes or cards).   The future isn't to far off here, really all you need is Ivy Bridge or some AMD APU technology in either card racks or compact enclosures.  

A Mac Mini packing an Ivy Bridge processor might be most interesting for this sort of application.   You get OpenCL capability along with enhanced CPU vector capability.  If the problem is amendable to cluster computing you basically have a self contained module to add to your cluster.   If you are sure that the problem set will map to the GPU an AMD APU would likely be a better choice.   In any event I see this sort of hardware getting rapid acceptance for low cost scientific computing.  

In any event I'm not sure why all this computing power is needed nor do I know if it will even map to a GPU.  I just have this feeling that there is a rush to GPU clusters before the need is established.  </snippet></document><document><title>Here's a practice test in Javascript that I developed for a CIS 122 Software Design course that I taught this quarter at Portland Community College. I may have tried to cover too much this term.</title><url>http://spot.pcc.edu/~mgoodman/Docs/Practice%20Test/practice%20test.html</url><snippet>  Cool. I got like 50%. I've been programming professionally for 6 years and graduated summa cum laude from a top CS program :D I didn't know any of the testing terms and blew it on other vocab type stuff. All the actual code and algorithm type questions I got. 

I don't like this question:

It is believed that efficient solutions are possible for all known problems.

    	False
    	True 

You are basically asking your students whether P=NP. That's just... not good. I guess, yes, it is *believed* that P!=NP, but asking it in a question like this makes it sound like there is actually a good proof that supports this notion, and there is not. 

I also don't like this question:

The following Javascript code defines a function that initializes some properties on a newly created object:

function Square(x, y, size)
{
  this.x=x;
  this.y=y;
  this.size=size;
}

This function is an example of:

    	Constructor
    	Subroutine
    	Test of equality
    	Test of inequality
    	Method
    	Assignment 

Constructor is the answer you chose. There is nothing that makes it have to be a constructor though. You don't say the object is called Square. You can initialize on a newly created object outside of a constructor as well. Even if it was definitely a constructor, constructors are still subroutines, and constructors are still methods, even if that is not the normal usage of the term. One could make a case for assignment as well. Scratch that Q.  Cool. I got like 50%. I've been programming professionally for 6 years and graduated summa cum laude from a top CS program :D I didn't know any of the testing terms and blew it on other vocab type stuff. All the actual code and algorithm type questions I got. 

I don't like this question:

It is believed that efficient solutions are possible for all known problems.

    	False
    	True 

You are basically asking your students whether P=NP. That's just... not good. I guess, yes, it is *believed* that P!=NP, but asking it in a question like this makes it sound like there is actually a good proof that supports this notion, and there is not. 

I also don't like this question:

The following Javascript code defines a function that initializes some properties on a newly created object:

function Square(x, y, size)
{
  this.x=x;
  this.y=y;
  this.size=size;
}

This function is an example of:

    	Constructor
    	Subroutine
    	Test of equality
    	Test of inequality
    	Method
    	Assignment 

Constructor is the answer you chose. There is nothing that makes it have to be a constructor though. You don't say the object is called Square. You can initialize on a newly created object outside of a constructor as well. Even if it was definitely a constructor, constructors are still subroutines, and constructors are still methods, even if that is not the normal usage of the term. One could make a case for assignment as well. Scratch that Q.  Cool. I got like 50%. I've been programming professionally for 6 years and graduated summa cum laude from a top CS program :D I didn't know any of the testing terms and blew it on other vocab type stuff. All the actual code and algorithm type questions I got. 

I don't like this question:

It is believed that efficient solutions are possible for all known problems.

    	False
    	True 

You are basically asking your students whether P=NP. That's just... not good. I guess, yes, it is *believed* that P!=NP, but asking it in a question like this makes it sound like there is actually a good proof that supports this notion, and there is not. 

I also don't like this question:

The following Javascript code defines a function that initializes some properties on a newly created object:

function Square(x, y, size)
{
  this.x=x;
  this.y=y;
  this.size=size;
}

This function is an example of:

    	Constructor
    	Subroutine
    	Test of equality
    	Test of inequality
    	Method
    	Assignment 

Constructor is the answer you chose. There is nothing that makes it have to be a constructor though. You don't say the object is called Square. You can initialize on a newly created object outside of a constructor as well. Even if it was definitely a constructor, constructors are still subroutines, and constructors are still methods, even if that is not the normal usage of the term. One could make a case for assignment as well. Scratch that Q.  I agree with your comment about the constructor.  What makes it a contructor is really that it's being used in something like
var x=new Square(x, y, size);
causing the prototype.constructor property of the object to be set to the Square() function. But that's a pretty advanced thing to communicate to students in an introductory course. Perhaps I have over simplified.

As far as P=NP, it is not generally believed that P=NP, in spite of the fact that we lack a proof right now, so I'm not sure what the objection is. &amp;gt;As far as P=NP, it is not generally believed that P=NP, in spite of the fact that we lack a proof right now, so I'm not sure what the objection is.

Because it's not proven either way. You really shouldn't be asking your students to report the educated guess some people have made on a critical problem. It suggests to the students that the answer is far more meaningful than it is. As scientists we need to be critical of things like this, else where do we end up as a society when everyone says "this is too hard, lets just go with our gut feeling?" If you really want to ask a problem about P=NP, why not do something like:

The theory that are efficient solutions are possible for all known problems is know as:

    P=NP
    &amp;lt;choice&amp;gt;
    &amp;lt;choice&amp;gt;
    &amp;lt;choice&amp;gt; What I don't want to happen is that some poor bastard programmer tells his manager (one of my students) that he can't solve the travelling salesman problem efficiently, and to have his manager tell him, "that's BS.  There's an efficient solution for every problem, you're just not trying hard enough."  YMMV. Huh? If you educate them that it's unsolved that won't happen. And no student is going to say that anyway after only having taken this elementary course; I think you overestimate how much they'll remember from this dense course, especially about complexity theory.

Anyway, you'd rather them go on believing that P!=NP is fact? That's just as bad, whether it is more likely to be the answer or not.  Well, in class I explained that there were a whole class of problems that if an efficient (polynomial time) algorithm existed for one, then an efficient algorithm for any of them could be derived by mapping, and that since no efficient algorithm for any of these problems had been found, it was currently believed (but not proven) that no efficient algorithm for any of them was likely to exist.  But, I could easily see your point that they could lose all of this fine detail and jump directly to P!=NP from it. What I don't want to happen is that some poor bastard programmer tells his manager (one of my students) that he can't solve the travelling salesman problem efficiently, and to have his manager tell him, "that's BS.  There's an efficient solution for every problem, you're just not trying hard enough."  YMMV. What I don't want to happen is that some poor bastard programmer tells his manager (one of my students) that he can't solve the travelling salesman problem efficiently, and to have his manager tell him, "that's BS.  There's an efficient solution for every problem, you're just not trying hard enough."  YMMV. You should be teaching your students about the existence of approximation algorithms to NP problems, and to be able to tell their manager that they can't solve the problem optimally because a whole whack of smart theorists can't either. They don't need to have P != NP drilled into them, they can know that P ?= NP, but that until someone solves the problem, they should implement an approximation instead.  

It is better this then having them believe that P != NP is factual. The truth is, hunch or not, we don't know. I agree with your comment about the constructor.  What makes it a contructor is really that it's being used in something like
var x=new Square(x, y, size);
causing the prototype.constructor property of the object to be set to the Square() function. But that's a pretty advanced thing to communicate to students in an introductory course. Perhaps I have over simplified.

As far as P=NP, it is not generally believed that P=NP, in spite of the fact that we lack a proof right now, so I'm not sure what the objection is. I agree with your comment about the constructor.  What makes it a contructor is really that it's being used in something like
var x=new Square(x, y, size);
causing the prototype.constructor property of the object to be set to the Square() function. But that's a pretty advanced thing to communicate to students in an introductory course. Perhaps I have over simplified.

As far as P=NP, it is not generally believed that P=NP, in spite of the fact that we lack a proof right now, so I'm not sure what the objection is. I agree with your comment about the constructor.  What makes it a contructor is really that it's being used in something like
var x=new Square(x, y, size);
causing the prototype.constructor property of the object to be set to the Square() function. But that's a pretty advanced thing to communicate to students in an introductory course. Perhaps I have over simplified.

As far as P=NP, it is not generally believed that P=NP, in spite of the fact that we lack a proof right now, so I'm not sure what the objection is. &amp;gt; As far as P=NP, it is not generally believed that P=NP, in spite of the fact that we lack a proof right now, so I'm not sure what the objection is.

I go to a top CS research school and one of my algorithms profs said that several top researchers in theoretical computer science hold the opposite opinion, and many researchers think that P!=NP is unprovable (or at least will never be proven).  Do they also believe that P=NP is proveable, or do they merely think that neither assertion can be proven? Do they also believe that P=NP is proveable, or do they merely think that neither assertion can be proven? Cool. I got like 50%. I've been programming professionally for 6 years and graduated summa cum laude from a top CS program :D I didn't know any of the testing terms and blew it on other vocab type stuff. All the actual code and algorithm type questions I got. 

I don't like this question:

It is believed that efficient solutions are possible for all known problems.

    	False
    	True 

You are basically asking your students whether P=NP. That's just... not good. I guess, yes, it is *believed* that P!=NP, but asking it in a question like this makes it sound like there is actually a good proof that supports this notion, and there is not. 

I also don't like this question:

The following Javascript code defines a function that initializes some properties on a newly created object:

function Square(x, y, size)
{
  this.x=x;
  this.y=y;
  this.size=size;
}

This function is an example of:

    	Constructor
    	Subroutine
    	Test of equality
    	Test of inequality
    	Method
    	Assignment 

Constructor is the answer you chose. There is nothing that makes it have to be a constructor though. You don't say the object is called Square. You can initialize on a newly created object outside of a constructor as well. Even if it was definitely a constructor, constructors are still subroutines, and constructors are still methods, even if that is not the normal usage of the term. One could make a case for assignment as well. Scratch that Q.   &amp;gt;The time required to insert an item into a tree is:

Where in a tree? I can insert an item into a tree in O(1). I'll just put the existing tree as the left child of a new root node containing the new item. You seem to mean a tree where only the leaves have values. This was in the context of a sorted, fairly well balanced binary tree.  Out of context, the answer could easily be O(1), O(log n), or O(n).  Of, course, you don't have all the context you'd need for some of these questions, making the test less useful for you. I think you should clarify that its a balanced/red-black tree and that you mean worst case. Otherwise that question is just a trick. The correct answer is O(n) unless you clarify, and teaching wrong things is bad in my book. This was in the context of a sorted, fairly well balanced binary tree.  Out of context, the answer could easily be O(1), O(log n), or O(n).  Of, course, you don't have all the context you'd need for some of these questions, making the test less useful for you. &amp;gt;The time required to insert an item into a tree is:

Where in a tree? I can insert an item into a tree in O(1). I'll just put the existing tree as the left child of a new root node containing the new item. You seem to mean a tree where only the leaves have values.   I feel like you went for quantity over quality.  "A module is a subroutine."? Hmm. Seems to me a module is more than just a subroutine. Another: "A cluster is a group of computers that share processing." What do you mean by they "share processing"? It would be better to say that they a problem can be given to a cluster, and each member will work on the problem at the same time. Or something along those lines. Cluster members don't usually share anything except a network...  I'm sorry, I really think this isn't a good quiz.  For example,

 6 is unclear, 

7 is highly debatable as most people agree that tight coupling is typically bad and leads to many problems,  

8 depends solely on industry term knowledge, 

9 is just annoying to answer, 

12 is just a loaded question (of course certain problems come up over and over again...they're called libraries), 

17 i personally think is wrong, as my opinion of "the best testers" includes those who test before they are done coding, 

18 i think is BS, but if you cover it in your class...i guess it fits, and 

19, i think that you can do "top down design" and have quite modular programs. 

 sorry for the rant, but this is the kind of stuff that i didn't like in school.  Is it right by the textbook? Sure maybe.  But a lot of this doesn't hold in actual programming.  aight lemme try this again. looks like the questions are randomized too so ill just type up what i can remember.

When you write a program using the Windows API, Microsoft gives you a different interface than they use themselves for Microsoft Office development.
 - i think is BS, but if you cover it in your class...i guess it fits

actually looking through a whole new set of questions kinda discourages me.  but honestly, even most of these i find problems with at some level or another.  PM me if you want more details, but overall...meh.  no offense intended, but just mah feelings &amp;gt; When you write a program using the Windows API, Microsoft gives you 
&amp;gt; a different interface than they use themselves for Microsoft Office 
&amp;gt; development. - i think is BS, but if you cover it in your class...i guess 
&amp;gt; it fits

This one is definitely false.  Microsoft absolutely does not provide a different interface for Office development than they provide to 3rd party developers.  So, why do you think the question is saying something different? my point was that i kinda think its one of those questions right out of the textbook, and doesn't really have any application except to see if they read that chapter of the textbook I'm sorry, I really think this isn't a good quiz.  For example,

 6 is unclear, 

7 is highly debatable as most people agree that tight coupling is typically bad and leads to many problems,  

8 depends solely on industry term knowledge, 

9 is just annoying to answer, 

12 is just a loaded question (of course certain problems come up over and over again...they're called libraries), 

17 i personally think is wrong, as my opinion of "the best testers" includes those who test before they are done coding, 

18 i think is BS, but if you cover it in your class...i guess it fits, and 

19, i think that you can do "top down design" and have quite modular programs. 

 sorry for the rant, but this is the kind of stuff that i didn't like in school.  Is it right by the textbook? Sure maybe.  But a lot of this doesn't hold in actual programming.  The questions are in random order, and the answers are in random order each time you load the quiz, so I'm not sure what questions you're addressing.  I'm sorry you went to the effort for nothing, because I really would have liked knowing what your issues were.

The only one I can tell for sure: Yes, top down design DOES usually result in quite modular programs, and if any of the questions/answers seemed to indicate that it didn't, that's a real problem.  &amp;gt; The time required to insert an item into an unsorted array is: [O(1)]

This seems ambiguous. It's unclear whether or not the array is already full. I assumed it wasn't, in which case O(n) would be needed for a reallocation (edit: and copy). You maybe want to say "The time required to insert an item into a non-full unsorted array", or something like that which gets rid of the ambiguity without making the answer too obvious.

Also, UI tip: I'd think the "Check all" and "Uncheck all" should be buttons, as opposed to check boxes themselves.  Thank you for your comments and feedback on the practice test.  I have made the following changes:

Q|The time required to insert an item into a tree is:
A|O(log n)
changed to
Q|The time required to insert an item into a well-balanced, sorted binary tree is:
A|O(log n)

Q|It is believed that efficient solutions are possible for all known problems.
A|False
changed to
Q|There are no known efficient solutions for some problems.
A|True

Q|The following Javascript code defines a function that initializes some properties on a newly created object:
    function Square(x, y, size)
    {
      this.x=x;
      this.y=y;
      this.size=size;
    }
This function is an example of:
A|Constructor
changed to
Q|The following Javascript code defines a function that initializes some properties on a newly created object:
    function Square(x, y, size)
    {
      this.x=x;
      this.y=y;
      this.size=size;
    }
    
    function f()
    {
      ...
      var s=new Square(2, 3, 3);
      ...
    }

The function Square() is an example of:
A|Constructor

Q|A term for how easily a module can be picked up and transported to a new program is:
A|Cohesion
changed to
Q|This is a term for how strongly related each piece of functionality within a module is:
A|Cohesion

Q|The time required to insert an item into an unsorted array is:
A|O(1)
changed to
Q|The time required to insert an item at the end of a non-full, unsorted array is:
A|O(1)

Q|You are designing a new media player.  What type of control should you use to let the user browse music?
A|Music.png
changed to
Q|You are designing a new media player.  What type of control should you use to let the user browse a collection of music?
A|Music.png

Q|You are designing a new media player.  What type of control should you use to let the user browse video?
A|Video.png
changed to
Q|You are designing a new media player.  What type of control should you use to let the user browse a collection of video?
A|Video.png

Q|You are designing a new media player.  What type of control should you use to let the user play media?
A|Play.png
changed to
Q|You are designing a new media player.  What type of control should you use to let the user play an individual song or movie?
A|Play.png  A term for how easily a module can be picked up and transported to a new program is: Cohesion?

From what I learnt, cohesion is about if the interface of a class is dedicated to a specific, common task. Usually, this mean low coupling. Coupling is the term for how hard a module can be picked up and transported to a new program, no? Both strong cohesion and loose coupling affect how easily a piece of software can be reused, but coupling has more effect on the interface to the reused code, while cohesion has more of an effect on how easy it is to tease out individual units for reuse. Sorry, that seems odd from my understanding as well. From [Wikipedia](http://en.wikipedia.org/wiki/Coupling_\(computer_programming\)#Coupling_versus_Cohesion): "Coupling talks about the interdependencies between the various modules while cohesion describes how related functions within a module are." So, cohesion is about the interface (how related the functions are), and coupling is about how connected a module is to other modules (just from the definition). If a module is highly connected (high coupling), I would then reason that it's more difficult to "tease out" as an individual unit. If a module is not connected at all to other modules (low coupling), it is easy to transport to another program. It would seem to me that "cohesion" as defined here (how related the functions in a module are) will have no effect on how easy it is to reuse the module -- it just depends on what the module's API actually does. A term for how easily a module can be picked up and transported to a new program is: Cohesion?

From what I learnt, cohesion is about if the interface of a class is dedicated to a specific, common task. Usually, this mean low coupling. Coupling is the term for how hard a module can be picked up and transported to a new program, no? I've been thinking some more about your objection, and I think part of the problem I've been having is that here's the definition of cohesion from the class text I've been working out of.  It sounds like you don't agree with this definition:

"It would be nice if when we are asked to write a new application, we could find some of the code that we need already written in an existing application. We could just &#8220;steal&#8221; the code and paste it into our new  application. Why reinvent the wheel? There isn&#8217;t any copyright violation if the &#8220;lifting&#8221; of code is done from a company owned program (as long as it is the same company). Being able to reuse code like this saves a lot of time, both from the standpoint of not having to design and code the module and also from not having to fix &#8220;bugs&#8221; and errors because the code is already working in another program (there may have to be a little modification of the code to get it to work in the new program). In fact nowadays with the innovation of object-oriented languages like C#, Java, VB.Net and C++, this is the most important reason to module code.

"Some code is more transportable than others. We say that the more transportable code is, the more cohesive it is. The more work that is involved making a &#8220;stolen&#8221; module function in a new program, the less cohesive it is. There are a number of things that can cause code to be less transportable. Another way to define cohesive code is that cohesive code can almost &#8220;stand alone&#8221;. It isn&#8217;t as dependent on other code. Code, in order for it to be useful must interface (depend on) with other code. While it is true that pure cohesive code doesn&#8217;t depend on other modules of code, no one writes code that is purely cohesive. The best way to visualize this is to think about cohesive code as being a spectrum. Think of a scale from zero to ten, where zero is code that is not cohesive at all and ten is pure cohesive code (see figure 4.5). On this scale, zero through five might be code that is too much trouble to transport. It would involve so much work in rewriting the code to get it to function in the new location that you might as well write the code from scratch. This code is not cohesive enough to bother transplanting it. Six on this scale would take considerable work to make the code fit into another program, seven a little less work, eight would take very little work and nine would function in a new program with no changes at all. Obviously it would be best if you could always write code that is nine on the cohesive scale. Anything in the code that is specific to an application lessens the cohesiveness of the code. [etc.]"

I can see how if you're not a fan of this definition, you wouldn't like the question either.  I have to think about the right way to handle this issue.  Thanks for your feedback, I appreciate it.  88/117. Not too bad, especially since I have never spent any time formally learning software engineering, OOP, or JavaScript. was it magic then?  I got a question wrong but, there's two right answere:
	

if profit==0 then
  output profit in black
else if profit&amp;gt;0 then
  output profit in green
else
  output profit in red
endif

if profit&amp;lt;0 then
  output profit in red
else if profit==0 then
  output profit in black
else
  output profit in green
endif

Both are identical, but only one is "right"   cool.  I'm taking your class this summer!!

edit: Damn that is a lot... nice...

edit 2: Holy crap, I actually don't know all these answers already... yay!!  (Based off all the other classes I've been taking at PCC, I had really low expectations)

edit 3: 45%,  cool, class is going to be educational. @treecoder in retrospect, I should have focused on the basics and algorithm design more. But, a lot of students are taking the course as a requirement for a business degree, so I tried to cover topics I thought would be more relevant for those students, such as basic vocabulary, development planning and software management. Next term, I'm teaching the course more "vanilla," and then I'll have a better idea what works and what doesn't.
  [deleted]  This is pretty cool stuff! Do you have a textbook you use or an easily transferable list of materials so we can brush up on our skills?     I'd be interested in seeing the text for this course since it seems to follow a pretty unconventional way of teaching.

I would like to see more algorithm based questions in there. Usually you can get the best out of your students (including business students) if you get them out of their comfort zone.

Also the practice test seems to repeat questions quite a lot. Might be a good idea to include something that prevents this.  You are designing a new media player. What type of control should you use to let the user browse music?

I put the slider because you can then scroll through music, the correct answer was a jpeg of a note -_- The point that I was hitting on in class is that good interfaces are consistent with the user's mental model of the task, and with existing applications.  When you launch Windows Media Player, the first thing you see is a bunch of icons for different types of media you can browse, including the notes (music) and the film strip (videos). I can see it from both our point of views undoubtedly! I just figured if it confused me it will likely confuse others too.  Agreed. Instead of "to browse music" it should be something like "to choose a music function".    Nice quiz!

* Not being a native speaker, i never heard of the term "Eat your own dog food".
* "Which software development model is most likely to result in bug-free code?" -&amp;gt; Extreme Programming.
Are you sure of that? My professor said, that it was a hype but long time studies can't confirm that. Nice quiz!

* Not being a native speaker, i never heard of the term "Eat your own dog food".
* "Which software development model is most likely to result in bug-free code?" -&amp;gt; Extreme Programming.
Are you sure of that? My professor said, that it was a hype but long time studies can't confirm that.    </snippet></document><document><title>Bitcoin is Worse is Better</title><url>http://www.gwern.net/Bitcoin%20is%20Worse%20is%20Better</url><snippet> </snippet></document><document><title>Is there any interest in a Computational Geometry subreddit?</title><url>http://imgur.com/x6TtZ</url><snippet>  What are some interesting things (e.g. problems, theorems, algorithms, etc.) about computational geometry? Tessellation and polygonal collision detection are the big problems that come to mind when I think of computational geometry. Those are both interesting problems. I'm a big fan of mesh generation, surface reconstruction, graph drawing, and various geometric data structures. There are tons of useful applications as geometric problems come up all the time in graphics, manufacturing, robotics, and engineering. Tessellation and polygonal collision detection are the big problems that come to mind when I think of computational geometry.    I'll go with no. It's not like /r/CompSci has 500,000 subscribers and is covered in memes and image macros.   It seems silly to splinter off a new subreddit that will have an even smaller number of readers. The benefit is that more minor developments in the field or very technical material would be more appropriate in a more specific subreddit. I could think of plenty of situations where /r/compgeo would be interested in a submission that the vast majority of /r/compsci subscribers wouldn't care about.  It seems like there is sufficient interest, so I went ahead and made [r/compgeom](http://reddit.com/r/compgeom). [deleted]         </snippet></document><document><title>Let's make a movement.  Teach computer science in schools!</title><url>http://wh.gov/Rir</url><snippet>  Fuck that, just teach logic.  Logical fallacies.  How to spot bad rhetoric.  U.S. public schools need more of that shit.    I'm not convinced that this is a necessarily good idea.

I had high school CS ("software design and development") classes, and they absolutely sucked. I got stuck writing VB6 code for 2 years. On top of that they made us do some bullshit project management things, like learning the differences between waterfall/prototyping/whatever. The curriculum was stuck in the 90s.

So while the classes boosted my grades, it was a complete waste of time. It might have been useful for people who were *not* going to continue with CS in college, but for me it was a complete waste of time. I'm not convinced that this is a necessarily good idea.

I had high school CS ("software design and development") classes, and they absolutely sucked. I got stuck writing VB6 code for 2 years. On top of that they made us do some bullshit project management things, like learning the differences between waterfall/prototyping/whatever. The curriculum was stuck in the 90s.

So while the classes boosted my grades, it was a complete waste of time. It might have been useful for people who were *not* going to continue with CS in college, but for me it was a complete waste of time. I also don't think this would be a great idea, albeit for different reasons. 

Computer programming is certainly important, but I'm not sure I'm convinced it's important enough to teach indiscriminately  to everyone.

As programmers our jobs are to build abstractions. You see this *everywhere* in real life. Automobiles are just fancy abstractions for controlling an incredibly complex set of machinery. Microwave Ovens are incredibly powerful abstractions, I'm sure the physics involved can get pretty complex.

Teaching logic and the art of abstraction is definitely useful: but programming isn't for everyone. It's a very tough subject, and I think a lot of people forget how much knowledge we have internalized about the field. Plus I think it ultimately runs contrary to what we, as programmers, are trying to do. We are building abstractions that *make* the iPad seem like a black box to the untrained eye. The iPad is only as fluid as it is because it has to be *so simple* that anyone, kids to senior citizens, can pick it up and start using it.

I'm just wary of "&amp;lt;x&amp;gt; should be taught in schools" posts in general. I know a lot of welders, and they all think welding is incredibly important, but they don't think it should be taught in K-12 schools.

Not to mention do you really want to indoctrinate an entire generation with the current programming trends? The implications of altering core curriculum are fairly staggering. This argument makes some sense, but looking back on high school... most of it was a complete waste of time, so even though there are arguments against CS, I don't think it's a completely terrible idea... It took me a good half an hour to even remember I had to take a shop class in middle school... How much of those 2 (or was it 3?) years of classes was useful to me? Exactly none.

I don't know if teaching CS is the best idea, but having some amount of programming in a science curriculum in middle school might be good, not to get kids up to speed with current trends, but to give them some exposure.

I don't think teaching kids that computers are magic black boxes is the best way forward; just like we teach kids how a light bulb or an engine works, why not teach them how computers work? You say that shop classes were not useful.

Let's use auto shop as an example: an automobile is practically a black box for all intents and purposes. If you thought that learning about *that* particular abstraction was a waste of time, then why would teaching someone about comp sci be worthwhile?

I guess on the flip-side there are plenty of people who got a lot of value out of shop classes, and so perhaps the exposure is important... but I think there's a better time and place for that kind of class. (As a summer program, not as a piece of core curriculum.)

 I went to school in Australia, so we didn't have autoshop, knowing how my car works would actually be pretty useful, since having someone else tell you what is going wrong with it usually costs you time &amp;amp; money.... What is the value of wood shop or metal shop unless you go and do that as a job? You probably don't even have the tools to do any of it again.

Computers, like cars, are something people are going to have to deal with in their lives for a long time to come, why not show them the basics?

There's a surprising number of people who end up 'programming' in Excel who never went through a CS curriculum because Excel is the tool they know how to use, why not expose them to some programming before that?

I also feel like as software (in particular all these 'apps') gets easier to develop, the more this makes sense in a core curriculum, since it would be more realistic for someone with no CS experience and no real interest in programming to bang something together if they really need it.   You should have linked to this:

http://csta.acm.org/Advocacy_Outreach/sub/CSEA_Fact_Sheet.pdf

As the description of the petition on the site you linked to is ~~extremely~~ a little vague (the fact sheet is two links deep from your link). On the other hand I totally agree.

Good luck.     </snippet></document><document><title>Diseased disciplines: The strange case of the inverted chart</title><url>http://lesswrong.com/lw/9sv/diseased_disciplines_the_strange_case_of_the/</url><snippet>  This happens far too often in all fields. To call this one a "diseased discipline" is to entirely miss the point. Don't blame software engineers (or doctors, with whom this happens). Don't blame the player, blame the game - and then change it to disallow cheating. Change academia so that the people that do this face reputation hits, disciplinary action, and, if the offense is severe enough, being beaten with a stick.

All well and good, you say, but how exactly do we do this?

I have no idea. I'm sorry.

*Edit*: the person that wrote this article should be ashamed for trying to sensationalize a misleading conclusion ("software engineering is a diseased field") when his misleading conclusion is about misleading use of citations. This article is the content of one chapter of a book containing many instances of various kinds of poor scholarship in software engineering&#8212;no doubt other fields (famously, medicine) have that problem too but he's setting out to illustrate it with reference to software engineering specifically. Out of the context of the rest of the book the claim does seem hyperbolic, I agree.  The claim is hyperbolic regardless as software engineering is not an academic field. I responded as a devil's advocate just because I was bored; the whole article was barely worth giving a moment's consideration before being dismissed. And we are all _so_ glad that you stopped by to share the results of your boredom. You agreed with me and I don't see any other top-level comments, so you can fuck right off, Keith. But look at those delicious negative votes on your replies. The people have spoken. Does it make you giddy that a few strangers agreed with you? If that's what you're after, congratulations. I'm here for interesting links and discussions and nobody has refuted my arguments here, so I couldn't care less whether I'm downvoted. If anything, it reflects poorly on the subreddit that people downvote a reasoned claim without providing any justification for why it is wrong.

So again, you can fuck right off, Keith.</snippet></document><document><title>Theoretical CS concepts for 8 - 14 age group</title><url>http://cstheory.stackexchange.com/questions/10365/concepts-in-theoretical-cs-that-would-be-approachable-ages-8-14</url><snippet /></document><document><title>I'm a TA for an intro to computing class. I'm looking for a numerical method that implements simply in code [xpost from r/math]</title><url>http://www.reddit.com/r/compsci/comments/qzm88/im_a_ta_for_an_intro_to_computing_class_im/</url><snippet>We just learned about recursion, so I thought I'd show students a basic Newton-Raphson method for finding zeros. Any other algorithm that would be appropriate to show first year students?

EDIT: from the xpost, I've gotten a few more ideas such as demonstrating the Collatz conjecture, Euclidean Algorithm, RSA encryption. Would like more!

EDIT: I'm getting a lot of good suggestions, thanks everybody! I should have mentioned this is an intro to computing class for engineers, so some of the more CS-y stuff may go over their heads. They're also first years and most are still in Calculus I, and may not be familiar with even basic number theory. Even though, keep em comin'!  You can cover greatest common divisor via [Euclid's method.](http://en.wikipedia.org/wiki/Greatest_common_divisor#Using_Euclid.27s_algorithm)

Covers recursion, and is very simple.  I would definitely not start with it this collapsed.

    def gcd(a,b):
        if b==0: return a
        return gcd(b, a%b)

You could start with an overly-complex recursive definition and pare it down to the elegant form above.

    def gcd(a,b):
        if a==b: return a #or b.  Doesn't matter.  
            #(remove this line because it will be returned when we get 
            #a%b==0 in the next step)
        if a &amp;lt; b: return gcd(b,a) #or (a,b)=(b,a) want b less than a 
            #(remove because: if a is less than b, a%b will just be a, 
            #and we will be calling it in the recursion anyway)
        if a&amp;lt;0 or b&amp;lt;0: return gcd(abs(a), abs(b)) #don't be passing negative numbers around 
            #(remove because: you don't actually need to rectify this. 
            #% returns a positive number anyway.  Good time to discuss 
            #the properties of mod in your language.  Some languages 
            #force it to be positive and coerce division so that a*(b/a) +b%a == b 
            #for all numbers, even negative ones.  Some let mod be 
            #negative if certain arguments are negative.  Some (stares at 
            #C/C++) let it be implementation defined)
        m = a%b
        return gcd(b, m)


or the iterative form:

    def gcd(a,b):
        #same base-casing as above, except without recursion
        while(a!=0 and b!=0):
            temp = a%b
            a=b
            b=temp
            
 I'm sure you are aware but Python doesn't need a temp variable to swap two variables.

    a, b = b, a%b

Works perfectly fine. not many languages need a temp variable, provided that you can xor... Could you give me an example in, say, C? I'm fresh out of intro and my professor said that it was impossible. I assumed any language that supported not needed a temp variable would have specifically written the trick in. http://en.wikipedia.org/wiki/XOR_swap_algorithm

There's an example in C I'm sure you are aware but Python doesn't need a temp variable to swap two variables.

    a, b = b, a%b

Works perfectly fine.  You could implement the square root function. That's always pretty neat.   Surprised no one has suggested the Towers of Hanoi, which is a pretty canonical recursive problem. There are also simpler ones, like generating Fibonacci numbers (which lets you talk about nasty time complexity, if you like). fibonacci is easy! [deleted] what's more straight forward than:

    def fib(n):
        if n &amp;lt;= 1: return 1
        return fib(n-1) + fib(n-2)

Am I right? what's more straight forward than:

    def fib(n):
        if n &amp;lt;= 1: return 1
        return fib(n-1) + fib(n-2)

Am I right? Surprised no one has suggested the Towers of Hanoi, which is a pretty canonical recursive problem. There are also simpler ones, like generating Fibonacci numbers (which lets you talk about nasty time complexity, if you like). Surprised no one has suggested the Towers of Hanoi, which is a pretty canonical recursive problem. There are also simpler ones, like generating Fibonacci numbers (which lets you talk about nasty time complexity, if you like).  How about something simple like the Sieve of Eratasthenes? It's really easy to visualize and doesn't require recursion unlike the Towers of Hanoi example. 

tl;dr Primes are cool. Primes ARE cool. Maybe for kicks, I'll show them how to get prime factorization, totient function, and encryption.  How about something simple like the Sieve of Eratasthenes? It's really easy to visualize and doesn't require recursion unlike the Towers of Hanoi example. 

tl;dr Primes are cool. but he wants recursion.  he said they just learned it How about something simple like the Sieve of Eratasthenes? It's really easy to visualize and doesn't require recursion unlike the Towers of Hanoi example. 

tl;dr Primes are cool.  Calculating factorials is an easy one. This can also be used as a good demonstration of tail-recursion. This can also be used as a good demonstration of tail-recursion. Calculating factorials is an easy one. Do this one. Don't confuse first-time programmers with complicated math. Recursion is a tricky enough concept by itself. Exactly. This is an easy intro because it can be demonstrated on a calculator, the white board or paper. The big difference is that the recursion can't be as easily explained via those media. 


OP: Do calculating factorials. You need to make sure that it's not just easy code, but _easy to understand_. There are a lot of good suggestions here, but not many are as easy to understand as factorials.  The 4th order Runge-Kutta method is only a handful of lines of code and beautifully demonstrates the power of intelligent sampling. Not precisely an algorithm, per se, but it was pretty neat when I first saw it.   [Euclid's Algorithm](http://en.wikipedia.org/wiki/Euclidean_algorithm) is a favorite of mine, it's easy and it's something I really wished they had mentioned in grade school.   How about binary search? That is pretty darn simple to implement. Doesn't work for every function though. [Actually, binary search is *surprisingly* hard to implement correctly.](http://googleresearch.blogspot.com/2006/06/extra-extra-read-all-about-it-nearly.html) No, seriously. 

Quote from the link: "While the first binary search was published in 1946, the first binary search that works correctly for all values of n did not appear until 1962" [Actually, binary search is *surprisingly* hard to implement correctly.](http://googleresearch.blogspot.com/2006/06/extra-extra-read-all-about-it-nearly.html) No, seriously. 

Quote from the link: "While the first binary search was published in 1946, the first binary search that works correctly for all values of n did not appear until 1962" &amp;gt; Specifically, it fails if the sum of low and high is greater than the maximum positive int value (2^31 - 1).  
  
Oh shut the fuck up. You only need a few more than a billion elements to start hitting this edge case. That's not really all that many. No, you only need them all to be large. If you have a bunch of elements, some small and some large, you won't run into this issue. No, you only need them all to be large. If you have a bunch of elements, some small and some large, you won't run into this issue. You _probably_ won't run into this issue, you can have bad luck and get two large values. uint64_t is a better solution than whatever crap is posted there. Seriously? 

There is a legitimate fix for the algorithm that doesn't matter what data type you use and you feel like "USE BIGGER NUMBER" is better?   first year we did a sudoku  solver. our teacher gave us the api and we just ahd to code according to it. It was pretty neat as a first programming assignment first year we did a sudoku  solver. our teacher gave us the api and we just ahd to code according to it. It was pretty neat as a first programming assignment I too am curious. Our professor just told us to solve sudoku and pushed us out of the nest... let me see if i can find it, but he literally did the whole the outline of the logic for us, in the api he pointed out all  the methods, parameters, pre conditions, and what the thing was supposed to do and what it returned. The whole thing worked using recursion, it was not the most efficient way of doing and it did take me a while to figure out how it worked but i felt so proud of myself when i finished it. He even added a scripplet that plugged into our code that would display the whole thing nicely. It was nothing fancy but it was just so we wouldnt have to check txt outputs.       I'd suggest the [Euclidean Algorithm](http://en.wikipedia.org/wiki/Euclidean_algorithm) for finding the GCD of two numbers.

Some pseudocode:

    def gcd(a,b):
        if b == 0:
            return a
        return gcd(b, a%b)

I love this algorithm! If you wanted, you could talk about it's relationship to RSA Encryption.
   interpolation. it'll help them out for thermo. Or taylor approximations.         QR algorithm is like 3 lines of matlab.             Matrix multiply?</snippet></document><document><title>A non-mathematical programmer oriented series on how regular expressions work</title><url>http://binarysculpting.com/2012/02/11/regular-expressions-how-do-they-really-work-automata-theory-for-programmers-part-1/</url><snippet> </snippet></document><document><title>How to Multiply Using Addition and Logs (A smart CS guy did this initially for mechanical cash registers back in the day)</title><url>http://youtu.be/m98FpD0XxUs</url><snippet>  &amp;gt;smart CS guy

You mean a mathematician - [John Napier](http://en.wikipedia.org/wiki/John_Napier)

That is the method that allows you to multiply on a slide rule.

And it far predates mechanical cash registers.

EDIT: Also...

There is an easier way in binary. It's called Peasant's Multiplication. http://en.wikipedia.org/wiki/Peasant_multiplication

For a computer Shift and Add is even easier. http://en.wikipedia.org/wiki/Multiplication_algorithm#Shift_and_add  I'm a little confused over why this is a big deal. Don't people learn logarithms in school anymore? Or is it directed at younger students?

Depressing fact - they made us use log tables to do complicated calculations for science classes back in high school. As with most aspects of the Indian education system, they lost sight of the point, and started taking off points if you avoided the logs with clever math tricks or noticing neat relationships between the numbers.  &amp;amp;#3232;\_&amp;amp;#3232; I'm a little confused over why this is a big deal. Don't people learn logarithms in school anymore? Or is it directed at younger students?

Depressing fact - they made us use log tables to do complicated calculations for science classes back in high school. As with most aspects of the Indian education system, they lost sight of the point, and started taking off points if you avoided the logs with clever math tricks or noticing neat relationships between the numbers.  &amp;amp;#3232;\_&amp;amp;#3232;  10^5 = 10,000? 10^5 = 10,000? </snippet></document><document><title>Judea Pearl Wins ACM A.M. Turing Award for Contributions that Transformed Artificial Intelligence
        &#8212;
        Association for Computing Machinery</title><url>http://www.acm.org/press-room/news-releases/2012/turing-award-11</url><snippet /></document><document><title>Judea Pearl - A.M. Turing Award Winner</title><url>http://amturing.acm.org/award_winners/pearl_2658896.cfm</url><snippet> </snippet></document><document><title>What would this "Languages and Computation" class be like?</title><url>http://www.reddit.com/r/compsci/comments/qy9hb/what_would_this_languages_and_computation_class/</url><snippet>Here's the course description:

&amp;gt;**CS 3240 - Languages and Computation**

&amp;gt;Interpreters as abstract machines and the tools used to construct them, such as scanners and parsers. An introduction to models of computation as embodied by different programming languages. Limits of and relationships between these models.


And an outline of topics:

&amp;gt;Lexical analysis, scanners, pattern matching

&amp;gt;Regular expressions, DFAs, NFAs and automata

&amp;gt;Limits on regular expressions, pumping lemma

&amp;gt;Practical parsing, LL and LR parsing

&amp;gt;Context-free languages, grammars, Chomsky Hierarchy

&amp;gt;Pushdown automata, deterministic vs. non-deterministic

&amp;gt;Turing Machines, Decidable vs. Undecidable problems, Halting Problem


It sounds like an interesting class. I'm doing a CS minor primarily to take more theoretical math-intensive courses, so in addition to algorithm analysis and advanced algorithms, this seems like a good fit. What should I expect from this class? Would it be too much to handle if the only real CS courses I've had are intro OOP, intro databases, and data structures/algorithms?

Also, what's weird is the prerequisite:

&amp;gt;CS 2340 - Objects and Design

&amp;gt;Object-oriented programming methods for dealing with large programs. Focus on quality processes, effective debugging techniques, and testing to assure a quality product.

Is it just me, or do these classes have nothing to do with each other?  I think you're correct that these classes have very little to do with each other.  The prereq at my school for this class was just mathematical maturity (can you do proofs?).  As long as you're good at math, you should be fine, even if you've never even touched a computer before. I think you're correct that these classes have very little to do with each other.  The prereq at my school for this class was just mathematical maturity (can you do proofs?).  As long as you're good at math, you should be fine, even if you've never even touched a computer before. I think you're correct that these classes have very little to do with each other.  The prereq at my school for this class was just mathematical maturity (can you do proofs?).  As long as you're good at math, you should be fine, even if you've never even touched a computer before.    Georgia Tech student, yeah?

CS2340 is completely unrelated to 3240. You can probably drop the prerequisite by talking to the CS advisers.

Your experiences in 3240 are going to be heavily dependent on the professor; ask some of the CS students which professors are good. If you enjoy theory classes, it should be a good choice. My understanding is that you only have one actual programming assignment, and it isn't something that great coding chops will help with.

You may also want to pop into [/r/gatech](/r/gatech) and ask some of the CS folks there.

Look into taking some of the graduate level theory classes if you are looking for a theory challenge. Computer Vision would also be a good choice if you meet the prereqs, as well as CS3600 AI. DSP over in the ECE department is another interesting class -- 101 ways to torture the Fourier Transform into doing what you want. The advisor said you can take a class without the prereq as long as the professor doesn't mind, and I don't think he will. I was considering taking 3600 in the fall. I actually took DSP last year, though that was the intro course and I think you're talking about the 4000-level one. I don't really think I want to delve into grad classes for this CS minor though lol.       I presume the prerequisite is a practical one about the languages used/discussed. For instance, the OOP one could be teaching you Object Oriented but one of the things that happens is you learn Java. Then in the CS 3240 they can assume everyone has a good knowledge of using Java, so they can dive right in and start talking about the JVM or whatever and practically discussing how the design of the Java language meshes with the sometimes-dry abstract knowledge of the Languages and Computation model. Maybe it's a coursework or something?
 Well I've had intro OOP and am taking data structures/algorithms now, which are both in Java. Well I've had intro OOP and am taking data structures/algorithms now, which are both in Java. Also, I'm assuming OOP = 1331, which is more of a "learn java class" not real OOD/OOA. That is taught in 2340.  I'm a CS major Georgia Tech, and have taken 2340. It's just meant to get you involved working on larger projects, in a group setting, and learning effective collaboration. It's kind of off that it's a prereq for 3240... a platforms class. 

I didn't read all the comments, but if your a minor in CS I don't see why you would take 3240 (though I have no clue what the minor requirements are) 

EDIT:
Look into the Theory thread if you haven't already. It should have what you're looking for. I'm not sure what the prereqs are for higher classes... prob Discrete for CS , and Combinatorics (MATH 3012)?</snippet></document><document><title>What is this data structure called?</title><url>http://www.reddit.com/r/compsci/comments/qxkz8/what_is_this_data_structure_called/</url><snippet>I have seen several implementations of them, I wrote some myself with slight variations.

It can be stored in a (doubly, singly) linked list or a vector (like in, array). Theoretically it could also be inside a number of different containers like trees etc.

They always contain an even number of elements. Those elements have an numeral type with a strict weak ordering. The elements are sorted, they cannot contain an element twice. In case of integral types, they can be considered something like a runlength compressed set. Like, if the data structure contains the elements [2,5,6,8], they represent the set [2,3,4,6,7]. Of each pair, the first is kind of an "on" value, and the second is an "off" value: the first one defines the first that is in the set, the second one is the first that is not in the set anymore. The principle can be (and is often) extended to non-integral types.

Typical operations run on them are boolean operation between two of them, including special cases that "add" or "substract" certain ranges of them, as well as a function that checks if a given number is inside the set. In case of an array as base structure, this query can be done in logarithmic time, in a list it takes linear time. Other operations that change the structure can be done "in place" if it's a list, but not (efficiently) if it's a vector in the general case.

There are variations (like allowing an odd number of elements to denote that infinity is inside the set and adding a bool that "inverts" the set, so that -inf can be inside, too). Another variation is that the "on" and "off" values that denote a range are sometimes crammed together into a "pair".

Those data structures can be found with a lot of different names. Just to name a few different ones, I came accross:

- interval list
- disjoint sorted interval list
- multi range
- range set
- multi nail

Since this kind of data structure can be helpful in a number of problem domains, it is quite strange that I never found a "generally approved" name for this kind of data structure.

Now two questions:

- Is there a generally accepted name for it, that I just never came accross?
- If not, or you don't know: what would you call it?

EDIT: Thanks for all the responses. I think I will go with pohatu's DSILs. If I would blog, I'd spread it :). But maybe I will create a wiki page at some point.   Interesting, I have used something like this, but didn't know the name either :P

I was able to find it under the name [Segment Tree](http://en.wikipedia.org/wiki/Segment_tree), but since it has 'Tree' in the name, it's specific to the representation. I guess, if it was stored as a list, I would call it a *segment list*.

Interestingly, I don't seem to see what happens in the segment tree algorithm when the segments are not disjoint, or if one is a subset of another. Do you think that algorithm still works without modification ?  I guess that would be the kind of questions perfect for http://cs.stackexchange.com, however they are still in private beta for a few more days, so you might not be able to ask the question now. I don't know how long they will stay in private, but that shouldn't be long now.     Could you name some problem domains for which this is useful?       </snippet></document><document><title>Searching and Sorting</title><url>http://www.reddit.com/r/compsci/comments/qy4aq/searching_and_sorting/</url><snippet>For an essay I am asked to write about what part of the history of computer science required the development of this topic. Could anyone point me in the direction of the history of the searching and sorting.  IIRC, it became a research topic around the time of [punch cards](http://en.wikipedia.org/wiki/IBM_card_sorter).  </snippet></document><document><title>Extracting blobs of bits out of an NxN 2-d array (or matrix if you prefer).</title><url>http://www.reddit.com/r/compsci/comments/qxwyt/extracting_blobs_of_bits_out_of_an_nxn_2d_array/</url><snippet>Hi all,
  I'm got an NxN matrix of 1s and 0s, not randomly distributed; instead, there are clumps of 1s (blobs).  I'm trying to find an efficient algorithm that can, given this matrix, calculate the areas of the blobs and, optionally, their centroid.  Anyone have leads?

Thanks all  What counts as a blob? 

How many blobs does this matrix contain?

    1 1 0 0 0
    1 1 0 0 0 
    0 0 1 0 0 
    0 0 0 1 1
    0 0 0 1 1 
   as cypherx showed, you have to be somewhat strict in defining what a 'blob' is. Probably can get over that by defining something stricter:
    1. a blob is any set of nodes that are nominally adjacent along **at most** one direction **and** contain more than 1 node.

so 

    1 1
    0 1


contains a 'blob'. But

    1 0
    0 1

contains no blobs. 


Next to find all 'blobs'

1. Create a set of all nodes
2. Create a set of checked nodes

3. scan through the array. placing each node in the checked node set as you go along.

4. If you encounter a 1 create a tentative set of nodes - this will be a tentative 'blob' set
5. Scan all of the neighbor nodes from step 4. Add each adjacent node to the set (using the previous definition).
6. For each neighbor node keep repeating 5 until you hit a node that has no neighbors not already in the tentative set.
7. Add the tentative step from 4 to a list of 'blobs' if the set of the tentative set is &amp;gt; 1


8. Do this for all nodes. 

Once you have a list of blobs you can find the centroid by any basic centroid algorithm based on their coordinates. You could even add weights to the nodes if you wanted to have fun... </snippet></document><document><title>Mario is hard, and that's mathematically official </title><url>http://www.newscientist.com/article/mg21328565.100-mario-is-hard-and-thats-mathematically-official.html?DCMP=OTC-rss&amp;amp;nsref=online-news</url><snippet>  Having certain subset/derivative of a game NP-hard is orthogonal to it being difficult for humans to play.  It's probably not orthogonal. I'm sure there is some degree of correlation.  Minimal. The actual level designs intentionally avoid this kind of hardness (or the actual game is an optimization problem, and this analyses only applies to completionism and speed-running, which would be generally relevant to the theoretical analyzes in these papers).


  I guess I would have to revise my statement to say with the same size input I think humans would have a harder times with instances of NP hard problems. Obviously some things we can do really well (like inverse optics), but I would venture a guess that given a random P problem and NP problem with the same size of input data, more often than not the P problem would be easier.  Having certain subset/derivative of a game NP-hard is orthogonal to it being difficult for humans to play.   We *must* know the mathematical difficulty of Battletoads. All other games should be graded by Battletoads difficulty.  No other game compares. What about TMNT?   The paper itself is a pretty easy read by academic standards: http://arxiv.org/abs/1203.1895 I think Theorem 7.2 in the paper is wrong.  
  
Specifically, it says   
  
&amp;gt; "To show that Pokemon with only enemy Trainers is in NP, note that once a Trainer has been battled, they become inert. Moreover, each actual battle is bounded in length by a constant, because eventually all Pokemon must expend all their PP for their moves and use Struggle, which damages themselves"

I'm not convinced by this. Firstly, if you assume that the game comes with a true RNG, things start getting extremely tricky once you have just a Mew with the attacks Swift, Dig, and Transform fighting against a Gengar (with just Dream Eater), Pidgey (with just Sand Attack), and a level 5 mew with the same abilities as you have. Your first 2 attacks can, in theory, always miss depending on your opponent's random substitution of Pokemon. Further, if you use Transform on the Mew, you reset each of your PP to 5, making the battle potentially endless.

If the game does not come with a true RNG, I'm still not sure that the battles necessarily take polynomial time. If the game uses something sufficiently complex as its seed (say, number of steps you've taken so far, which would be influenced by the size of the input game world), the PRNG could (in theory) have an exponentially-lengthed interval in which it would cause the trainer to engage in a Pokemon rotation battle with you (in the case where you both just have high-defense Pokemon who know just self-destruct), only to finally mess up after an exponentially-long period of time.

I'd imagine trainer-only Pokemon is likely in, say, &#928;_2^p or MA, but this (or any stronger claim) is still something I'd need to be convinced of (and I doubt it's a trivial proof). I think Theorem 7.2 in the paper is wrong.  
  
Specifically, it says   
  
&amp;gt; "To show that Pokemon with only enemy Trainers is in NP, note that once a Trainer has been battled, they become inert. Moreover, each actual battle is bounded in length by a constant, because eventually all Pokemon must expend all their PP for their moves and use Struggle, which damages themselves"

I'm not convinced by this. Firstly, if you assume that the game comes with a true RNG, things start getting extremely tricky once you have just a Mew with the attacks Swift, Dig, and Transform fighting against a Gengar (with just Dream Eater), Pidgey (with just Sand Attack), and a level 5 mew with the same abilities as you have. Your first 2 attacks can, in theory, always miss depending on your opponent's random substitution of Pokemon. Further, if you use Transform on the Mew, you reset each of your PP to 5, making the battle potentially endless.

If the game does not come with a true RNG, I'm still not sure that the battles necessarily take polynomial time. If the game uses something sufficiently complex as its seed (say, number of steps you've taken so far, which would be influenced by the size of the input game world), the PRNG could (in theory) have an exponentially-lengthed interval in which it would cause the trainer to engage in a Pokemon rotation battle with you (in the case where you both just have high-defense Pokemon who know just self-destruct), only to finally mess up after an exponentially-long period of time.

I'd imagine trainer-only Pokemon is likely in, say, &#928;_2^p or MA, but this (or any stronger claim) is still something I'd need to be convinced of (and I doubt it's a trivial proof).    This paper only proves that "you can design hard levels" in Mario. The levels you play in the actual game probably aren't that (NP-)hard. This paper only proves that "you can design hard levels" in Mario. The levels you play in the actual game probably aren't that (NP-)hard.  Could you pick up the turtle shells in the original Mario Brothers?  If so, I believe that there is an error in their "Crossover" construct for Mario Bros (Shown in Figure 6).  In order for the construct to work correctly, Mario must only be able to break the brick on the right if he enters on the left, and he must only be able to break the brick on the left if he enters from the right.  However, it is possible to jump on the turtle shell, pick it up, and then throw it in either direction. Could you pick up the turtle shells in the original Mario Brothers?  If so, I believe that there is an error in their "Crossover" construct for Mario Bros (Shown in Figure 6).  In order for the construct to work correctly, Mario must only be able to break the brick on the right if he enters on the left, and he must only be able to break the brick on the left if he enters from the right.  However, it is possible to jump on the turtle shell, pick it up, and then throw it in either direction. Could you pick up the turtle shells in the original Mario Brothers?  If so, I believe that there is an error in their "Crossover" construct for Mario Bros (Shown in Figure 6).  In order for the construct to work correctly, Mario must only be able to break the brick on the right if he enters on the left, and he must only be able to break the brick on the left if he enters from the right.  However, it is possible to jump on the turtle shell, pick it up, and then throw it in either direction.   Correction: the article states that if a problem is NP-complete, then that means that any problem in NP can be reduced to it. This is what NP-hard means (and is part of the definition of "at least as hard", as used in the description of NP-hardness).

NP-completeness just means that the "yes" answer has a short way to prove to everyone else that, in fact, "yes" is the right answer. For example, if you declare that point X in mario is reachable, then the short way to prove it to other people is to play the game and reach point X. (In some games, however, the playthrough might take exponentially long: such games would not be in NP, because the proof has to take little time) </snippet></document><document><title>Making a Connect 4 Ai, don't know where to start</title><url>http://www.reddit.com/r/compsci/comments/qx2ze/making_a_connect_4_ai_dont_know_where_to_start/</url><snippet>Hey guys,

I'm taking a C programming class for electrical engineering at my school, and for extra credit on our last and most important assignment, we have to create a good ai for connect4. Having never made a game algorithm, i'm not sure how to get started on this, and I really need the points. Any advice? 

Edit: It is an introductory C course, and as such, I am not familiar with the term 'depth'. Would anyone mind explaining in layman's terms?  Interestingly enough, I have a Connect4 AI written in C sitting right here on my desktop. You'll probably want to use a depth-first mini-max AI.

The basic steps to writing this are:

* Find an algorithm that determines the 'children' from a given state. Essentially, all the possible moves you can make.

* Find a heuristic that approximates how good a given state is. My suggestion (the one I've used) is to assign scores to various lengths of partially built rows of 4, then sum over all of them.

* Find an algorithm that determines when a game has ended.

* Running over a tree search, maximize the generated scores for player 1 (usually you), and minimize them for player 2, returning those. Also, [read Wikipedia](https://en.wikipedia.org/wiki/Minimax#Combinatorial_game_theory) (with pseudocode)

And that's basically it. If you have time, you might want to optimize it some. The best way to do this is probably with alpha-beta pruning, for which [Wikipedia](https://en.wikipedia.org/wiki/Alpha-beta_pruning) has both an excellent article and pseudocode. Interestingly enough, I have a Connect4 AI written in C sitting right here on my desktop. You'll probably want to use a depth-first mini-max AI.

The basic steps to writing this are:

* Find an algorithm that determines the 'children' from a given state. Essentially, all the possible moves you can make.

* Find a heuristic that approximates how good a given state is. My suggestion (the one I've used) is to assign scores to various lengths of partially built rows of 4, then sum over all of them.

* Find an algorithm that determines when a game has ended.

* Running over a tree search, maximize the generated scores for player 1 (usually you), and minimize them for player 2, returning those. Also, [read Wikipedia](https://en.wikipedia.org/wiki/Minimax#Combinatorial_game_theory) (with pseudocode)

And that's basically it. If you have time, you might want to optimize it some. The best way to do this is probably with alpha-beta pruning, for which [Wikipedia](https://en.wikipedia.org/wiki/Alpha-beta_pruning) has both an excellent article and pseudocode. how do you mean, "sum over all of them?" also, is the alpha_beta pruning useful if I'm not going for efficiency? By sum over all of them, I mean add up all the ones you find. Say, you find 2 lines of 3 of your color (which you could use to win), and you have some assigned score for 3-in-a-rows (call it x), then the heuristic score is x+x. This is just one suggestion for a heuristic scoring algorithm, you can use your own.

Alpha-beta pruning is generally not useful if you aren't going for efficiency, but it's very easy to implement (just copy the wiki pseudo-code as-is), provides a massive speed boost (cutting the branching factor from b to, on average, b^3/4 , and with decent move ordering, nearly to b^1/2 ), and thus can be used to build a significantly better AI (searching to depth 12 is significantly better than searching to depth 6). what is depth? Are you familiar with the concept of a tree (as a data structure)? If not, [read about it](https://en.wikipedia.org/wiki/Tree_data_structure) now - you won't get any further in any programming class without it. As it says on that page "The depth of a node is the length of the path to its root (i.e., its root path)."

A [depth-first search](https://en.wikipedia.org/wiki/Depth-first_search) is one which starts at the first child node of the root of a tree, then continues down the first child node of that node, and so on, until it hits a leaf node. Then it travels back until it can take the second child node of a certain node, and moves down that in the same fashion, repeating this process to access every leaf node in the tree.

In the minimax algorithm, you typically limit how many nodes down the tree you search to a certain number (otherwise you would be searching every possible game). This number is the depth of the search.

In the [minimax pseudocode](https://en.wikipedia.org/wiki/Minimax#Pseudocode) on the Wikipedia I previously linked, it makes specific reference to depth as an argument for the recursive function, decrementing it by 1 with each call, until it reaches 0, thus limiting the depth of the search.

When you search to a larger depth, your AI will see more moves, and thus will play better. so you are speaking of a tree in terms of the number of moves ahead I will be checking right?
 
If this is the case, I would like to first, if you have the time, ask for help in how to check the parent node, or any node for that matter. Right now, I am trying to divide checking the node up into horizontal, vertical, and diagonal components, and set up some sort of point system to evaluate the spaces around each potential move. I'm only on the horizontal check, and i'm already stuck and am not sure how to set up the point system.  You mean the heuristic algorithm for scoring? You can really set up the point system however you like - there's not really a definitive, objective, (non-exponential time) algorithm which will tell you the score of a node. Whatever happens to play the game best - you can mess around with it.

My heuristic can be represented more-or-less as follows (in python-esque pseudocode):

    def score(node):
        r = 0
        for every line of 4 in the node:
            if every item in the line is the same color or empty:
                value = 1 &amp;lt;&amp;lt; 4*(number of that color in the line) // ( 16^n )
                if color is you: r += value
                else: r -= value
        return r

This heuristic will definitely involve splitting it into horizontal, vertical, and diagonal components.

EDIT: Line "value = 1 &amp;lt;&amp;lt; 4*(number of that color in the line)" Originally said "value = (number of that color in the line) &amp;lt;&amp;lt; 4" Which was incorrect (based on what I intended, though you could use that as the heuristic instead) im unsure of how your 'value = ' line is working

specifically regarding the '16^n' what is depth?    </snippet></document><document><title>ACM-ICPC Live Archive, online code judge</title><url>http://livearchive.onlinejudge.org/index.php</url><snippet>  We should get a weekly competition going or something on reddit. I actually wrote the ptrace-jail/sandbox system that does online grading for the USACO training. System is built in C and Python. I could lend it out to someone knowledgeable, if this is something you're interested in pursuing. I, myself, have no time to organize this. I believe that already built systems like [domjudge](http://domjudge.sourceforge.net/intro) are what we need. I never really cared too much for domjudge. It forces a client/slave relationship between several machines that I found to be very unnecessary. We should get a weekly competition going or something on reddit.  </snippet></document><document><title>Need help understanding a problem and am not sure if this is the correct subreddit. [First Post]</title><url>http://www.reddit.com/r/compsci/comments/qx3yx/need_help_understanding_a_problem_and_am_not_sure/</url><snippet>My husband suggested that I come on to reddit to get help with my current homework.  I am studying to obtain my masters in computer science with a concentration in wireless networks.  I searched for what I thought was the best subreddit and this seemed fitting. (If this is not the right subreddit please suggest another)


My professor does not answer questions about homework and there is no TA.  I have been trying to get these first couple of questions done for the past 2 weeks with little to no success.  My issue is in regards to M/M/1/Infinity systems in the non-stationary state.  [Question 1 and 2](http://i.imgur.com/JWk3L.png) involve finding the differential equation that is needed for questions 3 and 4.  Once I have the equation I believe that I can figure out the rest, however, I am having difficulties starting this process.  In class we learned about the stationary state, but this is a learn on your own experience.  

If there is someone that can assist me in understanding and achieving the equation it would be greatly appreciated.  If this is not the right place, please direct me where I might be able to find assistance.

Thank you!   </snippet></document><document><title>Mathematics for Computer Science PDF Book</title><url>http://www.cs.princeton.edu/courses/archive/spr10/cos433/mathcs.pdf</url><snippet>  Most recent draft: http://courses.csail.mit.edu/6.042/spring12/mcsfull.pdf Most recent draft: http://courses.csail.mit.edu/6.042/spring12/mcsfull.pdf Is there a way to get a similary formatted version as the OP's one? Because this is not readable on my Kindle while the other is acceptable. (I would prefer an even bigger font version, though. Even pay money for it.) [mcsfull_cropped.pdf](http://dl.dropbox.com/u/1392012/mcsfull_cropped.pdf)

Cropped version, created with Briss ([http://sourceforge.net/projects/briss/](http://sourceforge.net/projects/briss/)) Most recent draft: http://courses.csail.mit.edu/6.042/spring12/mcsfull.pdf Most recent draft: http://courses.csail.mit.edu/6.042/spring12/mcsfull.pdf Most recent draft: http://courses.csail.mit.edu/6.042/spring12/mcsfull.pdf The formatting is much better in the originally linked article unfortunately. The border and header on each page kills it for me. The formatting is much better in the originally linked article unfortunately. The border and header on each page kills it for me. This is why it's called a draft. It's best to worry about what you are saying before you start worrying about how you are saying it.  This was seriously posted at the most perfect time.   Im glad I could help someone else out, this book is a treasure trove of goodies I'm in Discrete Mathematics as an undergrad right now, and I'm currently using Rosen's book Discrete Mathematics And Its Applications.  That book has walls of text for everything, whereas the PDF you posted explains the same thing in just a few lines. [deleted] I'm in Discrete Mathematics as an undergrad right now, and I'm currently using Rosen's book Discrete Mathematics And Its Applications.  That book has walls of text for everything, whereas the PDF you posted explains the same thing in just a few lines. UofM? UC Santa Cruz.  Only 80% of the stereotypes are true. UofM? UdeM? Michigan? Universit&#233; de Montr&#233;al.  Thought I had found a fellow CS student. I'm in Discrete Mathematics as an undergrad right now, and I'm currently using Rosen's book Discrete Mathematics And Its Applications.  That book has walls of text for everything, whereas the PDF you posted explains the same thing in just a few lines. I'm in Discrete Mathematics as an undergrad right now, and I'm currently using Rosen's book Discrete Mathematics And Its Applications.  That book has walls of text for everything, whereas the PDF you posted explains the same thing in just a few lines. I found Rosen's book to be a bit too long-winded.

If you need another comprehensive -but easy to follow- book, I'd heartily recommend Susanna Epp's.  I'm in Discrete Mathematics as an undergrad right now, and I'm currently using Rosen's book Discrete Mathematics And Its Applications.  That book has walls of text for everything, whereas the PDF you posted explains the same thing in just a few lines. This was seriously posted at the most perfect time.      How does this compare to Concrete Math? concrete math is a lot heavier!    EVERYTHING IS IN THERE ?

PROOF 

LOL          </snippet></document><document><title>Seemingly impossible functional programs</title><url>http://math.andrej.com/2007/09/28/seemingly-impossible-functional-programs/</url><snippet> </snippet></document><document><title>Linear time regular expressions + submatch extraction: Tagged NFA</title><url>http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.76.2799</url><snippet> </snippet></document><document><title>/r/compsci, what are some interesting (favourably esoteric) Models of Computation you have come across?</title><url>http://www.reddit.com/r/compsci/comments/qrf5x/rcompsci_what_are_some_interesting_favourably/</url><snippet>  Reversible turing machines. Every step is physically reversible, so the whole computation is thermodynamically reversible and theoretically can be achieved with 0 heat dissipation. Look for Bennett's classic paper, or Richard Feynman's  much more accessible review.  Reversible turing machines. Every step is physically reversible, so the whole computation is thermodynamically reversible and theoretically can be achieved with 0 heat dissipation. Look for Bennett's classic paper, or Richard Feynman's  much more accessible review.   Turing Machine's with tapes over arbitrary groups.

A standard TM has a 1-dimensional infinite tape, which corresponds to Z. We pick the generator a_1=1 for the group. After each step of the computation, the TM can move by either a_1 or a_1^-1 (aka, +1 or -1). 

We now generalize this by choosing an arbitrary group G with generators a_1,...,a_k. After each step of the computation, the machine moves according to a generator or its inverse. This takes the tap from state S to state S'=S*a_i.

For a paper on the topic, see http://arxiv.org/pdf/1005.2636.pdf  Not terribly esoteric, but cellular automata (e.g., Conway's Game of Life) are theoretically equivalent to Turing machines.

I'm now having visions of running arbitrary code on a sufficiently large monome programmed to run the Game of Life. Not terribly esoteric, but cellular automata (e.g., Conway's Game of Life) are theoretically equivalent to Turing machines.

I'm now having visions of running arbitrary code on a sufficiently large monome programmed to run the Game of Life.           Here's a nice lecture on "Temporally Quaquaversal Virtual Nanomachine Programming In Multiple Topologically Connected Quantum-Relativistic Parallel Timespaces...Made Easy".

http://blip.tv/open-source-developers-conference/temporally-quaquaversal-virtual-nanomachine-programming-in-multiple-topologically-4466153</snippet></document><document><title>How can a Turing machine execute every possible algorithm?</title><url>http://www.reddit.com/r/compsci/comments/qqqmt/how_can_a_turing_machine_execute_every_possible/</url><snippet>The wikipedia page didn't provide much explanation. Do I understand correctly that you could possibly implement any algorithm any program just by using the minimal set of instructions a turing machine has? How?

Thanks, I'd like to get at least to a basic level of CS.  &amp;gt;In computability theory, the Church&#8211;Turing thesis (also known as the Church&#8211;Turing conjecture, Church's thesis, Church's conjecture, and Turing's thesis) is a combined hypothesis ("thesis") about the nature of functions whose values are effectively calculable; in more modern terms, algorithmically computable. In simple terms, it states that "everything algorithmically computable is computable by a Turing machine."

It's essentially the foundation for computer science. Any computable algorithm can be simulated on some Turing machine, (or simulated on a universal Turing machine).

They're not particularly efficient, but any algorithm you can devise on a "Turing-complete" system has a corresponding Turing machine (not necessarily of the same time-complexity).

The Church-Turing thesis, by its nature, cannot be proven, but for practical purposes is usually assumed to be true. &amp;gt; They're not particularly efficient, but any algorithm you can devise on a "Turing-complete" system has a corresponding Turing machine (not necessarily of the same time-complexity).

I'm new to theory of comp, but just wondering, are there other models of computation that more accurately reflect real-world computation in terms of time-complexity?

Also, while I haven't formally been taught turing machines in my theory of comp course yet, we've covered DFAs, NFAs, and PDAs. How does the notion of time-complexity (which I more or less understand for say, actual C++ code) translate to different kinds of automata? For a DFA, my gut would tell me it just has to do with the list of states that correspond to a particular string? But that would imply that all DFAs run in O(n)? Is it actually the case that all regular languages can be recognized in O(n)?

What about (N)PDAs? Transitions that operate only based on the stack make it seem like it might be possible for execution of a PDA to take longer than linear time, but I'm not sure.  &amp;gt;I'm new to theory of comp, but just wondering, are there other models of computation that more accurately reflect real-world computation in terms of time-complexity?

If your computer had infinite memory, it would be one.

Stack based machines, lambda calculus are two simple ones. Most registers. Cellular automaton, Wolfram Rule #30 and Conway's Game of Life.

They're all basically equivalent, but in terms of actual physical ability to be constructed, register-like machines (modern computers) are the best performance/cost.

The architecture will certainly change the complexity, but the specific algorithm would matter, and I'm no expert in any of these.
 &amp;gt; They're not particularly efficient, but any algorithm you can devise on a "Turing-complete" system has a corresponding Turing machine (not necessarily of the same time-complexity).

I'm new to theory of comp, but just wondering, are there other models of computation that more accurately reflect real-world computation in terms of time-complexity?

Also, while I haven't formally been taught turing machines in my theory of comp course yet, we've covered DFAs, NFAs, and PDAs. How does the notion of time-complexity (which I more or less understand for say, actual C++ code) translate to different kinds of automata? For a DFA, my gut would tell me it just has to do with the list of states that correspond to a particular string? But that would imply that all DFAs run in O(n)? Is it actually the case that all regular languages can be recognized in O(n)?

What about (N)PDAs? Transitions that operate only based on the stack make it seem like it might be possible for execution of a PDA to take longer than linear time, but I'm not sure.      I think the simplest way to think about this is that you can implement any virtual machine on a touring machine. Algorithm/Program -&amp;gt; Virtual Machine -&amp;gt; Touring machine. Touring machines are pretty handy when you're away from your desk, too.</snippet></document><document><title>I'm a first year compsci student who can't get any of his CS classes next quarter. What should I learn in its place?</title><url>http://www.reddit.com/r/compsci/comments/qplzr/im_a_first_year_compsci_student_who_cant_get_any/</url><snippet>I'm a CS student at a very crowded uni. Both CS classes I was looking into (a software lab and 'intro to computer organization') filled up super quickly, and my low standing total unit-wise left me in the cold. However I'd like to keep up with CS so I don't fall behind, and so my current knowledge doesn't fade.

So far I'm 2 quarters in, have worked with c++ up to basic and STL data structures. What are some logical things to look into from here?

edit: thanks for all the ideas everyone!   Linear algebra is pretty useful if you're considering game programming. Linear algebra is pretty useful if you're considering game programming. Linear algebra is pretty useful if you're considering game programming. Probably required but probably not able to be taken till year 2. When I took it it had a corequsite with calc I or II ( I don't remember which ) Which is absolute bullshit since you require no calculus knowledge to take linear algebra (just mathematical maturity). but some linear algebra makes multivariable calculus much better. Probably required but probably not able to be taken till year 2.  Talk to the professors and try to get an override. Usually there's a bit of flexibility with class sizes, even after they reach the limit set in the registration software. Already tried. The lab is definitely full and so is the waitlist because it's a lab and is by nature small and dependent on specific class size. The class filled up really quickly and so did the waitlist. Mind you I'm not the only one not able to get these classes; it's an issue for several dozen students. They've been looking for a larger lecture hall/a prof to do more sections but no luck so far. Keep trying, many people end up dropping out of CS classes Already tried. The lab is definitely full and so is the waitlist because it's a lab and is by nature small and dependent on specific class size. The class filled up really quickly and so did the waitlist. Mind you I'm not the only one not able to get these classes; it's an issue for several dozen students. They've been looking for a larger lecture hall/a prof to do more sections but no luck so far. Already tried. The lab is definitely full and so is the waitlist because it's a lab and is by nature small and dependent on specific class size. The class filled up really quickly and so did the waitlist. Mind you I'm not the only one not able to get these classes; it's an issue for several dozen students. They've been looking for a larger lecture hall/a prof to do more sections but no luck so far.  Discrete math, graph theory, combinatorics, linear algebra, and probability are all math classes that are very useful for CS. Discrete math, graph theory, combinatorics, linear algebra, and probability are all math classes that are very useful for CS.  Fields that have a heavy potential application for the C.S. skill set, but are peripherally related. A numerical methods class (math), computational biology (bio, bioinformatics), There are probably some physics classes that would apply. If you've got the time and can stomach it take a math class that is outside of the calculus track and has a heavy emphasis on proof. It'll help down the road in most C.S. programs.  +1 on the computational biology - interesting stuff.  Electrical engineering courses are also extremely helpful for many CS related jobs.   a circuits 101 class would provide a big leg up for any embedded/arduino work, and perhaps provide a bit of context for a class on cpu architecture. If you want to learn embedded stuff, then go for broke and sign up for an embedded systems class. While you're waiting for it to start, check out a basic circuits book from the library and read the first few chapters. You'll be good to go. Or you'll fail horribly; that could happen, I guess.

Fun fact: in many schools, nobody actually checks to make sure that you've taken the prerequisites. This was how I got into most of the really cool classes. &amp;gt;Fun fact: in many schools, nobody actually checks to make sure that you've taken the prerequisites. This was how I got into most of the really cool classes.

*high five* 

Note: Metallurgy is HARD. Consider yourself warned.  +1 on the computational biology - interesting stuff.  Electrical engineering courses are also extremely helpful for many CS related jobs.   +1 on the computational biology - interesting stuff.  Electrical engineering courses are also extremely helpful for many CS related jobs.   Fields that have a heavy potential application for the C.S. skill set, but are peripherally related. A numerical methods class (math), computational biology (bio, bioinformatics), There are probably some physics classes that would apply. If you've got the time and can stomach it take a math class that is outside of the calculus track and has a heavy emphasis on proof. It'll help down the road in most C.S. programs.  I also recommend computational linguistics courses (but I'm a computational linguist, so I'm probably biased). relevant xkcd: http://xkcd.com/114/ ...not everything is xkcd, honestly. I also recommend computational linguistics courses (but I'm a computational linguist, so I'm probably biased). Fields that have a heavy potential application for the C.S. skill set, but are peripherally related. A numerical methods class (math), computational biology (bio, bioinformatics), There are probably some physics classes that would apply. If you've got the time and can stomach it take a math class that is outside of the calculus track and has a heavy emphasis on proof. It'll help down the road in most C.S. programs.  Another +1 for a proof-related course for CS majors.  Fields that have a heavy potential application for the C.S. skill set, but are peripherally related. A numerical methods class (math), computational biology (bio, bioinformatics), There are probably some physics classes that would apply. If you've got the time and can stomach it take a math class that is outside of the calculus track and has a heavy emphasis on proof. It'll help down the road in most C.S. programs.   Discrete math if you can. It'll really help you down the road as you learn more complex ideas. I'd also recommend probability and/or stats, can provide a good base for later work in machine learning. Definitely probability; stats classes tend to be aimed at people in majors like nursing who really don't want to be in a math class, but have to be because it's required. Probability is also more directly applicable to machine learning.      Set theory, group theory, linear algebra. 

See if your philosophy department offers a course in symbolic logic. That's a highly underrated discipline for budding Comp Sci students.   http://www.udacity.com/ is offering free courses. I would highly recommend you check them out.      I've seen a lot of answers that are all over the place. I think people are just throwing out their own personal favorite things.

I don't think we can give good advice unless you have some idea of what you might want to do with your degree, or where your interests lie. I'd suggest editing your post with some of that information. Do you think you might want to go to grad school, for example?  Take a marketing class or some niche liberal arts class on a topic that interests you. Rounding out your education and knowledge is extremely important for a few reasons. First it makes you a more well-rounded person. Secondly it helps you communicate with and understand the thought processes of non-engineers. I think that understanding is one of the biggest differences I've seen between successful and unsuccessful CSists   [deleted]   Improve your programming skill by writing programs to solve problems. [Project Euler](http://projecteuler.net/) and [previous topcoder competitions](http://community.topcoder.com/tc) are sources for small problems.

You need to establish a broad but (for now) relatively shallow understanding of discrete math to progress in your compsci education. Hopefully someone else can make suggestions for this.  You study Comp Sci without actually studying Comp Sci? Bloody Hell, US University is crazy, what do you learn instead? (Actually interestested, please elaborate)

So generally, i always found that actually just doing something helps the most. Just code something, does not matter what, does not have to be really good, just has to work, this is practice, you are supposed to not get it right the first few times. Just code whatever you are interested in, small tools which make your life easier, games, a personal web page. Try out interesting tech and programming languages if you feel like that is not too complicated for you just now.

Just knowing how to code, or rather feel a bit familiar with a language and the way these things work, helps tremendously with later homework assignments and semester projects.

Also, i am not sure if learning Math in advance is that helpful, you are supposed to learn that anyway and your professor(s) should be able to teach that to you better than you can do that for yourself. But decide for yourself.

I hope this helps! We don't usually study comp sci without studying comp sci. I'm technically supposed to be able to take at least 1-2 CS classes each quarter, but due to an abnormally large freshman class, especially in my major, it's been a struggle to get all the classes I'm supposed to be taking. I'm still studying physics and math next quarter, and have general ed requirements I'm getting done in the time I'd usually have for my cs courses. Hmm alright. Well i get the overcrowded courses, sucks but what can you do.

What are the general ed courses, can you describe some of those please.
The only things we had not really connected to Comp Sci was 1 course of law, but related to Comp Sci, and some really minor stuff on other things.

 American schools have you take a large amount of liberal studies courses in domains outside of your major.  There are ways around taking all of them, typically, but you generally have writing, arts***, history, math and sciences if you're not a math or science major, etc.  

*** Very broadly defined. 

**Edited for formatting. That is really strange to me. If you enroll into college for a buttload of money shouldn't you be learning what you are enrolled in. You learned that other stuff in high school didn't you? What an odd concept That is really strange to me. If you enroll into college for a buttload of money shouldn't you be learning what you are enrolled in. You learned that other stuff in high school didn't you? What an odd concept In theory, yes.  However I have tutored people who had never taken algebra before, and have met people who have not taken a science class since freshman year of high school.  

We have a significantly less standardized school system than the UK (I'm assuming this since you said bloody hell). States take care of their own public schools, but have to adhere to federal guidelines, many of which do not create better learning environments, e.g. No Child Left Behind.  Because of this there is an apparent need to reinforce these ideas.  I'll hop off my soapbox now.

In any event, some schools such as Brown (not sure of others), do not enforce this as stringently and allow greater flexibility in creating schedules (see: [wikipedia](http://en.wikipedia.org/wiki/Brown_University#New_curriculum)), but for the most part schools have you take a very large number.   Nah, Germany, not UK, but i am currently watching Sanctuary and Amanda Tapping sports a really nice british accent.

So for us, as i experienced it, our High School equivalent, has relatively stringent rules on stuff you are supposed to learn.

You got German, English, probably a 3rd language, Math, Biology, Physics, Chemistry, some religious study, which you can easily opt out and take something philosophy oriented, computer science, sports, history, geology and arts.

You have to do German and a 2nd language, Math, at least one science class, and some of the others. If you take more languages you can do less science and such with a minimum of stuff per week.

Hmm, as i write this, it seems a bit more complicated than i remembered it in my head.


What i wanted to say. I do think some stuff is basic stuff, like algebra as you mentioned. You can suck at it, but you should have heard of it. Then if you have to learn it at college, it should be because you "need" it for you major. Like math for Comp Sci. And if you enroll for physics, but have not taken science classes in forever, well too bad for you


Even Major sounds wrong. I don't want a Major and Minor for my studys, i want to study something, as my focus. The other side of the coin is that you have SO many things you have to take that you may find something you want to study that interests you more than your current focus. Many students change their major, often multiple times, before they find something they want to study. Hmm alright. Well i get the overcrowded courses, sucks but what can you do.

What are the general ed courses, can you describe some of those please.
The only things we had not really connected to Comp Sci was 1 course of law, but related to Comp Sci, and some really minor stuff on other things.

 The General Ed is a bit like reading, writing, and sums. (I'm being sarcastic.) ;)

3rd level education in the States have to deal with a lot students who appear not to have learned anything at secondary school, and the Uni's want to make sure they have those skills they missed at secondary school. 

It's not all bad, and I do think that it can be useful to learn things outside of one's degree; the more you know, then the more you know and the better your general understanding of things. However, this just doesn't seem to be the case with many US graduates I meet (unless they're doing sciences or engineering).  It's likely you'll need humanities and social sciences at some point in order to get your degree.  Perhaps you could take a philosophy course on logic?  Or any of your other needed electives?

You can always start up a pet project on the side, maybe develop an iPod or Android app in order to keep your coding skills sharp.                      Business. I got a business minor with my cs. 

1. The classes are relevant to everything around you
2. They're stupid easy (sounded like you might be at UCF)
3. Lots of programming is business stuff
4. Helps you invest so you can take that teaching job and live well
5. High frequency trading 

Lots of good reasons. It'll leave you well rounded. There have been multiple times that I understood conversations my bosses were having and my peers did not. Never underestimate the value of being able to talk to the vp.  There are a lot of things you can learn on your own, but certain things, like accounting, require a class.  There are a lot of things you can learn on your own, but certain things, like accounting, require a class.      Try taking a critical thinking class. It's philosophy for me at my university, but you probably have a ton of electives to fill. The class here deals with logical arguments and includes boolean statements and truth tables, which are handy to have a firm grasp on. It's also a PHILOSOPHY class, so you know it isn't hard.

BURY ME, PHILOSOPHY MAJORS! YOUR CHOSEN COURSE OF MAJOR IS A COP-OUT!  How to drink lots of beer - i hear that you'll need it for when you do tech support to start your career. A better option is to *not* start your career via tech support. If you can write some good stuff and stick it on GitHub, then you have career options that don't involve trying to slowly explain to someone what "right clicking" is. Agreed, but the sheer amount of tech support jobs compared with everything else  means its the only viable option sometimes </snippet></document><document><title>Classic Nintendo Games are (NP-)Hard</title><url>http://arxiv.org/abs/1203.1895</url><snippet>  Billy Mitchell can beat them in polynomial time.  Not just NP-Hard, *Nintendo-Hard*.   Title is misleading. They don't mean beating the game is NP-Hard. They mean finding the optimal path from the startpoint to the goalpoint is NP hard (given no prior knowledge of the level schematics). Which . . . well duh. Actually they prove that finding if a level is beatable is NP-hard. They construct levels that are solvable iff a corresponding 3-SAT problem is solvable. Finding if a level is beatable would be NP Complete.  NP Complete problems are decision problems, e.g. can World 1-1 of Super Mario Bros. be completed?  They reduced each game (problem) down to an instance of the 3-SAT problem to prove this.  

NP Hard problems are optimization problems, e.g. what's the best time that World 1-1 of Super Mario Bros. can be beat? Finding if a level is beatable would be NP Complete.  NP Complete problems are decision problems, e.g. can World 1-1 of Super Mario Bros. be completed?  They reduced each game (problem) down to an instance of the 3-SAT problem to prove this.  

NP Hard problems are optimization problems, e.g. what's the best time that World 1-1 of Super Mario Bros. can be beat?  Why don't they show that the platformers are NP-complete? Shouldn't it be trivial to show using a path as a certificate?

I wish they were more formal concerning their inputs. I did enjoy reading the referenced paper on block pushing games, though; it was surprising to me that PushPush-1 would be PSPACE complete. The decision problem ("Is this SMB level solvable?" or "Can I reach from point X to point Y.")  is NP-complete. They basically state that the path taken need only be polynomial in the size of the level (though they don't really prove that assertion), so the problem is in NP.

   What are all the different classifications of NP type P type complete... etc?     &amp;gt;... all Legend of Zelda games except Zelda II: The Adventure of Link...

I suspect someone has -- once again -- forgotten all about the CD-i Zelda games.</snippet></document><document><title>Can all unambiguous grammars be parsed in linear time?</title><url>http://cstheory.stackexchange.com/questions/10486/can-all-unambiguous-grammars-be-parsed-in-linear-time</url><snippet>  </snippet></document><document><title>Question about Complexity Classes and Runtimes</title><url>http://www.reddit.com/r/compsci/comments/qogz0/question_about_complexity_classes_and_runtimes/</url><snippet>We know that the fastest running time for a comparison-based sort is proportional to n log n, for n input elements.  Similarly, for other problems we can prove a lower bound on the running time.

But don't these running times assume a particular model of computation, say, a turing machine?  (I'm not even sure if a Turing machine can do sorting in O(n log n) time...)  Doesn't this kind of analysis depend on the primitive instructions of the computer (which presumably take constant time)?  For example, can't we assume a model of computation where one of the primitive operations is sorting in O(1) time, and then conclude that sorting takes O(1) time by construction?

Another example: it was recently shown that matrix multiplication can be done in O(n^2.3727) time.  Does this assume, for example, an x86-like architecture, where multiplying two numbers is assumed to take constant time?  Can we pick another architecture where multiplication can be done (asymptotically) faster?

The reason I'm asking is because I want to get excited about new results in complexity theory, but I'm having a hard time seeing the point if it's all contingent on an arbitrary model of computation of our choosing (although I do recognize that some computer systems are easier to build than others, and it's worth studying algorithms that will be running on these systems).

Thanks!  About your formatting, it's a bit messed up on your exponent.

    do this:         O(n^(2.3727))
    instead of this: O(n^2.3727)

Time complexity is based on the number of fundamental, atom operations it takes for a given input "size."

So all that matters is really what atomic operations you have, and what *their* complexities are (they'd all probably be O(1) for most applications) and *from* this you determine the complexity of the overall program. Is there any standardized notion of what an "atomic" operation is, though? Ie, most modern computers have random access to memory, but turing machines (from what little I understand of them) do not. So how can we say things like "matrix multiplication can be done in O(n^(2.3727))"? What kind of machine are we running this on? Which operations are considered atomic? The machine would determine this.

For example, a machine that deals with pointers and integers concats a linked list of length (n) with one of length (m) in O(n) time.

A machine that deals with linked lists concats them in O(1) time.

But which can be built in our reality cost-effectively? Only the former. Is there a formalization of which operations can be performed in constant-time, though? Without such a formalization, wouldn't a statement like "such and such an algorithm can run in O(f(n))" need to be qualified by explaining what kind of machine we're running it on?

And wouldn't this make something like a turing machine a bad model for computation, if we're concerned with time complexity (since they can't perform constant time random memory access, despite its real-life feasibility)? &amp;gt;Without such a formalization, wouldn't a statement like "such and such an algorithm can run in O(f(n))" need to be qualified by explaining what kind of machine we're running it on?

Everyone runs basically the same kind of machines. The only difference is overhead, which is not significant (and therefore ignored) for arbitrarily large n. More significant perhaps is the language- if you're agreeing on this you probably know the atomic operations.

&amp;gt;And wouldn't this make something like a turing machine a bad model for computation, if we're concerned with time complexity

When you're dealing with Turing machines, you're *not* concerned with time complexity. You're generally determining properties of computability, not of actually calculating things. No one designs programs for Turing machines and then evaluates their complexity unless they're trying to demonstrate something relevant to this, because Turing machines are very slow.
 So why don't we have a model for computation that accurately reflects "real world" computers, instead of turing machines? Wouldn't this allow us to formalize notions of time complexity? Or is that not really a useful goal? Time complexity can be formulated for existing languages.

Changing the simple Turing machine to a more complex register just makes it useless at doing everything it's supposed to do and allow it to kinda do something it's not intended to do. &amp;gt; Time complexity can be formulated for existing languages.

What exactly does that mean? Ie, what if someone claims that they have some lower bound on the running time of an algorithm, but someone else comes along and builds a computer than can do it faster. What does a lower bound on the running time of an algorithm even mean if we don't have a clearly defined model for what a computer is (one that accurately reflects running times)?

I feel like I might be too concerned with formality, but it just seems sloppy to say something like: the algorithm running time is bounded below by g(x) when run on "realistically-feasible computers". </snippet></document><document><title>The Theory of Relational Databases (1983)</title><url>http://web.cecs.pdx.edu/~maier/TheoryBook/TRD.html</url><snippet /></document><document><title>Worldcomp - The world's biggest sham computer science conference</title><url>http://sites.google.com/site/worlddump1/</url><snippet>  I get emails daily from these types of conferences. There are a few to stay away from that most folks know about. Any IADIS conference and really anything not sponsored by SIAM, IEEE, or ACM.

Unfortunately I didn't get the memo about Worldcomp. I submitted and attended and gave my talk. I did talk to the conference organizer, he seemed like an honest guy, and he knew that the conference was subpar.

I looked back into my email account. (I save everything). Here's the redacted emails when I asked for the reviews.

----

Prof. Arabnia,

Thank you for the great news. Do you know how I can access the reviewers comments?


hra@cs.uga.edu
4/8/09

to ***
Dear **:

You should receive the reviewers' comments (mandatory changes) by April 11.

Many thanks,
Hamid

*** **@***.edu via gmail.com 
4/20/09

to hra
Prof. Arabnia,

Is there any news about when the reviewers' comments will be available. I have not received anything.


----

...And nothing. 

If you want to hear about my experience at the conference. Let me know and I'll answer questions in this thread.
 1) You ever got the reviews?
2) Is true that there are sessions that do not exists? Do you had a chance to present your paper?
3) Do you attend to other sessions? If so, how you feel about the quality?

 1) No, I never got the reviews. At least my email doesn't have any record of them. This was 4ish years ago.

2) I don't know about that. I don't recall anything missing or odd about the program.

3) I did attend other sessions, the quality was hit or miss. There were some sincere scientists from regular universities (like me) who just didn't get the memo. And there were some suspect papers, but by the time I figured it all out I stopped caring enough to critically examine any papers.

Let me just say... I hold myself to be a serious scientist... I am about to graduate from a top 5 PhD program, and try to publish in top conferences -- sometimes I even get accepted ;-) The problem here is not the authors... we got duped. Personally, I figured that if the conference was listed on DBLP then it was legit -- I was wrong.

I no longer list that paper on my CV even though it was legitimate science. But I guess I did get a trip to Vegas. &amp;gt;I no longer list that paper on my CV even though it was legitimate science


Sorry, I'm not in high academics. What's wrong with keeping the paper and publishing it elsewhere? Or is that not how this works, (I honestly don't know). 1) No, I never got the reviews. At least my email doesn't have any record of them. This was 4ish years ago.

2) I don't know about that. I don't recall anything missing or odd about the program.

3) I did attend other sessions, the quality was hit or miss. There were some sincere scientists from regular universities (like me) who just didn't get the memo. And there were some suspect papers, but by the time I figured it all out I stopped caring enough to critically examine any papers.

Let me just say... I hold myself to be a serious scientist... I am about to graduate from a top 5 PhD program, and try to publish in top conferences -- sometimes I even get accepted ;-) The problem here is not the authors... we got duped. Personally, I figured that if the conference was listed on DBLP then it was legit -- I was wrong.

I no longer list that paper on my CV even though it was legitimate science. But I guess I did get a trip to Vegas. 1) You ever got the reviews?
2) Is true that there are sessions that do not exists? Do you had a chance to present your paper?
3) Do you attend to other sessions? If so, how you feel about the quality?

 Just found the footprint of the person who is sending comments about this conference. This appears to be the person behind it: www.cis.famu.edu/~prasad  This is interesting, and the fact that the critics of this conference got 2 fake papers accepted indicates that there is indeed no serious peer review.

Please do post follow-ups if you hear of any.. and warnings of other similar conferences, too! Just found the footprint of the person who is sending comments about this conference. This appears to be the person behind it: www.cis.famu.edu/~prasad  IAMA Request: Someone who submitted a paper or attended the conference I did :(

I don't wanna talk about it :( Ok now you REALLY need to do an ama IAMA Request: Someone who submitted a paper or attended the conference [deleted] [deleted] I'm sorry to hear that. What was your research on? [deleted]  It would be nice if there were an official blacklist of fake CS conferences. I don't know precisely how it would be maintained, though. (Certianly can't entrust it to IEEE or ACM.) [This list](http://core.edu.au/index.php/categories/conference%20rankings/1) is helpful, although the categories are broad. I doubt all the lower tier conferences are shams, but they should probably be avoided anyway. Is there an updated list?  The list you linked to seems to be from 2008. [This list](http://core.edu.au/index.php/categories/conference%20rankings/1) is helpful, although the categories are broad. I doubt all the lower tier conferences are shams, but they should probably be avoided anyway. It would be nice if there were an official blacklist of fake CS conferences. I don't know precisely how it would be maintained, though. (Certianly can't entrust it to IEEE or ACM.) &amp;gt;Certianly can't entrust it to IEEE or ACM.

Why not?  I'm surprised that any serious scientist got duped by this.  Just reading the e-mails that bombard my inbox from this and similar conferences suggests that its a sham.   Sorry, I don't quite understand. What is a fake conference? Someone trying to make money? No, that's a real conference.  I think the issue is that this one appears not be be peer-reviewed.  Basically, they're charging people large amounts of money to put them in a room and let them talk to eachother; there's no quality control. Exactly.. and the interesting part is that peer review takes time but it's not very costly; you find the experts of the field, send them papers, wait for their evaluations, rank the papers accordingly. I review papers occasionally and I've never got paid for that, but I get to see interesting papers earlier than most other people, sometimes.

Concerning WORLDCOMP, they have put lots of effort to the organization anyway, it puzzles me why they have neglected the peer review part. Maybe someone really wants to have conferences where you can publish "anything"? No, that's a real conference.  I think the issue is that this one appears not be be peer-reviewed.  Basically, they're charging people large amounts of money to put them in a room and let them talk to eachother; there's no quality control. Oh, so is there peer review at conferences like defcon? Or is it just at actual science conferences? Sorry, I don't quite understand. What is a fake conference? Someone trying to make money? To an academic the point of presenting in a conference is to get something published (in conference proceedings) and to get recognition.  Recognition helps your career, can even help your university get more funding, etc.

When you submit to a proper conference the paper you submit has to go through a process of peer review.

People who presented at this conference did indeed get their papers published, but the review process was misrepresented - there was no critical review - among other things.  When there is no proper peer review then it can't be considered a "peer-reviewed" publication and the benefit of publishing in it is significantly lower; for a lot of metrics, funding bodies within and outside the university, and other academics, it doesn't count at all.  You can read about scientific peer review elsewhere.

So it looks like academics are being encouraged to sign the rights of their research outputs (conference papers) over to these conferences on the understanding that they will be recognised as having published in a peer-reviewed proceedings, when this is a sham and their publication will be no more academically recognised than submitting a letter to a newspaper.

Now this is where my knowledge ends (and I may have gotten things wrong along the way).  So I have a question:

I know that the academic cannot publish the same paper elsewhere, but can he write a completely different paper based on the same research findings and submit that elsewhere or is that frowned upon?  Would it even be frowned upon in situations like this?  What about just using an earlier version of the same paper?  Would they be able to void any contract they have based on this misrepresentation (assuming they could prove it)?      Too bad Arabnia rakes in half a million a year from the state of Georgia in addition to a recent $2M grant for MRI devices.  Since the University probably gets 30-40% in grant overhead, I'm sure the administration loves him.  Ahhh University politics. Too bad Arabnia rakes in half a million a year from the state of Georgia in addition to a recent $2M grant for MRI devices.  Since the University probably gets 30-40% in grant overhead, I'm sure the administration loves him.  Ahhh University politics. Where are you getting this information? According to open.georgia.gove, Dr. Arabnia's salary was $102,712 last year and this includes money from grants. That's not half a million.  Grant money != salary.  The two aren't related in the least.  Check his CV from his website. I thought that university departments used a professor's grant money in order to pay a certain percentage of that professor's salary and that the rest of the money is used for funding grad students, travel, and equipment. I've never heard of this happening, but I guess it wouldn't be too surprising if it did happen. It may not be the case for all grants, however, for some it is (e.g., NIH grants). Please note that I'm claiming Dr. Arabnia has received any NIH grants (he may have). I'm just using them as an example.  Just found the footprint of the person who is sending all of these comments about this conference. This appears to be the person behind it: www.cis.famu.edu/~prasad    I dont seem to grasp the severity of this. Why is it considered a sham and how is it harmful? I am confused with the purpose of a computer science conference???903bu[jwi I dont seem to grasp the severity of this. Why is it considered a sham and how is it harmful? I am confused with the purpose of a computer science conference???903bu[jwi Probably answered above and there is no need to downvote. Just found the footprint of the person who is sending comments about this conference. This appears to be the person behind it: www.cis.famu.edu/~prasad Just found the footprint of the person who is sending comments about this conference. This appears to be the person behind it: www.cis.famu.edu/~prasad Probably answered above and there is no need to downvote. Just found the actual persons who are sending all these defamatory comments about WORLDCOMP and Hamid Arabnia. As of now, these are the persons behind it: 

http://www.cs.uga.edu/~thiab

http://www.cs.uga.edu/~tliu

http://www.cs.uga.edu/~erc 

http://ktwop.wordpress.com/about 

http://www.cs.fsu.edu/~tyson 

http://www.cis.famu.edu/~hchi 

http://www.scs.gatech.edu/people/mustaque-ahamad

http://www.cs.fsu.edu/~xyuan 

http://www.unf.edu/~ree 

http://www.johnlevine.com 

http://curly.cis.unf.edu

http://en.wikipedia.org/wiki/Albert_Shiryaev 

http://www.cse.sc.edu/~jtang

http://www.ninaringo.com 

http://www.cis.famu.edu/~prasad 

http://www.scs.gatech.edu/people/maria-balcan 

http://www.f4.htw-berlin.de/~weberwu 

http://www.iaria.org/speakers/PetreDini.html 

http://www.eecs.ucf.edu/index.php?id=profiles&amp;amp;link=joseph_laviola  

 (more names will be announced later on&#8230;)

These people formed a team and mailing to different forums, groups, blogs and individuals, heavily criticizing WORLDCOMP. Some of them have personal or professional enmity with Prof. Hamid Arabnia and some of them don&#8217;t like WORLDCOMP for one reason or the other.
</snippet></document><document><title>A Contrarian Worth Listening To: Caltech Engineering Professor John Doyle Thinks &#8220;Complexity&#8221; is Over-Simplified</title><url>http://h30565.www3.hp.com/t5/Feature-Articles/A-Contrarian-Worth-Listening-To-Caltech-Professor-John-Doyle/ba-p/1864</url><snippet /></document><document><title>Good design for an application involving finite-state machines</title><url>http://www.reddit.com/r/compsci/comments/qm1cb/good_design_for_an_application_involving/</url><snippet>Hello, everyone. I am studying computer science and I am very interested in theoretical computer science. About one year ago I finished a course on Formal Languages and Automata Theory. I enjoyed the concepts and understood them very well. Now, I am planning to implement an application that allows the user to specify finite-state machine (with an friendly interface or by providing an input file), simulate them step-by-step and, eventually, add more features like state minimization or converting a NFA to a DFA. The application will display the automaton drawn with all it's states and transitions.

I was wondering what would be a good design, regarding the open-closed principle of object oriented programming, for such an application considering that I will start with, for example, the DFA and later on add other types of finite-state machines (NFA, epsilon-free NFA, PDA, Moore Machine, Mealy Machine). However, I can't seem to decide on how abstract I should consider the states and the transitions. The ideal goal would be to have the logic of FSMs interfere as little as possible with the way I am drawing them.

Any tips and ideas are welcome as they would help me develop a better understanding on implementing such an application.  It's not object-oriented, but if you want to see an impressively modular design, check out the [ocamlgraph](http://ocamlgraph.lri.fr/) library.  Wait, can I clarify this? A system built in *objective* Caml *isn't* object-oriented?</snippet></document><document><title>Throwing Eggs outside windows</title><url>http://archive.ite.journal.informs.org/Vol4No1/Sniedovich/</url><snippet> </snippet></document><document><title>Udacity Web application class to be taught by Reddit Co-founder STEVE HUFFMAN</title><url>http://www.udacity.com/overview/Course/cs253</url><snippet>  Nice! Just started unit 2 of CS101. This is a great way to get into and learn programming. I'm doing the program your own robotic car course. It's very well explained and fun to participate! What is the time involved? I signed up for it but I'm not sure if I can handle it with Computer Vision and Natural Language Processing from Stanford. I spend about 2 hours per unit, so it's not too bad. The grading is now changed so that your exam grade will be 100% if it's higher than your homework grade. This way you can still join now without penalty. Okay thanks. I'm assuming it's one unit per week? I started the first unit last night and I like how he incorporates programming into the lessons.

It's very hard to decide what to take when Stanford is also offering Natural Language Processing and Computer Vision.  In the overview they are called weeks so I think it's one per week indeed.

If you're not under too heavy load already the next 6 weeks, you can easily squeeze this course in I think. It's probably a maximum of 4 hours per week of effort.

Can you send me a link to the computer vision course of Stanford by the way? I'm pretty busy but I may as well start it. Worst case I retake it this summer when it's offered again. 

Here's the [Computer Vision](http://www.vision-class.org/) url. I believe it starts on Monday, March 12th. If you scroll down you can see the other classes that are offered.  Thanks for the url! I'm really tempted to take that one too. Do you have an idea about the work load of that course? Sorry for the delay. It's now too late for NLP if you didn't already sign up but Computer Vision hasn't started yet. And about the timing, I haven't timed myself but I'd say it takes around an hour and a half to two hours for the videos and the programming problems can take a while. Hope this helps  What programming language is going to be used? The two current courses use Python, so I imagine that future courses will do the same.  Count me in. That and cryptology look like a good time. So far programming a robotic car has been as good as my IRL classes but it is a bit weak on theory ( never thought I'd miss it ) I would say way weak on theory. I'm trying to actually get a decent working knowledge of these topics, had to spend a few hours reading about Kalman filters last week after the class to actually understand what was happening in the last exercises on the HW. I really do have to agree, there are only about 50 minutes of video per week. That's like 1 class IRL  *The* Huffman of encoding fame?</snippet></document><document><title>At what number of dimensions is a kd-tree impractical?</title><url>http://www.reddit.com/r/compsci/comments/ql1wo/at_what_number_of_dimensions_is_a_kdtree/</url><snippet>When browsing open source kd-tree implementations (mostly in Python since that is what I work with in a daily basis), I noticed that most examples use only three dimensions. Furthermore, the SciPy implementation (http://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html) directly states:

&amp;gt; For large dimensions (20 is already large) do not expect this to run significantly faster than brute force.

I understand the curse of dimensionality, but what can I reasonably expect to get away with? I am working on an image retrieval engine in which the feature vector for each image is a vector of 40 entries. There are roughly 10k instances, each with a 40 dimensional vector. Is that too much for a nearest neighbor search in a kd-tree? Or should I look at either modifying my image descriptor to use less dimensions or apply PCA to reduce dimensions?

My other thought was to create a set of k clusters from a sample of the feature vectors, comparing each entry in the dataset to the generated clusters, and then assigning the instance to the cluster that minimized distance to the centroid (I believe this is related to prototype-clusters?). By using clusters I could just compare centroids and then search within the cluster with the minimum distance, but that solution sounds a little messy.  My first thought when I saw 40 dimensions was "Oh God why?" 

You're probably going to have to experiment a bit to find the the optimum number of dimensions to use in your kd-tree, but 40 definitely seems like way too much. As a general rule if you have N elements in a kd-tree you probably shouldn't use more than log2(N) dimensions at the most. So with a sample size of 10k you'll probably see efficiency take a nose dive after 12 or 13 dimensions, but even then I would still try to reduce the number of dimensions down to a minimum. 
</snippet></document><document><title>What level of rigor do you need to understand math for computer science?</title><url>http://www.reddit.com/r/compsci/comments/qk23o/what_level_of_rigor_do_you_need_to_understand/</url><snippet>I've heard computer scientists need to understand abstract algebra but do they need to know how to prove if there's a unique element in a vector space or things like that? 

Or do they need to know it from a computational standpoint? Obviously learning how to prove would be better than not but is it required for computer scientists?

And is Analysis/topology even applied to computer science? Would taking a real analysis course help me at all? What are the best mathematics to learn if you want to get into computer science?

What mathematics do you need to know to fully understand how a computer works and how a quantum compter works?  Graph Theory, Combinatorics, Linear Algebra, and Logic are all pretty widespread. Beyond that, it will depend on your specific area.  Someone who couldn't prove that the additive identity in a vector space was unique, I wouldn't expect to do too well as a computer scientist. I'd also expect to be able to give the definition of a vector space to a computer scientist who had beyond all reason somehow managed to avoid it, and have them be capable of proving things like that on their own, because that proof is really simple, and being able to think logically and apply formal definitions is a prerequisite for CS.

But also, linear algebra is just super-important for large swathes of computer science. If you want to fully understand how a real-world computer works, you'll probably also want to understand electronics, and it's absolutely central to understanding that also. (Circuit analysis makes use of linearity all the time.) If you want to understand quantum computers, then it's arguably even more important, as quantum mechanics is essentially just lots of functional analysis (which itself uses a lot of mostly infinite-dimensional linear algebra, and a great deal of real analysis). Quantum circuits are described by unitary operators on a Hilbert space, which is a vector space with an inner product which makes it topologically complete under the norm induced by that inner product. In the very definition of a Hilbert space, you have an interaction between concepts from linear algebra and real analysis.

Linear algebra over finite fields is also extremely important to coding theory, which involves itself in the construction of error detecting and error correcting codes.

Linear algebra is important in graph theory, which is important to many many algorithms. From what I understand, Google's machine power is in large part spent on doing gigantic computational linear algebra problems (computing the PageRank is fundamentally a problem of computing approximations to eigenvectors of a linear map on a vector space with a basis consisting of all the pages on the web).

I'm really only scratching the surface here with linear algebra.

I'll assume I've convinced you that's important. How about topology? Well, topology has deep connections to computation. For example, check out [these lecture notes by Mart&#237;n Escard&#243;](http://www.cs.bham.ac.uk/~mhe/papers/entcs87.pdf), which essentially go over how the study of computable functions between types is, if not the same, then at least extremely similar to the study of continuous maps between topological spaces.

There is a program on right now, started by Voevodsky, to develop something called Homotopy Type Theory, which crosses the lines between computer science (it is a form of lambda calculus), the foundations of mathematics and logic and homotopy theory from topology, which involves a bunch of category theory in the process. People are implementing this stuff on computers inside proof systems like Agda and Coq. It might be the future of type theory in programming languages.

The theory of monoids from abstract algebra is highly important to (if not just another way of looking at) the theory of formal languages. You've probably heard of regular expressions. Well, the languages of strings described by regular expressions are exactly those languages which are the preimage under a monoid homomorphism of a subset of a finite monoid in a free monoid. Understanding some things about monoids, through that lets us figure out things about regular languages (for one).

Order theory is important to the theory of semantics of programming languages and recursion theory, where types of values are given definedness orderings which make them into directed-complete partial orders. (This shines through especially well in non-strict functional programming languages like Haskell, where people use the language of 'bottom' to refer to nonterminating computations -- a completely undefined value being at the bottom of the order.)

Generalising the theory of monoids and order theory at the same time, you have category theory, which is finding more and more applications in the world of functional programming and type theory. (Monads being one popular example of an abstract concept stolen from category theory, used to describe programming language semantics, and then stolen directly as an abstraction for actual programming, capturing the similarities between a bunch of library and domain specific language APIs.)  I'm working for a game company at the moment, and some design aspects of the functional reactive library at the heart of our game engine are inspired by symmetric (or braided) monoidal categories. Stealing things from CT generally is a really easy way to get a large amount of elegance with a minimum of trial and error. I tend to look at it as a useful library of tools for figuring out what the "right definitions" to look at in a given context are, and abstractions which are well-developed and can be transplanted nicely into various new contexts which might arise.

In any case, this is rather long for a Reddit comment already. Hopefully it provides some useful examples. No matter what mathematics you can learn in an undergraduate course will probably be useful somewhere in computer science. Beyond that, the graduate level stuff will be useful to you as a researcher as it frequently contains ideas which are worth stealing and bringing into a more computational context. &amp;gt;[Hilbert space] which is a vector space with an inner product which makes it topologically complete under the norm induced by that inner product.

Completeness is a metric property, not a topological property. So it's complete with respect to the induced norm. Although in Quantum computing you mostly just work with finite dimensional complex vector spaces, so the functional analysis viewpoint isn't as important. Someone who couldn't prove that the additive identity in a vector space was unique, I wouldn't expect to do too well as a computer scientist. I'd also expect to be able to give the definition of a vector space to a computer scientist who had beyond all reason somehow managed to avoid it, and have them be capable of proving things like that on their own, because that proof is really simple, and being able to think logically and apply formal definitions is a prerequisite for CS.

But also, linear algebra is just super-important for large swathes of computer science. If you want to fully understand how a real-world computer works, you'll probably also want to understand electronics, and it's absolutely central to understanding that also. (Circuit analysis makes use of linearity all the time.) If you want to understand quantum computers, then it's arguably even more important, as quantum mechanics is essentially just lots of functional analysis (which itself uses a lot of mostly infinite-dimensional linear algebra, and a great deal of real analysis). Quantum circuits are described by unitary operators on a Hilbert space, which is a vector space with an inner product which makes it topologically complete under the norm induced by that inner product. In the very definition of a Hilbert space, you have an interaction between concepts from linear algebra and real analysis.

Linear algebra over finite fields is also extremely important to coding theory, which involves itself in the construction of error detecting and error correcting codes.

Linear algebra is important in graph theory, which is important to many many algorithms. From what I understand, Google's machine power is in large part spent on doing gigantic computational linear algebra problems (computing the PageRank is fundamentally a problem of computing approximations to eigenvectors of a linear map on a vector space with a basis consisting of all the pages on the web).

I'm really only scratching the surface here with linear algebra.

I'll assume I've convinced you that's important. How about topology? Well, topology has deep connections to computation. For example, check out [these lecture notes by Mart&#237;n Escard&#243;](http://www.cs.bham.ac.uk/~mhe/papers/entcs87.pdf), which essentially go over how the study of computable functions between types is, if not the same, then at least extremely similar to the study of continuous maps between topological spaces.

There is a program on right now, started by Voevodsky, to develop something called Homotopy Type Theory, which crosses the lines between computer science (it is a form of lambda calculus), the foundations of mathematics and logic and homotopy theory from topology, which involves a bunch of category theory in the process. People are implementing this stuff on computers inside proof systems like Agda and Coq. It might be the future of type theory in programming languages.

The theory of monoids from abstract algebra is highly important to (if not just another way of looking at) the theory of formal languages. You've probably heard of regular expressions. Well, the languages of strings described by regular expressions are exactly those languages which are the preimage under a monoid homomorphism of a subset of a finite monoid in a free monoid. Understanding some things about monoids, through that lets us figure out things about regular languages (for one).

Order theory is important to the theory of semantics of programming languages and recursion theory, where types of values are given definedness orderings which make them into directed-complete partial orders. (This shines through especially well in non-strict functional programming languages like Haskell, where people use the language of 'bottom' to refer to nonterminating computations -- a completely undefined value being at the bottom of the order.)

Generalising the theory of monoids and order theory at the same time, you have category theory, which is finding more and more applications in the world of functional programming and type theory. (Monads being one popular example of an abstract concept stolen from category theory, used to describe programming language semantics, and then stolen directly as an abstraction for actual programming, capturing the similarities between a bunch of library and domain specific language APIs.)  I'm working for a game company at the moment, and some design aspects of the functional reactive library at the heart of our game engine are inspired by symmetric (or braided) monoidal categories. Stealing things from CT generally is a really easy way to get a large amount of elegance with a minimum of trial and error. I tend to look at it as a useful library of tools for figuring out what the "right definitions" to look at in a given context are, and abstractions which are well-developed and can be transplanted nicely into various new contexts which might arise.

In any case, this is rather long for a Reddit comment already. Hopefully it provides some useful examples. No matter what mathematics you can learn in an undergraduate course will probably be useful somewhere in computer science. Beyond that, the graduate level stuff will be useful to you as a researcher as it frequently contains ideas which are worth stealing and bringing into a more computational context. I feel as if only a math degree is suitable to become a computer scientist. Do I learn the mathematics along the way or do I do the math degree and then go try and become a computer scientist? 

Would a statistics degree suffice? That's the program i'm in now.   So far i'm taking calculus 2(proof based course where we deal with epsilons and deltas. As it stands, I have a 80%+ in that class.), linear algebra (I took 2 semester linear algebra proof based math course but couldn't cut it. I got a 50% in the first semester and dropped out in second semester. So now i'm taking the non proof based math course and am getting 90%), Discrete math (already took this class and it was basically an intro to proof writing. It was very interesting and pretty challenging. Though algebra was by far 4x harder)


I'm going to later relearn everything in a more formal and rigorous manner for linear algebra. I really like thinking up of algorithms and like the idea of data mining.

What kind of math courses will i need to take to better understand algorithms and data mining?

I'll take real analysis regardless though. That's one thing for sure. I want to understand how the real numbers were constructed. Statistics. If you want to go into AI, statistics. Big way. So far i'm taking calculus 2(proof based course where we deal with epsilons and deltas. As it stands, I have a 80%+ in that class.), linear algebra (I took 2 semester linear algebra proof based math course but couldn't cut it. I got a 50% in the first semester and dropped out in second semester. So now i'm taking the non proof based math course and am getting 90%), Discrete math (already took this class and it was basically an intro to proof writing. It was very interesting and pretty challenging. Though algebra was by far 4x harder)


I'm going to later relearn everything in a more formal and rigorous manner for linear algebra. I really like thinking up of algorithms and like the idea of data mining.

What kind of math courses will i need to take to better understand algorithms and data mining?

I'll take real analysis regardless though. That's one thing for sure. I want to understand how the real numbers were constructed.   None if you really want to be a software engineer :) That's patently false. Expect to need to know logic rules, algebra, algorithm runtimes, and whatever your work's domain is.       99% of people with CS backgrounds don't need math beyond basic linear algebra.  If you want to do quantum computing, you'll need more advanced linear algebra.

Basically, linear algebra. That and: 

* Information Theory
* Logic
* Set Theory
* Graph Theory
* Probability

 That and: 

* Information Theory
* Logic
* Set Theory
* Graph Theory
* Probability

 I have never required any of that in 20 years in the commercial industry. I have never required any of that in 20 years in the commercial industry. I have never required any of that in 20 years in the commercial industry. That and: 

* Information Theory
* Logic
* Set Theory
* Graph Theory
* Probability

 Wow. Never considered how much stuff I need to know just to do fairly basic webdev. :S Fairly basic webdev is not compsci. Fuck off with this kind "jokes" (if it was ment as one anyway, which I hope it was) Fairly basic webdev is not compsci. Fuck off with this kind "jokes" (if it was ment as one anyway, which I hope it was) My degree was in CS. I've built a RISC processor from logic gates up (VLSI), an OS from ASM up, applications from there up, protocols, then distributed/networked applications on top.

I chose this profession as it's the level of the stack I'm most interested in. It involves all the pre-requisites above in this thread. 

If you think I'm joking, you misunderstand one or more of "basic", "webdev" or "compsci". Tell me please, how you used well-ordering theorem in building processors or protools or distributed networks. I didn't, because its obtuse. I use sorting and indexing on collections, and call them that because I want to be understood. Sorting is too high level for (RISC)chip+os, and theres no point writing your own sort in application layer when libraries exist that are way faster. As these weren't going into production, I would probably have just gone with bubble sort and called it a day.

I don't build 'protools'; [Avid](http://en.wikipedia.org/wiki/Pro_Tools) does that.

A 'distributed network' doesn't make sense. A network is by definition distributed, otherwise there is no 'net'. TCP takes care of packet ordering.

How is this relevant? Well it's clear YOU don't understand what set theory is :O. Well-ordering theorem is an equivalent theorem to Axiom of choice or Zorn's Lemma, and a key theorem in set and order theory. Those are principal parts of computer science... My degree was in CS. I've built a RISC processor from logic gates up (VLSI), an OS from ASM up, applications from there up, protocols, then distributed/networked applications on top.

I chose this profession as it's the level of the stack I'm most interested in. It involves all the pre-requisites above in this thread. 

If you think I'm joking, you misunderstand one or more of "basic", "webdev" or "compsci". Your specific choices in vocation aren't relevant to the discussion on CompSci as a whole.  Wow. Never considered how much stuff I need to know just to do fairly basic webdev. :S 99% of people with CS backgrounds don't need math beyond basic linear algebra.  If you want to do quantum computing, you'll need more advanced linear algebra.

Basically, linear algebra. [deleted]          To understand how a computer works? Not much, just vector calculus for a solid understanding of electromagnetic theory. I don't know much about quantum computers, but I guess you'd need to know quantum mechanics first. That requires an understanding of PDEs, probability theory and some complex analysis as well as vector calculus. Although the hardest part for me to try and learn quantum mechanics has been trying to understand Langrangian and Hamiltonian mechanics.   I've been writing computer software for 20 years as a professional in the industry and for nearly a decade before that. I currently write enterprise business applications for Oracle (yes, the 120,000+ DB company) targeted at installations of more than 10,000 users each. I have to say that the vast majority of comments on this thread are utter bullshit. I haven't had to use anything beyond basic algebra for my day-to-day job for my entire career.

On the other hand, I write 3D video games on the side for a variety of platforms, and I've had to learn a ton of math for my "hobby" programming, including lots of matrix/linear algebra, quaternion math, basic principles of linear and spherical-linear interpolation, tons of trigonometry, and a splash of calculus. I don't know what kind of niche programming these other guys are doing that require all of this other BS math, but I've never had to formulate proofs or even necessarily understand the fundamental principles behind much of the math that I do leverage. I just have to know how to translate formulas and papers from PhD theses into actual software code.

tl;dr You hardly need to know any math at all to have a very successful career in the software industry. career in the software industry != computer scientist

Similarly, many engineers need more math than architects.  Being able to design and build a house is a substantially different skill than being able to design and build a mile-long bridge or a CPU.  Writing enterprise applications is different from 3d game programming or designing a sound dependent type system or designing a novel cryptosystem, or proving a novel result in complexity theory.

That's not to say that one skill is better than the other, they're just different.  Is a Frank Lloyd Wright house any worse than the Golden Gate bridge because it required less math to design? I suppose that I did assume that by, "for computer science", he meant "for a career as a computer programmer". I suppose if he intends to get his PhD and develop new algorithms that the rest of us just leverage in our real-world software, then yes, I guess you're correct. Perhaps OP should clarify what he's asking. I suppose that I did assume that by, "for computer science", he meant "for a career as a computer programmer". I suppose if he intends to get his PhD and develop new algorithms that the rest of us just leverage in our real-world software, then yes, I guess you're correct. Perhaps OP should clarify what he's asking. </snippet></document><document><title>commit or rollback semantics for io?</title><url>http://www.reddit.com/r/compsci/comments/qkap1/commit_or_rollback_semantics_for_io/</url><snippet>Hey,

How would one implement commit/rollback semantics for io (especially multiple file io)?

For example, I want to:

    start()
    write("foo.txt", content)
    write("bar.txt", another_content)
    commit()

If something fails before commit() (or during executing commit()), I want to make sure both foo.txt and bar.txt aren't written. 

Some operating systems, you can write to temporary directory and call rename in commit(). But, what if something happens after renaming "/tmp/foo.txt"  but before renaming "/tmp/bar.txt" ?  Do you keep logs of io action and replay to recover? Writing to log is io, too. How do you know logging is always consistent, especially you're logging other io actions?

     I believe you can get atomic transactions with zfs. I don't know what a transaction is in the ZFS world, so this might not be at all what you are looking for.    </snippet></document><document><title>Help: is a subset unique?</title><url>http://www.reddit.com/r/compsci/comments/ql7f6/help_is_a_subset_unique/</url><snippet>Basically we&#8217;re given an array A[0 &#8230; n-1] of n elements. Given an index i where 0 &amp;lt;= i &amp;lt;= n-2 and index j where i &amp;lt; j &amp;lt;=n-1, we need to see if the subset of A[i &#8230; j] holds no duplicate values. We call this out "range uniqueness query." It returns a boolean whether or not the set holds unique values.  

Beginner&#8217;s approach: we can see a very easy to implement algorithm that requires no preprocessing simply checks each element in A[i &#8230; j]. This requires |A[i &#8230; j]|2 comparisons. This is also very bad. Conclusion: &#920;(n) space, &#920;(k pow2) RUQ run-time (given k = |A[i &#8230; j]|).  

Beginner&#8217;s approach 2: we can see multiple redundant comparisons are made when checling the array A[i &#8230; j]. First off, we needn&#8217;t compare it to itself. Also, one we check a pair of elements that pair is done. This requires ceiling((|A[i &#8230; j]| -1)2/2) comparisons. This is still very bad. &#920;(n) space, O(k pow2) RUQ run-time (given k = |A[i &#8230; j]|).  

We want to reduce the number of comparisons by constructing a datastructure to compute some of it beforehand. Then, the algorithm run on it takes care of the rest. Note that we only care about&#8230; A) The space of the resultant data structure B) The run-time of the RUQ  

We can take as long as we want to construct the data structure, as long as it doesn&#8217;t take up too much space.
We could compute all values of all ranges before hand in a lookup table, but that would take &#920;(n pow2) space, even though we get O(1) runtime.
Our prof gave us a hint saying it could be done looking at range minimum queries.
I was thinking, we could compute the uniqueness of subsets in the form of a segment tree, but if the elements of the subset do not form a whole branch of the tree, computations can get a bit out of hand.
Any ideas? Anyone have any links that could help me out?</snippet></document></searchresult>
<searchresult><compsci /><document><title>graduate student webpage template</title><url>http://www.reddit.com/r/compsci/comments/tfnu6/graduate_student_webpage_template/</url><snippet>i was working on my student webpage, after i thought for a second it seems most graduate students (I am in comp sci) seem to have dull wepages. I understand that its  not something that needs to be too flashy or anything , just something which gets the job done. 

Yet I was wondering what are the interesting graduate student webpages that  you hav come across.

(NOT directly related, to the compsci but i really didnot know where else to put it ) 
  [Pablo rauzy](http://pablo.rauzy.name/) [Guillaume allais](http://perso.ens-lyon.fr/guillaume.allais/) :-p.  I don't have links to anything, but agree that in recent times, UI/design is as important as function. There's a reason blackberry is having issues. How many times have you downloaded an app, had it work right, but uninstalled it because it looked a little dated/bad?    Another undergrad, with interactive canvas bird flocks.

[Eric Fruchter](http://www.ericfruchter.com/)</snippet></document><document><title>What document best exemplifies the field of computer science?</title><url>http://www.reddit.com/r/compsci/comments/tfk9h/what_document_best_exemplifies_the_field_of/</url><snippet>I'm taking an Advanced Writing in the Disciplines class over the summer, and my teacher has asked us to find a single document that best exemplifies our field. Since CS is such a broad field, with so many different areas of research, I'm having trouble narrowing it down to just one document. So what website, journal, blog or article do you all think would be the best to use as an example of computer science as a whole?  We started using MacCormick's new book: 
"9 Algorithms that Changed the Future" in our intro to comp sci course at my university. Of all the books I have come across it is by far the best at getting across why computer scientists love computer science to people who have no background in computer science. It does this by discussing 9 specific algorithms (page-rank, public-private key encryption, etc) in such a way that almost anyone can understand and relate to how the algorithm's work, without anything more than a basic (high school algebra) understanding of math.

http://www.amazon.com/Nine-Algorithms-That-Changed-Future/dp/0691147140   It's a large book The Art of Computer Programming Volume 1: Knuth  Northeastern? Yup!  Easy. Turing's paper on undecidability. It proves there is a limit to what we can compute. There are questions to which an answer does not exist. Computer scientists still have problems grasping the implications of this.  The main one that I'm considering is stackoverflow.com, because it shows the community aspects of the field well, which is what this assignment focuses on.

EDIT: I guess I should mention that this "document" can be just about anything, a website, a journal, an article, a blog, pretty much anything goes. This is a very broad and subjective question that totally depends on what "angle" you want to take.

When I first read it, my suggestion was going to be something like Turing's 1937 paper, "On computable numbers, with an application to the Entscheidungsproblem". In it he defines the Turing machine as well as proves the undecidability of the halting problem and Entscheidungsproblem. Basically, he sets the limits for what the entire future of what would be called "computer science" would be. Pretty monumental.

But if the assignment has to do with community stuff, then that's a whole 'nother ballgame. This is a very broad and subjective question that totally depends on what "angle" you want to take.

When I first read it, my suggestion was going to be something like Turing's 1937 paper, "On computable numbers, with an application to the Entscheidungsproblem". In it he defines the Turing machine as well as proves the undecidability of the halting problem and Entscheidungsproblem. Basically, he sets the limits for what the entire future of what would be called "computer science" would be. Pretty monumental.

But if the assignment has to do with community stuff, then that's a whole 'nother ballgame. The main one that I'm considering is stackoverflow.com, because it shows the community aspects of the field well, which is what this assignment focuses on.

EDIT: I guess I should mention that this "document" can be just about anything, a website, a journal, an article, a blog, pretty much anything goes. </snippet></document><document><title>Is the Burroughs architecture still relevant today?</title><url>http://www.reddit.com/r/compsci/comments/tf5hr/is_the_burroughs_architecture_still_relevant_today/</url><snippet>Reading [this interview](http://www.doc.ic.ac.uk/~susan/475/AlanKay.html) with Alan Kay, he makes a comment on the [Burroughs architecture](https://en.wikipedia.org/wiki/Burroughs_large_systems):

"In fact, the original machine had two CPUs, and it was described quite adequately in a 1961 paper by Bob Barton, who was the main designer. One of the great documents was called &#8220;The Descriptor&#8221; and laid it out in detail. The problem was that almost everything in this machine was quite different and what it was trying to achieve was quite different.

The reason that line lived on&#8212;even though the establishment didn&#8217;t like it&#8212;was precisely because it was almost impossible to crash it, and so the banking industry kept on buying this line of machines, starting with the B5000. Barton was one of my professors in college, and I had adapted some of the ideas on the first desktop machine that I did. Then we did a much better job of adapting the ideas at Xerox PARC (Palo Alto Research Center).

Neither Intel nor Motorola nor any other chip company understands the first thing about why that architecture was a good idea.

Just as an aside, to give you an interesting benchmark&#8212;on roughly the same system, roughly optimized the same way, a benchmark from 1979 at Xerox PARC runs only 50 times faster today. Moore&#8217;s law has given us somewhere between 40,000 and 60,000 times improvement in that time. So there&#8217;s approximately a factor of 1,000 in efficiency that has been lost by bad CPU architectures.

The myth that it doesn&#8217;t matter what your processor architecture is&#8212;that Moore&#8217;s law will take care of you&#8212;is totally false."

I'm wondering, if Alan Kay's comments are true, why such machine with that kind of performance isn't ubiquitous?   </snippet></document><document><title>Udacity Classes</title><url>http://www.reddit.com/r/compsci/comments/tf8mb/udacity_classes/</url><snippet>Hey! I was looking into doing a couple of the Udacity courses just to clean up on what I already know (just got my BA, starting the MA in the fall) or maybe learn something new. Have any of you taken a Udacity course? How was it? Did you learn something new?    </snippet></document><document><title>Any Portal 2 fans? Thoughts on Computability in Portal 2</title><url>http://www.reddit.com/r/compsci/comments/tdul2/any_portal_2_fans_thoughts_on_computability_in/</url><snippet>I hope this is an acceptable area to discuss this. Feel free to point me somewhere else if you feel it is more suitable for it. The Portal 2 map editor was released today and I was wondering how it could be used as a sandbox for computability like Minecraft is. I have an example map in the [workshop](http://steamcommunity.com/sharedfiles/filedetails/?id=68513261).  I'm just going to copy-paste the explanation I gave there:

&amp;gt; When I opened the editor my first thought was to build something crazy with the gels. I prompty became bored with the idea. However innovation never rests! My second instinct was to see if Portal 2 could be used to build basic logic gates and to consider the game in terms of computability.

&amp;gt; When you enter, to the left is and AND gate followed by an OR gate (I cheated somewhat and used AND and NOT to create OR rather than synthesis a unique approach). To you right is a NOT gate followed by a clock. 

&amp;gt; I've been able to make similar gates using mechanical components, but I prefer the "solid state" nature of the lasers.

&amp;gt; The nice part about constucting these is that (1) inputs and outputs can be connected in a many-to-many relationship and (2) outputs can be set to on or off initially (with a signal toggling them). So AND is simply two input piped into one output; it won't fire if they are not both on. NOT is piping an input into an output with the output on initially. OR I synthesised from the fact that a OR b &amp;lt;-&amp;gt; NOT(NOT(a) AND NOT(b)).

&amp;gt; I choose to use block buttons for inputs and lasters for outputs, but the choice is arbitrary.  Any of the inputs and outputs should work.

Thoughts comments suggestions?  For a proper OR gate you could just have two emitters aimed at one receptor, right?   </snippet></document><document><title>Estimating difficulty of instances of NP problems?</title><url>http://stackoverflow.com/questions/10508323/estimating-difficulty-of-instances-of-np-problems</url><snippet /></document><document><title>Why Tiny Transactions on Computer Science?</title><url>http://lmbgp.tumblr.com/post/22688605420/why-tinytocs</url><snippet>  </snippet></document><document><title>What should I learn this summer?</title><url>http://www.reddit.com/r/compsci/comments/taeqw/what_should_i_learn_this_summer/</url><snippet>Hi all,

I'm a soon to be college graduate with a math major, comp sci minor, and statistics minor. I am looking for something interesting and related to comp sci to learn this summer.  I hope whatever I study to be very interesting, and also improve my programming ability and problem solving ability.

Here are my ideas so far

1. Learn Haskell. I've never done anything functional, and I hear Haskell is interesting and makes you a better programmer.

2. Learn C. Haven't really done any low-level stuff.

3. Algorithms. I took an algorithms class, but it wasn't too rigorous.

4. Machine learning 5. Natural language processing. (These seem interesting)

6. Set theory and databases (My job next year will be working with databases)

I'd appreciate any input on what seems like the most interesting or what other suggestions you have. (Don't suggest Project Euler, I do that already). 

Thanks!

Edit: Thank you everybody! I think I'm going to learn a functional language, and that functional language will be Scheme (or Racket), as I found sicp to be more awesome than the Haskell resources. In conjunction with this, I'll be continuing project euler, and picking up emacs. Thanks for the advice!  if you're a math major i'd heavily recommend haskell. it'll connect your math and cs at an even deeper level.  &amp;gt; it'll connect your math and cs at an even deeper level.

Can you elaborate on this? with functional programming i find that everything has an additional level of abstraction that leads to close similarities between the mathematical representation of the problem and the solution, and the code that ends up being written.  &amp;gt; it'll connect your math and cs at an even deeper level.

Can you elaborate on this? &amp;gt; it'll connect your math and cs at an even deeper level.

Can you elaborate on this? &amp;gt; it'll connect your math and cs at an even deeper level.

Can you elaborate on this? [Because math!](http://hackage.haskell.org/package/category-extras) &amp;gt; it'll connect your math and cs at an even deeper level.

Can you elaborate on this? if you're a math major i'd heavily recommend haskell. it'll connect your math and cs at an even deeper level.   Algorithms! learn them and (also very important) implement some of them! its the best thing you can do to get that super amazing job. 

Good starting points:

http://uva.onlinejudge.org/index.php?option=com_onlinejudge&amp;amp;Itemid=8&amp;amp;category=3
http://code.google.com/codejam/contests.html

Do [at least] one everyday!  Being a cs/math double major myself, here's what I find most interesting:

**Algorithms:** I'm not sure how rigorous your class was, but here are some things to look at:

* Formally proving the correctness and runtime of algorithms.

* Proofs of NP-completeness, seeing that solving all of the well-known NP-complete problems are indeed equivalent.

* Solving/programming solutions to various problems. I guess this is what people usually think of when they think of algorithms. I'm sure there are plenty of books with interesting problems.

**Functional Programming:** Very awesome, a completely different way of thinking about programming. You should also consider looking at the Lisp family, unless your mind is set on Haskell. I love my fully parenthesized expressions.

**Kolmogorov Complexity:** Not practically useful in any way shape or form, but very interesting. Give the wikipedia article a thorough read; see if you find it interesting. This is definitely in the intersection of CS and math. Kolmogorov Complexity is very useful if you can find a way to approximate it...many people try zipping files. Shannon Mutual information is also cool.  [Chaitin's incompleteness theorem](http://en.wikipedia.org/wiki/Kolmogorov_complexity#Chaitin.27s_incompleteness_theorem) says you can't ever hope to even approximate the Kolmogorov Complexity of sufficiently long strings. In particular, there is a constant *c* such that for no string *s* can you prove that s has complexity greater than *c*.        If you're going for ML, then don't do the math by hand. It's very tedious and simple. Learn to implement it, you should be able to implement reinforcement learning to solve a maze.

I also recommend [Weka](http://en.wikipedia.org/wiki/Weka_(machine_learning\)) as their API is quite good. Why do you recommend not doing the math? It's simple, you're right, but it gives you a handle on what's actually going on.

I really dislike just learning to implement without knowing the math back and forth. Well I'm more talking about that he doesn't focus on only doing the math by hand (since he's a math major). Of course he should learn what's going on and doing a few examples by hand is a good way to do that. But don't go on solving large examples by hand, since it's very tedious and you get very little out of it. Haha, no, large examples are a little silly. I'd think proving optimization and maybe convergence results is pretty damn useful though.   Learn C, and Learn Haskell. Algorithms are simple to learn and implement if you have thorough knowledge of the languages used, and Language processing is outside your scope, if you're just beginning in the realm of AI. Try something a bit simpler, such as Neural Networks and Databases, as they often are conceptually intertwined, building up to Language processing. You've got a good list there with just the first 2. That will *definitely* take up your summer, and to learn it well, longer than that.

Cheers. &amp;gt; Algorithms are simple to learn and implement if you have thorough knowledge of the languages used.

What? No. How would knowledge of C help you to understand for exmple Shor's algorithm?

Algorithm doesn't mean sorting a list or finding maximal element. There is so much algorithms that you won't even know what they are useful for if you only know C or Haskell... Learn C, and Learn Haskell. Algorithms are simple to learn and implement if you have thorough knowledge of the languages used, and Language processing is outside your scope, if you're just beginning in the realm of AI. Try something a bit simpler, such as Neural Networks and Databases, as they often are conceptually intertwined, building up to Language processing. You've got a good list there with just the first 2. That will *definitely* take up your summer, and to learn it well, longer than that.

Cheers.    2 dot. C is always a good language to learn, even if you rarely use it in practice.

6 dot. If you haven't already, the foundation of Computer Science: [Theory of Computation](http://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/053494728X)
        Develop something for Android or iOS.  Learning Haskell well will be a great intro to algorithms and machine learning since it is really easy to implement. 

Don't bother with C unless you want to go low level... If you do I recommend you pick up an arduino, some leds, and sensors to play with. Arduino is nice a great way to get started with minimal C and you get more interesting results than command line output at the beginning. 

Finally set theory isn't databases. If you want to learn database design study the theory like the normal forms and then read through wp code, phpMyAdmin, and other database backed open source projects to get a feel for the application vs the theory. 

Just my 2 cents. Learning Haskell well will be a great intro to algorithms and machine learning since it is really easy to implement. 

Don't bother with C unless you want to go low level... If you do I recommend you pick up an arduino, some leds, and sensors to play with. Arduino is nice a great way to get started with minimal C and you get more interesting results than command line output at the beginning. 

Finally set theory isn't databases. If you want to learn database design study the theory like the normal forms and then read through wp code, phpMyAdmin, and other database backed open source projects to get a feel for the application vs the theory. 

Just my 2 cents. The Arduino's alright, but there are embedded limitations that can make it difficult if you're just learning C.  I used a Nano, so it might be somewhat different on other versions, but it used 16 bit integers, and had limited memory, so if I had too many string literals I'd run out of space and get bizarre errors.  It was a good experience for me learning to diagnose and debug those problems, but not a good environment for learning C.  1. Learn Haskell: Honestly, you won't need this right away. There are few Haskell jobs, however expect Haskell to increase in importance in the long term. Do it later in the middle term future. Priority 3.

1. Learn C: You don't need to learn C to get your job done in the current job market that's heavily based on C++, Java, C#, PHP, Python, Perl, Ruby, and ECMASCRIPT. The only thing of value you'll learn is pointers and memory management, which is not super important. You'll learn those with C++ anyway. You've got plenty of time to learn C proper. Do it later because there aren't many C only jobs these days. Priority 5.

1. Algorithms. I would focus on this. When you start interviewing for jobs, this will single-handedly be the most important subject you will have studied. Expect lots and lots of algorithm and data structure questions on your job interviews. Priority 1.

1. Machine learning. Unless you have a Master's or PhD degree, you won't be doing much detailed level work with this. Learn it later. Priority 4.

1. Natural language processing. There's a lot of this going on with search and indexing technology jobs, so I think this could come in handy. Learn it over the winter or following summer. Priority 3.

1. Set theory and databases. Learning how to solve typical problems with relational, multi-dimensional, and no-SQL databases is high priority. Expect relational queries and database design questions on the job interviews. Set theory proper (as in lots of maths) can wait, but you might as well learn it along with databases because they are so inter-related. This is priority 2.

1. Euler questions. Do these on occasion from now until you retire or die. Also look into general puzzles and riddles. They help get the brain flowing. Priority 1.

So, to recap (1 = do immediately, 5 = do later) my suggested TODO list in order of what to start first:

1. Algorithms &amp;amp; Euler puzzles
2. Databases and set theory
3. Haskell language &amp;amp; natural language processing
4. Machine learning
5. C language The OP never said they were looking to buffer their resume; they are looking to broaden their problem solving abilities and study interesting and *computer science* related topics.

Your list is better suited for someone looking to get a quick IT job in database management. The OP never said he wasn't either. He said:

&amp;gt; I'm a soon to be college graduate

He asked for advice and I gave him advice that would maximize his chances for getting a good job. What exactly is your problem with someone providing what he asked for? The issue I took with your advice was that you assume that a "good job" is one that, in your words:

1. Is not a "Haskell job"

2. Has no need for machine learning concepts/experience

3. Has no need for low-level programming concepts/experience (i.e. C)

Because of your assumptions, you gave advice that frankly isn't suited for someone looking for an interesting exploration into some rather important computer science topics. I mean no offense, but this is r/compsci, not r/programming. Now you're just plain making stuff up. I told him those were worthwhile and to learn them after. It was solid advice.

Secondly, people getting computer science degrees are doing it to get a job. Very few of us have the luxury of going to university and spending the rest of our lives following our interests completely unpaid. It's why I got a computer science degree. To work in the computer science field.

You know what? I'm getting really sick of you responding with your douchy comments. You're lying about what I wrote and for some reason you got a hard-on regarding my comments. How about this? Go fuck yourself and stop giving me your shit opinions. Just go away. 1. Learn Haskell: Honestly, you won't need this right away. There are few Haskell jobs, however expect Haskell to increase in importance in the long term. Do it later in the middle term future. Priority 3.

1. Learn C: You don't need to learn C to get your job done in the current job market that's heavily based on C++, Java, C#, PHP, Python, Perl, Ruby, and ECMASCRIPT. The only thing of value you'll learn is pointers and memory management, which is not super important. You'll learn those with C++ anyway. You've got plenty of time to learn C proper. Do it later because there aren't many C only jobs these days. Priority 5.

1. Algorithms. I would focus on this. When you start interviewing for jobs, this will single-handedly be the most important subject you will have studied. Expect lots and lots of algorithm and data structure questions on your job interviews. Priority 1.

1. Machine learning. Unless you have a Master's or PhD degree, you won't be doing much detailed level work with this. Learn it later. Priority 4.

1. Natural language processing. There's a lot of this going on with search and indexing technology jobs, so I think this could come in handy. Learn it over the winter or following summer. Priority 3.

1. Set theory and databases. Learning how to solve typical problems with relational, multi-dimensional, and no-SQL databases is high priority. Expect relational queries and database design questions on the job interviews. Set theory proper (as in lots of maths) can wait, but you might as well learn it along with databases because they are so inter-related. This is priority 2.

1. Euler questions. Do these on occasion from now until you retire or die. Also look into general puzzles and riddles. They help get the brain flowing. Priority 1.

So, to recap (1 = do immediately, 5 = do later) my suggested TODO list in order of what to start first:

1. Algorithms &amp;amp; Euler puzzles
2. Databases and set theory
3. Haskell language &amp;amp; natural language processing
4. Machine learning
5. C language who said there are no C jobs? According to http://www.tiobe.com/index.php/content/paperinfo/tpci/index.html C is the "most popular language". Also having a solid understanding of pointers will give you a leg up on a lot of other people in the industry as too many folk let the interpreter or jvm take care of all that stuff and don't ever think about it.  Are we *really* going to have this debate? Come off it. Tiobe is pure BS as a job metric.

Tiobe is a search metric. It is not a job metric. This is such an obvious point I feel no compulsion to bother debating this further.  Haskell Haskell Haskell Haskell Haskell Haskell Haskell Haskell!

Seriously, there's never been a better time.

Free tutorials

* [Try Haskell](http://tryhaskell.org/)
* [Learn You A Haskell](http://learnyouahaskell.com/)
* [Real World Haskell](http://book.realworldhaskell.org/read/)
* [Parallel Programming with Haskell](http://www.yellosoft.us/parallel-processing-with-haskell)            C is not fucking low level. Shits me to tears how many people say this.  C is not fucking low level. Shits me to tears how many people say this.  C is not fucking low level. Shits me to tears how many people say this.  C is not fucking low level. Shits me to tears how many people say this.  C is not fucking low level. Shits me to tears how many people say this.   </snippet></document><document><title>What happens when you use a linear random number generator to initialize a linear hash function</title><url>http://hpaste.org/68149</url><snippet>  (X-post /r/programming)

This is in Java, so we're using Java's PRNG which is linear. Its algorithm is fully described in the documentation: http://docs.oracle.com/javase/6/docs/api/java/util/Random.html#nextInt(int)

UPD: Using a Mersenne Twister gave me hashA = 763203480 and hashB = 765572306 which also gave terrible results. Is my hash broken? &amp;gt;long res = ((hashA * i + hashB) &amp;amp; ((1L &amp;lt;&amp;lt; 31) - 1)) % 1000;

So, ignoring the weird bit twiddling (it looks like you're trying to clear the sign bit, but I am not even sure that's the right number of bits for a long), once hashA and hashB are fixed, this isn't going to work so great for you.

Due to a property of modular fields, you can pretty much ignore hashB because all it does is offset the sequence of numbers hashA * i gives you. So what does hashA * i do? Well, if hashA and 1000 share any prime factors, then you will not cover all possible values in the field.

For example, if 1000 is divisible by (hashA % 1000) then, trivially, there exists an integer n such that (hashA % 1000) * n == 1000, namely 1000/(hashA % 1000). That number is, necessarily, less than 1000, which means your sequence resets to zero in less steps then there are possible values in the field. What this essentially means is that all elements that are not a multiple of (hashA % 1000) are skipped (once again, adding hashB just offsets the sequence).

Now then, you might think that this should be a relatively rare occurrence, that an integer perfectly divides 1000. But, actually this thing falls apart if (hashA % 1000) shares any prime factors with 1000 (p). Here's a proof:

    n = hashA % p
    p and n share prime factors, so there must exist i * r == n such that p is divisible by i
       pick the largest such i
    Solve: x * n % p == 0
      x * n == y * p, for some y
        -&amp;gt; x * (i * r) == y * p
        -&amp;gt; x * r == y * (p / i) (p / i is an integer value since p is divisible by i, as a given)
        -&amp;gt; x == (y / r) * (p / i) (r and p do not share any factors so they cannot divide)

Given this, there is a solution (that is, x * n % p == 0) for values of y that are divisible by r. The lowest such value is r itself, therefore x == p / i. Well, p / i, is, unfortunately, less than p. So you still haven't covered all the values of p, because when you hit that value of x, you reset your sequence because you're back at zero.

The solution is to ensure that hashA and p are relatively prime. Thanks!
Another error is that I should have used % instead of &amp;amp;, as this way I'm not computing modulo 2^31 - 1, but modulo simply 2^31. Interesting - What are your results now that you've changed your hash algorithm?

Some colleagues of mine used Bob Jenkins hash function work which might be interesting to you: 
http://www.burtleburtle.net/bob/hash/ &amp;gt;long res = ((hashA * i + hashB) &amp;amp; ((1L &amp;lt;&amp;lt; 31) - 1)) % 1000;

So, ignoring the weird bit twiddling (it looks like you're trying to clear the sign bit, but I am not even sure that's the right number of bits for a long), once hashA and hashB are fixed, this isn't going to work so great for you.

Due to a property of modular fields, you can pretty much ignore hashB because all it does is offset the sequence of numbers hashA * i gives you. So what does hashA * i do? Well, if hashA and 1000 share any prime factors, then you will not cover all possible values in the field.

For example, if 1000 is divisible by (hashA % 1000) then, trivially, there exists an integer n such that (hashA % 1000) * n == 1000, namely 1000/(hashA % 1000). That number is, necessarily, less than 1000, which means your sequence resets to zero in less steps then there are possible values in the field. What this essentially means is that all elements that are not a multiple of (hashA % 1000) are skipped (once again, adding hashB just offsets the sequence).

Now then, you might think that this should be a relatively rare occurrence, that an integer perfectly divides 1000. But, actually this thing falls apart if (hashA % 1000) shares any prime factors with 1000 (p). Here's a proof:

    n = hashA % p
    p and n share prime factors, so there must exist i * r == n such that p is divisible by i
       pick the largest such i
    Solve: x * n % p == 0
      x * n == y * p, for some y
        -&amp;gt; x * (i * r) == y * p
        -&amp;gt; x * r == y * (p / i) (p / i is an integer value since p is divisible by i, as a given)
        -&amp;gt; x == (y / r) * (p / i) (r and p do not share any factors so they cannot divide)

Given this, there is a solution (that is, x * n % p == 0) for values of y that are divisible by r. The lowest such value is r itself, therefore x == p / i. Well, p / i, is, unfortunately, less than p. So you still haven't covered all the values of p, because when you hit that value of x, you reset your sequence because you're back at zero.

The solution is to ensure that hashA and p are relatively prime. </snippet></document><document><title>A few questions on computer chess</title><url>http://www.reddit.com/r/compsci/comments/t7udq/a_few_questions_on_computer_chess/</url><snippet>1. Do we know exactly how big the full game tree for chess is? My AI book mentioned that it was about 10^40 nodes, but I was just curious if we had an exact value, and more importantly, how we computed that value.

2. Is solving a game like chess ever going to be feasible? About how long would it take our best computers right now to do? Is the problem more a matter of time or space?   With respect to question (2), the number of possible states is huge.  I don't mean "wait fifty years for memory to catch up" huge.  I mean "how many atoms are there in the solar system?" (if not more) huge.  There's a reason for opening book and closing book, but not middle-of-the-game book. With respect to question (2), the number of possible states is huge.  I don't mean "wait fifty years for memory to catch up" huge.  I mean "how many atoms are there in the solar system?" (if not more) huge.  There's a reason for opening book and closing book, but not middle-of-the-game book. With respect to question (2), the number of possible states is huge.  I don't mean "wait fifty years for memory to catch up" huge.  I mean "how many atoms are there in the solar system?" (if not more) huge.  There's a reason for opening book and closing book, but not middle-of-the-game book. In fifty years, according to Moores law, we will have around 40 000 000 TB of primary storage. The state of a chess board requires 32 B. That means you can, in fifty years time, keep 10^18 states of the board - just a teeny, tiny small fraction of the size of the tree &#8211; in primary memory. If my maths are not incorrect, we'll have to wait about 160 years for all possible 10^40 game states.

I think it's fair to say our model of computing might have changed a little by then. In fifty years, according to Moores law, we will have around 40 000 000 TB of primary storage. The state of a chess board requires 32 B. That means you can, in fifty years time, keep 10^18 states of the board - just a teeny, tiny small fraction of the size of the tree &#8211; in primary memory. If my maths are not incorrect, we'll have to wait about 160 years for all possible 10^40 game states.

I think it's fair to say our model of computing might have changed a little by then. Moore's law is already antiquated. Processor speed has not increased much lately, we've just started adding more cores. Moore's law never said anything about frequency, but rather the number of transistors. It's simply that the transistors are now divided into more cores than going into additional speed. Moore's law still applies, but now it counts the number of cores you have instead of frequency, to put it very bluntly. Ah yes, you're right.  
  
However, the benefit of dividing the transistors into more and more cores is not as great, because it only decreases run-time for tasks that can be run in parallel. And I believe many tasks can benefit from running in parallel. The problem is that it's bloody difficult to write that code.  From the paper stordoff posted:

"Another (equally impractical) method is to have a 'dictionary' of all possible positions of the chess pieces. For each possible position there is an entry giving the correct move (either calculated by the above process or supplied by a chess master.) At the machine's turn to move it merely looks up the position and makes the indicated move. The number of possible positions, of the general order of 64! / 32!(8!)2(2!)6, or roughly 10^43, naturally
makes such a design unfeasible."  
  
So 10^43 doesn't come from a tree, but rather a dictionary.  
  
On the subject of trees, they mention:  
  
"A typical game lasts about 40
moves to resignation of one party. This is conservative for our calculation since the
machine would calculate out to checkmate, not resignation.
However, even at this figure there will be 10^120 variations to be calculated from the initial
position." I'm not a chess player, but  when asked by others how chess could be solvable, I use the dictionary argument and came up with 2 * 13^64 (less than 10^72 ) as a very generous upper bound on the number of possible positions in chess.  

There are 6 pieces in chess (pawn, rook, knight, bishop, queen, king), so 12 pieces counting the different colors, and then we can consider a blank piece, giving 13 total "states" for each square on the board.  Thus there are at most 13^64 total board positions.  It can be either player's turn, so you multiply this by 2 for the total number of  game states.

Now I understand there are restrictions in chess which let us refine this number.  For example:

* At least 32 squares must be blank.
* No more than one king on each side.
* No more than 10 of any other piece per side.
* Some game positions cannot be reached.

But using these makes the calculation less clean.  Can someone explain the 10^43 number?  I don't have an answer for you but a question for anyone else that might see this.

Would it be feasible for the computer to look at the current board position and then look out 2-3 moves and find the one most favorable to it? 
Not all the way to checkmate (unless that is in the next few moves).

Like [this](http://imgur.com/1SoHt).

Would that work? I don't have an answer for you but a question for anyone else that might see this.

Would it be feasible for the computer to look at the current board position and then look out 2-3 moves and find the one most favorable to it? 
Not all the way to checkmate (unless that is in the next few moves).

Like [this](http://imgur.com/1SoHt).

Would that work? I think in a game like chess, there are no "favorable" moves when considering such a short time-span like two or three turns. Nothing so short-term as that is going to be optimal to the overall game, or even an overall strategy.

There is a realistically finite number of moves in two or three turns, but your definition of favorable is what would trip up programming something like this. Yah, I thought it would be something like that. When you're playing realistically you are thinking 10-15 ahead but from a programming perspective that doesn't seem efficient. 

Do you know where I could read up on actual chess AIs? I don't have an answer for you but a question for anyone else that might see this.

Would it be feasible for the computer to look at the current board position and then look out 2-3 moves and find the one most favorable to it? 
Not all the way to checkmate (unless that is in the next few moves).

Like [this](http://imgur.com/1SoHt).

Would that work? Yes, that is exactly how almost all chess engines work (they usually search deeper than 2-3 moves though). Wikipedia has a lot of good articles on the subject of [Computer chess](https://secure.wikimedia.org/wikipedia/en/wiki/Computer_Chess) if you want to read more. For search in particular, have a look at [Alpha-beta pruning](https://secure.wikimedia.org/wikipedia/en/wiki/Alpha-beta_pruning). Also, for more general information on game theory / AI, check out [game trees](http://en.wikipedia.org/wiki/Game_tree) and the [minimax algorithm](http://en.wikipedia.org/wiki/Minimax). Also just noticed that my AI professor is one of the sources for the game tree page. A [link](http://www.cs.umd.edu/class/spring2010/cmsc421/chapter06.pdf) to his lecture on "game playing" if you are interested. Alpha-beta pruning *IS* minimax. Only a bit optimized. I don't have an answer for you but a question for anyone else that might see this.

Would it be feasible for the computer to look at the current board position and then look out 2-3 moves and find the one most favorable to it? 
Not all the way to checkmate (unless that is in the next few moves).

Like [this](http://imgur.com/1SoHt).

Would that work? That's exactly how they work. I wrote one a few years ago, reached about 2600 ELO. At a high level it searches D moves deep and looks at the resulting board for all possible moves. Then it makes the move that leads to the most favourable board. As someone else already posted, Alpha-Beta pruning is the algorithm to use and there are many variations and tweaks to be done. Do you mean 2600? Kasparov was only 2850ish.    [deleted] Yes, chess on an nxn board is known to be EXP-complete.  A small nit, though: this only means that determining who wins from an arbitrary position on the board is hard.  As far as we know, there *could* still be a feasible strategy that wins the game from the start.

As an example of how this could be true for a slightly different game, consider playing two chess games at once, with the roles reversed in the two games.  There's a trivial strategy for the second player to draw: always just copy the opponent's move.  (Assume the order in which the players move makes this possible for one of the players.)  But even though it's so easy to force a draw *if you play that way from the start*, that doesn't make it any easier to say what will happen if someone just gives you an arbitrary position on both boards.

Not that anyone has suggested a trivial strategy exists for chess, but I haven't seen any work toward *proving* hardness of playing a game from the start.

(ETA: looks like rspeer beat me to it)  A relevant link from Redditor NoMaths on another thread: [Solving the King's Gambit](http://chessbase.com/newsdetail.asp?newsid=8047).

Edit: I see I was duped. Thanks to calebh for pointing it out.   &amp;gt;how we computed that value.

Recursively! Honestly I don't know, but it wouldn't surprise me. 

As for solving the game of chess, if you're talking about all possible moves, I don't think it's going to happen any time soon. [Here's some numbers](http://electronics.howstuffworks.com/chess1.htm) and it gets out of control fairly quickly.  Not all solutions involve evaluating every game state separately, though. It's possible that, after searching for long enough, a computer could collapse chess into a manageable number of dominant strategies. It seems unlikely, but it's possible.

Consider [Z&#233;rtz](http://www.boardgamegeek.com/boardgame/528/zertz).  It's one of a set of beautiful modern strategy games by Kris Burm. It has a highly branching game tree in its opening; I think that on a standard 37-tile board, there are 54 distinct opening moves (disregarding symmetries) and hundreds of distinct responses.

But surprisingly and disappointingly, the standard game of Z&#233;rtz [is nearly solved](http://www.brettspill.no/blog.aspx?blogId=371). I say "nearly solved" in that almost all of the moves that Player 1 can make lead to a dominant strategy in which Player 2 can force a win. You could think of Z&#233;rtz as splitting into 54 subgames after the first move, and 41 of them are completely solved.

(The fix, incidentally, is to play on a larger board.)

The people who solved most of Z&#233;rtz didn't even come close to analyzing all possible positions; they just analyzed strategies in certain positions where you can repeatedly force your opponent's move and exploit that to win, and showed how to construct such a position from most opening moves. &amp;gt;Not all solutions involve evaluating every game state separately, though. It's possible that, after searching for long enough, a computer could collapse chess into a manageable number of dominant strategies. It seems unlikely, but it's possible.

I'm assuming you're talking about repeating branches that were covered somewhere else in the tree. 

&amp;gt;The people who solved most of Z&#233;rtz didn't even come close to analyzing all possible positions; 

The reason I included something that includes all possible positions is because OP didn't define "solved". By "solved" I assumed every possible move, and possibly not including repetitive moves where you avoid checkmate. 

I was merely trying to say that it's virtually impossible for current computers to identify every and all possible moves in a long enough game of chess. Of course, not focusing on only winning games but instead on all possibilities. By "solved" I personally would assume an O(logn) search of a tree (where let's say we track likelyhood of success) containing all possibilities where rather than use algorithms the computer simply searches for next (most likely successful) move and follows the instructions in that node. 

But by all means, I could be wrong and would love to hear any input on the matter.  To solve a game means to determine one player's optimal strategy, taking into account all moves *their opponent* could make. If your solution is a win for player 1, you don't have to evaluate all possible moves player 1 could make, because player 1 is going to follow the strategy and win.

[Checkers is solved](http://www.sciencemag.org/content/317/5844/1518.abstract) (Schaeffer et al., 2007), even though not all possible positions are solved.

And you can solve many branches of a game with the same strategy without those branches being identical. Think of how to play King + Rook vs. King in chess. Do you memorize what to do in every position? Do you have to memorize it completely separately if there's a random pawn or two on the board? No, you memorize the *plan* of how to force the king into checkmate, and you make whichever move advances the plan. Quick question. Wouldn't this easily be solved by algorithms by simply thinking a few moves ahead with the likelihood of success (based on said algorithms, rather than a k-ary tree)? Which would easily answer/undo the original question. 

So far what I'm understanding from your arguments is that only the winning moves matter, anything else is "trash" wasting valuable computing resources.   

The second half of your comment is exactly what I'm conflicted about. There are algorithms that allow modern computers to play chess (possibly even better than humans) by simply using algorithms rather than going through all predefined possible moves. So far I've assumed OP wanted all of the possible chess moves calculated with every move leading to a new node and so forth. I'm not saying chess can't be solved by using AI on a case by case basis, I just assumed "solving" it involved finding ALL moves possible in the game of chess, where every possible start leads to every possible ending stored in a k-ary tree.  Quick question. Wouldn't this easily be solved by algorithms by simply thinking a few moves ahead with the likelihood of success (based on said algorithms, rather than a k-ary tree)? Which would easily answer/undo the original question. 

So far what I'm understanding from your arguments is that only the winning moves matter, anything else is "trash" wasting valuable computing resources.   

The second half of your comment is exactly what I'm conflicted about. There are algorithms that allow modern computers to play chess (possibly even better than humans) by simply using algorithms rather than going through all predefined possible moves. So far I've assumed OP wanted all of the possible chess moves calculated with every move leading to a new node and so forth. I'm not saying chess can't be solved by using AI on a case by case basis, I just assumed "solving" it involved finding ALL moves possible in the game of chess, where every possible start leads to every possible ending stored in a k-ary tree.  &amp;gt; Quick question. Wouldn't this easily be solved by algorithms by simply thinking a few moves ahead with the likelihood of success (based on said algorithms, rather than a k-ary tree)? Which would easily answer/undo the original question.

If you have a solution, your likelihood of success for each move is either 100% or 0%. If you have a heuristic that does things in terms of "win probabilities", that's just a game AI, and it can be beaten by a better AI or by a complete solution. Computers can play chess better than humans now, but some computers still play chess much better than other computers, and those computers will later be beaten by other computers. Playing better than humans is not what "solved" means.

&amp;gt; I just assumed "solving" it involved finding ALL moves possible in the game of chess, where every possible start leads to every possible ending stored in a k-ary tree.

But that's not what "solved" means either. Checkers is solved. This is a solid, peer-reviewed fact. But no computer has ever analyzed every possible position in checkers. (That would be "strongly solved", as DataWraith points out.)

I think what's going on here is you may not realize that the word "solved" is a term of game theory, with a definition. Honestly, I appreciate you being so patient and answering all of my questions. I'm not going to lie, some of this is still "out there" beyond my current understanding, but I will look into it. I understand what you mean, but it does honestly feel like "I'm here taking up oxygen" simply because there may be a simple, viable and complete solution (which I'm pretty sure fits your definition of solved) while I'm too busy thinking of irrelevant factors. From what you're telling me so far (sorry if I'm wrong) being able to determine either a or the next winning move fits the definition of "solved" more than being bale to determine the next moves(s) no matter what they or their consequences are. 

&amp;gt;I think what's going on here is you may not realize that the word "solved" is a term of game theory, with a definition.

This may be the answer I'm looking for. To be honest I don't have much experience with game theory but always considered looking into it. I think now would be a better time than any. 

So I may be wrong, but trying to oversimplify this "solved" seems to be along the "TL:DR" version of all the best moves in response to your current move. All the winning moves, disregarding how this all started, and  potentially the "least worst" move given a certain scenario. Without absolutely any prerequisite of the "program" being exposed to chess previously. Kind of like "here's the answer, don't let politics, religion, etc affect it, and we're terribly sorry if nothing goes as planned while expecting basic mathematical rules to "pull through"". </snippet></document><document><title>Data and Codata: a short sketch of the motivation for the concept of codata</title><url>http://blog.sigfpe.com/2007/07/data-and-codata.html</url><snippet>   </snippet></document><document><title>unique tuples in product of two ordered set-like lists.</title><url>http://www.reddit.com/r/compsci/comments/t7p9p/unique_tuples_in_product_of_two_ordered_setlike/</url><snippet>Say for instance we have two ordered set-like lists:
A,B,D,E and B,C,D

then the product of these sets is: {AB,AC,AD,BB,BC,BD,DB,DC,DD,EB,EC,ED}

of which if we don't consider order to be important, should have DB removed to end up with unique pairs.

At present, i'm achieving this like:

	foreach x in A
	  foreach y in B
	     if ! (x&#8712;B and y&#8712;A and y&amp;lt;x)
	       add (x,y) to result

I'm wondering if we can do better? Certainly I could use actual sets (rbtree maybe) instead of ordered lists so that the inclusion test is logarithmic instead of linear... but would be much nicer to be able to remove a need for such a test in the first place.  This belongs in [r/algorithms](/r/algorithms), but I'll answer here anyway.

First find the 3 sets in O(N) time:

- (U), difference of (A) and (B)
- (V), difference of (B) and (A)
- (W), intersection of A and (B)

the size of the product as defined is then equal to

|U| * |B| + |V| * |A| + |W| * (|W|+1)/2

This is because there are no duplicates of pairs containing elements of (U) and (V); further, all pairs in (W) are duplicated except those of the form (XX), where (X) &#8715; (W).

Using this logic it shouldn't be hard to find all members of the resulting set in O(|A|+|B|+|K|) time.</snippet></document><document><title>Parallel Computing Wrapper</title><url>http://www.reddit.com/r/compsci/comments/t5w36/parallel_computing_wrapper/</url><snippet>I have a fairly naive question for you lot. I have a program that is designed to be run in a multi-threaded environment (a simple command-line switch specifies the number of cores to use). My questions are:

1) Can I run the program using cores from 2 or more CPUs, or am I limited to the number of cores in one CPU?

2) Can I write an MPI wrapper for said program to take advantage of parallel computing clusters?

Thanks!   &amp;gt; 1) Can I run the program using cores from 2 or more CPUs, or am I limited to the number of cores in one CPU?

Nope. In fact, a process running inside an operating system shouldn't actually be able to distinguish between cores on one CPU, on multiple CPUs, or even "virtual CPUs" as HyperThreading provides. I'm not sure whether the OS sees the difference, though this seems unlikely, but you won't unless you're writing kernel code.

So, figure out the number of virtual cores you have, and tell it to use that many cores. On my Linux machine, this works:

    grep 'processor' /proc/cpuinfo | wc -l

It is sometimes advisable to run exactly one extra thread, in case the other threads get blocked doing something. On the other hand, depending on cache sizes and amount of RAM, it is also sometimes advisable to run *fewer* threads than you have cores available.

&amp;gt; 2) Can I write an MPI wrapper for said program to take advantage of parallel computing clusters?

MPI = "Message Passing Interface", so probably not. Ultimately, this depends how finely-grained your threading is, whether you have source, etc. If you can imagine doing this with multiple processes on the same machine (rather than threads), you can probably make it work on multiple machines.

Disclaimer: I've never done MPI.   1) no you could run as many processes as you want, but typically the kernel will just handle the processes and end up context switching between them when you have more processes than cores.


2) This is very vague - maybe try using CUDA or something else before this. MPI can be a bit of a pain / overhead if you've never used it before ( not just the api, but also making sure it works with the underlying system properly). Does CUDA have any advantages over OpenCL? Because from what I can tell, CUDA is nVidia-only, while OpenCL also works on AMD cards and on CPUs. Well, CUDA api is easier to get into, at least if you are only using single GPU. Also CUDA is more mature than OpenCL and there might be slight performance gain compared to OpenCL on Nvidia cards. But pretty much everything you can do with CUDA, can be done with OpenCL too, and you can write multithreaded code with OpenCL to CPUs as well. The kernel programming, which in my opinion is the hard part in GPGPU, is almost identical for CUDA and OpenCL.

Edit: while you can write code with OpenCL for both CPUs and GPUs you still need to optimize your kernels separately, so the benefit you from using just OpenCL instead of CUDA and e.g. OpenMP might not be that great. The CPU isn't the main gain, in my opinion -- though it is nice to know that your code will at least theoretically run unmodified on a CPU, sort of like how it's nice to have a software renderer for OpenGL.

The main gain is portability. Why lock yourself into exactly one vendor? Even if there is a performance gain, that plus the learning curve isn't a good enough reason, IMO. 1) no you could run as many processes as you want, but typically the kernel will just handle the processes and end up context switching between them when you have more processes than cores.


2) This is very vague - maybe try using CUDA or something else before this. MPI can be a bit of a pain / overhead if you've never used it before ( not just the api, but also making sure it works with the underlying system properly). Thanks for the reply. From a cursory Google search, CUDA would allow me to use the GPU, but I'm not sure this is necessary if I can just use more threads (as I would need multiple high-power GPUs). Would a simple threaded app be limited to the particular node/computer I run it on? &amp;gt; but I'm not sure this is necessary if I can just use more threads


Using the GPU does exactly this ( as long as you're not memory bound). As long as you don't use too much shared memory ( if the work is mostly SIMD ) CUDA can run hundreds of threads at a time (some of the moderately cheap gpus support up to 512 threads...). With the GPU you only have to worry about one piece of hardware functioning as expected. As far as an entry point to parallel computing I would STRONGLY suggest CUDA.


&amp;gt; Would a simple threaded app be limited to the particular node/computer I run it on?


What do you mean? Are you asking if you use a threads api (such as C++11 threads, pthreads, etc...) will your application only run on a single system? The answer for that is yes. You have to use something like mpi to utilize multiple physical systems.


MPI allows for a somewhat transparent transport protocol along with an environment similar to a typical threading api (minus easy access to shared memory).


Again I suggest cuda for a couple reasons:
1. Its a single piece of hardware - you can get one for your desktop.
2. You don't have to worry about any of the transport issues with mpi ( depending on if there is a cluster available and how well managed it is this could be easy or extremely difficult)
3. Threads. If you're doing things like vector addition or scalar vector operations then you can run hundreds of threads at once.
4. Cost. GPUs are much cheaper than a whole system.


I could go on but basically I suggest try CUDA. I've written hybrid gpu/cpu programs and understand the problems with each, and looking back I think understanding the GPU parallelism would have helped me more than MPI. They both have their places, but as an entry point, definitely gpus. Thank you very much for the intro. I hadn't realized how many threads a single GPU could handle, so I'll take your advice and look into CUDA. Just for example [this](http://www.newegg.com/Product/Product.aspx?Item=N82E16814125364) has 512 cores. As long as each thread doesn't need tons of memory to accomplish its task, you should be ok.


Compare that to 512 cores of cpus (price-wise) - and you'll see why. </snippet></document><document><title>Simons Foundation Chooses U.C. Berkeley for Theory of Computing Center - NYTimes.com</title><url>http://www.nytimes.com/2012/05/01/science/simons-foundation-chooses-uc-berkeley-for-computing-center.html</url><snippet> </snippet></document><document><title>What if P=NP?  The moral dilemma version.</title><url>http://www.youtube.com/watch?feature=player_embedded&amp;amp;v=6ybd5rbQ5rU</url><snippet>  I can't believe this isn't a joke. Right?  Right?  It's like, "Guys! Let's pretend proving P=NP would somehow break physics and allow us to unleash an Apocalypse.  THEN let's make it into a movie and pretend it makes SENSE!" I mean, it's a huge breakthrough, but his talk about "unleashing another dimension" and that weird part when he drilled into the guy's head? What the hell was that about? C'mon, everyone knows that time travel is NP-Complete. I mean, it's a huge breakthrough, but his talk about "unleashing another dimension" and that weird part when he drilled into the guy's head? What the hell was that about? I mean, it's a huge breakthrough, but his talk about "unleashing another dimension" and that weird part when he drilled into the guy's head? What the hell was that about? [deleted] I mean, it's a huge breakthrough, but his talk about "unleashing another dimension" and that weird part when he drilled into the guy's head? What the hell was that about? I mean, it's a huge breakthrough, but his talk about "unleashing another dimension" and that weird part when he drilled into the guy's head? What the hell was that about? Right?  Right?  It's like, "Guys! Let's pretend proving P=NP would somehow break physics and allow us to unleash an Apocalypse.  THEN let's make it into a movie and pretend it makes SENSE!" The problem they present isn't even an NP problem, its basically given this grid find a marked cell. That problem is in P and is fairly trivial. The solution they give about melting the sand to find the coin doesn't change the nature of the problem, it just shrinks the effective size of the grid. You might be able to argue that in the extreme case where the grid is shrunk to one cell reduces the problem out of P, but in that case the grid would be small enough that just itterating over each of the cells would still get you an answer very quickly so the practical gain would be minimal.   The problem they present isn't even an NP problem, its basically given this grid find a marked cell. That problem is in P and is fairly trivial. The solution they give about melting the sand to find the coin doesn't change the nature of the problem, it just shrinks the effective size of the grid. You might be able to argue that in the extreme case where the grid is shrunk to one cell reduces the problem out of P, but in that case the grid would be small enough that just itterating over each of the cells would still get you an answer very quickly so the practical gain would be minimal.   My impression of that was a) what you said, and b) that this 'problem' was some sort of epiphany the guy was having as to the nature of P = NP...like he was going to "melt" 3-SAT, Traveling Salesman, etc. into some sort of "simpler" problem.  The idea was patently ridiculous. "I want you to solve the traveling salesman problem in polynomial time: So there's a salesman and he needs to get..."

"Melt"

"What?"

"Melt him, melt him down, melt down all the cities too, burn it all"

"I... you want to what?"

"BURN IT ALL DOWN. WHERE YOU GOING TO GO NOW MOTHERFUCKER? YOU AND YOUR CITIES AND YOUR TOURS BURN THAT SHIT DOWN!"

"ok... ok... you know what let's try 3-SAT instead. So you have a bunch of clauses each of the form (x1 or x2..."

"melt it down"

"not this again, let's try rudrata cycle: you want"

"melt.

it.

down."



 "lets try 3-SAT, now take this boolean expression and figure out if..."

"melt it"

"what?"

"melt it so that all the Ts and Fs are partially melted blobs that look like small misshapen 1s, those count as True right? now plug it in"

"...ok we can try setting them all to true, but I dont think that will guarantee..."

"melt them some more, keep the heat on them until they evaporate... see? There aren't any variables left so you don't have a problem any more, you're welcome."

"You're fired." Alternatively:

"let's melt 3-SAT"

"you can't melt abstract concepts such as true and false"

"fine, let's reduce 3SAT to TSP, and then melt the salesman"

"... genius!" but what if he sells asbestos suits?  "I want you to solve the traveling salesman problem in polynomial time: So there's a salesman and he needs to get..."

"Melt"

"What?"

"Melt him, melt him down, melt down all the cities too, burn it all"

"I... you want to what?"

"BURN IT ALL DOWN. WHERE YOU GOING TO GO NOW MOTHERFUCKER? YOU AND YOUR CITIES AND YOUR TOURS BURN THAT SHIT DOWN!"

"ok... ok... you know what let's try 3-SAT instead. So you have a bunch of clauses each of the form (x1 or x2..."

"melt it down"

"not this again, let's try rudrata cycle: you want"

"melt.

it.

down."



 The problem they present isn't even an NP problem, its basically given this grid find a marked cell. That problem is in P and is fairly trivial. The solution they give about melting the sand to find the coin doesn't change the nature of the problem, it just shrinks the effective size of the grid. You might be able to argue that in the extreme case where the grid is shrunk to one cell reduces the problem out of P, but in that case the grid would be small enough that just itterating over each of the cells would still get you an answer very quickly so the practical gain would be minimal.   Right?  Right?  It's like, "Guys! Let's pretend proving P=NP would somehow break physics and allow us to unleash an Apocalypse.  THEN let's make it into a movie and pretend it makes SENSE!" Right?  Right?  It's like, "Guys! Let's pretend proving P=NP would somehow break physics and allow us to unleash an Apocalypse.  THEN let's make it into a movie and pretend it makes SENSE!" I haven't seen your link yet, but I do know there are lots of people who consider P!=NP to be a law of physics.  I can't find his link, but Scott Aaronson has written compelling essays linking the two subjects. Yeah, that's definitely the consensus. Right?  Right?  It's like, "Guys! Let's pretend proving P=NP would somehow break physics and allow us to unleash an Apocalypse.  THEN let's make it into a movie and pretend it makes SENSE!" &amp;gt; Let's pretend proving P=NP would somehow break physics

Uhh it kinda would actually. O.o what. This is simply not true. It might. Why not? Why should we think that P vs NP has nothing to do with the laws of physics? Because it doesn't. P=NP is a mathematical statement describing the equality of two sets. The first set (P) is the set of languages which can be decided by a Turing Machine in polynomial time. The second set (NP) is the set of languages which can be decided by a Non-deterministic Turing Machine in polynomial time. Turing Machines are purely abstract mathematical objects. They do not exist in real life. 

In fact, there *is* an answer to the P=NP question already. It is either true, false, or independent of our axioms. Somebody discovering which one of these three options is correct wouldn't somehow change the universe. The physical laws of the universe don't change based on an idea somebody had. That is like saying that there existed integers a,b,c such that a^3 + b^3 = c^3 until Wiles proved that they couldn't exist. No. They never existed and Wiles simply confirmed that fact for us. 


 Maxwell's equations are simply mathematical statements as well; they just happen to be very accurate descriptions of physical phenomena. A "point charge" is no more real than a Turing Machine, but it's an incredibly useful model. It turns out that when you prove something about the mathematical properties of point charges, it is very likely to hold true for the charged "particles" we encounter in real life experiments.

The same is, or could be, true for P vs. NP. Dismissing the problem as having nothing to do with our physical world is silly. There are lots of hard computational problems being solved in our universe. If we can model the computational process used to solve them, and we can prove things about the limitations of our model, then we can infer or hypothesize limitations of the physical world. That's science.

[Here](http://www.scottaaronson.com/blog/?cat=15) is an interesting post with a link to a paper relating the two topics, for starters.

Edit. A [stackexchange post](http://cstheory.stackexchange.com/questions/1425/intractability-of-np-complete-problems-as-a-principle-of-physics) and some other links. Its unclear if our universe is actually solving these hard computational problems. Protein folding is NP-Hard if you have to be *always right for any input*. I am not familiar with the properties of the actual problem, but many NP-Hard problems can be solved with epsilon of the optimal solution in polynomial time. As such, proving P != NP wouldn't imply that the universe is fundamentally more powerful than a TM unless you could demonstrate that the universe *always* solved the problem optimally. 

Also, this argument is a far cry from your original claim that P=NP "kinda would" break physics. 

Lastly, I don't understand how Aaronson's paper relates to the topic at hand. Granted, I am not an expert at theory work but it looks like he is just reproving one of Valient's results using a different model of computation. In this case, the model of computation appears to be based on a real world phenomena (Postselected linear optics). At this point I am way over my head since I have no idea what Postselected linear optics are nor am I familiar with complexity proofs using quantum computations. 

It is definitely possible to build a universal turing machine out of physical components (that's how real computers work) but that doesn't mean that all results about the power of turing machines are inexorably linked to the functioning of our universe. For example, I doubt any physicist would care whether the lower bound for matrix matrix multiplication is n^2 except for that it would make their simulations run faster.   &amp;gt; Its unclear if our universe is actually solving these hard computational problems.

It's unclear whether "point charges" actually exist (they probably don't), but the model is still extremely useful for describing our universe. Similarly,

&amp;gt; It is definitely possible to build a universal turing machine out of physical components (that's how real computers work) but that doesn't mean that all results about the power of turing machines are inexorably linked to the functioning of our universe.

You seem to be approaching science with a view that models in physics (such as the standard model, Einstein's special and general theories, etc) are somehow inviolate and real -- physical manifestations -- whereas computer science models are stuck forever in the mists of theory.

Rather, they're all theoretical models that can be applied to the real world. All we can do is prove things about our models, then try to show experimentally that our models describe the real world well. Newton proved that whenever two objects are moving toward each other, each sees the other approaching at the sum of their velocities, which is a nice result, but happens not to be true. Einstein proved him wrong at a time when nobody had observed any counterexamples in the real world. His result "broke" the physics of the time. The universe didn't change, but our understanding of its limitations changed radically.

You can view special relativity as a straightforward mathematical consequence of the speed of light being constant, or you can view it as a description of the physical world. Similarly, Turing machines are theoretical, but they're a model that may or may not be a good description of physical processes.

From what I've heard, Vinay Deolalikar's recent proposed proof of P != NP relied heavily on physics; he was trying to show, I think, something about locality of information and the time required for information to flow. His proof is probably not correct, but it illustrates the important (if implied) role of physics in computer science models. Similarly, Aaronson's post is just an example of how theoretical computer science can give insight into the laws of physics. (Or perhaps, can produce new laws of physics!)

So in summary, the natural sciences function by developing theoretical models, mathematically proving properties of these models, making predictions, then experimentally testing these predictions to understand how good this fit is to the real world. Newtonian physics was a great model until it failed to accurately describe certain behaviors. Then we needed a new model based on new assumptions.

In CS, we have a model of computation that many people think is somehow fundamental to mathematics and to the universe (the Church-Turing thesis). Proving theoretical properties of the model *would* have huge effects on our understanding of the real world. P != NP would confirm our intuitions that some problems are exponentially hard; it would explain why we see nature use approximation algorithms for things like protein folding; it would make testable predictions about problems that simply can't be solved. If these predictions were proven false, we'd have to reconsider the Church-Turing thesis and the whole foundations of computer science. P = NP, on the other hand ... well, I'd argue that it would shatter our current understanding of what is possible and what is not. Because it doesn't. P=NP is a mathematical statement describing the equality of two sets. The first set (P) is the set of languages which can be decided by a Turing Machine in polynomial time. The second set (NP) is the set of languages which can be decided by a Non-deterministic Turing Machine in polynomial time. Turing Machines are purely abstract mathematical objects. They do not exist in real life. 

In fact, there *is* an answer to the P=NP question already. It is either true, false, or independent of our axioms. Somebody discovering which one of these three options is correct wouldn't somehow change the universe. The physical laws of the universe don't change based on an idea somebody had. That is like saying that there existed integers a,b,c such that a^3 + b^3 = c^3 until Wiles proved that they couldn't exist. No. They never existed and Wiles simply confirmed that fact for us. 


 ...wat?

Of course it wouldn't physically change the laws of the universe, who ever said that.

That doesn't mean it wouldn't have incredibly far-reaching physical consequences. Read some papers/blogs by Scott Aaronson (and even David Deutsch) for an overview. ...wat?

Of course it wouldn't physically change the laws of the universe, who ever said that.

That doesn't mean it wouldn't have incredibly far-reaching physical consequences. Read some papers/blogs by Scott Aaronson (and even David Deutsch) for an overview. I can't believe this isn't a joke. I thought it was satire. It's satire, right? On their website it has a release date. They have a footnote on the P versus NP wikipedia page about making a movie on the topic.  In that case I'll just pretend I've never seen the trailer and not talk about this ever again... I thought it was satire. It's satire, right? It looked real to me. It's a plot device. Like that movie Val Kilmer invents a fusion power source in. If they're smart, they'll solve NP in P time with quantum weirdness, if not, they'll go with ridiculous pseudo science, either way it shouldn't affect the quality of the movie.   I stopped caring after "hacking algorithm" I love the pseudo-metasploit console:

**set rhost all**  I stopped caring after "hacking algorithm" Why? The implications for hacking are in fact the most interesting philosophical implications. Basically every single encryption algorithm ever is in NP, so the main political and technological advantage provided by a concealed, efficient algorithm for solving NP-complete problems is that you can break any encryption.

The philosophical quandary in Primer was "What would be the realistic outcome of creating a device that let you travel back in time to the moment it was created."

The philosophical quandary in Travelling Salesman is, or should be, "What would be the realistic outcome of creating a device that can hack into every single encrypted communication channel ever?"

Sounds like solid science (fiction) to me, with just one leap of faith. An algorithm to solve NP problems efficiently would be the single greatest hacking algorithm of all time, there's nothing bogus about that. Yes, but a "hacking algorithm" is not a thing.  You would call it a "decryption algorithm", or something along those lines...My laughing at it was not that the movie was about computers, just the term they used was..uh....childish....and they said "most sophisticated"...the only likely candidate for that would be http://en.wikipedia.org/wiki/Shor's_algorithm ...so any "hacking algorithm" would be as sophisticated anything else...Not laughing at the concept, just the term they used :P I agree, "hacking algorithm" sounds a bit sensationalist, but what else would you call a general purpose algorithm that can decrypt any encrypted file without knowing the key? Decryption Algorithm?  Integer factorization algorithm?  I think the general public would understand the first perfectly fine.  Decryption is not hacking, and hacking is not exclusively decryption... Why? The implications for hacking are in fact the most interesting philosophical implications. Basically every single encryption algorithm ever is in NP, so the main political and technological advantage provided by a concealed, efficient algorithm for solving NP-complete problems is that you can break any encryption.

The philosophical quandary in Primer was "What would be the realistic outcome of creating a device that let you travel back in time to the moment it was created."

The philosophical quandary in Travelling Salesman is, or should be, "What would be the realistic outcome of creating a device that can hack into every single encrypted communication channel ever?"

Sounds like solid science (fiction) to me, with just one leap of faith. An algorithm to solve NP problems efficiently would be the single greatest hacking algorithm of all time, there's nothing bogus about that. Good symmetric encryption techniques would me largely unaffected if someone was able to prove P=NP, what would be broken are asymmetric techniques like RSA because they typically rely on the attacker solving a problem that is NP-Hard or similar. This wouldn't necessarily allow someone to break AES, but all your https traffic would be compromised because that uses an asymmetric system.  That's not true at all. Symmetric algorithms such as AES are prime examples of problems that are presumed to be in NP but not P.

The decision problem is as follows: Given a certain string of bits 'ciphertext', do there exist strings 'plaintext' and 'key' such that AES(plaintext, key) == ciphertext? The proof that this is in NP is quite simple: the tuple (plaintext, key) serves as a certificate, and you can validate it by calculating AES(plaintext, key) which is necessarily in P because AES is designed to be an efficient encryption algorithm.

The whole value of a symmetric encryption algorithm is that if you don't know the key, it is supposed to be hard (harder than P) to figure out what the key is, but it is supposed to be easy (in P) to calculate the encrypted data, easy enough to do with cheap hardware very quickly. This is basically the definition of a problem in NP, and one that the designers of AES-256 hope very much is not also in P.

The flaws in RSA are totally different. There is a known algorithm for factoring large numbers efficiently with a quantum computer, whether or not P = NP. That is why this problem is more pressing: if quantum computers are developed, RSA is in trouble even without magically solving all NP-complete problems. when trying to decrypt something the question isn't "does there exist strings 'plaintext' and 'key' such that encryptAES(plaintext, key) == ciphertext" that question is answered just by having teh ciphertext. The problem is finding a key such that decryptAES(ciphertext, key)=actualplaintext. Given a ciphertext any key will let you decrypt it to a plaintext, it will almost certainly be random gibberish though if its the wrong key. Like you said it's designed to be fast if you have the right key, but difficult without it. Your problem adds in an extra variable, instead of just having to guess at the key, you're also trying to guess the plaintext. 

By definition a good symmetric system leaks no information about the key in the ciphertext, so there is no way to determine the key from the ciphertext you have. Your means of attack then becomes just guessing the key and checking if the plaintext makes sense.   

As far as RSA I never said it could only be broken by developing a quantum computer, doing so would certainly break it, but so would proving P=NP. The while the existence of Shor's algorithm provides a theoretical way to quickly break RSA, its still not a practical attack because practical quantum computers haven't been developed and it can't run on non quantum computers. Proving P=NP would change things and allow for presumably quickly defeating RSA using already existing hardware. I say presumably because the exponent might be too large (~10^9) to be of practical use.  That's not true at all. Symmetric algorithms such as AES are prime examples of problems that are presumed to be in NP but not P.

The decision problem is as follows: Given a certain string of bits 'ciphertext', do there exist strings 'plaintext' and 'key' such that AES(plaintext, key) == ciphertext? The proof that this is in NP is quite simple: the tuple (plaintext, key) serves as a certificate, and you can validate it by calculating AES(plaintext, key) which is necessarily in P because AES is designed to be an efficient encryption algorithm.

The whole value of a symmetric encryption algorithm is that if you don't know the key, it is supposed to be hard (harder than P) to figure out what the key is, but it is supposed to be easy (in P) to calculate the encrypted data, easy enough to do with cheap hardware very quickly. This is basically the definition of a problem in NP, and one that the designers of AES-256 hope very much is not also in P.

The flaws in RSA are totally different. There is a known algorithm for factoring large numbers efficiently with a quantum computer, whether or not P = NP. That is why this problem is more pressing: if quantum computers are developed, RSA is in trouble even without magically solving all NP-complete problems. Another problem with RSA is duplication of keys and the semi-relatedness of the keys; using the right kind of guessing (almost akin to Newton's method of iteratively finding roots), you can actually guess RSA keys in almost P times. This is actually not a problem with RSA per se, just a problem with some (many?) implementations of RSA that use poor sources of randomness. It's a particular problem for certain hardware devices, where the first thing that happens when the device is turned on after leaving the factory is that it generates an RSA keypair.

You might say that this is a meta-flaw in RSA, that it is really hard to implement properly. But it's not actually a flaw in the mathematical security of well-implemented RSA. I stopped caring after "hacking algorithm"  This is going to be terrible and I still want to see it (or at least watch it on netflix while I'm on the treadmill). Exercise. SIMPLIFIED. Exercise. SIMPLIFIED.  How is it that this subreddit manages to eerily copy all the material I'm currently studying in class....? Because if you're always studying and we're always posting, it's more than likely that we will intersect. Holy crap, I was just studying set theory! bravo. How is it that this subreddit manages to eerily copy all the material I'm currently studying in class....?  Reminds me of Primer. ([Trailer](http://youtu.be/4CC60HJvZRE))  ([Full Movie](http://youtu.be/gFJMcAB5SFU)) Reminds me of Primer. ([Trailer](http://youtu.be/4CC60HJvZRE))  ([Full Movie](http://youtu.be/gFJMcAB5SFU)) Reminds me of Primer. ([Trailer](http://youtu.be/4CC60HJvZRE))  ([Full Movie](http://youtu.be/gFJMcAB5SFU)) Reminds me of Primer. ([Trailer](http://youtu.be/4CC60HJvZRE))  ([Full Movie](http://youtu.be/gFJMcAB5SFU)) Never seen the movie, just watched the trailer. I now know less about the movie than I did before. Reminds me of Primer. ([Trailer](http://youtu.be/4CC60HJvZRE))  ([Full Movie](http://youtu.be/gFJMcAB5SFU)) Just like i_practice_santeria, I watched the trailer for *Primer* and was even more confused. Now after watching the movie, I'm even more confused than before.   But changing the sand to glass wouldn't work, most of the glass would be opaque without careful control of the melting and solidifying process. That's what I was thinking. Not to mention, the coin would most likely melt as well. But changing the sand to glass wouldn't work, most of the glass would be opaque without careful control of the melting and solidifying process.          the worst part is that it's "traveling".  I wish people stopped talking about p=np.</snippet></document><document><title>Should I take Formal Language and Automata?</title><url>http://www.reddit.com/r/compsci/comments/t3rei/should_i_take_formal_language_and_automata/</url><snippet>My major is CS. I screwed up this semester and got a D in Advanced Data Structures (we need a C and above for CS classes), so I cannot take Computer Graphics in the fall, :(. So, my other options are 1) System programming and design (dont like teacher: ive had him 3 times (B-, Withdrawal, and the D)). Clearly his classes are hard. and my second choice 2) Formal Language and Automata.

I 99% do not want to take that teacher for systems, but I want to take a course. I talked to the professor of the Automata class and he told me about it. It does not really interest me. I do not really like proofs and a lot of theoretical stuff, although I haven't had many classes like it, but still. (I did not really like Discrete Math). But should I take it?

Is it worthwhile? Will it set a good foundation to get a job later? He told me it is worth it to take if I am going to graduate school, which I want to, because I honestly feel like ive learned next to nothing going into my Junior year..

Edit: I thank you all for your very well written comments and how meaningful they were to me. I will be taking the class as it is offered only 2 years as it fits into the curriculum (And my schools CS program is tiny). Someone below recommended the course for Automata at coursera. I enrolled and am taking it for the next few weeks. Thanks a lot guys!

Thanks!  Absolutely yes.  These concepts are the basis of computer science, and gives a way to understand complexity at a high level.  Anyone who says the Halting Problem, Turing Machines, CFGs, NP-* classes, Regular languages etc etc etc. aren't relevant may not understand the gravity of these topics.  Personally, this was one of the classes I found most interesting, because what's not interesting about learning about the limitations of computers and what can and cannot be done with them? Absolutely yes.  These concepts are the basis of computer science, and gives a way to understand complexity at a high level.  Anyone who says the Halting Problem, Turing Machines, CFGs, NP-* classes, Regular languages etc etc etc. aren't relevant may not understand the gravity of these topics.  Personally, this was one of the classes I found most interesting, because what's not interesting about learning about the limitations of computers and what can and cannot be done with them? I'm a bit upset that the Halting problem and the computational complexity classes were left out of the lecture when I took that class this semester. I feel I need to read over the [Cinderella Book](http://en.wikipedia.org/wiki/Introduction_to_Automata_Theory,_Languages,_and_Computation) in my free time because of my professor's neglect. Wait, you took a computation class, and they _didn't_ go over the halting problem or complexity classes? What did you do then? Absolutely yes.  These concepts are the basis of computer science, and gives a way to understand complexity at a high level.  Anyone who says the Halting Problem, Turing Machines, CFGs, NP-* classes, Regular languages etc etc etc. aren't relevant may not understand the gravity of these topics.  Personally, this was one of the classes I found most interesting, because what's not interesting about learning about the limitations of computers and what can and cannot be done with them?   [deleted] &amp;gt; Finite state automata and regular languages.

Only now am I starting to understand that regular languages are *far* more useful than I had been lead to believe.  From university I had learned that regular expression were useful for tokenizing languages, and that was about it.  But now I'm finding them used in all sorts of places:

The first thing to understand is that regular expressions are the initial Kleene algebra, and Kleene algebras are everywhere.  Every equality and inequality about regular languages hold for every Kleene algebra.  [Kleene algebras occur a lot in graph problems](http://r6.ca/blog/20110808T035622Z.html) and more generally binary relations for a typed Kleene algebra.  Knowing how to reason about regular expressions can help you reason about these problems.  Moreover, we can [decide equalities (and inequalities) about regular expressions by comparing the induced finite automata](http://sardes.inrialpes.fr/~braibant/atbr/) to solve these problems automatically.

Very recently I noticed that the linear subspaces of an algebra form a Kleene algebra.  U + V = {x + y | x &#8712; U and y &#8712; V};  U &#215; V = the span of {x &#215; y | x &#8712; U and y &#8712; V}.  The 0 subspace is {0} and the 1 subspace is the span of {1}.  Then the Kleene star operator U* = 1 + U + U^2 +U^3 + ... is the smallest subalgebra containing U and 1.  Again, all theorems about regular expressions apply to this language about linear subspaces of algebras which is pretty far from parsing theory.

Of course, none of this is taught in any university course.  Honestly I haven't found any reference describing subspaces of algebras as a Kleene algebra, though I'm sure I'm not the first to see this. [deleted] [deleted]  http://pleasingfungus.com/Manufactoria/ &amp;lt;- If you can have fun and find that interesting automata might be not a bad choice. http://pleasingfungus.com/Manufactoria/ &amp;lt;- If you can have fun and find that interesting automata might be not a bad choice. http://pleasingfungus.com/Manufactoria/ &amp;lt;- If you can have fun and find that interesting automata might be not a bad choice.         Understanding how languages and automata work makes you a better programmer. At some point in your education you want to learn about functional programming and parsing. This class might be it.

I'm guessing that this Languages class won't satisfy the pre-requisites for something you'll be interested in. Don't you have any other electives you need to take care of? Formal languages and automata is typically a purely theoretical class. I've never heard of a formal language theory class talking about functional programming. Are you thinking of a programming languages class? Yeah, automata are kind of the anti-functional CS theory constructs. [deleted] [deleted] Formal languages and automata is typically a purely theoretical class. I've never heard of a formal language theory class talking about functional programming. Are you thinking of a programming languages class? Understanding how languages and automata work makes you a better programmer. At some point in your education you want to learn about functional programming and parsing. This class might be it.

I'm guessing that this Languages class won't satisfy the pre-requisites for something you'll be interested in. Don't you have any other electives you need to take care of? This class is not a prerequisite for any other class offered at my school (undergrad).  &amp;gt; I honestly feel like ive learned next to nothing going into my Junior year

Do you have an advisor that you can talk to?  Sounds like you are in the wrong major.  If you are not interested now, and not doing well .. . I just mean in general. Like I have learned how to code (Java,C++) but besides that i have only taken an assembly class, 2 data structure classes and the java class. c++ was bunched in with a data struct. class. I love cs and i love computers, i just dont know what direction I should be heading in. If it is like being a code monkey or being a boss at CS. &amp;gt; I love cs and i love computers,

Your post really doesn't sound like you do.  You are getting poor grades and expressing  lack of interest in the courses in front of you that the ACM recommends for CS people.  I believe that I am doing you a favor in suggesting that you rethink what you are majoring in.  

Could be wrong of course. fuck. I just typed up 3 paragraphs in here and accidentally hit the back button... Ill just type the tldr.

TL;DR: I get shitty grades because im a lazy asshole who needs to wake the fuck up and realize its college and my future. I feel I have not learned much, not because i dont like the classes, but because they are not that 'profound'. Data structures simply is not exciting to me. Its quite bland. Now O/S and architecture? Thats what I like. Just plain Coding? Thats what I like too. The only theoretical class I could think of that I have taken is Data Structures because it was mostly not programming and just using algorithms to do random shit.  I personally found it boring. Well, again, you should not be in a major for CS, then.  CS is not programming.

Anyway, my $0.02.  Carry on.  :-) The only thing I have learned thus far IS programming. 

That is why I am asking if I should take Formal Language and Automata.

Like I said, Data Structures are the only classes I have taken that are not 100% programming. My school started out freshman year with only coding classes. (Including Java with data structures, but it was very basic and we wrote code every day for it (DLL/SLL/Stacks/Queues).

This IS what I want to do. It is a matter of knowing which path to take class-wise.

So next semester I will be taking Computer Graphics and Formal Lang/Automata.    yes yes yes. I almost struggle to belief that this class is optional. it is the root of everything.  You should take it because it is awesome. reasoning?  Do you enjoy math?  Because you'll want to enjoy math.  Oh so much math.  But a good and valuable class.  There are direct and indirect implications of the material covered in diverse areas of computer science.

Also, if you think Advanced Data Structures is hard, you might want to reconsider your major before it's too late.    I went into it thinking that I wouldn't like it, but after taking it last semester, it's probably one of the most interesting classes I've taken. I got an A in it, but apparently the majority of the class didn't do too well.

Yes it's a lot of theory and proofs and terminology, but I actually found drawing out machines really fascinating and fun. Sure I might have spent 30 minutes to an hour solving one homework problem, but I enjoyed the experience. If you enjoyed the class, I might recommend spending a weekend making a regex parser from scratch for kicks (particularly in Haskell). I just completed one myself (still got a list of features to add) and I thought it was pretty fun (not to mention how useful such code could have been when I was taking the class!). Haskell is actually perfect for this sort of thing--going from a deterministic model (e.g. a DFA) to a nondeterminisitic model (e.g. an NFA) is actually very easy and elegant using the list monad. In fact, I think looking at a DFA and an NFA is one of the best ways to understand why monads matter and how the list monad works. I discoverd midway the list monad is not exactly what you want though-- it causes an exponential explosion in run time if you're not careful. You really need to use sets-- the catch is of course, that sets cannot take advantage of a analogous monad (I'm not quite sure why not) so it becomes a little more tricky than one might first think. But before that realization, I have to agree that the nondeterminism possible via the list monad is extremely sweet to behold.   Of course you should. I'm taking it right now and it's a great foundation to learning the differences between programming languages. They're crucial. Assuming it's anything like mine you don't do anything with *programming* languages at all. The "languages" in question are just sets of strings over a finite alphabet.  Nah, we studied Prolog, Scheme, Python, C and Java. Yeah, that means it was a different course. "Formal Languages and Automata" is about regular languages, context-free languages, context-sensitive languages, Turing-decideable and Turing-recognizable languages as well as complexity (e.g. P, NP, PSPACE, L, NL...). It's a very theoretical course.

You course, on the other hand, sounds like a survey of *programming* languages, which is very useful but also a completely different subject. No, I'm sorry, we also covered regular, context + context free languages, automata, DFA, NFA, PDAs, and turing machines.  Would you happen to live in Manitoba by any chance?

I got the feeling you might be going to the same university as I, then again it's most likely not the only university with those courses and teachers similar to the ones you described  Adel Atta is decent prof. at your school for it.           you go to penn state dont you and you are trying to not take roger christman?     You will almost certainly never use the class in real life. If it doesn't interest you then it sounds like it's not for you. I hated my class on automata. I don't know about that. I don't think more than 3-4 months have gone by that I haven't written a 20-line state machine to do complex input processing when a regex can't hack it. That class is rather math-heavy and usually boringly taught, but the things it deals with are everywhere.  What constitutes "math heavy"? :/ What constitutes "math heavy"? :/ I don't know about that. I don't think more than 3-4 months have gone by that I haven't written a 20-line state machine to do complex input processing when a regex can't hack it. That class is rather math-heavy and usually boringly taught, but the things it deals with are everywhere.  Could you give me an example of the kinds of things you do with text processing? Or an article with examples? I'm now wondering if I'm doing something the hard way :) Could you give me an example of the kinds of things you do with text processing? Or an article with examples? I'm now wondering if I'm doing something the hard way :) I don't know about that. I don't think more than 3-4 months have gone by that I haven't written a 20-line state machine to do complex input processing when a regex can't hack it. That class is rather math-heavy and usually boringly taught, but the things it deals with are everywhere.  You will almost certainly never use the class in real life. If it doesn't interest you then it sounds like it's not for you. I hated my class on automata. You will almost certainly never use the class in real life. If it doesn't interest you then it sounds like it's not for you. I hated my class on automata. You will almost certainly never use the class in real life. If it doesn't interest you then it sounds like it's not for you. I hated my class on automata. But will it give me sound understanding of CS?    This was a required class at my school when I was a CS grad.

It's also the only class in college (and since middle school) I got a 'C' in, but I blame that on the smart but incompetent-at-teaching instructor, poor quality book, and my own lack of resourcefulness for finding alternate higher quality material.

Make sure you quickly become familiar with the notation. The concepts and ideas are useful.   no reasoning? do something that makes you happy instead. That's faulty logic. I'm in school to learn and to get my degree and to ultimately get a good job to make money. Somethings you gotta do things that don't make you happy. If that's not the case, then I'd sit around on my ass all day and play skyrim.  no I have to agree with fox on this. When I have taken a class I don't really care about I have found it very easy to rationalize my way out of doing the work and ultimately into a bad grade. Do something you love, you'll do better at it.  I freaking hate this class, it's like what a computer scientist did 50 years ago without computers. It gave me some knowledge about types of problems and the understanding that a computer can't answer every question, but it has no real application to anything I plan to do in my future. This semester I might just pull off an A in all of my classes except this stupid class, which I might fail.

I'm more of a low-level computer systems kind of guy, and I can do math, but this theory stuff makes me suffer.</snippet></document><document><title>Any recommended reading for the following topics?</title><url>http://www.reddit.com/r/compsci/comments/t48w0/any_recommended_reading_for_the_following_topics/</url><snippet>I'm thinking about doing some reading over the summer to prepare for next year of classes, does anyone recommend any specific resources for the following topics?:

Linear algebra(II), machine learning, neural networks, graph theory, embedded systems(microprocessors I), operating systems.

I'm mostly concerned with the first 3 after linear algebra as they seem most interesting to me. Thanks in advance!   Diestel has a free graph theory textbook that can be read online [here](http://diestel-graph-theory.com/). However, it is a GTM book so it may be too advanced if you haven't taken courses like linear algebra. Try it and see if it's too much.   One of the best things I did was get started learning Intel assembly before going into a microcontrollers class (this class is your "first" exposure to assembly right?). After that, learning the instruction set more geared toward academics was a cinch -- plus you learn some neat things about Intel processors which you use on a daily basis.

I don't remember the book -- I just wandered the science library until I found something which taught assembly. I already took a class based on assembly;
I have a choice between taking the computer science microprocessor course (software oriented) or the ECE microprocessor course (hardware oriented) </snippet></document><document><title>Trying to track down a blog post linked here awhile ago. It was about hiding images within images by manipulating the least significant bit. Anyone recall the link? </title><url>http://www.reddit.com/r/compsci/comments/t3ja2/trying_to_track_down_a_blog_post_linked_here/</url><snippet>I've been searching all over for it, but no luck. Anyone recall what I'm referring to? 

Edit, I believe the technical term is steganography.    </snippet></document><document><title>Kolmogorov complexity on information/randomness?</title><url>http://www.reddit.com/r/compsci/comments/t45uf/kolmogorov_complexity_on_informationrandomness/</url><snippet> Does using the metric of Kolmogorov compressibility to measure randomness imply that randomness is completely independent from information, and only has to do with the representation of information? Is it also then true that given any "piece of information" encoded as as string, that there is some perfectly random (ie, Kolmogorov incompressible) representation of that information? (ie, take some arbitrary string. is it compressible? if yes, then compress it to get a shorter string that encodes the same information until you get to an incompressible string)  Not really. If a string of bytes is truly random, it is incompressible. However, the inverse is not necessarily true.  In your example, if you compress the string "Hello" so many times that the result is incompressible, it is still not a random sequence of bytes. It still contains all of the information in the original string. &amp;gt;Not really. If a string of bytes is truly random, it is incompressible.

Maybe I am misinterpreting what you mean by "truly random", but this is wrong, at least in the practical sense. If certain characters occur more often than others then the information is definitely compressible.
http://en.wikipedia.org/wiki/Huffman_code

http://en.wikipedia.org/wiki/Entropy_(information_theory) [deleted] Not really. If a string of bytes is truly random, it is incompressible. However, the inverse is not necessarily true.  In your example, if you compress the string "Hello" so many times that the result is incompressible, it is still not a random sequence of bytes. It still contains all of the information in the original string. Hm, ok, so Kolmogorov incompressibility isn't offering a definition/metric for randomness, it's just a quality that random strings happen to hold?</snippet></document><document><title>Full 3 hours of lectures for the Kurt G&#246;del Centenary (several lectures including one on G&#246;del and compsci)</title><url>http://youtu.be/IvbnxT3VL1E?hd=1</url><snippet>  Held at the Princeton IAS in 2006 and organized by the Kurt G&#246;del Society with support from the John Templeton Foundation. </snippet></document><document><title>Order of magnitude advance in quantum simulator: "The ion-crystal used is poised to create one of the most powerful computers ever developed," Full arXiv Paper</title><url>http://arxivindex.blogspot.com/2012/05/ion-crystal-used-is-poised-to-create.html</url><snippet /></document><document><title>Open source project to work on as a high school student...</title><url>http://www.reddit.com/r/compsci/comments/t1zrw/open_source_project_to_work_on_as_a_high_school/</url><snippet>I am currently a junior in high school and I've been interested in computer science for the past 3 years. 
After learning a couple of languages and doing some small projects on my own, I thought it would be both a useful learning experience and a wonderful thing on my resume to join an open source project for the summer.
I've looked into various websites hosting open source projects, such as SourceForge, and I was generally unsure on the project that I could or should contribute to -- here is where I was hoping you could help me out.
I currently know Python, Java, and C++ relatively well and on the same level -- projects featuring these languages would probably be most beneficial to me as they would help me expand my arsenal so to say; however, I do not think that a language besides these should be much of a barrier. 
Furthermore, I was hoping to find an open source project that has a more or less clear purpose and is relatively known. Going through various projects on sourceforge, I saw some interesting ones such as VLC, while at the same time, some of the project descriptions brought me no other reaction besides [this](http://i0.kym-cdn.com/photos/images/newsfeed/000/173/576/Wat8.jpg?1315930535). 

Thanks in advance.

   As a high school programmer myself, I'd recommend finding a problem before looking for a solution. As per your example, use VLC for a bit and see what could easily be improved. Only then would I download the code and try to implement your improvement. Once you have a patch, it's simply a matter of building a reputation as a reliable source. I had a similar experience with a piece of software called KISS IDE, which is an educational robotics IDE. I started contributing code, and then I was able to land a software development job with that company managing said project. I also started a project called CBCJVM, and after presenting my work at a conference, got several contributors.

So, there are three steps as I see it:

1. Find something worth fixing or implementing
2. Fix it
3. Get your project/idea/name out there

Github, as already mentioned, is awesome.

Here are links to some of my github accounts/organizations:

http://github.com/kipr

http://github.com/CBCJVM

http://github.com/bmcdorman

Edit:

It should be noted that age is not a factor when contributing code to a project. Skill level is all that is important. Don't be intimidated because you are a high schooler. If you can conduct yourself in a professional way and supply the goods, they won't ask any questions or judge you.   If I were you, I would go with a project I actually use and enjoy. You most likely already see bugs or something you would improve if you could in the software you already use and enjoy. Use this at you advantage and improve your own experience in that software. Also if you are looking for experiences you can put on your resume, choose a well known project. Most of the time, you are sending your CV to non-technical people and if you write that you did X bug fixes on some obscure library even technical people don't what it is, you won't stand out of the bunch of resume. The biggest gift you can do to yourself is making your resume stand out for non-technical people. They are usually the first to screen the batch. They are the one that get your resume into the hands of technical people.

The open source project managers don't know you and have no idea what your age is. Use that fact to your advantage and get some real experience on your hand. Take your time. Build a good name for yourself. Even if you contribute only 2 or 3 patch during the whole summer, be sure they actually fix the bug and don't create another one. I would have given anything to be able to get such experiences at an early age. This will help you get in college and even get a good jobs once you graduated.

Lastly, have a blast this summer. Hope you have fun doing it. Have a job you actually enjoy is a big plus in your life. So many people hate their job so use this opportunity to confirm your career choice before spending thousands and thousands on an education. Thanks for the encouragement and advice.

I've looked through various open source software that I use on my computer and different popular OS projects, and I've narrowed my search down to really 3 main ones: OpenOffice, VLC, FreeMind, and Mozilla (not sure whether I will be able to contribute to this one though -- it is rather expansive). Which project would you recommend I focus on? Forget about Mozilla. There is way too many people already contributing to it to be a real asset in the project. 

I'm not really a developer, I'm just a mechanical engineer that try to stay up to date on my programming stuff but real quick, I would go with FreeMind or VLC. I don't know FreeMind but that look like an awesome project, I will download it right away.

The project is not too large, it have some complexities and is actually useful. VLC is also always looking for contributors. 

If you look at OpenOffice, don't forget the fork, LibreOffice. &amp;gt;Forget about Mozilla. There is way too many people already contributing to it to be a real asset in the project. 

False. Case in point: I am _the_ guy who is working on implementing per-window private browsing (think Chrome's incognito mode) for Firefox at this moment. I am also a recently-graduated student, and in the midst of traveling around Europe for two months, so the amount of time I have to work on this hotly-anticipated feature is very little. There are more than 25 unclaimed tasks ([evidence](https://bugzilla.mozilla.org/show_bug.cgi?id=pbngen)) that need volunteers to step up and get them done. This could be you! I also happen to be the coding contributor engagement lead, so I can get you hooked up with a mentor in a jiffy. Contact me!

Edit: this is not just a one-off example of making a difference. I can think of a bunch of different projects/bugs that we would love to have someone work on, but there simply aren't enough people resources for them. C++/Javascript/Java/Python/Objective-C (ohgodyes, 10.7 integration needs you) are all welcome.      </snippet></document><document><title>Do you think this is a secure way to store passwords in a database?</title><url>http://www.reddit.com/r/compsci/comments/t0p9r/do_you_think_this_is_a_secure_way_to_store/</url><snippet>Alright so now my question is bcrypt or pdkdf2? From what I've read bcrypt is supposed to be more secure than pdkdf2 however it's supposedly harder to implement...




**NOT IMPORTANT ANYMORE!**
Alright so I'm working on a project that will involve storing users passwords and I've been trying to come up with a good way to store them and I think I've (finally) come up with a way that seems secure to me.

**At Registration**

When the user signs up a token will be generated that will be saved in a db along with their login information. This token contains 8 numbers between 0 and 7. It is used later on to split up the hashed password.

**How It Works**

1. The password first gets hashed(sha512), this will be used as the salt.
2. The password (plain text, not hashed) is broken up into 4 (mostly equal) parts.
3. The password and hash are then mixed together.
4. This new salted password is then hashed(sha512)
5. The hashed value is split up into 8 equal parts.
6. The token created earlier is split up into an array of 8 elements.
7. The broken up hashed value is put back together according to the token.
8. What is created in step 7 is what gets stored in the database.

**Why I Think It Is Good**

For starters this manipulates the password quite a bit before it even gets hashed. Even if the hacker figured out what the hashed value was it would just be a jumble of random numbers and letters. This however didn't satisfy one requirement of a good salt which was two users with the exact same password should not have the same hash. I didn't want to simply add the user id to the password because that seemed pretty trivial. Instead the hash value that is stored is based on a random token that the user is assigned they sign up. This token could simply create a new hash of the 7th array element being repeated 8 times if the token was 77777777.

**Criticize Me!**

Alright so what do you think? Do you think this is a secure way of doing it, what recommendations do you have for me?  I'm not a crypto expert, but everything I do know about crypto says this:
"If you're writing your own system, especially something with multiple parts, you're probably doing something wrong."

What's your reason for not using [PDKDF2](http://en.wikipedia.org/wiki/PBKDF2) or bcrypt? I'm not a crypto expert, but everything I do know about crypto says this:
"If you're writing your own system, especially something with multiple parts, you're probably doing something wrong."

What's your reason for not using [PDKDF2](http://en.wikipedia.org/wiki/PBKDF2) or bcrypt? I'm not a crypto expert, but everything I do know about crypto says this:
"If you're writing your own system, especially something with multiple parts, you're probably doing something wrong."

What's your reason for not using [PDKDF2](http://en.wikipedia.org/wiki/PBKDF2) or bcrypt? i agree... [deleted]  No it's not secure. Your hashing algorithm is only slightly slower than two SHA512 hashes; in other words, it's very, very fast. Therefore, if an attacker knows your algorithm, he can rapidly brute-force any given hash. See [bcrypt](http://en.wikipedia.org/wiki/Bcrypt) for a better approach.  Alright so throwing that out the window, bcrypt or pdkdf2? Alright so throwing that out the window, bcrypt or pdkdf2?   I'm haven't studied crypto before, but I can't think of any compelling reason why this is more secure than just salted SHA-512. It might be harder to break, but the increased risk of bugs (IMO) counter-acts this. AFAIK, it isn't feasible to brute-force salted SHA-512, so why complicate matters? Yeah. A new 32 random character salt for every password and SHA512 should do the trick. [deleted] [deleted] I'm haven't studied crypto before, but I can't think of any compelling reason why this is more secure than just salted SHA-512. It might be harder to break, but the increased risk of bugs (IMO) counter-acts this. AFAIK, it isn't feasible to brute-force salted SHA-512, so why complicate matters? What if I get a hold of your salt?

The Sha hash algorithm family are designed (or rather chosen) to be amazingly fast (at least faster than crypto algorithms, otherwise there is no point in having the hash algorithm), and AES is already very fast.

The whole point of pdkdf2 and bcrypt are the load factor that forces the hashing to be slow so you cannot bruteforce, even a simple password. From what I understand, even if a hacker had the salt, it would be very hard to figure out the password. The salt is designed to combat the use of rainbow tables. If the adversary have your salt, he can bruteforce the hash *easier*, and the only thing holding him back is the strength of the password and the speed of the hashing algorithm. Which is why you want a slow hash algorithm for password hashes. The point of a salt is that they must bruteforce a single account at a time rather being able to get their hands on many in one go by investing the time into constructing a rainbow table. What if I get a hold of your salt?

The Sha hash algorithm family are designed (or rather chosen) to be amazingly fast (at least faster than crypto algorithms, otherwise there is no point in having the hash algorithm), and AES is already very fast.

The whole point of pdkdf2 and bcrypt are the load factor that forces the hashing to be slow so you cannot bruteforce, even a simple password. I'm haven't studied crypto before, but I can't think of any compelling reason why this is more secure than just salted SHA-512. It might be harder to break, but the increased risk of bugs (IMO) counter-acts this. AFAIK, it isn't feasible to brute-force salted SHA-512, so why complicate matters? I second this.  Nothing wrong with just a plain old salted sha-512 Isn't SHA considered insecure for things like password hashing? I was under the impression that PPKDF2 or bcrypt should be used exclusively for password hashing (and by "I was under the impression" I mean "[Jeff Atwood said](http://www.codinghorror.com/blog/2012/04/speed-hashing.html)")

edit: fixed the link Not with the advent of GPU clusters. The advent of GPU clusters makes SHA a better choice than PBKDF2/bcrypt, or it makes PBKDF2/bcrypt bad choices and it makes SHA an even worse choice?  I think you're trying to reinvent the wheel.  And unless you're an expert in cryptography (I'm not), you don't have the expertise to say whether or not this is more secure.

[bcrypt](http://en.wikipedia.org/wiki/Bcrypt) is a popular password hashing algorithm that is also adaptive and can be made slower as computers become faster.

[There are also others based on MD5 and SHA-2](http://en.wikipedia.org/wiki/Crypt_\(Unix\)#Library_Function_crypt.283.29)

I'm guessing you're doing this for either school or a personal project, because in the "real world" trying to come up with your own solution to something that has already been solved in many other ways (i.e. libraries exist to do it already) is generally frowned upon.  Especially when it comes to encryption/security.

Edit: Also note, using SHA or MD5 for passwords is starting to be frowned upon since they are designed to hash a lot of data very quickly.  With a password it's better to have something that hashes a tiny amount of data relatively slowly.  Hence the rise in popularity in bcrypt. Thanks I'm going to go ahead and use bcrypt I think. Either that or pdkdf2.  *"From what I've read bcrypt is supposed to be more secure than pdkdf2 however it's supposedly harder to implement..."*

Holy bejezzuz don't implement your own bcrypt! That's insane. There are plenty of bcrypt libraries around, and they are dead simple to use properly. I don't know much about pdkdf2, but it seems to just be multi-round sha, which would still be relativity easy to break with a GPU/FPGA/ASIC where bcrypt is much more memory intensive, so harder to implement in hardware.

Just use bcrypt. From what I've read php supports bcrypt with the crypt() function. It generates a 60 string hash, so, yes? [deleted]  http://diovo.com/2009/02/wrote-your-own-encryption-algorithm-duh/

I would recommend going with something that is actually peer reviewed and has a reputation behind it.     ~~Okay first off, **why do you need to store the password?**~~ This goes strongly against best practices. ~~Unless you need to recover the plaintext of the password (which in 99.9% of cases you should never have to), you should be storing a salted hash of the password.~~ 

Here's a short description of the best way to authenticate users through passwords.

* Create a random and unique string or number for *each* user when they create their account. This is the **salt**. A SHA hash of the time and date down to the millisecond of account creation or a simple call to Math.random() would probably suffice. 

**That's it. One Step.**


If all you want to do is authentication then all you need to store in a database is... 

&amp;gt;      [username, salt, hash(password,salt)]   **or**   [username, salt, hash(hash(password),salt)]

Notice that the plaintext of the password is unrecoverable without a brute force search through all possible passwords and hashing each one with the salt of each user. Then at runtime you just need to compare the hash in the database with the one calculated at runtime and there is no need to ever have access to the plaintext password.

The salt forces an attacker to start a brute force search over again for each user, so even if he gets lucky and succeeds in cracking one password, the rest of the database is still safe. If you try and encrypt the the database or come up with your own algorithm, then all it takes is **one** mistake and the whole thing is now available to an attacker.


**TL;DR - If the password can possibly be recovered, then you have to assume in the worst case scenario an attacker will be able to recover them. You should therefore MAKE PASSWORDS UNRECOVERABLE.**

-----

**Edit:** It seems that I misread what you were doing. I thought that you were mixing the plaintext password and it's hash together and then storing that in the database. But in any case there is still a glaring error in step 1. The whole point of a salt is to introduce randomness into hashes so that if 2 users have the same password they will have a different hash value in the database as well as to force attackers to restart brute force attacks that try every possible password for each user. Hashing the password and then claiming that to be the salt is just wrong because it is entirely deterministic and adds no randomness. In your implementation the token acts as a sort of psuedo-salt.     Kudos on the call to "bring it", though. It's a great thought experiment to try to come up with your own stuff, and it brought about a bunch of great discussion.  Actually, you shouldn't be storing the password at all. Store only the hash. Then when the user enters their password in the future, rehash it and compare that to the stores hash. That's the only truly secure way to do it. That way, even the DBAs can't find out the user passwords. I'm storing the hashed value...? I'm storing the hashed value...?  When it comes to cryptography: don't - *ever* - roll your own. I think this statement is incorrect for maybe 10 people in the world.   sha512 is fine by itself, and it supports salting.


Find a good java script that implements sha512 at the client side. 
DO NOT HAVE THE PASSWORD SENT TO THE SERVER IN CLEAR TEXT. Not even over in SSL/TLS!!

Let the server generate a random number, that is send to the clients, have that be a hiden field in in the password input form. Also put a random number inside a cookie that is sent to the browser. Finally have the browser generate a random number. There should now be three random tokens available to the java script to choose from for doing things like salting. 


Basically the client and the server can negotiate on the salt to permutate the SHA512, but that is about it.... Don't try to improve something that is already great. sha512 is fine by itself, and it supports salting.


Find a good java script that implements sha512 at the client side. 
DO NOT HAVE THE PASSWORD SENT TO THE SERVER IN CLEAR TEXT. Not even over in SSL/TLS!!

Let the server generate a random number, that is send to the clients, have that be a hiden field in in the password input form. Also put a random number inside a cookie that is sent to the browser. Finally have the browser generate a random number. There should now be three random tokens available to the java script to choose from for doing things like salting. 


Basically the client and the server can negotiate on the salt to permutate the SHA512, but that is about it.... Don't try to improve something that is already great. Spectacular, you've just rendered pointless the entire process of storing an encrypted password.  Random numbers in hidden fields?  Are you trying to protect from someone looking over your shoulder and working it out with a calculator? </snippet></document><document><title>File System Alternatives?</title><url>http://c2.com/cgi/wiki?FileSystemAlternatives</url><snippet>  This post reads like a weird stream of consciousness.

Far as I can tell, you don't like the tree data structure. Okay, whatever. Amazon's S3 is getting a lot of people in the habit of using keys anyways. You can make whatever pointers you like to them in your app ( tags, trees, whatever )

Though ultimately, what problem are you trying to solve here? This post reads like a weird stream of consciousness.

Far as I can tell, you don't like the tree data structure. Okay, whatever. Amazon's S3 is getting a lot of people in the habit of using keys anyways. You can make whatever pointers you like to them in your app ( tags, trees, whatever )

Though ultimately, what problem are you trying to solve here? &amp;gt; This post reads like a weird stream of consciousness.

It's basically a threaded discussion collapsed into one page. I like it. :)

&amp;gt; Though ultimately, what problem are you trying to solve here?

Hierarchical organization of files in useful and necessary, but also annoyingly limiting. The workarounds (unix "find", windows search, etc) are not well integrated and provide a completely separate interface.</snippet></document><document><title>What should I study if my goal is to work in bioinformatics?</title><url>http://www.reddit.com/r/compsci/comments/t0b6b/what_should_i_study_if_my_goal_is_to_work_in/</url><snippet>  Computer Science and Statistics. Statistics is probably more important. Biology is probably the least important aspect to study, surprisingly enough.

Stats &amp;gt; CS &amp;gt; Biology Computer Science and Statistics. Statistics is probably more important. Biology is probably the least important aspect to study, surprisingly enough.

Stats &amp;gt; CS &amp;gt; Biology  If you're intending to study formally, why not choose a Bioinformatics program? The University of Waterloo in Canada offers one, for example. I don't think I want to study formally, just so that I am not tied down to that field and can go into other areas of cs if I decide to. Then study normal CS and Stat in uni, and pay more attention to the Stat courses than your classmates do. Study Biology/pharmaceuticals on your own. If you're intending to study formally, why not choose a Bioinformatics program? The University of Waterloo in Canada offers one, for example. If you're intending to study formally, why not choose a Bioinformatics program? The University of Waterloo in Canada offers one, for example.   Computer science and biology.          Undergrad working in a bioinformatics lab here.  What Pentapus says is good advice, but if that's not possible I'd recommend you double major in computer science and chemistry (or biochemistry if your university has that major).  We have a fair number of each bio and chem majors in our lab, but the chem majors all seem pick things up more quickly. I think, actually, that biochemistry students would do the best because of the focus in bioinformatics on biochemistry and molecular genetics. Undergrad working in a bioinformatics lab here.  What Pentapus says is good advice, but if that's not possible I'd recommend you double major in computer science and chemistry (or biochemistry if your university has that major).  We have a fair number of each bio and chem majors in our lab, but the chem majors all seem pick things up more quickly.      Both answers that are already here are good and correct, but what I'm reading from your question is, "if I have to pick between Biology and Computer Science, which should I do?"

If that's the actual case, study computer science. The "bio" in the name refers to the way biology inspires the subject, but you don't need a lot of knowledge of biology to understand the subject well. In a way you could say you're just applying biology to the field of computer science.

Similarly, if you wanted to solve biological problems through information technology, you'd study biology: in that case you're merely applying computer science to the field of biology, and a deeper knowledge of biology becomes more important. But... bioinformatics *is* application of compsci to biology, not the other way around. For example, they research algorithsm for things like protein folding or DNA matching etc (which are ages old problems in compsci). That's what I'm generally interested in. I want to research how the different patterns relate to each other, and hopefully find new patterns. Kinda like a stats/applied math deal. I'm an undergrad who is currently receiving a scholarship in bioinformatics. For this, I'm required to do a research project (plus, it's fun! :)). This year I'm researching RNA secondary structure prediction (very related to folding). It may simply be that I'm coming from a Math/CS base, but the CS was way more important in this than the biology related aspect. As in, I would expect the average person doing this sort of thing to be able to pick up the biology knowledge required. The computer science, not so much. My adviser on the project (from the CS department) said the same thing to me last year: The biology knowledge is much easier to pick up than the CS knowledge.

Now, if you're still looking for areas that interest you, biology might very well be the better choice. Especially if you're trying to solve a particular problem/more interested in biology. However, your talk of patterns makes me believe you'd enjoy the CS side better and would benefit more from it. But... bioinformatics *is* application of compsci to biology, not the other way around. For example, they research algorithsm for things like protein folding or DNA matching etc (which are ages old problems in compsci). Both answers that are already here are good and correct, but what I'm reading from your question is, "if I have to pick between Biology and Computer Science, which should I do?"

If that's the actual case, study computer science. The "bio" in the name refers to the way biology inspires the subject, but you don't need a lot of knowledge of biology to understand the subject well. In a way you could say you're just applying biology to the field of computer science.

Similarly, if you wanted to solve biological problems through information technology, you'd study biology: in that case you're merely applying computer science to the field of biology, and a deeper knowledge of biology becomes more important. </snippet></document><document><title>Good compsci schools in Europe?</title><url>http://www.reddit.com/r/compsci/comments/t0x9y/good_compsci_schools_in_europe/</url><snippet>I'm looking for good Computer Science schools in Europe, for Master's or possibly a doctorate.  Any ideas?

**edit:** Thanks for the info and tips, everyone!  Obviously the University of Cambridge and the University of Oxford are top institutions (especially Cambridge). They're both fantastic schools (with a high level of prestige) and they offer fantastic computer science programs. Perhaps tied with Cambridge and Oxford in terms of tech programs is ETH Zurich.

Other top programs include Imperial College London, University of Edinburgh, University of Manchester, Ecole Polytechnique F&#233;d&#233;rale de Lausanne, &#201;cole Normale Sup&#233;rieure, University of New South Wales, University of Basel, and University of Amsterdam (largely in that order). Obviously the University of Cambridge and the University of Oxford are top institutions (especially Cambridge). They're both fantastic schools (with a high level of prestige) and they offer fantastic computer science programs. Perhaps tied with Cambridge and Oxford in terms of tech programs is ETH Zurich.

Other top programs include Imperial College London, University of Edinburgh, University of Manchester, Ecole Polytechnique F&#233;d&#233;rale de Lausanne, &#201;cole Normale Sup&#233;rieure, University of New South Wales, University of Basel, and University of Amsterdam (largely in that order). Obviously the University of Cambridge and the University of Oxford are top institutions (especially Cambridge). They're both fantastic schools (with a high level of prestige) and they offer fantastic computer science programs. Perhaps tied with Cambridge and Oxford in terms of tech programs is ETH Zurich.

Other top programs include Imperial College London, University of Edinburgh, University of Manchester, Ecole Polytechnique F&#233;d&#233;rale de Lausanne, &#201;cole Normale Sup&#233;rieure, University of New South Wales, University of Basel, and University of Amsterdam (largely in that order). Obviously the University of Cambridge and the University of Oxford are top institutions (especially Cambridge). They're both fantastic schools (with a high level of prestige) and they offer fantastic computer science programs. Perhaps tied with Cambridge and Oxford in terms of tech programs is ETH Zurich.

Other top programs include Imperial College London, University of Edinburgh, University of Manchester, Ecole Polytechnique F&#233;d&#233;rale de Lausanne, &#201;cole Normale Sup&#233;rieure, University of New South Wales, University of Basel, and University of Amsterdam (largely in that order). Obviously the University of Cambridge and the University of Oxford are top institutions (especially Cambridge). They're both fantastic schools (with a high level of prestige) and they offer fantastic computer science programs. Perhaps tied with Cambridge and Oxford in terms of tech programs is ETH Zurich.

Other top programs include Imperial College London, University of Edinburgh, University of Manchester, Ecole Polytechnique F&#233;d&#233;rale de Lausanne, &#201;cole Normale Sup&#233;rieure, University of New South Wales, University of Basel, and University of Amsterdam (largely in that order). &amp;gt; Obviously the University of Cambridge and the University of Oxford are top institutions (especially Cambridge). 

Yup, but they're not in Europe. They're in the UK. Don't for the sake of all that is holy tell anyone from the UK that you think they're in Europe. I am from the UK, and I can confirm that the UK is in Europe, we're part of the EU, UEFA and the Eurovision song contest (we're not proud of that one though) . We're just not European. Strange concept I know. **whoosh** I am from the UK as well, and I can confirm that the popular culture is to claim that Europe is full of crazy people with silly voices and national stereotypes.  I go to the University of Southampton. Which, I'm told, has a good research group. Sir Tim Berners Lee does research here (although I'm told he also does a lot of research at MIT) Sweet.  Have you actually done research with him?  I may be slightly biased as it's my alma mater, but the [University of Manchester](http://www.cs.manchester.ac.uk/) is pretty good, with an excellent heritage. It's the birthplace of the stored-programme computer and home to Alan Turing. I thoroughly enjoyed the course and the city.  I finished a Master's at Edinburgh in 2009 and it was fantastic. The professors were excellent teachers, they cared about your success, and we're happy to help. Their research is second to none and the School of Informatics is consistently ranked one of the top in the UK. If you're interested in AI or HPC, give it a look.  European Master's student here.
It all depends on what kind of CS you want to do...

* High performance computing
* Networking
* Machine learning
* Formal languages/Compilers
* Databases
* Theoretical
* Applied

At a Master's level there are many european universities that are very good.
When I applied for my Master's I was looking at the following:

* Technical University Delft (Netherlands)
* Leiden University (Netherlands)
* &#197;rhus University (Denmark)
* KTH (Sweden)
* NTNU (Norway)

However, even the smallest, lesser known universities (especially to USA people) can have excellent research groups.
So I would suggest that you decide what kind of work you want to do and then look at what universities have good groups.

At the Graduate level it isn't really worth looking at the reputation of the university rather than the research group you would be working with.

If you have any further questions, let me know. Maybe I'll know a good uni depending on the research area. European Master's student here.
It all depends on what kind of CS you want to do...

* High performance computing
* Networking
* Machine learning
* Formal languages/Compilers
* Databases
* Theoretical
* Applied

At a Master's level there are many european universities that are very good.
When I applied for my Master's I was looking at the following:

* Technical University Delft (Netherlands)
* Leiden University (Netherlands)
* &#197;rhus University (Denmark)
* KTH (Sweden)
* NTNU (Norway)

However, even the smallest, lesser known universities (especially to USA people) can have excellent research groups.
So I would suggest that you decide what kind of work you want to do and then look at what universities have good groups.

At the Graduate level it isn't really worth looking at the reputation of the university rather than the research group you would be working with.

If you have any further questions, let me know. Maybe I'll know a good uni depending on the research area.      For general CS studies, there are countless excellent schools.

For research, it very much depends on what you want to study as you should be looking at proximity to a top research group in the area. If you do a PhD, what fields can you see yourself working in?    </snippet></document><document><title>ASK: Recommend a book on technical writing?</title><url>http://www.reddit.com/r/compsci/comments/szfef/ask_recommend_a_book_on_technical_writing/</url><snippet>I would like to improve (start creating) my ability to write cohesively and succinctly on technical subjects, mainly programming. I much enjoyed [Programming in Lua](http://www.amazon.com/Programming-Second-Edition-Roberto-Ierusalimschy/dp/8590379825) as an example of good technical writing. What can folks recommend to learn this craft?    Justin Zobel's [Writing for Computer Science](http://www.justinzobel.com/index.htm) is quite good, though it's focus is a bit more constrained than general technical writing.

Edit: Also, [BUGS in Writing](http://www.amazon.com/BUGS-Writing-Revised-Edition-Debugging/dp/020137921X) is outstanding. I lost my copy a few years ago, but remembered it when responding to this. New version is on the way.     &amp;gt; What can folks recommend to learn this craft?

Others comments have recommended some very good books, and reading some of these would be completely worth your while. Unfortunately, reading books isn't enough.

I used to write fairly poorly, and would frequently get papers sent back because referees where unable to understand them. I found that the most effective way to improve my ability to communicate on technical topics was to practice. I started blogging frequently, and actively request feedback on things I write from people I respect. I then make an effort to incorporate this feedback into my next piece of writing.

I would encourage you to do the same. Write a lot, ask people to read your writing, then learn from what they have to say. Not everybody's advice is going to be good, but I think you'll be surprised at the high quality of comments you get.    </snippet></document><document><title>Lauren Ipsum and the Timing Attack</title><url>http://carlos.bueno.org/2011/10/timing.html</url><snippet>  How would you implement this over a network, considering that latency varies? How would you implement this over a network, considering that latency varies? How would you implement this over a network, considering that latency varies?  Doesnt hashing passwords before checking them defeat this type of attack? Somewhat.

The timing attack would work against the hashes as well.  If we assume that my password hashes to 5baa61e4c9b93f3f0682250b6cf8331b7ee68fd8, and you are trying to guess it, then the timing information can reveal characteristics of my hashed password (without revealing the password itself).  If you submit a password whose hash begins with '5bc', then that will be rejected more quickly than a password whose hash begins with '5baa61e5'

Using this, you can eventually discover the value of my hashed password.

Using hashes makes the process more difficult, because with good hash algorithms it's hard to control the output (for example, to create a string whose hash starts with 'aaaaaaaa' should require a lot of brute force calculation).

But, leaking any information at all about my password should be viewed as a security hole.

For example, say you want to use a dictionary attack against all the users on my system, hoping that someone has chosen an english word as a password.

If I have implemented a policy that 10 password failures locks out an account, then a dictionary attack is unlikely to succeed, because you are unlikely to guess a password within your first 10 attempts.
  
But, if you can use the timing information to direct your search, then you can eliminate swaths of the dictionary without having to test them.

 Somewhat.

The timing attack would work against the hashes as well.  If we assume that my password hashes to 5baa61e4c9b93f3f0682250b6cf8331b7ee68fd8, and you are trying to guess it, then the timing information can reveal characteristics of my hashed password (without revealing the password itself).  If you submit a password whose hash begins with '5bc', then that will be rejected more quickly than a password whose hash begins with '5baa61e5'

Using this, you can eventually discover the value of my hashed password.

Using hashes makes the process more difficult, because with good hash algorithms it's hard to control the output (for example, to create a string whose hash starts with 'aaaaaaaa' should require a lot of brute force calculation).

But, leaking any information at all about my password should be viewed as a security hole.

For example, say you want to use a dictionary attack against all the users on my system, hoping that someone has chosen an english word as a password.

If I have implemented a policy that 10 password failures locks out an account, then a dictionary attack is unlikely to succeed, because you are unlikely to guess a password within your first 10 attempts.
  
But, if you can use the timing information to direct your search, then you can eliminate swaths of the dictionary without having to test them.

 Isn't the whole point of a hashed password that the hash could leak and the password would still be secure? I feel like if you're not confident that this is the case, you need a better hash algorithm.

Not that you should intentionally leak the hashes, of course. Isn't the whole point of a hashed password that the hash could leak and the password would still be secure? I feel like if you're not confident that this is the case, you need a better hash algorithm.

Not that you should intentionally leak the hashes, of course. Some hashes (SHA1/MD5) that are widely used are known to be easy to bruteforce. It seems to me like if a hash algorithm is easy to bruteforce, it offers literally zero security. Am I wrong? Do you actually get something out of hashing passwords using such a weak hashing algorithms? Is an MD5-hashed password even slightly more secure than an unhashed password? It seems to me like if a hash algorithm is easy to bruteforce, it offers literally zero security. Am I wrong? Do you actually get something out of hashing passwords using such a weak hashing algorithms? Is an MD5-hashed password even slightly more secure than an unhashed password?   </snippet></document><document><title>A poem about division from Hacker's Delight</title><url>http://www.catonmat.net/blog/poem-from-hackers-delight/</url><snippet> </snippet></document><document><title>TEDx talk about the first ever computer</title><url>http://blog.jgc.org/2012/04/greatest-machine-that-never-was.html</url><snippet>  </snippet></document><document><title>Is there a harder alternative to integer factorization for the purposes of cryptography?</title><url>http://www.reddit.com/r/compsci/comments/suar1/is_there_a_harder_alternative_to_integer/</url><snippet>I'm thinking of the (unlikely) case where P=NP is found. Since factorization is known to be in NP, this would likely lead to RSA being considered broken.

Is there a harder alternative that could be used in place of current systems for the purposes of public-key cryptography; or does the fact that if a problem can be verified in P then it is in NP break everything?  There's also [Elliptic Curve Cryptography](http://en.wikipedia.org/wiki/Elliptic_curve_cryptography), which my cryptography professor described as "what we'll use when some Russian teenager finds a way to factor numbers in polynomial time." It's still in NP, though. Or, at least, finds a way to factor integers with exactly two factors of similar magnitude in average-case polynomial time.

RSA and similar algorithms are an extremely special case of the problem they rely on for their strength. I wouldn't be the least bit shocked if someone came up with an algorithm to crack them that was not fast at integer factorization in general.

In contrast, ECC is very dense. It is not a particularly special case, so you'd need a very general algorithm (probably a constructive proof that P=NP) to solve it quickly on conventional computers. &amp;gt; probably a constructive proof that P=NP

There already exist algorithms that solve NP problems in polynomial time if P=NP.  As far as I understand, you can run them today and they will run in polynomial time if P=NP. Really? Can you cite some of these algorithms? It seems like they would provide strong experimental evidence of whether P=NP or not. Yes, really. They're not at all enlightening, and the constants are (likely) enormous, so it doesn't really have any experimental value. I believe the technique is known as Levin search. It goes something like this.

Let M1, M2, ... be an enumeration of algorithms.

    for i = 1, 2, ...:
        for j = 1, 2, ..., i:
            run Mj for i steps
            if it output two numbers, check whether they are factors of the input
            if yes, halt

If Mj solves factorization in time O(f(n)), then this algorithm solves factorization in something like time O(j\*j\*f(n)). So if there is a j such that Mj solves factorization in polytime, then so does this algorithm. Yes, really. They're not at all enlightening, and the constants are (likely) enormous, so it doesn't really have any experimental value. I believe the technique is known as Levin search. It goes something like this.

Let M1, M2, ... be an enumeration of algorithms.

    for i = 1, 2, ...:
        for j = 1, 2, ..., i:
            run Mj for i steps
            if it output two numbers, check whether they are factors of the input
            if yes, halt

If Mj solves factorization in time O(f(n)), then this algorithm solves factorization in something like time O(j\*j\*f(n)). So if there is a j such that Mj solves factorization in polytime, then so does this algorithm. Interesting, it seems to rely on a claim that there are a finite amount of polynomial time algorithms though, otherwise the algorithm could run forever. I feel like that can't be true. Yes, really. They're not at all enlightening, and the constants are (likely) enormous, so it doesn't really have any experimental value. I believe the technique is known as Levin search. It goes something like this.

Let M1, M2, ... be an enumeration of algorithms.

    for i = 1, 2, ...:
        for j = 1, 2, ..., i:
            run Mj for i steps
            if it output two numbers, check whether they are factors of the input
            if yes, halt

If Mj solves factorization in time O(f(n)), then this algorithm solves factorization in something like time O(j\*j\*f(n)). So if there is a j such that Mj solves factorization in polytime, then so does this algorithm. Problems:
1. Checking one input/output pair is a factorisation is not the same as showing the algorithm factors.
2. The problem of checking if Mj solves factorisation is undecidable.
3. The ordering of the algorithms determines j.
4. The set Mj is dependent on the input size n. &amp;gt; probably a constructive proof that P=NP

There already exist algorithms that solve NP problems in polynomial time if P=NP.  As far as I understand, you can run them today and they will run in polynomial time if P=NP. There's also [Elliptic Curve Cryptography](http://en.wikipedia.org/wiki/Elliptic_curve_cryptography), which my cryptography professor described as "what we'll use when some Russian teenager finds a way to factor numbers in polynomial time." It's still in NP, though. Compton? Go Blue? Compton? Go Blue? There's also [Elliptic Curve Cryptography](http://en.wikipedia.org/wiki/Elliptic_curve_cryptography), which my cryptography professor described as "what we'll use when some Russian teenager finds a way to factor numbers in polynomial time." It's still in NP, though.   If P=NP then there can not be any one way functions with one direction in P and the other not, because if the forward direction is P, the reverse direction must be verifiable in P, so it must be in NP, which is = P. In practical terms, it's hard to imagine what kind of one-way function could then exist (and afaik some sort of one way function is needed for public key encryption). I guess maybe someting in EXP whose reverse is in NEXP, but then that requires calculating an EXP function to encrypt, which is iirc harder than NP already.

Rekai correctly points out some work on cryptography envisioning a post-quantum computing world, but I must point out, quantum computing does not mean P = NP, and many of the algorithms that are probably more resistant to quantum computing would still break if P = NP. This. 

Factorization is not NP-complete, meaning it can be solved in polynomial time without implying P=NP.

If P=NP, then any crypto-system that can be encrypted and decrypted in polynomial time can also be broken in poly time. So in that case, we are really out of luck.

And to emphasize, quantum computing wouldn't really alter this world of complexity-based cryptography as they are not known to solve NPC problems in poly time (but can do factorization).  This. 

Factorization is not NP-complete, meaning it can be solved in polynomial time without implying P=NP.

If P=NP, then any crypto-system that can be encrypted and decrypted in polynomial time can also be broken in poly time. So in that case, we are really out of luck.

And to emphasize, quantum computing wouldn't really alter this world of complexity-based cryptography as they are not known to solve NPC problems in poly time (but can do factorization).  This. 

Factorization is not NP-complete, meaning it can be solved in polynomial time without implying P=NP.

If P=NP, then any crypto-system that can be encrypted and decrypted in polynomial time can also be broken in poly time. So in that case, we are really out of luck.

And to emphasize, quantum computing wouldn't really alter this world of complexity-based cryptography as they are not known to solve NPC problems in poly time (but can do factorization).  [deleted] The article you link doesn't really support your assertions, it just implies physics would be very slightly stranger than it already is.  [deleted] If P=NP then there can not be any one way functions with one direction in P and the other not, because if the forward direction is P, the reverse direction must be verifiable in P, so it must be in NP, which is = P. In practical terms, it's hard to imagine what kind of one-way function could then exist (and afaik some sort of one way function is needed for public key encryption). I guess maybe someting in EXP whose reverse is in NEXP, but then that requires calculating an EXP function to encrypt, which is iirc harder than NP already.

Rekai correctly points out some work on cryptography envisioning a post-quantum computing world, but I must point out, quantum computing does not mean P = NP, and many of the algorithms that are probably more resistant to quantum computing would still break if P = NP. If P=NP then there can not be any one way functions with one direction in P and the other not, because if the forward direction is P, the reverse direction must be verifiable in P, so it must be in NP, which is = P. In practical terms, it's hard to imagine what kind of one-way function could then exist (and afaik some sort of one way function is needed for public key encryption). I guess maybe someting in EXP whose reverse is in NEXP, but then that requires calculating an EXP function to encrypt, which is iirc harder than NP already.

Rekai correctly points out some work on cryptography envisioning a post-quantum computing world, but I must point out, quantum computing does not mean P = NP, and many of the algorithms that are probably more resistant to quantum computing would still break if P = NP. &amp;gt; I guess maybe someting in EXP whose reverse is in NEXP, but then that requires calculating an EXP function to encrypt, which is iirc harder than NP already.

That would probably depend on *n*. If P=NP then there can not be any one way functions with one direction in P and the other not, because if the forward direction is P, the reverse direction must be verifiable in P, so it must be in NP, which is = P. In practical terms, it's hard to imagine what kind of one-way function could then exist (and afaik some sort of one way function is needed for public key encryption). I guess maybe someting in EXP whose reverse is in NEXP, but then that requires calculating an EXP function to encrypt, which is iirc harder than NP already.

Rekai correctly points out some work on cryptography envisioning a post-quantum computing world, but I must point out, quantum computing does not mean P = NP, and many of the algorithms that are probably more resistant to quantum computing would still break if P = NP.  Just because something has a polynomial-time solution doesn't mean squat in terms of whether it's actually practical to solve in less than a few million years.  There are plenty of examples of [galactic algorithms](http://rjlipton.wordpress.com/2010/10/23/galactic-algorithms/), which are interesting only in theory but cannot be applied in any meaningful way.  The race to find better matrix multiplication algorithms is the perfect example: the current "fastest" is O( n^2.3727 ) but this is not actually usable for any kind of real world work.  The original Strassen algorithm is O( n^2.807 ) and is still faster for any actual computation.  Does P=NP imply that all hash functions are second-preimage insecure? If not, [Merkle signatures](https://en.wikipedia.org/wiki/Merkle_signature_scheme) are still useful. hash functions are truly one-way. You're going from a infinite space to a finite space, so no matter what algorithm you come up with, you will never find *that* particular value knowingly from the hash alone; you would however be able to find a collision. That is, if P = NP, then you could find a second preimage in poly time.

That collision, as undesirable as it is, is most likely going to be garbage, and it's all down to using it correctly, so you cannot just pad the message you're trying to forge. But that still only gives us authentication, not confidentiality. Does P=NP imply that all hash functions are second-preimage insecure? If not, [Merkle signatures](https://en.wikipedia.org/wiki/Merkle_signature_scheme) are still useful. P=NP implies that you can build files with the same hash value in a time polynomial in a few hash sizes at most.   Every public-key encryption scheme can be broken in polynomial time if P = NP. This is because it is an (F)NP problem to determine the randomness used during key generation (which would determine the secret key). Moving away from factoring as a hard problem won't salvage this.

More generally, every cryptographic "thing" depends on the existence of one-way functions (except for those cryptographic "things" that we can construct unconditionally, like one-time pad, secret sharing, etc), and one-way functions don't exist if P = NP. &amp;gt; This is because it is an (F)NP problem to determine the randomness used during key generation (which would determine the secret key).

I don't quite understand this. A computer can have a hardware RNG which derives values from quantum effects. A PRNG could be mutated several times over the course of key generation. How would you recover enough information to predict the random values? The public key is a deterministic function of the randomness used in the key generation process. It doesn't matter whether the randomness *came from* a perfect hardware RNG, quantum effects, whatever. The attack to which I refer doesn't involve breaking the *source* of the randomness, it involves breaking the cryptographically hard function that was *applied* to the randomness. This is trivial if you're given the power of NP ("which inputs, when fed through this function, produce this output?") I'm still not sure why randomness comes into this at all.

Surely the fastest way to break RSA is to find the factors of an integer. A general number field sieve could be used, which does not rely on finding any random properties.

Could you link me to a source to read up on? I'm still not sure why randomness comes into this at all.

Surely the fastest way to break RSA is to find the factors of an integer. A general number field sieve could be used, which does not rely on finding any random properties.

Could you link me to a source to read up on? There's a [survey paper](http://cseweb.ucsd.edu/~russell/average.ps) by Russell Impagliazzo which talks about what would happen if P=NP.  

Basically, if P=NP, then computers can do just about anything -- break any cryptosystem, perform natural language parsing, prove any theorem, etc.  The way Impagliazzo puts it is:
&amp;gt; In short, as soon as a feasible algorithm for an NP-complete problem is found, the capacity of computers will become that currently depicted in science fiction. I'm still not sure why randomness comes into this at all.

Surely the fastest way to break RSA is to find the factors of an integer. A general number field sieve could be used, which does not rely on finding any random properties.

Could you link me to a source to read up on?       one time pad?

can't be verified in p, so even if p=np, it is safe (as long as the pad is safe... turtles all the way down) That doesn't help with public-key cryptography afaik. You wouldn't be able to sign files to be verified by third parties or do a DH key exchange with a one time pad.</snippet></document><document><title>Computer Science or Information Technology?</title><url>http://www.reddit.com/r/compsci/comments/stc80/computer_science_or_information_technology/</url><snippet>Hello Reddit,

I am a nurse and I am thinking of going back to school to study Computer Science or Information Technology. The problem is, I'm not sure which one of the two I should take. 

What are the advantages/disadvantages of both? Which one will most likely land me a job or make it easier for me to start one? What do you think will the trend be like in 3 years (more towards IT or CS)?

I apologize if this post is poorly worded, though I am interested in the field I have not really looked into the finer details yet.

Thanks!  IT: Learn how to select, implement and manage existing technology solutions in a business setting.

CS: Learn about computation itself. Traditionally, CS has been more about math and theory than actual computers, but nowadays many CS grads go on to apply their knowledge to real world tasks, so now there is practical training in every program out there. You will learn to create software (and hardware at a theoretical level).

Software Engineering: Learn to be a software developer.

To be honest, if you are thinking "CS or IT", you do not understand these fields at a basic level, because they are *quite* different, despite the fact that they often share the same department at a university. I made the same mistake when I was looking at degree programs. I would start looking into the finer details now, before you apply anywhere. &amp;gt;  if you are thinking "CS or IT", you do not understand these fields at a basic level

Perhaps,  but to be fair,  many schools tend to have confusing nomenclatures...   moreover,  we are talking about a newbie and there is nothing wrong with being a bit confused when getting started in a new field. I completely agree.  I was in CS for a good year in undergrad before I had a real grasp on the idea this isnt what I was looking for.  Then I switched to web design (just kidding). What are you trying to say about web design? :P That it doesn't require a CS degree. IT: Learn how to select, implement and manage existing technology solutions in a business setting.

CS: Learn about computation itself. Traditionally, CS has been more about math and theory than actual computers, but nowadays many CS grads go on to apply their knowledge to real world tasks, so now there is practical training in every program out there. You will learn to create software (and hardware at a theoretical level).

Software Engineering: Learn to be a software developer.

To be honest, if you are thinking "CS or IT", you do not understand these fields at a basic level, because they are *quite* different, despite the fact that they often share the same department at a university. I made the same mistake when I was looking at degree programs. I would start looking into the finer details now, before you apply anywhere. IT: Learn how to select, implement and manage existing technology solutions in a business setting.

CS: Learn about computation itself. Traditionally, CS has been more about math and theory than actual computers, but nowadays many CS grads go on to apply their knowledge to real world tasks, so now there is practical training in every program out there. You will learn to create software (and hardware at a theoretical level).

Software Engineering: Learn to be a software developer.

To be honest, if you are thinking "CS or IT", you do not understand these fields at a basic level, because they are *quite* different, despite the fact that they often share the same department at a university. I made the same mistake when I was looking at degree programs. I would start looking into the finer details now, before you apply anywhere. At least in my uni where I'm studying for an IT degree, you can learn loads of CS (and you can do a major/minor in CS). In fact, a lot of CS is required in the degree. I've also heard that CS students at another uni learn a lot of IT stuff. And students of course can decide what courses they want to focus on. EDIT: This is in Finland. IT: Learn how to select, implement and manage existing technology solutions in a business setting.

CS: Learn about computation itself. Traditionally, CS has been more about math and theory than actual computers, but nowadays many CS grads go on to apply their knowledge to real world tasks, so now there is practical training in every program out there. You will learn to create software (and hardware at a theoretical level).

Software Engineering: Learn to be a software developer.

To be honest, if you are thinking "CS or IT", you do not understand these fields at a basic level, because they are *quite* different, despite the fact that they often share the same department at a university. I made the same mistake when I was looking at degree programs. I would start looking into the finer details now, before you apply anywhere. Thank you very much! After all the replies I definitely need to look more into what CS and IT are.  If you have an IT department at your hospital, then you can ask them for a better idea. What they do is IT.

One way to think about this is people who do IT are technicians, whereas people who do CS are software developers or researchers. IT is way more practical than CS as a field. Beyond basic knowledges in the field, it is all about learning new technologies, use them, and support the use in your organization. CS is all about theories, coding, and designing software.

IT has little to do with CS, although they are both about computers. Some CS knowledge could help a technician in understanding the technical terms used in a specification, and vice versa, some IT knowledge helps a developer to be productive. If you have an IT department at your hospital, then you can ask them for a better idea. What they do is IT.

One way to think about this is people who do IT are technicians, whereas people who do CS are software developers or researchers. IT is way more practical than CS as a field. Beyond basic knowledges in the field, it is all about learning new technologies, use them, and support the use in your organization. CS is all about theories, coding, and designing software.

IT has little to do with CS, although they are both about computers. Some CS knowledge could help a technician in understanding the technical terms used in a specification, and vice versa, some IT knowledge helps a developer to be productive. Technicians is false in so many ways. A technician is someone that applies specific technical skills in a standardized fashion in order to attain an defined objective. I wish in so many ways IT was like this, because it is not. Not one bit. Hence, why there are so many implementation failures. "Selecting, implementing, and managing existing technologies" means your dealing with people; not with computers. The most technical you get is through configuring ERPs; but even that requires dealing with people. (people cannot be "processed", they are as unstandard and unstable as the weather!... too many uncontrollable variables to establish a solid technique)

In programming, you apply a technique that is very standard. It is necessary to be like that, because in the object oriented development mindset, all the pieces of the software are replaceable parts that have to fit together. The software engineer plans all the different "parts" of the software to ensure that it responds to the specifications. 

Computer science, in it's actual application, is research and development. The reason why it is taught is to bring the computer practice further; beyond current computers. This is not focused on programming, or developing software, or organizations ; hell you don't even learn SQL.  IT: Learn how to select, implement and manage existing technology solutions in a business setting.

CS: Learn about computation itself. Traditionally, CS has been more about math and theory than actual computers, but nowadays many CS grads go on to apply their knowledge to real world tasks, so now there is practical training in every program out there. You will learn to create software (and hardware at a theoretical level).

Software Engineering: Learn to be a software developer.

To be honest, if you are thinking "CS or IT", you do not understand these fields at a basic level, because they are *quite* different, despite the fact that they often share the same department at a university. I made the same mistake when I was looking at degree programs. I would start looking into the finer details now, before you apply anywhere. Can you define Software Engineering and CS please? I am very undecided what I want to major in and currently I am a CS&amp;amp;Math major. I will start this fall SE is a form of applied computer science. Microsoft and Google hire people with a CS background to be Software Engineers and develop their commercial software applications. Many schools include an SE component in their CS curriculum and many CS grads go on to be professional SEs. Take a look at the wikipedia pages for both topics. They are pretty good. Can you define Software Engineering and CS please? I am very undecided what I want to major in and currently I am a CS&amp;amp;Math major. I will start this fall   I will tell you that in the US, an IT degree isn't looked upon very favorably.

It's the computer equivalent of getting a auto mechanic certification- it prepares you to fix end user issues not be a software engineer/architect/programmer.

If the IT program in the Philippines is a combination of CS and business, then that may be the best direction. Especially if you don't actually want to be a programmer at the startup. Just as an alternative perspective, I got an IT degree from Rochester Institute of Technology, and their IT program was the first accredited IT program in the country. The classes I took included 1 year of Java programming, multiple years of system &amp;amp; network administration (including building large networks with really expensive Cisco gear), multiple years of web &amp;amp; database programming (PHP, Perl, JavaScript, MySQL), and then for my personal track within the program I concentrated on interface design, human-computer interaction and usability testing.

It may have just been the program at my school, but I definitely disagree about IT majors not being suited to be actual engineers, architects or programmers. Tons and tons of my IT classmates now have jobs as engineers, architects, programmers, network/system administrators, HCI professionals and more. Personally, I learned C &amp;amp; Obj-C on my own and now design &amp;amp; build iOS, desktop and web apps and have been very successful, but the web programming, networking and database fundamentals I learned in college were very useful.

However, many IT programs are essentially MIS or IS programs (information systems) which are what you describe: maintaining and using technology, not actively building it. It's very important to actually *look* at the classes within a program like IT and not just rely on the name, and also to see if the program is accredited, because what you learn within an "IT" program may vary widely from school to school.

And as an allegory, my wife graduated from RIT's Computer Engineering program and is now a front-end architect working on web software. Our careers have converged even though she was in bunny suits building microprocessors in college while I was writing JavaScript :) Just as an alternative perspective, I got an IT degree from Rochester Institute of Technology, and their IT program was the first accredited IT program in the country. The classes I took included 1 year of Java programming, multiple years of system &amp;amp; network administration (including building large networks with really expensive Cisco gear), multiple years of web &amp;amp; database programming (PHP, Perl, JavaScript, MySQL), and then for my personal track within the program I concentrated on interface design, human-computer interaction and usability testing.

It may have just been the program at my school, but I definitely disagree about IT majors not being suited to be actual engineers, architects or programmers. Tons and tons of my IT classmates now have jobs as engineers, architects, programmers, network/system administrators, HCI professionals and more. Personally, I learned C &amp;amp; Obj-C on my own and now design &amp;amp; build iOS, desktop and web apps and have been very successful, but the web programming, networking and database fundamentals I learned in college were very useful.

However, many IT programs are essentially MIS or IS programs (information systems) which are what you describe: maintaining and using technology, not actively building it. It's very important to actually *look* at the classes within a program like IT and not just rely on the name, and also to see if the program is accredited, because what you learn within an "IT" program may vary widely from school to school.

And as an allegory, my wife graduated from RIT's Computer Engineering program and is now a front-end architect working on web software. Our careers have converged even though she was in bunny suits building microprocessors in college while I was writing JavaScript :) I will tell you that in the US, an IT degree isn't looked upon very favorably.

It's the computer equivalent of getting a auto mechanic certification- it prepares you to fix end user issues not be a software engineer/architect/programmer.

If the IT program in the Philippines is a combination of CS and business, then that may be the best direction. Especially if you don't actually want to be a programmer at the startup. I can tell you from personal experience that your assessment is incorrect.

Of course, it depends on the school's curriculum for each degree, and your opinion may hold true for some, but definitely not all. I can tell you from personal experience that your assessment is incorrect.

Of course, it depends on the school's curriculum for each degree, and your opinion may hold true for some, but definitely not all. And I can say from personal experience that it is.      What are you intersted in doing after you get out of school? Start-ups definitely.

I worked as a nurse but resigned after 4 months (should have included this in the post), it was too stressful and had too little pay (Philippines salary is just sad). Now I've been working online as a Social Media Marketer, self taught all the way. 

During one of my stints as an outsourced worker, I was hired by a start-up based in Thailand, unfortunately the CEO called quits. I really liked the start-up environment and I got lots of ideas that might just work in the Philippines. 

So, I am thinking of starting my own business when I'm done with my degree. Be sure to check out [/r/startups](/r/startups). I'm a comp sci major on a year out in a startup right now, it's definitely been useful. I have been subscribed for around a year now. Thanks!  Be sure to check out [/r/startups](/r/startups). I'm a comp sci major on a year out in a startup right now, it's definitely been useful. Start-ups definitely.

I worked as a nurse but resigned after 4 months (should have included this in the post), it was too stressful and had too little pay (Philippines salary is just sad). Now I've been working online as a Social Media Marketer, self taught all the way. 

During one of my stints as an outsourced worker, I was hired by a start-up based in Thailand, unfortunately the CEO called quits. I really liked the start-up environment and I got lots of ideas that might just work in the Philippines. 

So, I am thinking of starting my own business when I'm done with my degree. If you really want to start a business, **just do it**. You don't need a degree to start a business, that being said.
I don't know how the job market for IT or CS grads is or will be in the Philippines, but I remember reading an article ([here](http://www.businessweek.com/globalbiz/blog/eyeonasia/archives/2009/05/philippines_it.html)) about an IT boom in Cebu City, and I don't think IT or CS job are going away anytime soon. If you want to go into Software Engineering I would recommend doing CS, IMHO the problem solving involved in a CS degree would be better prep for a programming type job (but that does depened on the school and curriculum, an IT major structred something like [this](http://www.ist.rit.edu/node/15) would be fine).
I sugest checking out MIT OCW, Udacity, coursera, cs50.tv and see.stanford.edu. 
edit:spelling
 Start-ups definitely.

I worked as a nurse but resigned after 4 months (should have included this in the post), it was too stressful and had too little pay (Philippines salary is just sad). Now I've been working online as a Social Media Marketer, self taught all the way. 

During one of my stints as an outsourced worker, I was hired by a start-up based in Thailand, unfortunately the CEO called quits. I really liked the start-up environment and I got lots of ideas that might just work in the Philippines. 

So, I am thinking of starting my own business when I'm done with my degree. What university are you planning on taking CS from? From what I know the curriculum varies greatly. (I'm from the Philippines as well) Start-ups definitely.

I worked as a nurse but resigned after 4 months (should have included this in the post), it was too stressful and had too little pay (Philippines salary is just sad). Now I've been working online as a Social Media Marketer, self taught all the way. 

During one of my stints as an outsourced worker, I was hired by a start-up based in Thailand, unfortunately the CEO called quits. I really liked the start-up environment and I got lots of ideas that might just work in the Philippines. 

So, I am thinking of starting my own business when I'm done with my degree. Start-ups definitely.

I worked as a nurse but resigned after 4 months (should have included this in the post), it was too stressful and had too little pay (Philippines salary is just sad). Now I've been working online as a Social Media Marketer, self taught all the way. 

During one of my stints as an outsourced worker, I was hired by a start-up based in Thailand, unfortunately the CEO called quits. I really liked the start-up environment and I got lots of ideas that might just work in the Philippines. 

So, I am thinking of starting my own business when I'm done with my degree. I _think_ the general difference is CompSci will give you a more theoretical grounding but is less hands on, whereas IT will give you immediate practical experience though not necessarily in the area you want to go on to use.  (Depends on the uni course itself)

If you're thinking of starting your own thing afterwards, I definitely look at add some business modules to it (general business, accounting, marketing, strategy etc) if you think that will help.

If you're after a fast track to being able to provide a professional service then your choice is EITHER:

1. go for CompSci to get a solid grounding in logic, maths, db theory, programming theory AND put in the hours to do your own mini-projects in the specific thing you want to do in your business.  Take bit-work, contract work, join open source projects, chip in.  Learn the myriad of details you get from actually doing it that a course won't necessarily give you (eg source control!).
CompSci will be helpful as technology evolves over the next decades.  (you'll pick up new stuff quicker because you'll appreciate the thinking behind it)    OR

2.  Find an IT course that gives plenty of hands on experience with the specific thing you want to do.   Still put in all the practice to be more rounded in it in your spare time but don't worry about the theory stuff so much.  Read up on that while you get your start up off the ground, and put the effort in to learn while IT evolves.

TL;DR a life in IT involves constantly learning.  The course you choose depends on whether you want more help learning the theory or a specific practice    Judging by your past experience and your desire to work, I would say that Information Technology would be your best choice. The market is loaded with opportunities for IT specialists and almost all companies need some form of help with IT. The starting pay is decent, however, the field of IT is not incredibly lucrative; at least not compared to Computer Science.

Computer Science requires a lot more work and mathematical understanding in addition to the practice taught. That being said, the field of Compute Science is so diverse that it allows you to specify what you want to do while maintaining expertise in most professions (including IT). If you are willing (or have) to take off a bit of time to understand and conceptualize the theory associated with Computer Science, the opportunities are far more vast and much more lucrative.

In the short term --&amp;gt; IT, long term --&amp;gt; Computer Science     Software Engineering? where can you study software E? in my country there is only CS or IT. Nerver heard there was a grade on just the software part. A few universities in the states have it as a stand alone degree. Most just have a Computer Science or Computer Engineering degree and a few courses in SE that you can elect to take. so its not a degree by itself, right?           </snippet></document><document><title>Computational Thinking [PDF]</title><url>http://www.cs.cmu.edu/~wing/publications/Wing06.pdf</url><snippet>   It's a little old, but I really like this text. Some of the examples are a little weird, like the girl packing her bag (That's not so much computational thinking, it's more just doing something). But I could really see this concept of computational thinking being smart as a "recipe" for doing certain tasks. I love how 6 years is now "a little old" in computer science. I love how 6 years is now "a little old" in computer science.  Very interesting, but I think it would have benefited from some more practical, in-depth examples of solving problems outside of computer science with computational thinking. The examples of prefetching/caching, backtracking, online algorithms, etc. don't really illustrate why thinking about those particular things (planning a lunch, retracing your steps, deciding to stop renting in favor of a purchase, etc.)  computationally is an advantage. (Except perhaps for the CAPTCHA one, but that's not outside the realm of computers to begin with.)    </snippet></document><document><title>All of you Computer Science majors, what job do you currently have?</title><url>http://www.reddit.com/r/compsci/comments/srxqe/all_of_you_computer_science_majors_what_job_do/</url><snippet>I am currently about to be a sophomore Computer Science major and love it, but am a little worried that my career will consist of literally just programming. Whether you do a ton of programming or not, what job do you currently hold?   I worked for a few years as a programmer ("software engineer") and started to get sick of it.  I love programming, but it was too much.  I worked for a decent company, but working on the same project all day every day started to grind me down.  When I went home at night, I didn't want anything to do with computers.  This was upsetting to me, because I've loved programming since I started at age 10.

I left and went to graduate school, then took a position teaching computer science at a teaching college (no research).  This has been a great job.  I get to know a lot of great students, I have a lot of control over the classes that I teach, and I work on projects I love on the side, just for the fun of it.  I also get summers off, long holidays, and I work many fewer hours than I used to.

I don't get paid nearly as well as if I'd stayed in a commercial job, but I love what I do and I can honestly see myself doing this for the rest of my life without learning to hate it. I want your job so badly. Thanks for the vote of confidence!  It was a hard decision at the time.  My starting salary with a PhD was $1000 less than my first job out of college, and working at a small state college lacks the prestige of a bigger research university.  After going to famous colleges and working in a startup environment, I had to swallow my pride a bit to go this route, but I think it was a good choice for me and my family (who I get to spend time with).  I'm not saying this is the way to go for everyone, but it is working out well for me.  It doesn't hurt that my wife got a teaching job at the same college, so we have coordinated holidays. Thanks for the vote of confidence!  It was a hard decision at the time.  My starting salary with a PhD was $1000 less than my first job out of college, and working at a small state college lacks the prestige of a bigger research university.  After going to famous colleges and working in a startup environment, I had to swallow my pride a bit to go this route, but I think it was a good choice for me and my family (who I get to spend time with).  I'm not saying this is the way to go for everyone, but it is working out well for me.  It doesn't hurt that my wife got a teaching job at the same college, so we have coordinated holidays. This is my current gameplan. I was a little surprised to find out how little a long career in industry means to the academic world. I guess the best you can do is save some money in the commercial job, and make some contacts so that you can work contracts in the summer. Working can even be a bit of a disadvantage sometimes.  Letters of recommendation are about the most important part of grad school admission, and they really only care about those from other academics.  If your undergrad professors have all forgotten you, it can be hard to get strong letters.

The hardest part of going back after working is losing your income, though, and this is what keeps most people out. I worked for a few years as a programmer ("software engineer") and started to get sick of it.  I love programming, but it was too much.  I worked for a decent company, but working on the same project all day every day started to grind me down.  When I went home at night, I didn't want anything to do with computers.  This was upsetting to me, because I've loved programming since I started at age 10.

I left and went to graduate school, then took a position teaching computer science at a teaching college (no research).  This has been a great job.  I get to know a lot of great students, I have a lot of control over the classes that I teach, and I work on projects I love on the side, just for the fun of it.  I also get summers off, long holidays, and I work many fewer hours than I used to.

I don't get paid nearly as well as if I'd stayed in a commercial job, but I love what I do and I can honestly see myself doing this for the rest of my life without learning to hate it. Did you get an MS or PhD? PhD.  I went to England, where PhD programs are 3-4 years.  I defended my dissertation a few days before my 30th birthday. How different is funding in phd programs in England?  High frequency trading systems engineer.  That's a pretty niche area -- how did you break into it? While I was in school I worked part time doing basic IT work for a local trading firm and eventually moved into writing their automated trading software. Around the time I was graduating my company was closing shop and I was recruited by my current company. So effectively, networking got me into the field.  Just curious, I hear alot about Q,K and J being used in financial institutions, does your firm rely on any of them? High frequency trading systems engineer.  I would like to ask you some questions, if you don't mind:)

- Do you know much about the particulars?
- Some smaller firms use principles around the law of averages and regression to mean, do you know of any processes that use different strategies?

 I doubt you have any exposure to this sort of stuff doing sys engineering work  I work for a large defense contractor writing c++ and python applications on linux.  The apps read in various scientific data (imagery, sensor data) and compute statistics and composite data sets.  In addition, we write visualization tools (2d, 3d) that view the data sets.  Some of these statistics tools run on Linux clusters using technologies such as MPI, OpenMP, and CUDA. I work for a large defense contractor writing c++ and python applications on linux.  The apps read in various scientific data (imagery, sensor data) and compute statistics and composite data sets.  In addition, we write visualization tools (2d, 3d) that view the data sets.  Some of these statistics tools run on Linux clusters using technologies such as MPI, OpenMP, and CUDA. Seems like a perfect job, quick question do they take international PhD students ?   Querying help desk metrics in SQL, formatting them with HTML and Javascript, and sitting in meetings. Lots of meetings. Also, interacting (discussing video games and vehicles, mostly) with all of the other guys my age who work there.  Is it bad that I would almost kill for this job when I graduate? Which part of it do you like the most? The SQL coding? The meetings? The chatting with co-workers? Having meetings is almost a given, having co-workers with similar interests is fairly likely, and the SQL part is just applying for the right job and proving you can do it well. Well, it's obviously harder than that in this economy, but I'd imagine you could end up in a similar position given enough time and effort.

One of the benefits of working for a relatively small IT department is that when I told my manager about my passion for SQL she said she'd see what she could find for me to do with it. Then, a few months later, she asked if I wanted to be the assistant to the lead software engineer, and now I'm essentially a junior SQL developer.  Senior Security Researcher - My job consists of digital forensics investigations, pentests, research and development (grant chasing).. and loving it.  There's a lot more to CS than just software development.   Mobile augmented reality developer. Even though my job is coding, I (and the rest of the developers) have significant input in design and all that stuff as well. Probably only 65% of my time is actual coding.  coolio , what languages , apis do you use ?        Soon to be robotics software engineer. What level of degree do you hold? 
Did you take robotics courses in your undergrad?  Code is everything.                     Firmware Engineer at a Major Printer company, I started as a Test Engineer (Most laid back Company/Job ever). Great Job, Entry Level QA is honestly a pretty sweet start, since I got to Test and Code some (Which got me adapted to coding in the real world). Feels good man. did you program in forth/postscript ? No why? i tought some firmware programming was wrote using forth :/  </snippet></document><document><title>ACM Members: It's Election Time. Be sure to vote for candidates who support Open Access! And Be Sure to Vote!</title><url>http://youinfinitesnake.blogspot.com/2012/04/statements-of-acm-candidates-on-open.html</url><snippet>  Huh. There are surprisingly few candidates who support the open-access model. </snippet></document><document><title>Giving a CS lecture to a class of non-CS students.</title><url>http://www.reddit.com/r/compsci/comments/sr1fa/giving_a_cs_lecture_to_a_class_of_noncs_students/</url><snippet>I'm a senior in high school doing an independent study on Genetic Algorithms. I've already done two lectures on the basics of GA/GP, so I'm going to cover something different and interesting.

I'd like to talk about three classic, non-trivial problems in CS (e.g. n-queens). The class will be split into three or four groups, each group will design a (presumably naive) solution to a presented problem, and then we'd discuss it as a class. Rinse and repeat.

**Does anybody have any suggestions for interesting problems I can cover?** Something to hold the attention of a class of about eighteen high school seniors (of which four are CS students). Thank you in advance.

**Edit: you guys and gals are awesome, thank you for all your help. Time to hit 'em over the head with CS!**  I thought Dijkstra's algorithm was fascinating my first year in college. (http://en.wikipedia.org/wiki/Dijkstra's_algorithm)


You could ask them to find the shortest distance to an object from a starting point with obstacles, then show them the algorithm.

Also, my favorite intro to CS for non-CS students is to ask them to make a list of instructions of how to make a PB&amp;amp;J sandwich. It's really easy to forget little details, and that's something that is very applicable to CS. Ahh, this is beautiful. Thank you! You could even do something like "Your job is to design GoogleMaps, how do you find the shortest route to your destination?" or something similar to that to relate it to real world problems. Ahh, this is beautiful. Thank you!   Just spouting some ideas off of the top of my head-
-The 0-1 Knapsack problem (or some variant)
-
-Tiling an N x N grid with triominos
-  The Knapsack problem is such a great idea, thank you. Heh, I read that as "tomatoes" as first and was pretty confused for a few seconds... I wasn't sure how technical you wanted to get, but I've paged through my Discrete Mathematics textbook from a few years ago, here are some good ones I recall having major "Aha!" moments about

-The Travelling Salesman problem (already mentioned)
-
-Postage Stamp Problem
-
(Prove that every amount of postage 12 cents or more can be formed using just 4 and 5 cent stamps - proof by induction)


The Birthday Problem
-

The Ramsey theory (Pidgeonhole Principle)
-
Assuming that in a group of six people, each pair of individuals consists of two friends or two enemies. Show that there are either three mutual friends or three mutual enemies in the group.

How complicated did you want to get?   Network flows - Used to solve problems that can be modelled as maximum flow through a graph, including: bi-partite matching, assignment, scheduling, project selection - solves min cut as a bonus. Pretty badass.

The great part is that one algorithm to solve them is a simple "try something and iteratively fix it up". A solution that someone is likely to come up with.

http://en.wikipedia.org/wiki/Flow_network#Applications
http://en.wikipedia.org/wiki/Ford-Fulkerson_algorithm#Algorithm

Wikipedia has a crap explanation of FF - too much formalism. You might want to check out the slides from my favourite algorithms book: http://www.cs.princeton.edu/~wayne/kleinberg-tardos/  It might be a little trite, but if you want to get them thinking about recursion, you could try the Tower of Hanoi.  If you're talking about classic, non-trivial problems in CS, this seems like a great opportunity to introduce the idea of easy (P) and hard (NP) problems.  You can use the traveling salesman problem as an example of a fairly easy-to-grasp NP problem, and you can consider a wide variety of problems for the P class.

This could be a valuable idea to introduce since the idea that some problems are much harder than others is a very intuitive concept, but it could be interesting and intriguing to know that of all the smart people working on this problem, none have been able to prove it.  You could also bring in EXPTIME if you wanted a complexity class that is known to be strictly harder than P.  I just finished an assignment for a CS class on the [Dining Philosophers Problem](http://en.wikipedia.org/wiki/Dining_philosophers_problem).    Don't forget the bread and butter of algos. Sorting. 

There is also coloring a map with 4  colors, gale shapely pairing, Floyd warshall algo which is O(n^3) compared to dijksters O(n^2) and tons more. 

Since your talking about less than 10 classes with laypeople I would try to stick to either depth or breadth in your subject. So stick to all the ways to sort a random list of integers which is boring but necessary or graph theory and trees which is going to be difficult. Another option is breadth where you just break each class up into sorting, graph, evaluating algos big O, and finally cool examples.  4-color problem is an excellent idea (my mentor suggested the same thing just today). I already covered sorting algorithms in my explanation of big-O notation, but it'd be a really good way to introduce a new topic in something that they are already familiar with.  Any math/number people? I like this problem:

Is there a partition of `[; \{\sqrt{1},\sqrt{2},\sqrt{3},\ldots,\sqrt{99},\sqrt{100}\} ;]` into two sets A and B such that `[; \sum_{a \in A} a = \sum_{b \in B}b ;]`, or what partition results in the minimal difference of the sums. I like it; where can I find more information? Uh, don't know really. A math/cs teacher in high school told me about it and suggested that using genetic algorithms might work for it. Encode a potential solution as a 100-length bit string, 0 if the number at index n-1 is in one set and not the other, and visa versa. If a perfect partition is possible, each set will sum to just half the total sum. So store that as your target and check how far from it you are, using that as your "health" metric, or whatever they're called for genetic programming. Hmm, from what I can find it seems to have been originally presented by Knuth and is a variant of the subset-sum problem... I might use this, except maybe without square roots to simplify things a little bit. Thank you! Well, I think the thing that makes it particularly difficult is the square roots, otherwise I think you can just DP it. But the normal non-fancy subset sum problem is nice, too. Is it obvious that a solution exists? I'm not going to get into depth with it, but a solution (where difference = 0) doesn't exist for n=2, and I don't think it exists for n=4. Perhaps when n=100 it's a different story, though.

If you can show me that a solution exists, then we can definitely find one, although the algorithm might be in NP. No, I don't think it's obvious at all. My teacher brought it up in the context of genetic algorithms (which is why I brought it up here, since OP mentioned genetic algorithms). He suggested trying to solve it with GA's, but he didn't know if it would work or not. He implied it was an open problem. But it might be fun to try and minimize the difference with GAs.

edit: well, I'd point out that I stated the problem as "does there exist a partition", so in that sense, yes, a solution of "yes" or "no" definitely exists. Aha. Yeah, it's definitely possible to try and find a minimal difference, but I wasn't so sure about no difference.

GAs are cool and sexy because of their inspiration, but an equally cool method is simulated annealing. Essentially, you do hill climbing with some random component. I personally feel like SA is better because it can be implemented more directly in the solution than in a GA, and essentially the outcome is the same.

For instance, SA is often times implemented as randomly selecting a parameter, and performing some adjustment to improve the results. You could easily implement this for our problem as randomly selecting a number and seeing if moving it to the other set would improve the difference between to the two sets. If so, keep the improved copy and repeat the process. There's an easy field theory proof of the contrary for all n &amp;gt; 1 using [Bertrand's postulate](http://en.wikipedia.org/wiki/Bertrand's_postulate). Aha. Yeah, it's definitely possible to try and find a minimal difference, but I wasn't so sure about no difference.

GAs are cool and sexy because of their inspiration, but an equally cool method is simulated annealing. Essentially, you do hill climbing with some random component. I personally feel like SA is better because it can be implemented more directly in the solution than in a GA, and essentially the outcome is the same.

For instance, SA is often times implemented as randomly selecting a parameter, and performing some adjustment to improve the results. You could easily implement this for our problem as randomly selecting a number and seeing if moving it to the other set would improve the difference between to the two sets. If so, keep the improved copy and repeat the process. I think I actually attempted to solve it with SA instead of GA because I was more comfortable with SA at the time, but why do you think SA would be better? My teacher seemed to think GA might work out better for some reason (and he wasn't just some schmuck, he used to teach math at stanford -- ie, I have reason to trust his gut a bit). I like it; where can I find more information?    How did it go? Which examples did you use? Is there a detailed lesson plan you could post? </snippet></document><document><title>Looking for a specific paper about compilers and programming languages, details inside.</title><url>http://www.reddit.com/r/compsci/comments/ss55n/looking_for_a_specific_paper_about_compilers_and/</url><snippet>I read a part of a paper about compilers, programming languages and their performance a while ago but I no longer can find it.

It had some stuff about implementing a llvm-like intermediate representation. Also I believe managed languages was a topic in there too (it was mentioned multiple times in the beginning).

This is not much information but I hope someone knows it. I think it isn't an old paper because stuff like llvm was mentioned there.  There was a comparison of [Scala/Go/Java/C++ in a Google paper](https://days2011.scala-lang.org/sites/days2011/files/ws3-1-Hundt.pdf) a while back.  It discussed the use of virtual machines (particularly the JVM), but not LLVM specifically.

Check it out anyway, it's a pretty neat paper. That's not the paper I'm looking for but it looks interesting, I'll check it when I have time. :) </snippet></document><document><title>The Quine Page(self-reproducing code)</title><url>http://www.nyx.net/~gthompso/quine.htm</url><snippet>   </snippet></document><document><title>Computer Science Classes I Would Like To See Offered</title><url>http://theexceptioncatcher.com/blog/2012/04/computer-science-classes-i-would-like-to-see-offered/#.T5eJRQuH9VM.reddit</url><snippet>  I'd like to see

*  Intro to Category Theory
*  Monoids and Dioids and Kleene Algebras
*  Logic and Dependent Type Theory. I hear that branch of computer science is called "math" (or "maths", if you're British).  
  
Occasionally, schools tend to offer that as its own department. I hear that branch of computer science is called "math" (or "maths", if you're British).  
  
Occasionally, schools tend to offer that as its own department. I'd like to see

*  Intro to Category Theory
*  Monoids and Dioids and Kleene Algebras
*  Logic and Dependent Type Theory. That sounds interesting.... Could you go into that a little more? I don't know much about those. [I've heard of monoids, but I don't fully remember or understand it]
Feel free to add it to the comment on the blog. [No its not for self promotion] Its more of for those who find the article from outside of reddit. Monoid is just a set of values with an operation that has an identity, obeys closure and has an inverse for all possible values.

Example: Integers and addition form a monoid. 0 is the identity, any integers you add together will always yield another integer, and for every intenger I, there exists an integer -I which when added to it yields the identity.

Edit: I was wrong; I described a group, not a monoid. What you describe is closer to a group than a monoid. An element in a monoid need not have an inverse. In either case you need associativity. Monoid is just a set of values with an operation that has an identity, obeys closure and has an inverse for all possible values.

Example: Integers and addition form a monoid. 0 is the identity, any integers you add together will always yield another integer, and for every intenger I, there exists an integer -I which when added to it yields the identity.

Edit: I was wrong; I described a group, not a monoid.  &amp;gt; Software Development Tools:

This is really in the domain of Software Engineering and not Computer Science. It should be available for CompSci students to take as an elective. Remember, the sole goal of CompSci isn't to produce Software Engineers, it's to produce people who understand the principles of Computer Science. That does't mean programmers; graduates could end up in a range of non-programming roles as well (Business Analyst?)

&amp;gt; Debugging &amp;amp; Testing software:

I thought this was fairly common practice. 

&amp;gt; Computer Vision

This is a challenge for undergraduate courses, simply because of the amount of CompSci you need to have under your belt to be able to get into CV relatively quickly. 

&amp;gt; How to design an API/Surveying APIs:

This should probably be another elective within Software Engineering that CompSci students should consider. However, I don't think Universities could do it justice. There isn't a formal method of writing APIs which makes it difficult to teach. Especially when the students are still, effectively, novices when it comes to producing code. 

&amp;gt; Open Source: Not really a history of open source. But this is to take an open source project, and extend it. The goal of this project is to get students involved with working with one project, and demonstrating improvement to the project.

The most common case I see is students submitting really shitty contributions to things and increasing the administration duties of the administrators. 

&amp;gt; Marketing: How to market your work, or someone else&#8217;s software. This isn&#8217;t designed to replace developers with marketers, but it just helps the developer understand how their work is sold.

Introduction to Marketing Fundamentals should be available as an elective for all students. 

What would be a much better approach is simply teaching students how to present there work in a variety of places. 

&amp;gt; Automation: How to automate manual tasks with software. This could be with build scripts, batch scripts, testing automation software, or even simple system scheduling.

Another vocational output. What country are you in? In AU students have a general maximum of 24 subjects that will make up their CompSci degree (32 if they complete their honours year). So far, you have taken out 6 CompSci subjects and replaced them with 4 SoftEng, 
 1 Marketing, and 1 InfoSys courses. 

&amp;gt; From hardware to software:

I thought this was fairly common in CompSci degrees? 

&amp;gt; How Computer Science relates to [Field x, y or z]: 

Really? Really? Fuck, if students can't work out how computer science relates to fields that use computers they have bigger problems. Like breathing and walking at the same time. 

&amp;gt; How to crack software/re-engineer a binary: [Probably with the permission of the publisher] 

This should probably already be done in the debugging course. MIT has a subject where one of the debugging solutions is done via reverse engineering. 

But other than that, it's probably more SE than CS. 

&amp;gt; Alternative Language Survey: Yes, this is technically a standard class. But I&#8217;d like to see one on functional languages, Groovy, BF, or even creating your own domain specific languages be taught.

How did you get taught compilers and interpretors? 

&amp;gt; The goal of this class is to research market need, create something usable and getting their fellow peers involved in using it. 

Considering you already put marketing as a section. How about you teach the Interaction Design methodology instead? 

&amp;gt; Hands on Software Optimization class: Take an open source system, and optimize it. This class would teach formal procedures on how to optimize an existing application to perform as quickly as possible, monitor, and document the improvements.

This would probably be impossible to mark fairly. 

&amp;gt; He suggested that given the available of free CS courses (OpenCourseWare and the like) and experts at your fingertips (Blogs and Stackoverflow); wheres the value of a formal CS degree?

Also, your friend is a moron. Stop listening to him. 

CompSci isn't handled well by things like Stack Overflow, or blogs. It's much better for low level Business based Information Technology, or shitty SoftEng programs.  &amp;gt;&amp;gt; Debugging &amp;amp; Testing software:

&amp;gt; I thought this was fairly common practice.

I don't think any of the debugging and testing skills I have were acquired from university courses. Most probably, they were from reading programming blogs/forums, and being an innocent bystander of the Extreme Programming/TDD fad.

&amp;gt;&amp;gt; From hardware to software:

&amp;gt; I thought this was fairly common in CompSci degrees?

I don't think I ever did any hardware either. Closest I got was working with some assembler. There was no work that I did that required interaction with the physical world other than touching the keyboard and mouse.

&amp;gt;&amp;gt; Alternative Language Survey: Yes, this is technically a standard class. But I&#8217;d like to see one on functional languages, Groovy, BF, or even creating your own domain specific languages be taught.

&amp;gt; How did you get taught compilers and interpretors?

Not the OP, but if I recall correctly, [my course](http://www.cs.mcgill.ca/~cs520/2011/index.html) did not do a "survey" of different languages. We looked at parsers, ASTs, symbol tables, type checking, code generation, virtual machines, optimization, etc.

Since your and my university experiences seems to differ so much, I'll preemptively mention I went to McGill in Montreal, Quebec. CMU has "bomb lab" for teaching debugging and reverse engineering, it's an awesome lab. Apparently the course is sold to other schools as well (but not mine, unfortunately).

"A 'binary bomb' is a program provided to students as an object code file. When run, it prompts the user to type in 6 different strings. If any of these is incorrect, the bomb 'explodes,'' printing an error message and logging the event on a grading server. Students must 'defuse' their own unique bomb by disassembling and reverse engineering the program to determine what the 6 strings should be. The lab teaches students to understand assembly language, and also forces them to learn how to use a debugger. It's also great fun. A legendary lab among the CMU undergrads." &amp;gt;&amp;gt; Debugging &amp;amp; Testing software:

&amp;gt; I thought this was fairly common practice.

I don't think any of the debugging and testing skills I have were acquired from university courses. Most probably, they were from reading programming blogs/forums, and being an innocent bystander of the Extreme Programming/TDD fad.

&amp;gt;&amp;gt; From hardware to software:

&amp;gt; I thought this was fairly common in CompSci degrees?

I don't think I ever did any hardware either. Closest I got was working with some assembler. There was no work that I did that required interaction with the physical world other than touching the keyboard and mouse.

&amp;gt;&amp;gt; Alternative Language Survey: Yes, this is technically a standard class. But I&#8217;d like to see one on functional languages, Groovy, BF, or even creating your own domain specific languages be taught.

&amp;gt; How did you get taught compilers and interpretors?

Not the OP, but if I recall correctly, [my course](http://www.cs.mcgill.ca/~cs520/2011/index.html) did not do a "survey" of different languages. We looked at parsers, ASTs, symbol tables, type checking, code generation, virtual machines, optimization, etc.

Since your and my university experiences seems to differ so much, I'll preemptively mention I went to McGill in Montreal, Quebec. &amp;gt; Software Development Tools:

This is really in the domain of Software Engineering and not Computer Science. It should be available for CompSci students to take as an elective. Remember, the sole goal of CompSci isn't to produce Software Engineers, it's to produce people who understand the principles of Computer Science. That does't mean programmers; graduates could end up in a range of non-programming roles as well (Business Analyst?)

&amp;gt; Debugging &amp;amp; Testing software:

I thought this was fairly common practice. 

&amp;gt; Computer Vision

This is a challenge for undergraduate courses, simply because of the amount of CompSci you need to have under your belt to be able to get into CV relatively quickly. 

&amp;gt; How to design an API/Surveying APIs:

This should probably be another elective within Software Engineering that CompSci students should consider. However, I don't think Universities could do it justice. There isn't a formal method of writing APIs which makes it difficult to teach. Especially when the students are still, effectively, novices when it comes to producing code. 

&amp;gt; Open Source: Not really a history of open source. But this is to take an open source project, and extend it. The goal of this project is to get students involved with working with one project, and demonstrating improvement to the project.

The most common case I see is students submitting really shitty contributions to things and increasing the administration duties of the administrators. 

&amp;gt; Marketing: How to market your work, or someone else&#8217;s software. This isn&#8217;t designed to replace developers with marketers, but it just helps the developer understand how their work is sold.

Introduction to Marketing Fundamentals should be available as an elective for all students. 

What would be a much better approach is simply teaching students how to present there work in a variety of places. 

&amp;gt; Automation: How to automate manual tasks with software. This could be with build scripts, batch scripts, testing automation software, or even simple system scheduling.

Another vocational output. What country are you in? In AU students have a general maximum of 24 subjects that will make up their CompSci degree (32 if they complete their honours year). So far, you have taken out 6 CompSci subjects and replaced them with 4 SoftEng, 
 1 Marketing, and 1 InfoSys courses. 

&amp;gt; From hardware to software:

I thought this was fairly common in CompSci degrees? 

&amp;gt; How Computer Science relates to [Field x, y or z]: 

Really? Really? Fuck, if students can't work out how computer science relates to fields that use computers they have bigger problems. Like breathing and walking at the same time. 

&amp;gt; How to crack software/re-engineer a binary: [Probably with the permission of the publisher] 

This should probably already be done in the debugging course. MIT has a subject where one of the debugging solutions is done via reverse engineering. 

But other than that, it's probably more SE than CS. 

&amp;gt; Alternative Language Survey: Yes, this is technically a standard class. But I&#8217;d like to see one on functional languages, Groovy, BF, or even creating your own domain specific languages be taught.

How did you get taught compilers and interpretors? 

&amp;gt; The goal of this class is to research market need, create something usable and getting their fellow peers involved in using it. 

Considering you already put marketing as a section. How about you teach the Interaction Design methodology instead? 

&amp;gt; Hands on Software Optimization class: Take an open source system, and optimize it. This class would teach formal procedures on how to optimize an existing application to perform as quickly as possible, monitor, and document the improvements.

This would probably be impossible to mark fairly. 

&amp;gt; He suggested that given the available of free CS courses (OpenCourseWare and the like) and experts at your fingertips (Blogs and Stackoverflow); wheres the value of a formal CS degree?

Also, your friend is a moron. Stop listening to him. 

CompSci isn't handled well by things like Stack Overflow, or blogs. It's much better for low level Business based Information Technology, or shitty SoftEng programs.  &amp;gt;&amp;gt;Computer Vision

&amp;gt;This is a challenge for undergraduate courses, simply because of the amount of CompSci you need to have under your belt to be able to get into CV relatively quickly.

My university offers IVR (Introduction to Vision and Robotics) in 3rd year
undergrad (penultimate year). I'll admit it was (is, I'm revising for it now) a tricky course but it's doable. [deleted] Maybe so but it's Herrmann that will set the impossible coursework. [deleted] &amp;gt;&amp;gt;Computer Vision

&amp;gt;This is a challenge for undergraduate courses, simply because of the amount of CompSci you need to have under your belt to be able to get into CV relatively quickly.

My university offers IVR (Introduction to Vision and Robotics) in 3rd year
undergrad (penultimate year). I'll admit it was (is, I'm revising for it now) a tricky course but it's doable. &amp;gt; Software Development Tools:

This is really in the domain of Software Engineering and not Computer Science. It should be available for CompSci students to take as an elective. Remember, the sole goal of CompSci isn't to produce Software Engineers, it's to produce people who understand the principles of Computer Science. That does't mean programmers; graduates could end up in a range of non-programming roles as well (Business Analyst?)

&amp;gt; Debugging &amp;amp; Testing software:

I thought this was fairly common practice. 

&amp;gt; Computer Vision

This is a challenge for undergraduate courses, simply because of the amount of CompSci you need to have under your belt to be able to get into CV relatively quickly. 

&amp;gt; How to design an API/Surveying APIs:

This should probably be another elective within Software Engineering that CompSci students should consider. However, I don't think Universities could do it justice. There isn't a formal method of writing APIs which makes it difficult to teach. Especially when the students are still, effectively, novices when it comes to producing code. 

&amp;gt; Open Source: Not really a history of open source. But this is to take an open source project, and extend it. The goal of this project is to get students involved with working with one project, and demonstrating improvement to the project.

The most common case I see is students submitting really shitty contributions to things and increasing the administration duties of the administrators. 

&amp;gt; Marketing: How to market your work, or someone else&#8217;s software. This isn&#8217;t designed to replace developers with marketers, but it just helps the developer understand how their work is sold.

Introduction to Marketing Fundamentals should be available as an elective for all students. 

What would be a much better approach is simply teaching students how to present there work in a variety of places. 

&amp;gt; Automation: How to automate manual tasks with software. This could be with build scripts, batch scripts, testing automation software, or even simple system scheduling.

Another vocational output. What country are you in? In AU students have a general maximum of 24 subjects that will make up their CompSci degree (32 if they complete their honours year). So far, you have taken out 6 CompSci subjects and replaced them with 4 SoftEng, 
 1 Marketing, and 1 InfoSys courses. 

&amp;gt; From hardware to software:

I thought this was fairly common in CompSci degrees? 

&amp;gt; How Computer Science relates to [Field x, y or z]: 

Really? Really? Fuck, if students can't work out how computer science relates to fields that use computers they have bigger problems. Like breathing and walking at the same time. 

&amp;gt; How to crack software/re-engineer a binary: [Probably with the permission of the publisher] 

This should probably already be done in the debugging course. MIT has a subject where one of the debugging solutions is done via reverse engineering. 

But other than that, it's probably more SE than CS. 

&amp;gt; Alternative Language Survey: Yes, this is technically a standard class. But I&#8217;d like to see one on functional languages, Groovy, BF, or even creating your own domain specific languages be taught.

How did you get taught compilers and interpretors? 

&amp;gt; The goal of this class is to research market need, create something usable and getting their fellow peers involved in using it. 

Considering you already put marketing as a section. How about you teach the Interaction Design methodology instead? 

&amp;gt; Hands on Software Optimization class: Take an open source system, and optimize it. This class would teach formal procedures on how to optimize an existing application to perform as quickly as possible, monitor, and document the improvements.

This would probably be impossible to mark fairly. 

&amp;gt; He suggested that given the available of free CS courses (OpenCourseWare and the like) and experts at your fingertips (Blogs and Stackoverflow); wheres the value of a formal CS degree?

Also, your friend is a moron. Stop listening to him. 

CompSci isn't handled well by things like Stack Overflow, or blogs. It's much better for low level Business based Information Technology, or shitty SoftEng programs.  Lol, you live up to your name. :)

So as an overview... I wasn't trying to advise vocational training within a CS degree. [Although it might help]. I have no idea about AU [other than having interviewed over the phone once] Part of the problem in the US, as I'm sure you've seen, is the claim by employers that "new hires [college kids] are bad" I've come from 2 completely different schools. One [my undergraduate] addressed software engineering as separate "smaller"/more thorough classes. The other tried to lump everything into one class, pick an all-in-one software engineering book, and taught by a professor that went straight to teaching from post doc. Design patterns are enough for one class in themselves. 


**Debugging and testing:** 

I cannot for the life of me recall when everyone was taught how to debug. The only formal tutorial I had was for the GDB, because well its quite "special." I'm not suggesting a semester long tutorial on teaching how to set break points. I'm suggesting on techniques on how to debug across messages [JMS], catching and analyzing network traffic, techniques for decoding non-text output, how to catch and find memory leaks, debugging across containers and components, and how to debug across those fucking MFC message handling. 

**Computer Vision**

You're right. That is one of the fields I really liked. Something to the degree of "Computational Photography" would suffice as well. 

**APIs**

Well at least introduce the student to well designed and poorly designed APIs. There are books written on this.

**Open Source:**

This is one of your responses that kinda irks me and proves my [unstated] point. The reason why those contributions are so crap is that the students are unmotivated to submit quality things. If there is a grade depending on it, there is at least some desire to do well. 


**Marketing**

I kinda don't want to suggest that marketing should be taught to developers. We do a good job with the techincal related items. I think it would be a good idea to have a course with a focus on how to get your software out there. [And not annoy the shit out of your users]

**Automation**

All of the tools suggested within the program have to deal with this: Heres a hammer use that hammer to hammer things [The compiler/ide]. My suggestion is to teach students [later] how to turn the hammer into an industrial machine. [Build systems] After being a TA, you learn quite quickly that non-technical things cut at the ability to learn the raw material. [See my previous blog post about "I am a student what should I learn"]

**Hardware to Software**

The only time I've seen this remotely setup is for Electronic Engineer grads. I was send down the route of using a diagramming tool for logic circuits. It was fun, but it never pushed you to be hands on. Also, who here has written a device driver for windows or linux? I'm not convinced that there are many. 

**How Computer Science relates to [Field x, y or z]:**

Without googling:

*  Please name a library an open source library that I could transform a string mathmatical into an integral, and then solve it
*  Name a major bioinformatics framework, also do you know how to use it?
*  Name a protocol for submitting stock transactions to most NASDAQ brokers.

**Alternative Languages**

Most programming language courses stick with the classics. My suggestion was to teach some of the newer or unfamilar languages. COBOL Maybe? Or maybe this could be a look at how languages evolved. 


&amp;gt;The goal of this class is to research market need, create something usable and getting their fellow peers involved in using it.
&amp;gt; Considering you already put marketing as a section. How about you teach the Interaction Design methodology instead?

Busted... I was wondering if anyone was paying attention. I didn't suggest an HCI course, because that is already being taught. Instead I was wanting to get students to be able to understand the needs of the market. Heathcare needs electronic records? Who knew. [That is to combat the naivity of "I want to make video games" then only to find out... oh thats a saturated market]

**Optimization**

Eh, optimization is more than just loop unrolling. Its about profiling, documenting, finding less than optimal algorithms, and changing protocols to boost speed. **Sometimes its even about re-evaluating the design or use-case.** Its not impossible. The suggestion about optimizing an open source app was more of an end of class project in a group. [In a compeition] I believe that would reinforce: O notation, understanding of algorithms, exploit alternative (but not so obvious) algorithms, low level bit manipulation, getting rid of those annoying XML based web services (hessian for the win!) In a manner that Jeremy Clarkson would put it... reading the book and siting in the class is like driving a Honda Civic, its nice its safe, and it gets the job done... but its boring. Taking a reworld application and optimizing the hell out of it is like taking a 60s american muscle car and replacing the engine with one from the Bugatti. Well he didn't say that, and the analogy probably doesn't fit perfectly... but hey its Jeremy Clarksoneqse. &amp;gt; Lol, you live up to your name. :)

Hah, I wasn't even that angry. Except at your mate. 

I think almost all of your suggestions, in themselves have merit. It's just that a lot of them don't relate to the foundation of Computer Science. They are, predominately, subjects that should be found in the Software Engineering and (in Australia) Information Technology &amp;amp; Communication majors. 

That being said, I am a proponent of students taking cross electives. I think it's healthy for a CompSci major to take multiple SoftEng courses during their undergraduate (or at least sit in them and sponge knowledge). 

I'm also a fan of CV.   Dear god no.

CS is not vocational training.  Not everyone will be a college professor, and if you only cater to that crowd then your market will be tiny and out of touch with reality. Unless you propose to take the theoretical parts and secede to form some tiny niche for PhD-bound students, some vocational training is essential to anyone in the subject. Most people don't view college as a luxury vacation for learning things that are interesting, they view it as preparation for knowledge-based careers. There's no reason why a good curriculum can't have some balance, but time is limited so it's better to have somewhat more theory. *All* theory or *all* engineering are both bad strategies. Not everyone will be a college professor, and if you only cater to that crowd then your market will be tiny and out of touch with reality. Unless you propose to take the theoretical parts and secede to form some tiny niche for PhD-bound students, some vocational training is essential to anyone in the subject. Most people don't view college as a luxury vacation for learning things that are interesting, they view it as preparation for knowledge-based careers. There's no reason why a good curriculum can't have some balance, but time is limited so it's better to have somewhat more theory. *All* theory or *all* engineering are both bad strategies. I think it's funny that CS "purists" think vocational training is somehow "bad" - yet most college curricula are *entirely* aimed at producing a functional, vocational citizen (medicine (undergrad and graduate), law, education, civil engineering, marketing, etc).

I've graduated from a two-year college and a four-year university, and both had an optional-but-encouraged intern program because they recognized that pure CS theory is useless in the real world: you have to have honest-to-god practical application of the theory.  A lot of these (tools, designing APIs, debugging, automation, etc.) are good *concepts* that a comp sci major should know, but not necessarily good classes in of themselves. They can probably be integrated well into existing courses. For example, you can ensure that students are competent in using common tools if you require that students use git and ant for a team project. You don't need an entire class on that material though.  My school actually offers the majority of those of those courses, like other people have mentioned though, several of them are listed under software engineering      Interesting. Many classes at my uni have classes like that focus on these topics or include them as part of studies.

&amp;gt;He suggested that given the available of free CS courses (OpenCourseWare and the like) and experts at your fingertips (Blogs and Stackoverflow); wheres the value of a formal CS degree? 

This is a bit extreme though. It is a bit. However, with Florida dropping its CS program, some of the programs becoming basically overpriced book readers, or legal research institutions with slave labor. I can't blame him for the comment. 

EDIT: Yes I am the original author, I like and want for discussion on the posts.  It is a bit. However, with Florida dropping its CS program, some of the programs becoming basically overpriced book readers, or legal research institutions with slave labor. I can't blame him for the comment. 

EDIT: Yes I am the original author, I like and want for discussion on the posts.  The amount of college courses that are actually overpriced book readers angers me, compsci or not   Most of those offerings are more software engineering than real computer science courses. Not to be nit-picky, but most CS should be more theoretically geared, with applications - NLP, OSes, DB, Compilers, Algorithms, Networking, Crypto, Language Design, etc. But you are right, we should have those classes, for a S.E. specialization. I just don't think they should be part of a core curriculum like the aforementioned ones. The techniques taught in S.E. tend to be outdated, and are more focused on how to use tools than actually learning how things work and the concepts behind them, which will remain constant even after the technologies upgrade. </snippet></document><document><title>Functional Programming for the Rest of us</title><url>http://www.defmacro.org/ramblings/fp.html</url><snippet>  Great link. For those looking to get a more hands on approach to functional programming, try coding in Scheme (its a language similar to LISP for those who don't know). The book called "The Little Schemer" is great for learning it. Great link. For those looking to get a more hands on approach to functional programming, try coding in Scheme (its a language similar to LISP for those who don't know). The book called "The Little Schemer" is great for learning it. Yes! Scheme is a great way to learn FP. I'm pretty they changed the name of the language to "Racket" now though. There are many different "versions" of scheme,  racket is just the most popular. Its still scheme at the core, the company that releases it just wanted to rename it I guess :D       I hated functional programming so much (Scheme) that I almost quit my computer science degree because of it. I managed to do good on my tests, so now it's just a bad memory.

Parenthesis, THEY ARE EVERYWHERE !!!! ARRRRGH!  This article is terrible and the author is asserting some glaring mistakes here.  Erlang is far from an "academic" language.  It was designed by Ericsson to power telecom switches which need to be robust and "industrial strength" (as he puts it).


 </snippet></document><document><title>Are there structures that cannot be encoded using graphs?</title><url>http://www.reddit.com/r/compsci/comments/sr6or/are_there_structures_that_cannot_be_encoded_using/</url><snippet>Lists: graphs

Arrays: lists, hence graphs

Trees: DAGs, hence graphs

Hashmaps: h() mapping objects to buckets, hence graphs  This is kind of a sidestepping answer, but when you step out of the realm of the discrete, I wouldn't guess those structures can be represented fully as graphs.

Otherwise, without being too profound, a graph may be a somewhat limiting term. If you mean "finite graph" then there are some things which are inherently infinite and therefore can't be represented by a finite graph. Also, hypergraphs are generalizations of graph where edges can join more than two vertices. Some structures (such as statecharts) can't be represented by simple graphs. Although you can "hack" it to be very graph like (e.g., treat the edge like a typed vertex in your graph, suddenly many-to-many edges).

EDIT: So I know statecharts can be flattened into an FSA which is basically a decorated graph. The more I think about it, the more it sounds like a lot of things can be represented by a graph. When you start thinking of tricky things like UML Class Diagrams or something like that, you might have to do something that isn't so 'graph-like' and maybe not allowable by your definition of 'graph' by decorating edges or vertices or something.

tl;dr: I think we need to firm up our definition of graph because I think I'm speaking more from a SE POV than theoretical CS.  How would you encode the list or array [1, 2, 1, 3]? node (weight = 1) - node (weight = 2) - node (weight = 1) - node (weight = 3) Do you use values on the edge weights?

I guess you encode anything in graphs then, if you use graphs similar to [RDF](https://secure.wikimedia.org/wikipedia/en/wiki/Resource_Description_Framework) triples. </snippet></document><document><title>Are let-expressions 'bad code smell' for functional programming?</title><url>http://www.reddit.com/r/compsci/comments/splwm/are_letexpressions_bad_code_smell_for_functional/</url><snippet>A friend and I were talking about functional programming and he mentioned that 'let expressions' should be avoided and that their use indicates a tendency towards imperative programming being shoe-horned in to a functional environment. I, personally, think they are perfectly fine and a natural tool in a functional programmer's toolbox - the crux of my argument being that they are easily defined as lambda expressions, and thus are just an abstraction to lambda functions.

I was wondering what other people feel about the situation.

It might be interesting to note that I come from a Lisp-ish FP background, whereas he comes from a Haskell background. Although in all fairness I should also note that he knows Lisp fairly well, whereas I still haven't learned Haskell. Really need to do that.   There's nothing wrong with `let`, although it does tend to be verbose in Lisp-like languages. I much prefer internal definitions. e.g. the `helper` in

    (define (map f lst)
      (define (helper lst acc)
        (if (null? lst)
            (reverse acc)
            (helper (cdr lst) (cons (f (car lst)) acc))))
      (helper lst '()))

and `where` is nice in Haskell. I'm not entirely sure and I can't test it right now due to a stupidly broken package, but doesn't `define` automatically import the definition to the global scope? That would be fairly different than `let`, if that's the case. But maybe I fundamentally misunderstand what `define` does. Nope. Internally defined names are only bound locally inside the `lambda`, `let`, etc at least for Scheme. This is specified in the standard for Scheme too: see relevant section of [R6RS](http://www.r6rs.org/final/html/r6rs/r6rs-Z-H-2.html#node_toc_node_sec_11.3). Some implementations also give you internal definitions in even more places than specified in the standard (which is pretty useful).

Proof:

    $ racket
    Welcome to Racket v5.2.900.2.
    -&amp;gt; (define (map f lst)
         (define (helper lst acc)
           (if (null? lst)
               (reverse acc)
               (helper (cdr lst) (cons (f (car lst)) acc))))
         (helper lst '()))
    -&amp;gt; helper
    ; reference to undefined identifier: helper [,bt for context]
 There's nothing wrong with `let`, although it does tend to be verbose in Lisp-like languages. I much prefer internal definitions. e.g. the `helper` in

    (define (map f lst)
      (define (helper lst acc)
        (if (null? lst)
            (reverse acc)
            (helper (cdr lst) (cons (f (car lst)) acc))))
      (helper lst '()))

and `where` is nice in Haskell. I haven't programmed in LISP before, but is there any reason why you'd use a tail recursive definition like this, instead of something like&#8230; (let's see if I can write my first lisp ever&#8230;):

    (define (map f lst)
        (if (null? lst)
            ()
            (cons (f (car lst)) 
                  (map f (cdr lst))
            )
        )
    )

(sorry if the indentation style annoys anyone, but it makes more sense to me)

Anyway, my thoughts on using let in haskell is that you should use it if it makes something clearer. Got a complex subecpression? Give it a meaningful name. Also there are times when it is more efficient to name an expression so it's only evaluated once, instead of repeating it and having it execute several times. In the first year course I tutor at a university, I try to encourage people to give names to subexpressions if it makes it clearer, because code should be self documenting, to some degree at least.

  Let it be. It's documentation.   First, not computer science.

Second, in the non-recursive case, let can be seen as sugar over lambda

    let x = exp1 in exp2

is equivalent to

    (\x -&amp;gt; exp2) exp1 Side comment: I learned recently that this equivalence doesn't hold for dependently typed languages. With `let x = e1 : T in e2` the typing environment for e2 knows that `x` is convertible (aka definitionally equivalent) to `e1` while with `(&#955;x:T. e2) e1`, the typing environment for `e2` only knows `x` of type `T`.  So some let expressions cannot be converted into lambda expressions. First, not computer science.

Second, in the non-recursive case, let can be seen as sugar over lambda

    let x = exp1 in exp2

is equivalent to

    (\x -&amp;gt; exp2) exp1 First, not computer science.

Second, in the non-recursive case, let can be seen as sugar over lambda

    let x = exp1 in exp2

is equivalent to

    (\x -&amp;gt; exp2) exp1 Thank you for your very helpful post! Two questions:

1. Which subreddit would you expect something like this to be posted in? I thought that /r/compsci would be a suitable place to talk about functional programming constructs and whether or not a given expression is functionally 'safe' or not.
2. As to your second point, if you read my post, you'll see that I actually knew that already, but thanks for the example! I wonder if `let*` and `letrec`, available in Scheme and I assume in other functional languages as well, are also macro rewrites for lambda expressions? I don't know what let* does, but letrec is also sugar for a lambda expression. It is harder to explain than the one for "let" since it involves the fixed point operator, so you will just have to trust me.  Yup, I'm familiar with the basics of the idea (assuming you mean 'fixed point combinator') - good to know!

Thanks. Thank you for your very helpful post! Two questions:

1. Which subreddit would you expect something like this to be posted in? I thought that /r/compsci would be a suitable place to talk about functional programming constructs and whether or not a given expression is functionally 'safe' or not.
2. As to your second point, if you read my post, you'll see that I actually knew that already, but thanks for the example! I wonder if `let*` and `letrec`, available in Scheme and I assume in other functional languages as well, are also macro rewrites for lambda expressions?</snippet></document><document><title>MIT's Nancy Lynch Named A.C.M. "Athena Lecturer" for Advances in Distributed Systems that Enable Dependable Internet and Wireless Network Applications
</title><url>http://www.acm.org/press-room/news-releases/2012/athena-award-2012</url><snippet /></document><document><title>How many of you went on to get a graduate degree in CS without a CS undergraduate degree?</title><url>http://www.reddit.com/r/compsci/comments/so8j3/how_many_of_you_went_on_to_get_a_graduate_degree/</url><snippet>Hey guys,

I'm graduating with a humanities B.A. I just wanted to see if there were people here that managed to get into a CS program for graduate school (who didn't get a CS degree in undergrad) and what kind of a journey they took to get there.

I guess I'm looking to hear inspiring stories.

Thanks for anybody willing to share!  I did. I have a Master's degree in CS which I pursued immediately after my Bachelor's degree in music. And now I'm working on my PhD in CS.  
  
My university just had me take some "leveling" courses before I was allowed to start the program. It was about a year's worth of classes, and that's including the prerequisites to the leveling courses. I did. I have a Master's degree in CS which I pursued immediately after my Bachelor's degree in music. And now I'm working on my PhD in CS.  
  
My university just had me take some "leveling" courses before I was allowed to start the program. It was about a year's worth of classes, and that's including the prerequisites to the leveling courses. That's excellent! Does that particular university allow other humanities students to take that route or were you a special case? I know another guy who's starting a Master's degree in CS after a Bachelor's in business. Seems like they were less willing to cooperate with him, for one reason or another.  
  
For example, after I applied, they gave me an official acceptance letter stating that I was accepted pending the completion of the leveling courses. For him, they didn't accept him to the program. They told him he had to take the leveling courses and then apply. I'm not sure if it was a change in policy, or some other reason. I did. I have a Master's degree in CS which I pursued immediately after my Bachelor's degree in music. And now I'm working on my PhD in CS.  
  
My university just had me take some "leveling" courses before I was allowed to start the program. It was about a year's worth of classes, and that's including the prerequisites to the leveling courses.   I'm currently a CS graduate at the University of Milwaukee - Wisconsin. I received a BFA in Film and a CS minor from the same university.

They had no problem admitting me, but the department has an "undergraduate requirements" section (which includes classes like Discrete Mathematics, Operating Systems, Computer Architecture, Programming Language Concepts, Algorithms, etc). If you hadn't taken those with a minor or an equivalent from a different university, you had three semesters during your graduate career to fulfill them. I thought I was the only CS graduate student with a BA in film.

o/  Biology BS here.  Started my Masters in CS about five years later, after doing various things I thought would be fun (like living abroad for two years).  Didn't do it for the career -- I did it because I wanted to learn about computer stuff.

Turns out that I learned a lot about computers, but didn't really learn how to program until I got my first job.  But you'll hear that a lot.

There were a few engineering-related undergrad requirements I had to fulfill, but since I'd done most of the math for the Bio degree, thankfully I didn't have to repeat those.

I'm still looking for an interesting way to link my life sciences background with the computer stuff.  Gene sequencing?  Genetic algorithms?  Any ideas? &amp;gt; I'm still looking for an interesting way to link my life sciences background with the computer stuff.

Shouldn't be tough.  Bioinformatics is so hot right now. Biology BS here.  Started my Masters in CS about five years later, after doing various things I thought would be fun (like living abroad for two years).  Didn't do it for the career -- I did it because I wanted to learn about computer stuff.

Turns out that I learned a lot about computers, but didn't really learn how to program until I got my first job.  But you'll hear that a lot.

There were a few engineering-related undergrad requirements I had to fulfill, but since I'd done most of the math for the Bio degree, thankfully I didn't have to repeat those.

I'm still looking for an interesting way to link my life sciences background with the computer stuff.  Gene sequencing?  Genetic algorithms?  Any ideas? interesting. i am a biology/chemistry undergrad as well. i think itd be cool to get some cs background too. i have an informal knowledge of programming but id love to take some more classes on it too (and I am starting next sem). bioinformatics seems like a cool synthesis of those two ideas (im premed so i think its a really fascinating combination). Looking at job listings for bioinformatics, it would seem that if you have a strong background in Bio (expecially Molecular Bio) then just pick up a working knowledge of databases and SQL (which isn't that hard). 

Either that or pick up a Ph.D. in Bioinformatics.  Doesn't say how much they pay, though ... do you mind linking to where  you saw some? i'm interested in looking at the requirements if its not too much trouble do you mind linking to where  you saw some? i'm interested in looking at the requirements if its not too much trouble     Just completed my MS in CS. A fellow classmate that I was in several courses with had an undergraduate degree in psychology. So, it's definitely doable. Due to not much of a technical background, some remedial courses may need to be taken in order to get you up to speed on the relevant areas (operating system, programming languages, discrete mathematics, algorithm analysis and design, etc).       Hi guys,

I guess I'm in a similar boat.. i just did my undergrad in economics and came across [this course.](http://www-typo3.cs.ucl.ac.uk/admissions/msc_computer_science/)

Any thoughts would be appreciated on how good this course may be.. 

Thanks. Hey man, keep me updated on your progress in CS. I'm in the same position. am applying to courses in UK, you in US ?     </snippet></document><document><title>Computer Science PostDocs</title><url>http://www.todaysengineer.org/2012/Apr/career-focus.asp</url><snippet>  I am graduating this Summer (PhD) from a US university (top 20 CS department) and planning to take a post doc in Europe in Fall. Based on my experience, it seems that post docs are the most accessible way to enter the European research job market (if you don't know people there). I want to get additional experience and more publications to apply for faculty position. What are your experiences and opinions about taking a post doc?  Also, any comments/suggestions about moving from US to Europe are welcome.    I am graduating this Summer (PhD) from a US university (top 20 CS department) and planning to take a post doc in Europe in Fall. Based on my experience, it seems that post docs are the most accessible way to enter the European research job market (if you don't know people there). I want to get additional experience and more publications to apply for faculty position. What are your experiences and opinions about taking a post doc?  Also, any comments/suggestions about moving from US to Europe are welcome.   </snippet></document><document><title>Math, for programmers.</title><url>http://www.reddit.com/r/compsci/comments/sn3q2/math_for_programmers/</url><snippet>I have already posted this on AskReddit with no response, this seems like a better place to post it. I just downloaded a huge array of mathematics books with every branch of math included. However I am feeling a bit overwhelmed about which order to read them in. I'm interested in math to be a better programmer, so in which order and what type of books should I be reading. I've already covered the high school math courses and calculus.  You may need to ask a more detailed question.  As it stands, I'm reminded of this conversation from 'Alice in Wonderland':

`Would you tell me, please, which way I ought to go from here?'
 Alice speaks to Cheshire Cat
`That depends a good deal on where you want to get to,' said the Cat.
`I don't much care where--' said Alice.
`Then it doesn't matter which way you go,' said the Cat.
`--so long as I get somewhere,' Alice added as an explanation.
`Oh, you're sure to do that,' said the Cat, `if you only walk long enough.'

 "Except Actionscript alley. Don't bother going down there." "Except Actionscript alley. Don't bother going down there." You may need to ask a more detailed question.  As it stands, I'm reminded of this conversation from 'Alice in Wonderland':

`Would you tell me, please, which way I ought to go from here?'
 Alice speaks to Cheshire Cat
`That depends a good deal on where you want to get to,' said the Cat.
`I don't much care where--' said Alice.
`Then it doesn't matter which way you go,' said the Cat.
`--so long as I get somewhere,' Alice added as an explanation.
`Oh, you're sure to do that,' said the Cat, `if you only walk long enough.'

 You may need to ask a more detailed question.  As it stands, I'm reminded of this conversation from 'Alice in Wonderland':

`Would you tell me, please, which way I ought to go from here?'
 Alice speaks to Cheshire Cat
`That depends a good deal on where you want to get to,' said the Cat.
`I don't much care where--' said Alice.
`Then it doesn't matter which way you go,' said the Cat.
`--so long as I get somewhere,' Alice added as an explanation.
`Oh, you're sure to do that,' said the Cat, `if you only walk long enough.'

  If you're really starting out with a high school understanding of math then absolutely the first thing you need to master is discrete math. That's a prerequisite for designing and analyzing algorithms, which is in my opinion pretty much the obvious #1 use for math in programming. Here's what you need to know before tackling an intro algorithms course:

* [Propositional logic](https://en.wikipedia.org/wiki/Sentential_logic)
* Basic [predicate logic](https://en.wikipedia.org/wiki/First-order_logic) 
* Proofs by [contradiction](https://en.wikipedia.org/wiki/Proof_by_contradiction), [case analysis](https://en.wikipedia.org/wiki/Disjunction_elimination), [induction](https://en.wikipedia.org/wiki/Mathematical_induction), etc
* Solving [recurrences](https://en.wikipedia.org/wiki/Recurrence_relation)
* Get comfortable with [these properties of summation](https://en.wikipedia.org/wiki/Summation#Identities)
* [Naive set theory](https://en.wikipedia.org/wiki/Naive_set_theory)
* Basic [graph theory](https://en.wikipedia.org/wiki/Graph_theory), especially trees, inductive proofs on trees, graph traversals Linear Algebra! Not entirely compsci related but important if you want to do anything with computer graphics.  As a CS major college student 2 weeks away from finishing a linear algebra course and who was told the same thing you just said, please explain how exactly Lin. Al. deals with graphics because I just don't see it. As a CS major college student 2 weeks away from finishing a linear algebra course and who was told the same thing you just said, please explain how exactly Lin. Al. deals with graphics because I just don't see it. Well, let's see. Think of a 3d model. It is defined by it's coordinates relative to some center. This is model space. But it also has a position in the 3d world with a different origin. This is world space. Now on the screen, it has different coordinates as well. These depend on where the camera is and what direction it's facing. This is View Space. 

Now, starting with model space, you put those coordinates into a nice little matrix and you're wondering, hmm, wouldn't it be great if this model could be rotated by about 10 degrees. How do you think they know which are the new coordinates? They apply a transformation by multiplication with a matrix that contains that 10 degrees in there somewhere and guess what, the resultant matrix contains the new coordinates! Similarly, you just keep on applying transformations (just matrix multiplication) and these coordinates you entered into a matrix change so that they reflect the model in world space and then view space until you ultimately have 2d coordinates that specify where exactly on the screen that coordinate of the original model should display. 

Obviously, there's more stuff than just linear transformations but that's a crucial part of it. To be able to understand how exactly your graphics library turns that model into screen coordinates is pretty useful and necessary if you want to ever dig deeper. 

Edit: Yeah that long explanation was unnecessary. I'm just not too good with words. hah. tel's answer is nice and concise.  Well, let's see. Think of a 3d model. It is defined by it's coordinates relative to some center. This is model space. But it also has a position in the 3d world with a different origin. This is world space. Now on the screen, it has different coordinates as well. These depend on where the camera is and what direction it's facing. This is View Space. 

Now, starting with model space, you put those coordinates into a nice little matrix and you're wondering, hmm, wouldn't it be great if this model could be rotated by about 10 degrees. How do you think they know which are the new coordinates? They apply a transformation by multiplication with a matrix that contains that 10 degrees in there somewhere and guess what, the resultant matrix contains the new coordinates! Similarly, you just keep on applying transformations (just matrix multiplication) and these coordinates you entered into a matrix change so that they reflect the model in world space and then view space until you ultimately have 2d coordinates that specify where exactly on the screen that coordinate of the original model should display. 

Obviously, there's more stuff than just linear transformations but that's a crucial part of it. To be able to understand how exactly your graphics library turns that model into screen coordinates is pretty useful and necessary if you want to ever dig deeper. 

Edit: Yeah that long explanation was unnecessary. I'm just not too good with words. hah. tel's answer is nice and concise.  As a CS major college student 2 weeks away from finishing a linear algebra course and who was told the same thing you just said, please explain how exactly Lin. Al. deals with graphics because I just don't see it. As a CS major college student 2 weeks away from finishing a linear algebra course and who was told the same thing you just said, please explain how exactly Lin. Al. deals with graphics because I just don't see it. As a CS major college student 2 weeks away from finishing a linear algebra course and who was told the same thing you just said, please explain how exactly Lin. Al. deals with graphics because I just don't see it. As a CS major college student 2 weeks away from finishing a linear algebra course and who was told the same thing you just said, please explain how exactly Lin. Al. deals with graphics because I just don't see it. Matrix Math is pretty much the entirety of 3 dimensional graphics.  Fire up [OpenGL](http://www.glprogramming.com/red/) and play awhile.    As a CS major college student 2 weeks away from finishing a linear algebra course and who was told the same thing you just said, please explain how exactly Lin. Al. deals with graphics because I just don't see it. Then I would say that your professor failed you as a student, or you failed your professor. Well, it was an intro to lin. al. course, partly modeled by Strang's. The professor is good, its probably because the course was strictly mathematical and no applications were ever discussed. Funny, I think mine was based off of Strang as well(not quite certain, I just recall that name. It may have been my matrix theory class that was based on it), and was entirely mathematical. I left the class mind blown and seeing everything as vectors and matrices for months. I literally couldn't look at the world without seeing math. That's awesome, I'm so jealous! I'm starting to get it a bit (and maybe enjoy it a bit too) but no lightbulb moment such as that yet =\ Well, it was an intro to lin. al. course, partly modeled by Strang's. The professor is good, its probably because the course was strictly mathematical and no applications were ever discussed. I'm in a Lin. Al. Class now, and my prof is very involved in computer science, so we spent thirty minutes looking at the applications for programming and graphics. It is actually surprisingly simple and cool! I envy you my friend, I truly do.  Linear Algebra! Not entirely compsci related but important if you want to do anything with computer graphics.  Linear Algebra! Not entirely compsci related but important if you want to do anything with computer graphics.  Linear Algebra! Not entirely compsci related but important if you want to do anything with computer graphics.  Linear Algebra! Not entirely compsci related but important if you want to do anything with computer graphics.  If you're really starting out with a high school understanding of math then absolutely the first thing you need to master is discrete math. That's a prerequisite for designing and analyzing algorithms, which is in my opinion pretty much the obvious #1 use for math in programming. Here's what you need to know before tackling an intro algorithms course:

* [Propositional logic](https://en.wikipedia.org/wiki/Sentential_logic)
* Basic [predicate logic](https://en.wikipedia.org/wiki/First-order_logic) 
* Proofs by [contradiction](https://en.wikipedia.org/wiki/Proof_by_contradiction), [case analysis](https://en.wikipedia.org/wiki/Disjunction_elimination), [induction](https://en.wikipedia.org/wiki/Mathematical_induction), etc
* Solving [recurrences](https://en.wikipedia.org/wiki/Recurrence_relation)
* Get comfortable with [these properties of summation](https://en.wikipedia.org/wiki/Summation#Identities)
* [Naive set theory](https://en.wikipedia.org/wiki/Naive_set_theory)
* Basic [graph theory](https://en.wikipedia.org/wiki/Graph_theory), especially trees, inductive proofs on trees, graph traversals You have stroked the right cord...

These are must basics for Programmer... Unless you program professionally. In which case you'll maybe use one of these at best. 

Not to say they aren't worth learning for their own sake, of course.  Unless you program professionally. In which case you'll maybe use one of these at best. 

Not to say they aren't worth learning for their own sake, of course.  Are you sure about that?

You can use de Morgan's laws to simplify complicated conditionals. Predicate logic is useful for expressing invariants in comments and annotations. A solid inductive understanding of trees and subtrees is required to design linked data structures, and depth/breadth first search is useful for searching through certain types of data structures. Solving recurrences and knowing summation properties is useful for back-of-the-envelope analysis of the runtime of an algorithm. &amp;gt;Are you sure about that?
You can use de Morgan's laws to simplify complicated conditionals. Predicate logic is useful for expressing invariants in comments and annotations. A solid inductive understanding of trees and subtrees is required to design linked data structures, and depth/breadth first search is useful for searching through certain types of data structures. Solving recurrences and knowing summation properties is useful for back-of-the-envelope analysis of the runtime of an algorithm.


Yes yes, it's all very useful in theory. Except in practice you're mostly just glueing functions together. For the most part, the general rule of thumb is "if you're writing something with more than two nested for loops, you should be using a function someone else wrote." Well, there's programming, and there's plumbing.  Someone needs to write the compilers. Less than 0.01% of programmers will be writing a compiler professionally. Again, it's unlikely that most programmers will need a significant portion of the skills listed.  I hate to reply in an already dead thread, and I mean no offense by this, but it's bugging me: I just can't think of a programming domain where a solid math background is not useful.  And I'm saying this as someone without an academic background in math, but where the necessity of learning math has come from practical experience.  Aside from compilers (which are relevant far beyond the simple case of general purpose programming languages), I've worked with databases and ended up applying set theory and graph theory heavily; image and audio processing almost go without saying, except that it's more than just linear algebra -- there was some fascinating math behind correcting shaky video frames, detecting "aesthetically pleasing" photos, and detecting musical repeats to optimize music playback routines; videogames and simulations, not just linear algebra -- group theory comes up when thinking about symmetry, machine learning, game theory comes up when thinking about AI, being able to write a proof came in handy when optimizing a gameplay-specific range query, constructing spaces with Voroni diagrams, calculus for rigid body simulation and other stuff; finance, plenty of stats and calculus, but also error estimation (also useful in game physics); cryptography, but that's a niche thing, obviously most people shouldn't roll their own.  Web stuff, Bayesian stats, figuring out formulas for point systems, proofs about concurrency.  I worked on screenwriting software once where there ended up being some hairy graph theory behind supporting concurrent editing and structured versioning.  I think the only thing I've worked on that hasn't had some math directly applicable would be device drivers, and there I would still argue that the austerity of mathematical thought is useful (certainly basic Boolean logic was still essential).  Not to mention profiling and benchmarking, which is essential pretty much everywhere, and where a solid grasp of stats is essential (http://zedshaw.com/essays/programmer_stats.html).  Constructing good test cases (especially &#224; la QuickCheck) requires thinking about the domains and ranges of functions.  And there's still a fuckton of math I want to learn about that I'm sure is going to further inform my work in the future.

On the other hand, I worked at a place where central functions of the main commercial asset could be said to completely revolve around queuing theory, and very few people there bothered to learn anything about it or apply it to the software at hand.  But the software sucked and so did most of the programmers.

There are many libraries, yes, but there is always novel software to write.  If something isn't novel or an improvement on what exists, why write it?  I find it hard to imagine anyone working in the field for more than a decade without having desperate need to apply some of these ideas.  Most of my colleagues who, like me, didn't study formally, have all been driven to study math independently for this reason.  http://courses.csail.mit.edu/6.042/fall10/mcs-ftl.pdf Here is a very recent version (2012-04-20 vs 2010-09-08):

http://courses.csail.mit.edu/6.042/spring12/mcsfull.pdf http://courses.csail.mit.edu/6.042/fall10/mcs-ftl.pdf  Linear algebra, statistics, probability (including combinatorics) and set theory, in no particular order, are the ones I can think about now. Linear algebra, statistics, probability (including combinatorics) and set theory, in no particular order, are the ones I can think about now. And graph theory can be useful for algorithms later on Linear algebra, statistics, probability (including combinatorics) and set theory, in no particular order, are the ones I can think about now. What are some applications of linear algebra and statistics? Those don't seem relevant to day-to-day software development. What are some applications of linear algebra and statistics? Those don't seem relevant to day-to-day software development. What are some applications of linear algebra and statistics? Those don't seem relevant to day-to-day software development. What are some applications of linear algebra and statistics? Those don't seem relevant to day-to-day software development. Linear algebra, statistics, probability (including combinatorics) and set theory, in no particular order, are the ones I can think about now. Linear algebra, statistics, probability (including combinatorics) and set theory, in no particular order, are the ones I can think about now. You people always leave out calculus... Problem is, the fields I love (Computer Animation, Graphics, Vision, Machine Learning) are flooded with it. That's because calculus is concerned with whatsitsface the thing where you have continuous possible values, where most of the math programmers are concerned with is discrete. It's different in specialised fields of programming, of course, like graphics, sound and stuff.

Edit: Can you tell I just woke up? Because I did. Linear algebra, statistics, probability (including combinatorics) and set theory, in no particular order, are the ones I can think about now. I concur with this basic list.  The basic elements of calculus are assumed I suppose?  I'm not sure what calculus means exactly. I assume basic knowledge of equation solving (like exponentials, quadratic and stuff, but not differential) and a very intuitive grasp of differentials and integrals in general, (especially enough to be able to use Newton-Raphson) but nothing detailed. I'm not sure what calculus means exactly. I assume basic knowledge of equation solving (like exponentials, quadratic and stuff, but not differential) and a very intuitive grasp of differentials and integrals in general, (especially enough to be able to use Newton-Raphson) but nothing detailed. Basically, functions in calculus are not unlike functions in a program.  Similarly, summation is a lot like looping.  There's lambda calculus which I guess would map to recursion, but that's not really in the 101 curriculum. You may become intimately familiar with other terms (logs, factorials, etc) that overlap with things you'd learn in CS.  Other calculus techniques you might apply algorithmically to address specific challenges--someone elsewhere mentioned graphics (or anything dealing with 2/3D geometry)--but I was speaking more broadly. However, set theory should convey the concept of functions a lot clearer than calculus does. Summation was not in calculus when I learned it, but in discrete mathematics. Lambda calculus is not related to the calculus you normally refer to when talking about calculus. Factorials were in combinatorics for me, and not calculus. I agree with logs (and different function types) and the specific challenges, though, but those are the only ones.     Logic, Statistics, Set Theory, Graph Theory. Mostly in that order of importance. Some others that are fun(if you're into that stuff) are Information Theory and Game Theory. Logic, Statistics, Set Theory, Graph Theory. Mostly in that order of importance. Some others that are fun(if you're into that stuff) are Information Theory and Game Theory.   Statistics is probably one of the most relevant fields you should study, it has very widespread application, as well as granting you a better understanding of quite a lot of things people take for granted. Probabilistic models are the best available solution to about half of the problems out there. The interesting and difficult half.

Stats doesn't quite teach you probabilistic methods any more than calculus teaches you numerical methods, but it's background that anyone who's trying to solve problems by programming should have. Statistics is probably one of the most relevant fields you should study, it has very widespread application, as well as granting you a better understanding of quite a lot of things people take for granted.   Although it's a real pain in the ass Linear Algebra can be helpful.   I hope this doesn't get buried, but probably will since I'm showing up a little late to the party.. But for anyone who can answer this question for me I would be very thankful. Alright so I am currently a junior in high school and am in the process of choosing my courses for next year. I plan on majoring in CS in college because I love everything there is in the subject. But anyways I was wondering which class would be the most beneficial for myself if I plan on majoring in CS, AP calculus or AP stats? Thanks I want to say both, but Calculus seems to be the biggest weeder course in CS.

On a related note, I've seen a lot of otherwise smart and capable people fail calculus. Not a single one of them was good at all of the following:

**Algebra**

**Fractions**

**Trigonometry**

Get these skills up to par and Calculus should be completely doable. Without one or more of them, you'll spend a lot of time struggling with arithmetic when you should be learning higher concepts. I want to say both, but Calculus seems to be the biggest weeder course in CS.

On a related note, I've seen a lot of otherwise smart and capable people fail calculus. Not a single one of them was good at all of the following:

**Algebra**

**Fractions**

**Trigonometry**

Get these skills up to par and Calculus should be completely doable. Without one or more of them, you'll spend a lot of time struggling with arithmetic when you should be learning higher concepts. I hope this doesn't get buried, but probably will since I'm showing up a little late to the party.. But for anyone who can answer this question for me I would be very thankful. Alright so I am currently a junior in high school and am in the process of choosing my courses for next year. I plan on majoring in CS in college because I love everything there is in the subject. But anyways I was wondering which class would be the most beneficial for myself if I plan on majoring in CS, AP calculus or AP stats? Thanks AP Calculus is a much more rewarding class. Neither is necessarily the best choice, but calculus will do more to open your mind and challenge you. BC especially. I hope this doesn't get buried, but probably will since I'm showing up a little late to the party.. But for anyone who can answer this question for me I would be very thankful. Alright so I am currently a junior in high school and am in the process of choosing my courses for next year. I plan on majoring in CS in college because I love everything there is in the subject. But anyways I was wondering which class would be the most beneficial for myself if I plan on majoring in CS, AP calculus or AP stats? Thanks You can get credit for Calc 1 and possibly Calc 2 with AP Calculus.  And, you will absolutely need to take Calc 1 so you may as well get it out of the way in high school.  

You may or may not be able to get credit for the required statistics course with AP Stats, since the level of calculus needed as a prerequisite to that course would be at least the BC level.

You should check the AP Credit mappings of the school(s) you plan to apply to since this will be public information.  For example, a 3 on the AP Calc AB may give you Calc 1 credit but a 5 on the BC may give you Calc 1 and 2.

tldr probably AP Calc since it's pretty much guaranteed required college credit.   [Foundations of Computer Science](http://infolab.stanford.edu/~ullman/focs.html) by Ullman &amp;amp; Aho. It resembles a discrete mathematics book, and covers theory of computation as well as abstract mathematical concepts/structures that pervade computer science.

People will suggest them, but avoid any books by Knuth (Concrete Mathematics, TAOCP), they're really intended for an advanced audience such as graduate students and/or researchers; they're not really good for introducing topics. People that don't read or use them will recommend them, perhaps due to their novelty, but try to stick with books that are actually useful for teaching introductory topics.  [Concrete Mathematics](http://en.wikipedia.org/wiki/Concrete_Mathematics) [found pdf online](http://www.matematica.net/portal/e-books/Graham%20-%20Knuth%20-%20Patashnik%20-%20%20Concrete%20Mathematics.pdf)  Discrete mathematics and its Applications by Kenneth Rosen. The definitive text in introductory discrete Math. [deleted] What is so bad about this book?  It gives a pretty good introduction to many of the major concepts, including logic, induction, combinatorics, recursion relations, set theory, graph theory, etc. [deleted] [deleted]          to be a better programmer, you'd better program a lot. 

studying math to be a better programmer is like studying music theory to be a better violinist. while theory is really important, practicing violin has much more impact in building virtuosity.  to be a better programmer, you'd better program a lot. 

studying math to be a better programmer is like studying music theory to be a better violinist. while theory is really important, practicing violin has much more impact in building virtuosity.  to be a better programmer, you'd better program a lot. 

studying math to be a better programmer is like studying music theory to be a better violinist. while theory is really important, practicing violin has much more impact in building virtuosity.  to be a better programmer, you'd better program a lot. 

studying math to be a better programmer is like studying music theory to be a better violinist. while theory is really important, practicing violin has much more impact in building virtuosity. </snippet></document><document><title>Lower bound for TSP when extending set of cities by one</title><url>http://www.reddit.com/r/compsci/comments/smyqz/lower_bound_for_tsp_when_extending_set_of_cities/</url><snippet>I'm curious if there's any kind of interesting lower bound for the optimal path in a TSP when you add a city to an already solved TSP. A trivial lower bound would be the length of the optimal path prior to adding the city. Without taking into consideration any other factors I'm pretty sure that's the best you can get. However, if we take into account the edge lengths from every other city to the new city, can we construct a better lower bound?

How about if we assume a Euclidean-metric?  &amp;gt; A trivial lower bound would be the length of the optimal path prior to adding the city

This bound doesn't hold for costs that violate the triangle inequality.

Consider adding a city that has 0 transportation costs to any other city. The shortest circuit will be smaller than the previous optimal circuit minus the cost of the largest edge.

It does hold for metrics though. </snippet></document><document><title>Which are the worthwhile courses from Coursera?</title><url>http://www.reddit.com/r/compsci/comments/sm0ks/which_are_the_worthwhile_courses_from_coursera/</url><snippet>I'm signed up for a few of them that start in a couple days. Just wondering if they are going to be interesting. I have a little bit of programming background, but haven't done anything for a year or two. So I decided to take CS 101, Intro to Logic, Machine Learning and Automata. Thoughts on any or all of these?

Edit: I've started on a couple of them and the formatting is awesome. Feels like it's better structured than when I attended classes at university. I would definitely recommend these if someone wants to learn about computers. Or anything if Coursera offers it. I've also started a couple from Udacity, they are just as great. All of the material is easily found if you have the slightest knowledge of the internet. And if you don't then why are you on Reddit?

Also, with the Udacity classes, at least 212, it wouldn't be a bad idea if you try to read into Python a little before you start. I know a lot of C++ and I was getting confused because they weren't initializing any variables before they used them. Damn modern languages.  If you haven't studied algorithms and discrete Maths before, automata is probably not the right course for you. I didn't study the subject until the third year of my CS degree (not formally anyway; we touched on it where it linked to other areas), and still found it pretty tricky.

Machine Learning is a great course (I took it last time it was offered) - it does a great job of introducing the theory without stressing about the details too much, and it really gives a great overview of how to use ML in practice.  If I've learned a lot of C++ do you think it will be too bad? Assuming I'm willing to put in a lot of time and effort because these online classes are pretty much the only thing I do other than walk my dog. If you're willing to put the time and effort in, I'm sure you'll do fine. The course website gives a good overview of the knowledge you are expected to have, so you can go learn/review that before starting:

&amp;gt;You should have had a second course in Computer Science &#8212; one that covers basic data structures (e.g., lists, trees, hashing), and basic algorithms (e.g., tree traversals, recursive programming, big-oh running time). In addition, a course in discrete mathematics covering propositional logic, graphs, and inductive proofs is valuable background.

&amp;gt;If you need to review or learn some of these topics, there is a free on-line textbook Foundations of Computer Science, written by Al Aho and me, available at http://i.stanford.edu/~ullman/focs.html. Recommended chapters include 2 (Recursion and Induction), 3 (Running Time of Programs), 5 (Trees), 6 (Lists), 7 (Sets), 9 (Graphs), and 12 (Propositional Logic). You will also find introductions to finite automata, regular expressions, and context-free grammars in Chapters 10 and 11. Reading Chapter 10 would be good preparation for the first week of the course.  I'm following Game Theory course and I think it is a great introduction; they hint at some real applications such as auctions, and the course is relatively easy - bachelor level.  Are you talking about Udacity?   I am taking a bunch of Udacity and Coursera courses. I am taking CS101 with Coursera just so I can see how it compares to the CS101 from Udacity. 
The CS courses from coursera look interesting, but their prerequisites scare me off for now. I will probably take them the next time they are offered.

For Automata:
&amp;gt;You should have had a second course in Computer Science &#8212; one that covers basic data structures (e.g., lists, trees, hashing), and basic algorithms (e.g., tree traversals, recursive programming, big-oh running time). In addition, a course in discrete mathematics covering propositional logic, graphs, and inductive proofs is valuable background.

That is more than a little programming background.

[Udacity's courses](www.udacity.com) cover much more middle ground. Thanks! I didn't know about Udacity. What courses have you taken or are taking now? Udacity just finished its first round of classes, and just started the second round, where they added classes and are re-running the original two. I took CS101. All of the courses use Python. Now I am taking all of the classes except CS101 and CS373(robotic car). So I am taking:

* CS 212 Design of Computer Programs
* CS 253 Web app engineering
* CS 262 Programming languages
* CS 387 Cryptography

The first homework is due Mon/Tuesday, so you can still start without missing anything.

One of the great things about Udacity is that CS 101 is meant for the absolute beginner and then all the other classes only require the level of CS 101. It's self contained whereas Coursera is a random, discrete mix of classes for the most part.    Try also udacity, they are really good. I can recommend all the courses (I finished first 2- cs101 and robot car and now I attend 4 new ones). One is given by the creator of reddit :) I also plan to try coursera, but the bar was set high by udacity and I am afraid if the experience would be equally good...  I think "Introduction to Logic" course is starting on april 23rd on coursera.com will be invaluable to you. Just gets you the right start...

good luck The class is a bit confusing because they don't give you all of the material in the slides/videos that is expected to be known for the homework. But with a little help from Google it's right there. And I've actually taken some programming courses when I was at university, I just didn't finish my degree so I'm hoping to use these to prepare me to go back to school. That is what its all about...going through confusion and coming out successfully.To say the truth, Logic is very very wide like all others.
But the material condensed in that course is all what you need to make the great start. Just finish the course and catch some book on proofs. you will enjoy proofs more than before....
Pefected at proofs means u understood the basic principles very well....hope you will understand wat i am saying..</snippet></document><document><title>Anybody do biology related work with a programming degree?</title><url>http://www.reddit.com/r/compsci/comments/slpg1/anybody_do_biology_related_work_with_a/</url><snippet>What type of work should could I do studying programming and biology?

Is there an area of study that is good to companion cs?

If I would be happy with finding new ways of doing data entry is CS going to be perfect for me?  There's a field known as bioinformatic. It's fairly new.. but I assume with things like [OpenPCR](http://openpcr.org/) there are a lot of programmers working with biology.

After all, all the fields nowadays are merging with math and compsci. [http://www.amia.org/](http://www.amia.org/)

-edit for formatting ~~Well you certainly couldn't have fucked that up any worse.~~ he fixed it are you kidding?  it could have been way worse. It didn't cause an infinite loop or crash the program, it wasn't that bad. There's a field known as bioinformatic. It's fairly new.. but I assume with things like [OpenPCR](http://openpcr.org/) there are a lot of programmers working with biology.

After all, all the fields nowadays are merging with math and compsci.  I did my undergrad in Physics but joined a computational biology graduate program 2 years ago. Interesting subfields which require lots of programming include:

* Genetics - Lots of programming and CS
* Systems Biology - Little personal experience with it
* Molecular Dynamics - Can be LOTS of coding, lots of physics, using huge clusters.
* Structural Biology - Maybe not as much coding as the previous three

I do a sort of Molecular Dynamics, and I spend maybe 50% of my day programming. So what else do you do other than the programming? Read papers and stay current on the newest research, write papers and give talks for class and qualifier exams, make figures for papers for publication, meet with visitors to our lab and show our latest research, change the water cooler tank.

Personally, I like the programming stuff the most and it's a struggle sometimes to not completely neglect the reading/writing stuff. But you have to read and know the science to do the fun coding.  I graduated with a degree in Bioinformatics from UCSD. I'll be happy to answer any questions you may have. Answer any you feel like.

What are you doing with your degree now?

What type of classes did you take while in school?

Would you recommend graduate work?

What did you want to do when you first decided to take that major?

Do you have any minors?

Are there any other areas you wish you would have taken classes in? I graduated with a degree in biochem at UCSF, was a lead computer scientist at a major genomics-based drug discovery company,  and  directed a computational group at Harvard.  I love computing, and am not using my biology degree at all now.  I would not recommend you pursue computational biology.  So my vote -- "no" to graduate school. 

There is a fundamental economic problem - a vast oversupply of PhDs to the number of jobs in both academia and industry.  Each lab produces several PhDs to each professor - however, the number of jobs is roughly constant.  And biotech has always had and continues to have a net negative ROI.  This means that for every dollar invested, less than $1 comes back. As a computer scientist, you are at a very low rung in the biological hierarchy.  Biologists control the research funding, and what they respect are experimental results about biology.  In biotech, value is produced (for the most part) by selling drugs.  As a computational biologist, you are far down the value chain - you are a cost center.  Chemists create the drugs; biologists do research that is helpful to making the drugs, and computational biologists provide tools to the biologists.  Computer science is a servant.  It is not in the drivers seat.

This economic structure has practical effects.  It means that as a computer scientist in biology, you will be at the periphery.  Not in power.  When I was in graduate school, one of my advisors was a very gifted technologist who produced many fundamental and widely used innovations in microscopy.  I remember when he came up for review of his Howard Hughes grant.  The head of the department, a yeast biologist, in his review said that he did not do enough biology.  Poof.  There went his grant.  

Biologists don't give a shit about technology unless it produces a biological result they respect, and even then it's only a tool, and will only ever be one of many tools they might use.  It doesn't matter the intellectual depth that goes into creating the tool - biologists can't understand it and won't respect it. Such results will be relegated to journals on the periphery of science.  Case in point, even the journal Bioinformatics relegates reports that don't contain novel biological results (no matter what it is) to 2 pages.  As an exercise, go try to figure out what journal you could publish a computational biology algorithm that doesn't contain actual novel biology.  Biologists ultimately, control the funding in their field.  That will affect everything you do and can do in your career.

**TL;DR** If you are going to do computer science for biology, you better want to do biology first, and have a clear idea of how you will apply computer science to solve particular biological problems.  The political and economic forces are not in your favor.      I'm a Computer Science Bachelor currently working on my Master's Thesis at the Bioinformatics programme offered jointly by [Leiden University and TU Delft](http://bio.leidendelft.nl/) in the Netherlands.

Bioinformatics is a large field, and even two bioinformaticians can sit down and not have a clue what the other is saying.
Here are some examples of the subfields that might explain the size of this 'specialization':

 * **Molecular Computational biology**: In this area, you would generally be operating on biological sequences. You have probably heard of some of the problems we tackle here, sequence alignment of protein or DNA/RNA sequences and structural prediction of RNA and proteins, each of which are quite large subfields on their own but there are many others such as finding genes on a dna sequence, motif finding and dna sequencing and phylogeny. In this field we often use heuristics instead of exact calculations, because there is so much data.
 * **Functional genomics**: Using genetic data to understand and model the functions and interactions between genes or proteins. Here you would find, among others, problems involving function or interaction prediction, prediction of expression levels.
 * **Imaging**: A large part of bioinformatics is interested in imagine, particularly medical bioinformatics. In this field you receive a bunch of images from medical imaging equipment or microscopy, from which something must be calculated. A friend of mine was accepted at a conference recently for his work on tracking the growth of tuberculosis in zebrafish.
 * **Systems Biology**: Here you model components in a cell. For example, we had a course on Metabolic analysis, where you model the fluxes of metabolites in the cell. By analyzing the steady state of the network, you can find the primary 'extreme pathways' that make up the system. You can use the methods on this field to, for example, optimize the production of a certain metabolite under for an organism by seeing the effects of knocking out proteins or modifying the conditions under which the cell operates.
 * **Visualization**: Once you have some output, you'll probably have a lot of it! Looking at all the data is retarded, so you would visualize it somehow in order to understand the results.
 * **Databases**: This is of vital importance, but together with visualization is often ignored, when in reality, without well designed data storage we wouldn't be able to have many of our most used tools such as BLAST.

Many of these fields overlap, and in almost all of them we are using statistics and machine learning techniques to solve the problems.
The focus of your education will vary depending upon the institution you study at; at mine there is a strong focus on pattern recognition/machine learning, while at others the focus may be more on structure prediction, or sequencing.
As far as programming is concerned, I have yet to see a field where you don't.
Programming is a tool and we use it all the time.
It is important to not forget that while we do all this, we are constantly using tools developed by computer scientists, mathematicians and even physicists.

If you have any questions feel free to ask me.
  I'm a grad student doing protein structure prediction.  Undergrad was cs/biochem.  Lots of cs post docs in my lab.

You also have the whole field of genomics.

Feel free to pm any questions, since your initial question is kind of vague. [deleted] Well I really want to do protein design/engineering, but after reading some papers/talking to some PIs I realized that design is still at a very immature stage: sure you can iterate quickly on the computer, but if that doesn't reflect what it really looks like when expressed, it's not very helpful.

For structure prediction, check out the CASP and CAMEO competitions, look at the best labs, and read papers coming out of those.  Zhang, soeding, baker etc Are there any good subs on here to snoop around? dunno, don't really mix reddit and work :P I'm a grad student doing protein structure prediction.  Undergrad was cs/biochem.  Lots of cs post docs in my lab.

You also have the whole field of genomics.

Feel free to pm any questions, since your initial question is kind of vague. Do the CS people typically have strong bio backgrounds? Nope, usually none at all.

From my experience and from them, the bio part of most problems in protein structure prediction is fairly easy to pick up. Nope, usually none at all.

From my experience and from them, the bio part of most problems in protein structure prediction is fairly easy to pick up. Do you think being a programmer makes it easier to understand things in general? I think the skill of programming is a multiplier effect on your output.  That said, CS is not programming, though many tend to learn both side by side.

I'd say programming helps understand what you can't do in programming, which happens to be fairly important when you're doing protein structure prediction.  You can say the same about biochem, physics, or math though.

Some cs people can't pick up bio for the life of them though... How do you differ between cs and programming? I'm a grad student doing protein structure prediction.  Undergrad was cs/biochem.  Lots of cs post docs in my lab.

You also have the whole field of genomics.

Feel free to pm any questions, since your initial question is kind of vague. What did you choose for graduate studies? What do you look at to predict protein structures? Would CS have been enough to study or would you have liked to gotten any other classes in for undergrad? biochem.

that second question has a very long involved answer, but it's essentially any empirical or theoretical correlation between sequence and final structure we can get our hands on.  Alignments like HHpred, Raptor, consensus between those, contact predictions, structural homolog, lookback iterations, and then of course a force field with all your energy components, solvation, vanderwaals, electrostatics, etc.

TBH, I think that a math major that can program would be the most useful for the field.  In my opinion (and David Shaw's), the hard part right is now is developing a better force field, and to do that you need some clever math tricks to get around the sampling problem.  Sure, we can iterate slowly, but groudbreaking stuff like http://bioinformatics.oxfordjournals.org/content/28/4/510.abstract will get us there that much faster. Do you think a math minor would be good enough?       </snippet></document><document><title>Hey /r/Compsci I've got a quick question about majors and CS opportunities</title><url>http://www.reddit.com/r/compsci/comments/sm3z7/hey_rcompsci_ive_got_a_quick_question_about/</url><snippet>Hey y'all! I've come to a fork in my career, and I'm looking for advice as to which major I should choose, and what opportunities are available to me once I do pick one. I'm currently in the enviable position of having been formally invited into the CS major at my university by a professor, and I have extensive programming experience, but I'm also very interested in doing Applied Mathematics, or a Math major.

The Question: If I DO end up picking Applied Math or Math, what are my chances of being able to continue in areas that are heavy in CS, like computational numerics or modelling? The requirements for Applied Math end with basic C programming or basic OOP, and that scares me into thinking I won't be able to continue with the level of programming I'd like. 

Also, if I DO do CS, how hard would it be to say, continue into a Math MA? or Physics MA?

Thanks for listening!!!!

EDIT: I forgot to specify, I only have enough time to do 1 major, no minor. Pressure from parents.     </snippet></document></searchresult>
<searchresult><compsci /><document><title>Crazy Man's Operating System Now Has New WebSite with Source Code Posted On-Line</title><url>http://www.losethos.com/CrazyOS.html</url><snippet>  I really don't care if he is crazy, it really doesn't matter.  This is something that is cool, and I'm impressed that he did this on his own.   Nice work Terry, props to you for the hard work on this. I really don't care if he is crazy, it really doesn't matter.  This is something that is cool, and I'm impressed that he did this on his own.   Nice work Terry, props to you for the hard work on this. Check the submitter's name -- his online handle is "Crazy Man"; he's not saying the original dev is crazy ;).  truly bizarre.  It's not a unix clone. Its not a windows clone. It is truly something different. Your OSMain.SPZ bootstraps everything in a modular fashion using includes. very clever. It would (in theory) be easy to add on a new subsystem by extending and injecting said functionality here. Totally unexpected.
http://www.losethos.com/LTWeb/OSMain/OSMain.html#l1

Beyond that, everything seems to be in functions. 

**EDIT:** It really is a hackable structure. There is some crazy asm going on in places, but the architecture of this OS couldn't be any more drop dead simple. I didn't look at the source code, but from what I understand, you couldn't really write a decent OS without SOME assembly code (as in it is not possible, not merely difficult). You technically could, but it wouldn't be a good idea. 

My main point was just on the surprise on just how hackable this code is. It has a very simple design.  You technically could, but it wouldn't be a good idea. 

My main point was just on the surprise on just how hackable this code is. It has a very simple design.  How? machine code for one.  You know what I wish?  I wish I could convert the Intel manuals from PDF to TXT in a pretty format.  Then, I would include them on the distribution.  I have links in LoseThos to make documentation fun.  I would put links to the Intel manuals.

My niche is recreational programming.  Assembly language appeals to every red blooded programmer!  What's wrong with you?  Aren't you curious at all?

There's very little assembly language in LoseThos.  You'll see  PUSHFD CLI POPFD.  That turns-off interrupts.  I use the same names for bit operation functions as the intel assembly instructions -- Bts Btr Bt Btc Bsf Bsr.

Bsf(i)  returns the first bit scanning forward.

Bsr(i) returns the first bit scanning in reverse.

I love those, but don't use them often.

    while (LBts(&amp;amp;lock_flags,0)) //This sets a bit to one and returns old state so it can loop if it was one
      Yield();

    {code}

    LBtr(&amp;amp;lock_flags,0); //This sets a bit back to zero

That's how I protect something with a lock bit.

The "L" on "LBts" or "LBtr" means use the LOCK assembly prefix.  You need that for multicore safe locks.

Using the Yield() loop is unusual.  Normally, they make a structure or something and wake it exactly when the thing is ready.  Those structures are nasty -- you have to worry about ownership when tasks die and stuff.  My operating system can switch tasks in half a microsecond -- much faster than others because I don't mess with memory maps or change privilege levels.  Also, I only target systems for home users -- not main frames where you might have a thousand users. &amp;gt; My niche is recreational programming. Assembly language appeals to every red blooded programmer! 

Out of curiosity. I have some AMA questions.

1. The Atari 2600 homebrew community has been making huge strides in the last 10 years by expanding the capability of the original platform, way beyond what was originally envisioned. The 2600 offers some really challenging problems because the system is such a limited platform. Do you have any interest in coding 8-bit Atari games?

2. Same question for 8-bit Nintendo. Any interest there?

3. Do you follow the homebrew communities such as [AtariAge](http://www.atariage.com) and others for Nintendo, PlayStation, Xbox, and Wii?

4. What kind of keyboard do you use?

   Demo Video: http://www.youtube.com/watch?v=Ql9yQERcQL4 I feel violated by all those blinking buttons and scrolling texts. I feel violated by all those blinking buttons and scrolling texts. Demo Video: http://www.youtube.com/watch?v=Ql9yQERcQL4 Looks cool! What's up with the WordStat window, care to explain what's that for / how it works? (Sorry if you explain it in the video; I watched it at work without sound.) WordStat looks at the word under the cursor of the focus window.  It checks for autocompletion possibilities and lets you jump to source code for symbols.  CTRL-F1 will autocomplete with 1st.  CTRL-SHIFT-F1 will jump to symbol source code. Demo Video: http://www.youtube.com/watch?v=Ql9yQERcQL4 Did you give up trying to make money off of it? :) I knew I couldn't charge for it.  Linux is free, for one thing.  More importantly, I can't imagine offering a warranty.  PC hardware is really diverse and even if you got 90% compatibility, that'd be lots of refunds.

I tried shareware with SimStructure.  6000 downloads and only one check for $20.00

There are 380,000 reddit/r/programming people.  LoseThos is for programmers.  Maybe, if 100,000 people installed LoseThos, it might make a market for applications?  It doesn't look good.  

I've had well over 10,000 downloads over the last 5 years and so far I haven't see one program written by someone else.  Nothing.  Something, doesn't make sense.  I guess people sell token logos and merchandise, as one way of making a little money.

I'm on disability, so it's not terrible.  I'm not in debt -- I don't owe anything.  I can keep doing what I'm doing.  I hired an artist.  That didn't go well.  I could blow money on advertising.  Wouldn't change anything. No one knows how to use it well. You'd have to do a tutorial intended for people who've never used a computer before, because your system is very different. Most people don't want to invest time learning or developing for something no one else uses.

If you would dedicate yourself to making freebsd or linux better, work to find security issues, or improve software that can run on linux, I'm sure you'd be greatly rewarded and we would all appreciate it. Not that your project isn't amazing, but if you want recognition and love from the programming community we would suck your dick if you fixed gnome. You Linux people need to accept your inner nerd and make nerd paradise, instead of being greedy.  LoseThos is just for programmers.

I have made programmer paradise.

Linux is full of #ifdefs because they want so many architectures.  They got greedy.  Now, you have hell.  How many people long for "open source" code which isn't so unpleasant, it's hell?  Your code sucks.  Look at my in-line assembly support and look at gcc in-line assembly!

LoseThos is not a general purpose OS.  You will use it in addition to Linux or Windows.  I'm not in direct competition with Linux, except in mind-share.  You go ahead and keep all the atheists and I'll give Heaven to the good boys and girls like Karate Kid, the movie. Why not make a compiler for your language that works on linux? People would start to use it and that would make it easier for them to switch over to LoseThos. The D programming language has "static if" and "mixins" to support compile time changes much more cleanly and it also has a more portable assembly which I can use in any OS that supports GDC. Your language is amazing in the fact you're just a 1 man army, but modern languages provide the the lessons learned from lifetimes of programming language theory with the efforts of large and continually changing teams. The fact is I and so many other would benefit from your work if you worked on something everyone uses, even if it's only a minor tiny bit of your time each day. We can tell you are good at it. God and I.  God advised me.

I did everything I could for compile speed because I want to operate almost everything on a JIT basis.  I haven't compared to gcc.  I suspect, they do many expensive optimizations which only help poor programmers.  The C language is pretty close to assembly, if you think about it.  It seems like many things can be solved by writing good code in the first place.

You try talking to God.  Run the program "After Egypt" in LoseThos.

God says...
eased cup courage Platonists swine applauses thread degrees 
vain Churches presented project HAVE gallantry Jacob husbands 
copyright contention knottiness fields effaces knowest 
regeneration laying well-known I hedged goes exceedeth 
speedily -though devoutly mankind header baggage needeth 
deceived excepted caresses Better battle Creator intelligence 
coals didst class distractedly often feels dissolution 
show products virus murder University disputed redeem 
black yielding apprehension password summit strife devilish 
slumbers cleansing Church's re-collect walls name persuasive 
male point reading sweetnesses hither bore preachings 
att fearful unalterable revenges sorrow-bringing discovereth 
remembered ear discontent briefly resisting -the courage 
attempt Psalm combined called struck thirsts unteachable 
pressure stowed Meanwhile illusion understand madness 
proceed retire Etexts surmounted woe melt betters approaching 
offend embittered equipment sever detain Ambition bulk 
Paradise jeer fifty Jove troubles held gave toiled Pylades 
mists repel sufficiently remain wasting wanton undertook 
abstract Life avoiding despite silently program initiation 
profess showeth habitation waterest novice silent Victorinus 
desired wickedly Galatians uncertain jeering pursue brackishness 
throughout inspired forge ABOUT fraud 300 harmonising 
divorceth virginity yields Terence sympathy conversation  And it's amazing, but come play with the programming community every once and awhile and use the things we use. God will punish you for masturbating. [Share your code](http://i.imgur.com/qBxCv.gif). It almost seems highs and lows balance.  I sat pondering if pleasures and pains balance.

----

20 Looking at his disciples, he said:


&#8220;Blessed are you who are poor,
    for yours is the kingdom of God.
21 Blessed are you who hunger now,
    for you will be satisfied.
Blessed are you who weep now,
    for you will laugh.
22 Blessed are you when people hate you,
    when they exclude you and insult you
    and reject your name as evil,
        because of the Son of Man.
 
23 &#8220;Rejoice in that day and leap for joy, because great is your reward in heaven. For that is how their ancestors treated the prophets.
 

24 &#8220;But woe to you who are rich,
    for you have already received your comfort.
25 Woe to you who are well fed now,
    for you will go hungry.
Woe to you who laugh now,
    for you will mourn and weep.
26 Woe to you when everyone speaks well of you,
    for that is how their ancestors treated the false prophets.
   Why Christian themed? He has schizophrenia, which quite often tends to have religious beliefs/paranoias associated with it.  He ended up joining another forum I'm on, and while he uses the Bible and talks about God, I don't know if I'd consider his beliefs "Christian" per se.

The OS itself is very very fascinating and the videos show what I think would have been the future of command line computing (as interpreted by someone who has schizophrenia or taken lots of drugs) if the WIMP metaphor hadn't taken hold.  

I think too many people will see "crazy man" and discount all the work he's done (or laugh at him).  It really is a beautiful thing to behold.  I hope some people will take it seriously, and perhaps take up the source code and run with it.  I tried to run it under virtualbox, but I'm using 32-bit Windows XP, and apparently it couldn't detect my core2 duo as being dual core.  But if anyone has Windows 7, I think that can run it fine under virtualization (I have seen some reports in my previously mentioned thread that people were able to get it to run under Virtualbox). For the sane version of the same ideas there is [Oberon](http://en.wikipedia.org/wiki/Oberon_%28operating_system%29). This project was started by Niklaus Wirth in 1985 but it looks like the site is dead since 2008. For a quick test the [old version](http://www.ethoberon.ethz.ch/windows.html) running on top of Windows may still work.

edit: It seems that this project is still active and is mirrored in [google code](http://code.google.com/p/ethz-oberon-repository-mirror/source/browse/). He has schizophrenia, which quite often tends to have religious beliefs/paranoias associated with it.  He ended up joining another forum I'm on, and while he uses the Bible and talks about God, I don't know if I'd consider his beliefs "Christian" per se.

The OS itself is very very fascinating and the videos show what I think would have been the future of command line computing (as interpreted by someone who has schizophrenia or taken lots of drugs) if the WIMP metaphor hadn't taken hold.  

I think too many people will see "crazy man" and discount all the work he's done (or laugh at him).  It really is a beautiful thing to behold.  I hope some people will take it seriously, and perhaps take up the source code and run with it.  I tried to run it under virtualbox, but I'm using 32-bit Windows XP, and apparently it couldn't detect my core2 duo as being dual core.  But if anyone has Windows 7, I think that can run it fine under virtualization (I have seen some reports in my previously mentioned thread that people were able to get it to run under Virtualbox). Could you give a Cliff-notes version of how it changes the command-line? everything you type in the command-line is compiled into a program and run, because [Everything you type is in C](http://www.losethos.com/LTWeb/Doc/Welcome.html#l1)

It's taking bash to the next level? Could you give a Cliff-notes version of how it changes the command-line? This has some command-line examples:

http://www.losethos.com/LTWeb/Doc/Welcome.html#l1

http://www.losethos.com/LTWeb/Doc/HelpMain.html#l1

Press F1, for help and go look in the Help Index for "CmdLine (typical)".  It lists the commands you usually type at the command-line.  Any function is fair game, but those are the usual.
 whaa....?

&amp;gt;There are no object files in LoseThos and, normally, you don't make executable  files either, but you can.  That's known as "static" compilation.  Instead, you  "just-in-time" compile.

So wait. Are you saying that you are doing JIT assembler?
~~I don't know how to feel about that.~~ It is an interesting idea.

The idea of being able to meta-program in assembler is interesting.
 This has some command-line examples:

http://www.losethos.com/LTWeb/Doc/Welcome.html#l1

http://www.losethos.com/LTWeb/Doc/HelpMain.html#l1

Press F1, for help and go look in the Help Index for "CmdLine (typical)".  It lists the commands you usually type at the command-line.  Any function is fair game, but those are the usual.
 Also, I don't understand this:
&amp;gt;12) Backward compatibility is NOT guaranteed.  You are safest working with JIT  compilation instead of AOT static binaries because I can add members to classes,  reorder members in classes, add default args to existing functions, change #define values, etc. all without breaking compatibility.

When juxtaposed with:
&amp;gt;There are no object files in LoseThos and, normally, you don't make executable files either, but you can.  That's known as "static" compilation.  Instead, you "just-in-time" compile.

So does that mean that you CAN AOT statically compile or not? You can AOT static compile.  I just think it's neat that with JIT, I can change constants and rearrange structures without breaking compatibility. Could you give a Cliff-notes version of how it changes the command-line? He has schizophrenia, which quite often tends to have religious beliefs/paranoias associated with it.  He ended up joining another forum I'm on, and while he uses the Bible and talks about God, I don't know if I'd consider his beliefs "Christian" per se.

The OS itself is very very fascinating and the videos show what I think would have been the future of command line computing (as interpreted by someone who has schizophrenia or taken lots of drugs) if the WIMP metaphor hadn't taken hold.  

I think too many people will see "crazy man" and discount all the work he's done (or laugh at him).  It really is a beautiful thing to behold.  I hope some people will take it seriously, and perhaps take up the source code and run with it.  I tried to run it under virtualbox, but I'm using 32-bit Windows XP, and apparently it couldn't detect my core2 duo as being dual core.  But if anyone has Windows 7, I think that can run it fine under virtualization (I have seen some reports in my previously mentioned thread that people were able to get it to run under Virtualbox). Why Christian themed? My mom's schizo too, and has "christian" beliefs involving angels, "psychic vampires" and telepathic messages being directed at her. She doesn't do cool stuff like write operating systems though, she can barely use e-mail software :'(

Anyways, having lived with a schizophrenic mother for years, I can tell you that the illness doesn't necessarily affect their intelligence. My mother believed all sorts of insane things and I consider her kind of unbearable, but she always kept a budget, paid all the bills, did groceries, cooked supper, etc. without requiring any assistance. She has multiple hobbies like reading books, painting, sewing, etc. She's insane, but actually very smart and *mostly* functions like a normal person.

I commend Mr. Crazy on what he has achieved in his holy mission to create an operating system capable of digitizing humanity's legacy before the apocalypse. May your opcodes be blessed.    Whatever's going on with LoseThos or its author, your post's title and your puppet-account name are really distasteful. Many people here know that the author of LoseThos has mental health issues. That doesn't make his work a "crazy man's operating system." Show a little class. I'm pretty sure he _is_ the author. Sigh. I hadn't considered that possibility. He is either being banned repeatedly or he is making multiple accounts for unknown reasons. [Here is another one from a few days ago.](http://www.reddit.com/user/MusicOS) God likes Highlander.  He also likes soap operas and Beverly Hillbillies. Hey Terry, I have been meaning to ask you this but I haven't had the chance yet. What do you have planned next for Losethos? I know stability is a big thing, but are you bringing more cool features up soon? I don't have any more ideas, but I get new ideas, now and then.  Win8 with secure boot is really depressing.
 

I decided I'm not going to do PCI drivers.  My code is aging and devices might not be supported.  I figure VMWare, etc, will provide a compatibility layer far into the future, however.  Those takes the fun out of it -- not real hardware.  If it's not THE operating system, it's like an application, and begins to seem silly.

UEFI is the new boot standard, but they should have backward compatibility.  They put ELF object files and various propriatary graphic files into the standard.  Mine supports none of those.  They made a massive bunch of nonsense in the UEFI standard.
 I don't have any more ideas, but I get new ideas, now and then.  Win8 with secure boot is really depressing.
 

I decided I'm not going to do PCI drivers.  My code is aging and devices might not be supported.  I figure VMWare, etc, will provide a compatibility layer far into the future, however.  Those takes the fun out of it -- not real hardware.  If it's not THE operating system, it's like an application, and begins to seem silly.

UEFI is the new boot standard, but they should have backward compatibility.  They put ELF object files and various propriatary graphic files into the standard.  Mine supports none of those.  They made a massive bunch of nonsense in the UEFI standard.
 What does god think of Node.js and Ruby? What does god think of Node.js and Ruby? God likes Highlander.  He also likes soap operas and Beverly Hillbillies. Hey, sorry for hijacking this thread but I have a quick question... are you the same TAD that used to compete in some asm size coding competitions?  If not, that's quite a coincidence! Whatever's going on with LoseThos or its author, your post's title and your puppet-account name are really distasteful. Many people here know that the author of LoseThos has mental health issues. That doesn't make his work a "crazy man's operating system." Show a little class. Whatever's going on with LoseThos or its author, your post's title and your puppet-account name are really distasteful. Many people here know that the author of LoseThos has mental health issues. That doesn't make his work a "crazy man's operating system." Show a little class. We prefer the term mentally ill, it is more correct. I am mentally ill myself, with schizoaffective disorder, not crazy and not insane. I know the OP is posting the link to his own website and trying to use the "crazy man" Meme, but it just hurts the rest of us who got labeled as crazy because we are mentally ill.

I mean it is a good OS, looks like a Commodore 64 ported to 64 bit processors with some sort of Mutant C/C++ language in use. The deal breakers are that it does not have serial port or networking support so you really cannot do a lot of fun stuff with it like control stuff via serial ports or hook up to other devices over a network. I mean it would be great for PLC/PIC control, and as a web server and database server because there are no known viruses for it, and you cannot add in a lot of crap because there isn't very many programs available for it.

The part that makes it hard to believe or impossible is the whole "talking to God" and "God sings hymns" part. Just basically a random word generator pretending to be God. It's just like the Bible:

http://www.biblegateway.com/passage/?search=1+Corinthians+14&amp;amp;version=NIV

The most important story in the Bible is Cain and Abel.

http://www.biblegateway.com/passage/?search=genesis%204&amp;amp;version=NIV

They offered what they thought God would like and He gave feedback.  If the offering was not good, they should have tried something else.  Cain loved God so much he killed Abel!  This being was very real to the people in the Bible.

The Greeks believed in muses giving music. Your interpretation is unusual to me. How do you conclude that Cain's murder of Abel was out of love for God? To me it has always seemed like jealousy is the more accurate interpretation. Yeah, like lover's jealousy. It was because God liked Abel's offering better than Cain's. Yeah, like lover's jealousy. It was because God liked Abel's offering better than Cain's. Abel was the greater sacrifice?  Beloved of God.  Cain's own brother... So what better sacrifice to offer to God, to make up and atone for his lousy previous sacrifices than Abel? When you have absolute proof of God, atheists are very funny.  I like the genius that said, "How could a loving God not cause the discovery of anesthetics centuries sooner?"  Or, "Simple logic says God cannot be proven or disproven."  When you have absolute proof of God, atheists are very funny.  I like the genius that said, "How could a loving God not cause the discovery of anesthetics centuries sooner?"  Or, "Simple logic says God cannot be proven or disproven."  Abel was the greater sacrifice?  Beloved of God.  Cain's own brother... So what better sacrifice to offer to God, to make up and atone for his lousy previous sacrifices than Abel? s/Abel was/Abel had/

IMO. Well that was the idea - the traditional story is Cain was jealous and slew Abel in a fit of jealous rage, but I like this new spin on it where Abel became a greater sacrifice via the fact that he was more beloved of god and also Cain's brother, and a human, thus greater than plant or (non-human) animal, thus should appeal to god even more.

It's really just a twisted retelling of the idea, but intrigues me nonetheless. It's just like the Bible:

http://www.biblegateway.com/passage/?search=1+Corinthians+14&amp;amp;version=NIV

The most important story in the Bible is Cain and Abel.

http://www.biblegateway.com/passage/?search=genesis%204&amp;amp;version=NIV

They offered what they thought God would like and He gave feedback.  If the offering was not good, they should have tried something else.  Cain loved God so much he killed Abel!  This being was very real to the people in the Bible.

The Greeks believed in muses giving music. But it is not the true word of God if it is some random word generator that does not make sense.

Cain murdered Abel over jealousy because God accepted his offering but rejected Cain's. The most important story in the Bible is that of Jesus Christ that put an end to these offerings and sacrifices, and did away with an eye for an eye and tooth for a tooth to love thy neighbor and to have mercy, compassion, and kindness towards all. I'm not religious but I love playing devil's advocate (maybe the wrong phrase to use in this context heh)

&amp;gt;But it is not the true word of God if it is some random word generator that does not make sense.

If God is everything than a random word generator's output would still be via God's design and thus could be said to come from him.  Yes - I believe he sees it as a sort of bibliomancy/numerology style divination.  I may be wrong, of course, but I think that's what he's doing with the God program. We prefer the term mentally ill, it is more correct. I am mentally ill myself, with schizoaffective disorder, not crazy and not insane. I know the OP is posting the link to his own website and trying to use the "crazy man" Meme, but it just hurts the rest of us who got labeled as crazy because we are mentally ill.

I mean it is a good OS, looks like a Commodore 64 ported to 64 bit processors with some sort of Mutant C/C++ language in use. The deal breakers are that it does not have serial port or networking support so you really cannot do a lot of fun stuff with it like control stuff via serial ports or hook up to other devices over a network. I mean it would be great for PLC/PIC control, and as a web server and database server because there are no known viruses for it, and you cannot add in a lot of crap because there isn't very many programs available for it.

The part that makes it hard to believe or impossible is the whole "talking to God" and "God sings hymns" part. Just basically a random word generator pretending to be God. &amp;gt; and as a web server and database server because there are no known viruses for it

I remember watching the video when it was posted to slashdot a few years ago, and he specifically said that a total lack of memory protection was a design feature of the OS. You can literally write to any page from any process. DOS worked just fine.  It's faster without messing with all that.  I do it for simplicity of code and because it's nice debugging any location you feel like from one task in another tasks's space. 

Memory protection is only when you have bugs or malware, otherwise it never does anything. 

It's a special purpose toy-kind-of-thing.  Memory protection is good in a normal OS.  If you want to have fun though, use LoseThos.  Maybe you need to be younger.  I spent hours exploring.  I loved a disk editor that let me explore disk blocks.  You can do that.   Edit: It's the author posting this ... previous comment redacted. Stop treating like there's something wrong. And STFU.  If you are Terry, then it's ok. If you aren't, then it's not. It is I.

People say stuff like, "You better take your meds."

I say, "STFU nigger."  (On disability with no employer to worry about.)

Some stuff is really offensive.  Patronizing is really offensive.  IQ of 145. Just as an FYI, having a high IQ doesn't make it OK to yell racist slurs at people, dude. I get racially slurred for being Irish all the time, but on a higher level than just saying "nigger".  I listen to the BBC a lot.
 Huh... you don't sound particularly Irish in your youtube videos.  TIL. Ethnicity.

"Take your meds" is as insulting as "nigger".  That's my point. Ethnicity.

"Take your meds" is as insulting as "nigger".  That's my point. It is I.

People say stuff like, "You better take your meds."

I say, "STFU nigger."  (On disability with no employer to worry about.)

Some stuff is really offensive.  Patronizing is really offensive.  IQ of 145. It is I.

People say stuff like, "You better take your meds."

I say, "STFU nigger."  (On disability with no employer to worry about.)

Some stuff is really offensive.  Patronizing is really offensive.  IQ of 145. I realise 99% of the time you're batshit insane but do you seriously have to resort to using racial slurs? Equally offensive as "take your meds."  It's is saying, "You are subhuman." Equally offensive as "take your meds."  It's is saying, "You are subhuman." Equally offensive as "take your meds."  It's is saying, "You are subhuman." Ape shit crazy used to be my favorite kind of crazy, but I think CrazyOSMan crazy is my new favorite.  You might like this then http://qaa.ath.cx/LoseThos.html Equally offensive as "take your meds."  It's is saying, "You are subhuman." Equally offensive as "take your meds."  It's is saying, "You are subhuman." [deleted] You're drunk, dude.  What are you rambling? [deleted] [deleted] Equally offensive as "take your meds."  It's is saying, "You are subhuman." No it isn't. "Take your meds" isn't even nomative. It's an injunction like, "chew a breath mint before you speak in this elevator". Insulting? Sure. But no one has ever argued about whether or not take-your-med-people should be bought and sold like livestock. No, but it's certainly been argued that we should lock them all up so we never have to interact with them again.  Many are as scared of them as they are murderers or rapists but with more vague pity and condescension.

Considering "Take your meds" contains all the above connotations (whether intended or not), I'd say "chew a breath mint" is maybe the worst, most unfitting, and most disgustingly dismissive thing you could have possibly tried to compare it to. Well, the difference between "take your meds" and "you stink" is larger to you. The difference between "take your meds" and "nigger" is larger to me. No it isn't. "Take your meds" isn't even nomative. It's an injunction like, "chew a breath mint before you speak in this elevator". Insulting? Sure. But no one has ever argued about whether or not take-your-med-people should be bought and sold like livestock.  The recent thread on metafilter about LoseThos that Terry participated in was fascinating.

http://www.metafilter.com/119424/An-Operating-System-for-Songs-from-God The recent thread on metafilter about LoseThos that Terry participated in was fascinating.

http://www.metafilter.com/119424/An-Operating-System-for-Songs-from-God Rule number 1) You do not talk about metafilter on reddit.
Rule number 2) You DO NOT  talk about metafilter on reddit.

(fellow mefite?)     &amp;gt; I got furious at Visual-C when they added "const char *".

Can you explain this? I'd like to know what your reasoning is here.     So is CrazyOSMan too crazy for github? I don't have networking, so transferring files involves booting another operating system and uploading.  My files have per-file compression.  My files can have graphics and crazy non-ASCII codes.  The ASCII#5, for example, is the stored editor's cursor location.     How about a port to PandaBoard and Raspberry Pi? How about a port to PandaBoard and Raspberry Pi?    This guy is as crazy as the reddit admins that let /r/SubredditDrama still exist and downvote brigade! How can you give enough of a shit about an online forum to spend your time doing this</snippet></document><document><title>Easy papers to read?</title><url>http://www.reddit.com/r/compsci/comments/zctgf/easy_papers_to_read/</url><snippet>Hi everyone,

I'm most certainly going to go for a Masters after I'm done with my B.Sc. this winter.  I understand that a large part of doing a masters will be to read many papers relating to my research area.  I've read one paper so far for a presentation I had to do a couple weeks ago, and I found that it's hard to read those.  I've been told that after you've read a few, the basic framework becomes more ingrained and reading further papers becomes easier and faster.

With that in mind, I was wondering if you had any suggestions for papers that are relatively easy, so that even as an undergrad, I can read them and practice my reading skills.

Thanks a lot!  Yo.  I spent 5 years out of school, not doing research.  I have always wanted a PhD, and I am a year in.  Adjustment was difficult to say the least.  No one gave me any direction into reading papers and I learned by reading a LOT of shitty papers (I didn't even know HOW to find papers).  Then I read some decent papers and learned even more.

Sorry, you're going to start your training here, because I gots a lot to say.  Grad school is very different than undergrad.  Grad classes go into less background depth, expecting you to pick up the slack where needed.  You start to realize that your networking teacher would probably learn a LOT taking an OS class, despite how close those two topics may be.  The truth is that these guys/gals are specialists.  The undergrad classes are filled with mostly well understood material, but graduate classes are not so firm.  Grad classes go from very little background to 100mph on a dime it can seem.  But, software classes aren't so bad.  That said, you do need to do a lot of reading, and you need to learn to do it fast.  So, don't beat yourself up for not already knowing the material.  BUT, do plan on picking up the basics and quickly.  Research papers are written such that smart people familiar with research from ANY field can read them.  They have a little background, then they go.  The writing can be dense, and they are often written by students, not professors (that means they're rarely written well).  The format has its challenges, but such is life.

I'd like to link to my grad class page, but I'd like to remain somewhat anonymous.  Fortunately for you, computer science papers are almost ALWAYS disseminated for free (this is not true for many fields).  This is fortunate for me too, because I am too lazy to add in links :)

Here are some GREAT papers:

 * The UNIX Time-Sharing System,
Dennis M. Ritchie and Ken Thompson
 * A History and Evolution of System R,
Donald D. Chamberlin, et al.
 * Congestion Avoidance and Control,
Van Jacobson and Michael J. Karels

Here's a great non-research paper:

 * Some Hints for Computer System Design,
Butler W. Lampson

Read some modern industry papers (MSR is argueably not "industry"):

 * Dynamo: Amazon's Highly Available Key-value Store,
Giuseppe DeCandia, et al.
 * Finding and Reproducing Heisenbugs in Concurrent Programs
M. Musuvathi, S. Qadeer, T. Ball, G. Basler, P. A. Nainar, I. Neamtiu
 * If you want, read the BigTable paper, but I don't love BigTable, so fuck it.

I love scheduler activations (and linux has today solved many of the problems this paper aimed to solve:

 * Scheduler Activations,
T. E. Anderson, B. N. Bershad, E.D. Lazowska, H.M. Levy, 

These are all systems papers, which is what I am into.  There's some networking and some databases.  It's a good list, but it's just covering a few areas of CS (albeit popular ones).  Happy reading :) Good list, I've read a number of those. I'd also recommend the paper below. This was the paper that introduced the semaphore. What I find amazing is that the semaphore is described in the appendix.

* The structure of the ``THE''-multiprogramming system, Edsger W. Dijkstra

     </snippet></document><document><title>What is loop unwrapping?</title><url>http://www.reddit.com/r/compsci/comments/zclx9/what_is_loop_unwrapping/</url><snippet>Can someone please explain &#8211; not like I'm five, but more like I'm a sophomore computer science major that's never heard the term &#8211; what loop unwrapping is? I've been reading about it in the context of the optimization of the quick sort algorithm. Thanks!  Let's say you have a loop that goes over the elements of an array and adds them together:

    for(int i in array) {
        sum += i;
    }

If the compiler can figure out the size of `array` ahead of time, then it can throw away the loop and expand it out like:

    sum += array[0];
    sum += array[1];
    ...
    sum += array[n];

When you look at it down at the assembly code level, it's effectively throwing away comparison and jump instructions, and perhaps some moves as well. The tradeoff is a larger executable size, but that's usually acceptable with modern hard drives and transfer speeds except in very specific cases. Wow, that's cool. What kind of speed increase are we looking at there? Maybe 50% faster? Wow, that's cool. What kind of speed increase are we looking at there? Maybe 50% faster? Wow, that's cool. What kind of speed increase are we looking at there? Maybe 50% faster? Wow, that's cool. What kind of speed increase are we looking at there? Maybe 50% faster? Wow, that's cool. What kind of speed increase are we looking at there? Maybe 50% faster? Wow, that's cool. What kind of speed increase are we looking at there? Maybe 50% faster? You also have to consider the operations each loop is doing.  If you're doing complex number math, divides and square roots, you're not gonna get many gains anyways because the pipeline is gonna have to stall/flush when you unroll those loops.  Again, as mentioned, very specific to the hardware architecture.
 Let's say you have a loop that goes over the elements of an array and adds them together:

    for(int i in array) {
        sum += i;
    }

If the compiler can figure out the size of `array` ahead of time, then it can throw away the loop and expand it out like:

    sum += array[0];
    sum += array[1];
    ...
    sum += array[n];

When you look at it down at the assembly code level, it's effectively throwing away comparison and jump instructions, and perhaps some moves as well. The tradeoff is a larger executable size, but that's usually acceptable with modern hard drives and transfer speeds except in very specific cases. The top link on /r/programming had a pretty good example of loop unwrapping: http://stackoverflow.com/questions/12264970/why-is-my-program-slow-when-looping-over-exactly-8192-elements (See the top answer in that SO question.) It's worth noting that the improvement in that example was more due to swapping the outer loops (and improving the cache hit/miss ratio) than unrolling the inner ones, but this is a case where unrolling the short inner loops made the code somewhat clearer (if more verbose) as well as faster (assuming the compiler wouldn't unroll them -- which it probably would).   I was reading about Strassen's algorithm the other day, and now that I saw your examples in this thread I thought that Strassen's algorithm would be a good example of loop unwrapping. The text I was reading showed that instead of having 3 loops, Strassen's algorithm had 2 loops where the 3rd was unwrapped to make it faster. Have I misunderstood or is that accurate?  It's accurate enough. The algorithm is just unrolling the loop by hand rather than having a compiler do it. Probably because there was no such thing as a Sufficiently Smart Compiler for this case.  </snippet></document><document><title>CS Questions (Radix Sort and Java Server)</title><url>http://www.reddit.com/r/compsci/comments/zcf0y/cs_questions_radix_sort_and_java_server/</url><snippet>So random questions that's been bothering me...

1. Radix sorting is considered to be best if you're sorting items of a set length (IE: Phone numbers, or Social Security Numbers, etc.).  Why not have this work, but "Pad" w/e you're sorting.  
For example, if I want to sort this set of numbers (I know this is a puny dataset, but imagine it's a lot larger). {40,12,6,1,43,76323}.  Obviously they aren't the same length, but why not pad them all so that it's like: {00040, 00012,00006,00001,00043,76323} so that you can then use the bucket-&amp;gt;Radix sort?  I can sort of see that this could be inefficient, but I feel like you could imply that if no value exists where it should, you just default to a null/0.

2.  I was reading Steve Yegge's Drunkin Blog essays, yes, I understand they're older, however it seems to apply still.  A lot of people mention using Java as a back end.  As a server.  Etc.  But what I don't understand is, Java is considered slow and w/e, so why would you use Java in a setting that would be most important to a live application that thousands/millions would use?  Would this not cause a bottle neck and wouldn't using a quicker native language like C make this quicker?  If anyone has any articles as to how/why people use Java and not C or another language, I'd appreciate it.

That's it for now, thanks!  1. Why not pad them? Because you don't have to :P

2. Because java runs on many plaforms, big bundles from big companies are written once and run almost anywhere. This far outweights the 1 milisecond difference in the computation time. Also, Java IS NOT SLOW! Whoever told you that either lives in 1995 or is a complete idiot. And also, user don't care if he's served in 1ms or 10ms. Even if java was 20 times slower than C (which it is NOT), it wouldn't matter for the kind of job it's doing. 1. Why not pad them? Because you don't have to :P

2. Because java runs on many plaforms, big bundles from big companies are written once and run almost anywhere. This far outweights the 1 milisecond difference in the computation time. Also, Java IS NOT SLOW! Whoever told you that either lives in 1995 or is a complete idiot. And also, user don't care if he's served in 1ms or 10ms. Even if java was 20 times slower than C (which it is NOT), it wouldn't matter for the kind of job it's doing. Why is Radix sort not more widely used then something like Quicksort then?  I mean, with 5 million elements of length 10, it'd take half the time (According to O()) to sort it using radix, which seems like a bit much to do. 

And I understand the reasoning of Java.  Especially aided with Drumming's chart, makes more sense.  Thanks to both of you.  Radix sort is O(k&#183;n) for n keys which have k or fewer digits

Radix sort becomes slower as keys get bigger. The point about set length is that you want a known, small, *maximum* length.   </snippet></document><document><title>Probably an extremely basic question but...</title><url>http://www.reddit.com/r/compsci/comments/zcqtv/probably_an_extremely_basic_question_but/</url><snippet>what is the difference between Calculating &amp;amp; Computing?  There's no difference, they're synonyms.

But then, we may establish some connotations and nuances.

A calculator (or pocket calculator, "calculette" in French) would compute only mathematical operations, while a computer ("ordinateur" in French) would be able to compute also logical and symbolic operations or all kinds.

It depends also of the time when you use those words.  In the past, a computer was a person able to calculate. Big companies had rooms full of computers, calculating by hand accounting computations.  Between 1940 and the late 1950s, the first computers were called calculators (and indeed, were used only to compute numerical mathematical tables at first).  The French word "ordinateur" meaning computer has been defined in 1962 (it has other older meanings that are unrelated). So since the 1960's, we call them computers, because they are used to do more things than just calculating mathematical operations.

Since the 1990's, we started to call computers "phones" instead, because they're small, and you can use them to phone.  Lately, some of them are called "tablets", because they're flat.

</snippet></document><document><title>Does anyone know of machine learning techniques to sequence video frames?</title><url>http://www.reddit.com/r/compsci/comments/zadm7/does_anyone_know_of_machine_learning_techniques/</url><snippet>Say I have a collection of images that I know are stills from one or more videos.  Are there any techniques to probabilistically sort and sequence them to try to reproduce the original sequence(s)?

EDIT for more info on the problem:

What I have is a bunch of images that have been recovered from disk by file carving.  That means I don't have any file name information.  They appear to be stills from a recorded video.  I'm thinking it was one of those old school programs that would take a screenshot of a webcam every X seconds.  The images appear to be from multiple videos.  I want to try to sort and sequence these images.  It's certainly a non-trivial problem.  [deleted] [deleted] That was my first thought. If I'm not mistaken, certain video compression formats only tell the difference between frames instead of a whole frame. My only problem would be what if in the video the camera moved a bit and went back to its original position. *All* video formats use only the difference AFAIK. Do you know how ridiculously huge a video file would be if every frame was stored individually? Like, say you have 30 FPS video, consisting of bitmap frames, 8-bit color, 640x480. The size of 1 minute of video would be 1 byte times 640 times 480 times 30 FPS times 60 seconds. The end result is 552,960,000 bytes, or ~527.3 MB, or a little more than half a gigabyte. Not ideal at all! Especially considering modern framerates of 60 FPS and modern resolutions of 1920p. Kill me now if each frame is stored individually! 1080p *All* video formats use only the difference AFAIK. Do you know how ridiculously huge a video file would be if every frame was stored individually? Like, say you have 30 FPS video, consisting of bitmap frames, 8-bit color, 640x480. The size of 1 minute of video would be 1 byte times 640 times 480 times 30 FPS times 60 seconds. The end result is 552,960,000 bytes, or ~527.3 MB, or a little more than half a gigabyte. Not ideal at all! Especially considering modern framerates of 60 FPS and modern resolutions of 1920p. Kill me now if each frame is stored individually! There are widely used video formats that store every full frame. A lot of older cameras had jpg compression hardware, and the mjpg format is just essentially concatenated jpgs. Usually those videos are low res and have a time limit of 15 seconds or so. *All* video formats use only the difference AFAIK. Do you know how ridiculously huge a video file would be if every frame was stored individually? Like, say you have 30 FPS video, consisting of bitmap frames, 8-bit color, 640x480. The size of 1 minute of video would be 1 byte times 640 times 480 times 30 FPS times 60 seconds. The end result is 552,960,000 bytes, or ~527.3 MB, or a little more than half a gigabyte. Not ideal at all! Especially considering modern framerates of 60 FPS and modern resolutions of 1920p. Kill me now if each frame is stored individually! There are compressed intra-frame (eg ProRes), compressed inter-frame (eg H.264) and uncompressed (eg DNG) video formats. And yes, the raw formats can get rather large - 4 to 6MB per frame is quite normal for High Def footage.

Typically the footage is captured at the higher quality for use in post-production. Delivery formats are usually compressed to save time, storage and money.

So the inter-frame delta compression to which you refer is just one flavour of video. If nothing else, there are many formats and standards to choose from!   Depending on the activity in the scene and the time between images there may not be any continuity between successive images, think of a photo booth. Have you scanned the files for any Exif type data?
If they are close enough then a correlation approach could be used for sorting.     If the frames are at the same interval of time apart from each other you might try throwing those together into a video file and then using an optical flow interpolator (like Twixtor or whatnot) repeatedly to double the number frames each iteration.

If the frames are not even in time... you could still use the optical flow approach but I don't know of any pre-made tools that work with uneven frame intervals.

*Edit: For some reason I initially  missed the requirement that the frames start out of order and need to be sorted. I have no idea how you'd go about doing this.*    &amp;gt; It's certainly a non-trivial problem. 

Sounds like an NP-hard problem. 

EDIT:  [It is indeed NP-hard](http://www.reddit.com/r/compsci/comments/zadm7/does_anyone_know_of_machine_learning_techniques/c6351px), (and in fact NP complete), by trivial reduction from the Travelling Salesman Problem. I don't know why you were downvoted. You're probably right. I don't know why you were downvoted. You're probably right. &amp;gt; It's certainly a non-trivial problem. 

Sounds like an NP-hard problem. 

EDIT:  [It is indeed NP-hard](http://www.reddit.com/r/compsci/comments/zadm7/does_anyone_know_of_machine_learning_techniques/c6351px), (and in fact NP complete), by trivial reduction from the Travelling Salesman Problem. ~~No way in hell is this NP hard. This is effectively equivalent to sorting, but with difficult to define means of comparing different frames. The problem's solution is hard to define, but that doesn't mean that the problem is NP hard.~~ &amp;gt; No way in hell is this NP hard. 

Here is a reduction from the Travelling Salesman Problem:

Define a similarity metric F(x,y) which equals the similarity of objects (video frames) x and y.  

The problem is now:  given a set of objects X = (x1, x2, ..., xN), and a real number R, does an ordering y1, y2, y3, exist such that the sum from i = 2 to N of F(y(i-1), y(i)) is less than or equal to R?  (where there is a bijection between the y and the x, i.e. the sequence y1, y2, ... represents a re-ordering of the x1, x2, ...)  Clearly this is in NP. 

Now the map from TSP:  Let each vertex v in the graph G=(V,E) each correspond to a video frame, and let the weight on each edge e=(vi, vj) correspond to the value of F(xi, xj), where xi corresponds to vi and xj corresponds to vj.  The TSP problem is to find a path that visits each vertex once (corresponding to an ordered sequence of video frames), which has a distance less than or equal to a given minimum R (in this case, the same numeric value).

Therefore, if we are able to solve the video sequence problem, we are able to solve the Travelling Salesman Problem, within a polynomial reduction.  &#9633;

 Sorry, I didn't know what I was thinking earlier. I guess I didn't fully understand the problem or something. 

Edit: Although, what's nice in this case is that approximate solutions are good enough for this particular application. I'm not that familiar with this, but if a reduction to TSP exists for this problem, does that mean that the set of approximating algorithms could be used on this particular problem? In other words, ant colony optimization, etc?</snippet></document><document><title>The Maths Book of the Future</title><url>http://world.mathigon.org</url><snippet>  The best math book I ever saw was a grade 12 book published in the 60s. It was a perfect size to hold, got straight to the point, and did not have shiny pages or any of that crap they stick in the books now that only serve to make it more expensive.

That is what I want in a math book. Interactive stuff may have some place, but be careful in its application. Agreed.  The best math books I read were from the 50s and 60s.  They explain why things work in a concise manner.  The books now days take 50 pages to only show how so kids can pass the standardized tests without explaining the foundational concepts to understand why it works that way. There's been a LOT of educational advancement since the 60s - especially around mathematics and the sciences. The old style of texts may have worked for you, but have been shown time and time again to be difficult to learn from.

That's not to say that all textbooks now are fantastic. In fact some of the introductory physics texts I have to work with are complained about quite a bit by both faculty and students. We get review copies of new texts every now and then, and some of them are ridiculously bad. However, there are also some fantastic books that blend explanation, example, and challenging problems together. &amp;gt; The old style of texts may have worked for you, but have been shown time and time again to be difficult to learn from.

Serious question: Shown by whom, how? Do you have references? I do have some papers I can cite when I get to work (don't have the exact authors on my home computer.) 

However in general, science and technology education has been being solidly researched for the last 20 years (it goes further back than that, but IIRC the big developments started in the 90s) The University of Colorado has put out some good stuff on meta-cognition and learning styles. I'll update later with some sources.

 I do have some papers I can cite when I get to work (don't have the exact authors on my home computer.) 

However in general, science and technology education has been being solidly researched for the last 20 years (it goes further back than that, but IIRC the big developments started in the 90s) The University of Colorado has put out some good stuff on meta-cognition and learning styles. I'll update later with some sources.

 I do have some papers I can cite when I get to work (don't have the exact authors on my home computer.) 

However in general, science and technology education has been being solidly researched for the last 20 years (it goes further back than that, but IIRC the big developments started in the 90s) The University of Colorado has put out some good stuff on meta-cognition and learning styles. I'll update later with some sources.

 The best math book I ever saw was a grade 12 book published in the 60s. It was a perfect size to hold, got straight to the point, and did not have shiny pages or any of that crap they stick in the books now that only serve to make it more expensive.

That is what I want in a math book. Interactive stuff may have some place, but be careful in its application.   It looks really interesting and promising. However for the moment being, it's kind of "introductory". What I mean by that it is that it gives you a brief overview of some basic principles and ideas, but does not deeply go into them.

Take for example the part about sequences. It talks about what conversing and diversing functions are, but not how to determine of which kind a function is, and how to determine the limit.

Since this is still a new website I assume that they are already working on this cause and the likes. In short: looks promising! @DoctorProfessorson Thank you for your detailed feedback. We want to make most of the content accessible to GCSE / Grade 9 students, so it is difficult to go into too much detail. But of course we will continue to expand the articles and add more advanced content, including examples and exercises.

There will be more on the convergence of series (sums of sequences) in the article on 'Functions and Series' (to come). If you have any additional ideas please email info@mathigon.org. What about having a button that a hidden box with more detail if the student clicks on it?  


It might be nice to have an optional "Go deeper..." link to expose the detail to more advanced students or interested laypeople like us. That's a great idea, anachronic. But first we have to finish some of the other missing articles.    Does anyone else think the interface is way too cluttered and seizure inducing? [Sort of like this parody](http://www.mspaintadventures.com/?s=6&amp;amp;p=002725) and the following page. I think I could make parallax scrolling spacey images like that, too. But I wouldn't.  Wow this site seems really intuitive. I'm actually taking a Discrete Mathematics course right now and this seems like this could really help me out. Specifically the probability, codes and ciphers, combinatorics and logic courses. When is your projected completion ? @bbraunst We are pleased to hear that you like The World of Mathematics and we hope to finish the majority of the articles during the next month or two. Unfortunately the articles on probability are towards the bottom of our to-do-list, especially because we want them to include many interactive games and probability experiments. The articles on Combinatorics and Logic should be finished during the next couple of weeks.

If you have any suggestions for particular topics to be included, or if you even want to contribute, please email info@mathigon.org! @bbraunst We are pleased to hear that you like The World of Mathematics and we hope to finish the majority of the articles during the next month or two. Unfortunately the articles on probability are towards the bottom of our to-do-list, especially because we want them to include many interactive games and probability experiments. The articles on Combinatorics and Logic should be finished during the next couple of weeks.

If you have any suggestions for particular topics to be included, or if you even want to contribute, please email info@mathigon.org!      No. 

The textbook of the future will still be a textbook. One or two column layout, inline or wrapped figures, with sections of setup-example-explaination-exercise. In the future the figures may be interactive, or you may place this side-by-side with an interactive environment (e.g. Mathematica) on an interactive device, but it will still be a book.

Perhaps this might be a good supplement, or a good reference to have side-by-side.  Why do you say that? I doubt this example will be the one that heralds the transition to the new format - but I don't see why most textbooks won't transition to electronic formats within the next decade or two...

If I could have all of my textbooks on a color e-ink tablet that has a touch screen interface.. yeah, I'd jump on that in a heartbeat. Of course -- ebook would be great. With very *sparse* use of color. In fact, I think that's the ideal world; an e-book side by side with a worksheet environment. That's why I mentioned Mathematica, but Maple would do, even a Python repl in a pinch.

I'd jump on it too. </snippet></document><document><title>supermetacompilers [pdf]</title><url>http://reference.kfupm.edu.sa/content/s/e/a_self_applicable_supercompiler_77986.pdf</url><snippet>  Super Meta Compilation HD Turbo X-treeeeeeeeme! Super Meta Compilation HD Turbo X-treeeeeeeeme! Based on your reply, I was expecting the paper to be a spoof. Was disappoint.

/s - actually, looks like a nice paper, even if just for the citations, to get up to speed on partial evaluation methods. I must be missing the font though, because the kerning was atrocious! You mean keming. I never, in my life, expected to laugh at a typography joke. 

Damn good work. You mean keming.</snippet></document><document><title>Why is MATLAB inefficient compared to C/C++ and Fortran?</title><url>http://www.reddit.com/r/compsci/comments/z8f1u/why_is_matlab_inefficient_compared_to_cc_and/</url><snippet>In our Numerical Analysis class, we are going to be using only MATLAB to do calculations. One of the first problems we solved was approximating e^(x) where x = 0.5. The professor gave us a piece of code that was rather inefficient and we had to make it more efficient. No problem there.

He asked us to think and write about why MATLAB is inefficient in doing calculations like the one above compared to code written in C/C++ or FORTRAN. We were stumped. It was assigned as a "group worksheet" where we could bang our heads together to come up with the answer and none of us really have any low-level programming experience. Two of us (including myself) have taken computational physics classes where we have written tons of code in multiple programming languages. But these languages were just tools for us and we didn't study them in-depth.

 He mentioned something about interpretive languages vs. compiled languages, but I'm not sure what he meant by that. He also asked us to look up "JIT" for MATLAB and start from there but I don't quite understand the whole compilation deal.

So, why is MATLAB inefficient in doing such simple computations? I am assuming that it is rather efficient in doing matrix algebra compared to C/C++.

Thank you for your help, in advance and this is not a real priority since we are taking a math class. I think the question was posed to help us get an understanding why we shouldn't use MATLAB for every single problem we come across.   Imagine your office has to collaborate with a foreign office. For the sake of an example, I'll assume your office's native language to be English and the foreign language to be German.

Now there are two obvious possibilities: each email you send to the German office, you run through Google Translate to send in German and they puzzle out what it means and each email you receive back you run through Google Translate back from German. Every so often you might need a couple of round trips to get the gist of what is being asked. This approach has flexibility, any member of your office can provide input, but is slow because all communication is bottle-necked through a translation layer. This is strategy A.

An alternative approach is to train up a native German speaker in the work your office does. Then there is no translation bottleneck and round-trips to reduce confusion/increase clarity are reduced. This is less flexible and requires a ahead-of-time training of the native speaker but this is a constant time overhead so as time goes on you'll win. This is strategy B.

One might also get the native speaker to come to you for specific training as soon as they get a question from the German office they can't answer. This removes the ahead-of-time overhead from training the native speaker but introduces the odd slow down in communications since every so oftern the native speaker has to stop what they're doing and get some training 'just in time' to answer the German office's question. This is strategy C.

None of A, B or C are inherently superior but they will be better suited to particular needs. For example if training a new person in the work of your office takes more time than is wasted by using Google Translate, strategy A works.

In general you might start out with strategy A and as time progresses and your working patterns with the German office become better known it might prove simple enough to train a native speaker.

MATLAB is strategy A, C/C++ is strategy B and JIT-ed languages like .NET/Java are strategy C. Which you choose depends at what point on the initial cost vs. ongoing cost curve you want to occupy. Hah, glad to see I'm not the only one using the translator metaphor for explaining JITs. FWIW here's how I usually present it:

You are going to Japan to give a series of lectures, in Japanese. You don't speak japanese. You have a few options:

a) You have someone translate it for you before leaving your home. This takes the least work for them, but gives you no flexibility in changing what you're saying on the spot.

b) You bring a translator with you, and he or she translates everything you say, as you say it. This gives you complete flexibility, but it's a bit slower and a lot of work.

c) Finally, you bring a translator with you, he or she translates what you say, but also writes down the translation. The next time you give your talk you can just read their translation unless you want to change something, in which case they translate. This gives you all the flexibility of choice (b), and most of the convenience of (a), it's just a little worse on the first talk, and you pay a small price if you ever do decide to deviate from your planned remarks. Imagine your office has to collaborate with a foreign office. For the sake of an example, I'll assume your office's native language to be English and the foreign language to be German.

Now there are two obvious possibilities: each email you send to the German office, you run through Google Translate to send in German and they puzzle out what it means and each email you receive back you run through Google Translate back from German. Every so often you might need a couple of round trips to get the gist of what is being asked. This approach has flexibility, any member of your office can provide input, but is slow because all communication is bottle-necked through a translation layer. This is strategy A.

An alternative approach is to train up a native German speaker in the work your office does. Then there is no translation bottleneck and round-trips to reduce confusion/increase clarity are reduced. This is less flexible and requires a ahead-of-time training of the native speaker but this is a constant time overhead so as time goes on you'll win. This is strategy B.

One might also get the native speaker to come to you for specific training as soon as they get a question from the German office they can't answer. This removes the ahead-of-time overhead from training the native speaker but introduces the odd slow down in communications since every so oftern the native speaker has to stop what they're doing and get some training 'just in time' to answer the German office's question. This is strategy C.

None of A, B or C are inherently superior but they will be better suited to particular needs. For example if training a new person in the work of your office takes more time than is wasted by using Google Translate, strategy A works.

In general you might start out with strategy A and as time progresses and your working patterns with the German office become better known it might prove simple enough to train a native speaker.

MATLAB is strategy A, C/C++ is strategy B and JIT-ed languages like .NET/Java are strategy C. Which you choose depends at what point on the initial cost vs. ongoing cost curve you want to occupy. What does Germany have to do with LinPack?

Germany did rocket science.  America did Computers.  I got an A in nonlinear controls.  Lapunov was Russian.

I wrote LoseThos, 135,000 LOC kernel compiler graphics library, editor boot loader.  That was easy.  I wrote SimStructure for rocket science.  Rocket science is hard.  Download both at http://www.losethos.com


DID YOU FUCKEN READ!!!  MATLAB IS LINPACK!

http://www.mathworks.com/company/newsletters/news_notes/clevescorner/winter2000.cleve.html hmm...not sure if troll Look at the site!  It's mathworks, maker of MATLAB!
http://www.mathworks.com/company/newsletters/news_notes/clevescorner/winter2000.cleve.html

You can look at my ASU transcripts at the LoseThos site.  I studied control theory for 2 years.  Not especially good at it.  Without giving you the straight answer:

Think about what he mentioned.  How does MATLAB work?  How does MATLAB run on your computer?  What about the other computer (other operating system, 32-bit vs 64-bit)?

Who made MATLAB?  What is MATLAB?  Why do you code in MATLAB then run?  Why does your MATLAB code work on your computer (lets say Windows) then work on another computer (lets say Mac) with the same version of MATLAB?

If your still stumped, look at how Java works and how it runs on multiple platforms with little to no modification to the code. Java isn't that slow though, with recent optimizations. It's much faster than MATLAB.  Java isn't that slow though, with recent optimizations. It's much faster than MATLAB.  Isn't matlab build with java?  At least the gui is.  ~~I believe the entire language is, in order to make it easier to move it from one platform to another.  I know for sure that Octave (the open source implementation) is written in java.~~

EDIT: I'm wrong

EDIT 2: I'm doubly wrong, don't listen to me. There's a -nojvm switch to matlab that only turns off the GUI, some plotting stuff, and Java interop, so that seems unlikely. ~~I believe the entire language is, in order to make it easier to move it from one platform to another.  I know for sure that Octave (the open source implementation) is written in java.~~

EDIT: I'm wrong

EDIT 2: I'm doubly wrong, don't listen to me. Java isn't that slow though, with recent optimizations. It's much faster than MATLAB.  Without giving you the straight answer:

Think about what he mentioned.  How does MATLAB work?  How does MATLAB run on your computer?  What about the other computer (other operating system, 32-bit vs 64-bit)?

Who made MATLAB?  What is MATLAB?  Why do you code in MATLAB then run?  Why does your MATLAB code work on your computer (lets say Windows) then work on another computer (lets say Mac) with the same version of MATLAB?

If your still stumped, look at how Java works and how it runs on multiple platforms with little to no modification to the code. I don't see how this is better tips. What the OP has posted is better than this. And the "tip" to look to Java instead? Java is more complex than Matlab in terms of inner working. Why would the OP has better chance of understanding Java than understanding Matlab?

To the OP, look to the other responses below for much better tips. (At the moment, this comment is top comment.)  Here's a great example of some of the overheads certain languages impose on computations like matrix multiply (skip about 20 slides in). 

http://beowulf.lcs.mit.edu/18.337-2009/lectslides/Intro_and_MxM.pdf

This example compares java with C then assembly, but many of the same lessons apply to matlab. Here's a great example of some of the overheads certain languages impose on computations like matrix multiply (skip about 20 slides in). 

http://beowulf.lcs.mit.edu/18.337-2009/lectslides/Intro_and_MxM.pdf

This example compares java with C then assembly, but many of the same lessons apply to matlab. Here's a great example of some of the overheads certain languages impose on computations like matrix multiply (skip about 20 slides in). 

http://beowulf.lcs.mit.edu/18.337-2009/lectslides/Intro_and_MxM.pdf

This example compares java with C then assembly, but many of the same lessons apply to matlab.  The key to getting good performance in MATLAB is to do things the "MATLAB" way so that it can optimize the code very well.  MATLAB is designed to optimize vector calculations - i.e. doing the same operation to a lot of things at the same time.

If you are just doing some kind of simple processing MATLAB won't have extremely fast routines written in FORTRAN/C/C++ under the hood that it will use, so you won't get great performance out of it.  If you write MATLAB code that MATLAB can optimize well, it can be on par with compiled languages.   In the end, even though the MATLAB code has to be interpreted, it results into the same processor system architecture operations. What this means, is that after the MATLAB has processed the code, the final execution of that code will be just as fast as machine code compiled with languages like C/C++.

Computers nowadays are very fast and JIT compilers with their respective languages are improved to an outstanding level of quality. The major drawbacks that made these kind of programs slow a couple of years ago are in practise neglible. The loss in speed nowadays only matters if you're working on a very low level. For 'simple' mathematic problems it shouldn't matter what you pick, and I would even recommend using MATLAB instead of C/C++ as it removes a lot of frustration, given it's specifically designed for computations.

Booting MATLAB code up and getting it to run may take a bit longer than with a compiled language, but we're talking about milliseconds here, and once it's running it's about just as fast. The overhead is neglible.

**Edit**

I forgot to say that JIT compilers are so advanced, that they can match the to-interpeted code to your machine and optimise it better than some normal compilers can. This could even mean in theory that it's faster than a compiled language. In the end, even though the MATLAB code has to be interpreted, it results into the same processor system architecture operations. What this means, is that after the MATLAB has processed the code, the final execution of that code will be just as fast as machine code compiled with languages like C/C++.

Computers nowadays are very fast and JIT compilers with their respective languages are improved to an outstanding level of quality. The major drawbacks that made these kind of programs slow a couple of years ago are in practise neglible. The loss in speed nowadays only matters if you're working on a very low level. For 'simple' mathematic problems it shouldn't matter what you pick, and I would even recommend using MATLAB instead of C/C++ as it removes a lot of frustration, given it's specifically designed for computations.

Booting MATLAB code up and getting it to run may take a bit longer than with a compiled language, but we're talking about milliseconds here, and once it's running it's about just as fast. The overhead is neglible.

**Edit**

I forgot to say that JIT compilers are so advanced, that they can match the to-interpeted code to your machine and optimise it better than some normal compilers can. This could even mean in theory that it's faster than a compiled language. JIT are trivial.  I wrote one for LoseThos.  I thought it was hard, but it's not.

Optimization is not needed anymore because the CPU does it.

You won't believe me, you little shit.  </snippet></document><document><title>How are large numbers stored in a system?</title><url>http://www.reddit.com/r/compsci/comments/z7e27/how_are_large_numbers_stored_in_a_system/</url><snippet>This has been bothering me for a while now, and I'm not sure how to find the answer: how are large numbers stored in a system? For example, take Linux: you use the command 'ssh-keygen' and create keys using the RSA algorithm. The key values are just stored in text in files, ie the 'id_rsa' file for the private key, but obviously there were some operations performed on very large numbers to generate these keys. How does Linux work with these incredibly large numbers when it generates the keys and also when it encrypts/decrypts using the keys? I mean, raising some arbitrary number to the power of say 65537 for encryption or raising some cipher-text to the power of some 1024-bit number is obviously more than say a 64-bit integer value can store. 

The only other examples I can think of would be a scientific calculator and a programming language like Java (the java.math.BigInteger class). 

Any insight from /r/compsci? 

EDIT: thanks to everyone for all the replies, this has been most enlightening.   There are multiple representations, each with their own tradeoff. First off, normally we typically about large numbers as Big Numbers, bignum for short. Bignums can be unbounded/arbitrary in size (native to LISP and Python languages), or bounded depending on the application.

We treat bignums as a sequence of digits. We can then store the digits in a vector or linked-list.

To be space (and time) efficient, you would want to use a large base. A good size for a 32-bit architecture might be 2^31 or 2^32.

From there, you can perform addition, subtraction, multiplication, division, modulus (a side-effect of division algorithm), modular exponentiation, modular multiplication, etc. over the data structure chosen. Multiplication and division tend to be expensive, and there are many algorithms which perform better than grade-school multiplication algorithm (what we were taught in school using shift and add).

The above, only serves to give an idea on how bignums can be implemented. There are many ways to do it, each with their own tradeoffs, and a plethora of algorithms and optimizations.

Wikipedia has a nice list of the [complexity for various operations](http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations). It's normally in terms of the number of digits. This immediately implies that a higher base results in more efficiency since it reduces the number of digits to represent a number. Of course, that's in the abstract -- the reality is your choice of base will always depend on the platform. I'd imagine the best size for an n-bit architecture would be a n/2 base system. That way you could perform multiplication with causing an overflow.

I wonder why BigNumber libraries use arrays of bytes then. n-bit architecture have often instructions to do the multiplication n-bit x n-bit -&amp;gt; 2n-bit results and the division 2n-bit by n-bit giving n-bit quotient and n-bit remainder. In that case, n-bit digits make sense. Note that if you care about decimal representation, using a base which is a power of 10 make sense even if you loose some density.

BigNum libraries which use arrays of bytes are proof of concept or student exercises, not for production. Yes.  On x86, the result of MUL is placed in two registers.  The dividend of the DIV instruction is taken from two registers.  If you were to program bignum mul/div in a HLL like C then you would have to use half word digits.  Bignum arithmetic is therefore a good example of the shortcomings of HLLs and why ASM is still important. Yes.  On x86, the result of MUL is placed in two registers.  The dividend of the DIV instruction is taken from two registers.  If you were to program bignum mul/div in a HLL like C then you would have to use half word digits.  Bignum arithmetic is therefore a good example of the shortcomings of HLLs and why ASM is still important. I'd imagine the best size for an n-bit architecture would be a n/2 base system. That way you could perform multiplication with causing an overflow.

I wonder why BigNumber libraries use arrays of bytes then. I'd imagine the best size for an n-bit architecture would be a n/2 base system. That way you could perform multiplication with causing an overflow.

I wonder why BigNumber libraries use arrays of bytes then. There are multiple representations, each with their own tradeoff. First off, normally we typically about large numbers as Big Numbers, bignum for short. Bignums can be unbounded/arbitrary in size (native to LISP and Python languages), or bounded depending on the application.

We treat bignums as a sequence of digits. We can then store the digits in a vector or linked-list.

To be space (and time) efficient, you would want to use a large base. A good size for a 32-bit architecture might be 2^31 or 2^32.

From there, you can perform addition, subtraction, multiplication, division, modulus (a side-effect of division algorithm), modular exponentiation, modular multiplication, etc. over the data structure chosen. Multiplication and division tend to be expensive, and there are many algorithms which perform better than grade-school multiplication algorithm (what we were taught in school using shift and add).

The above, only serves to give an idea on how bignums can be implemented. There are many ways to do it, each with their own tradeoffs, and a plethora of algorithms and optimizations.

Wikipedia has a nice list of the [complexity for various operations](http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations). It's normally in terms of the number of digits. This immediately implies that a higher base results in more efficiency since it reduces the number of digits to represent a number. Of course, that's in the abstract -- the reality is your choice of base will always depend on the platform. How would it be any more efficient to use a 2^32 -base number system instead of binary?  There are multiple representations, each with their own tradeoff. First off, normally we typically about large numbers as Big Numbers, bignum for short. Bignums can be unbounded/arbitrary in size (native to LISP and Python languages), or bounded depending on the application.

We treat bignums as a sequence of digits. We can then store the digits in a vector or linked-list.

To be space (and time) efficient, you would want to use a large base. A good size for a 32-bit architecture might be 2^31 or 2^32.

From there, you can perform addition, subtraction, multiplication, division, modulus (a side-effect of division algorithm), modular exponentiation, modular multiplication, etc. over the data structure chosen. Multiplication and division tend to be expensive, and there are many algorithms which perform better than grade-school multiplication algorithm (what we were taught in school using shift and add).

The above, only serves to give an idea on how bignums can be implemented. There are many ways to do it, each with their own tradeoffs, and a plethora of algorithms and optimizations.

Wikipedia has a nice list of the [complexity for various operations](http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations). It's normally in terms of the number of digits. This immediately implies that a higher base results in more efficiency since it reduces the number of digits to represent a number. Of course, that's in the abstract -- the reality is your choice of base will always depend on the platform. &amp;gt;Wikipedia has a nice list of the complexity for various operations[1] . It's normally in terms of the number of digits. This immediately implies that a higher base results in more efficiency since it reduces the number of digits to represent a number.


No it doesn't and that isn't how it works. To quote 
&amp;gt;complexity refers to the time complexity of performing computations on a multitape Turing machine.

In addition changing the radix has other effects on complexity as well. Turing machines work off binary information, one bit(eg digit) at a time. If you change the radix you suddenly have to account for the increase in complexity of dealing with that.

 http://en.wikipedia.org/wiki/Radix_economy 


BigNum implementations vary but the ones I have seen translate the numbers into strings. Which in Java at least means you get into issues like 0.0 != 0 unless you use the correct comparator. I'm not all that great or up to snuff with complexity theory, but this warrants a response. I think it's good you brought up the complexity aspect, but there seems to be some misconceptions about the [definition of a Turing Machine](http://en.wikipedia.org/wiki/Turing_machine#Formal_definition).

In general, a TM, and all variants allow for arbitrary alphabet. Time complexity is defined as the number of operations (transitions) performed before a TM halts.

For a TM with a binary alphabet, grade-school multiplication of two k-bit integers will require O(k^2) time. However, the same TM with an alphabet of {0,...,2^(k-1)} can multiply two k-bit integers in O(1) (just using transitions).

This is why the wikipedia page can represent complexity in terms of a digit, tacitly assuming the alphabet of the underlying TM can represent a digit of arbitrary size.

As for Radix economy. I've never come across it in my studies, and while admittedly interesting, the complexity of using a higher base is dealt with by the underlying platform.

Like I said, complexity theory is not something I'm well-versed in, and hopefully a CS theory guru can step in and sort out this misunderstanding. Complexity is not, in fact, defined as DasWood has declared above; it a relation between the number of occurrences of a particular operation or operations and some property of the input. Note that the precise operation measured and the precise input property considered are *not* part of that definition, and indeed, many complexity analyses use something other than Turing machine steps and input bits; most number-theoretic algorithms have complexity given in terms of input *magnitude* (i.e., 2&#8319; where n is the number of input bits).

Radix economy is utterly irrelevant to this discussion because it requires a difference in the cost of handling numbers of different radices, e.g. a bit being more efficiently handled than a word, which does not hold on modern architectures.

Furthermore, only a catastrophically bad BigNum would store data as textual strings, and the special case of BigNums which store decimal fractions as such are usually called Decimal (or BigDecimal) and *still* don't use strings if they're any good. Every decent BigNum uses contiguous arrays of words to store numbers.  In many cases, you don't actually have to raise a number to the power of 65537 because the next step will involve dividing it by something, just as one example.  It's stored exactly as you'd store it on paper, add more digits until you got the whole number down. Operations with more digits become more complex (as the CPU is only able to operate on a fixed number of bits at a time, (typically 8 - 128 bits), but it's not exactly a huge deal for 1024 bit keys. For most cipher-texts, the algorithms work on discrete blocks of the text. In practice, to store the number (which the computer represents internally in binary, of course) in a copy-and-paste-friendly format, it is stored encoded as base64 in your id_rsa. (Don't copy-and-paste your id_rsa anywhere, though, only the id_rsa.pub!)

If you want to try this out yourself, there are libraries available like GMP, which allow you to work with arbitrarily large numbers. It's usually slow compared to working with smaller numbers, but still fast enough for most practical purposes. &amp;gt; (Don't copy-and-paste your id_rsa anywhere, though, only the id_rsa.pub!)

hunter2  If you were a kid in the 80's, you would know because everybody programmed in 8-bit assembly language.  (If you know how to do 16 or 32 bit math on an 8-bit, you know how to do arbitrarily large integer math.)

The 6502 8-bit CPU had no DIV or MUL instruction, so you did them by hand in binary.  I know how to do MUL and DIV with shift/compare/add/sub but I think you want to use a MUL if it's available to make bigger ints.

Believe it or not, doing it in ASM is the easiest because you have the carry available.
 Doing 32bit math (for example) on an 8bit computer is called multiple precision arithmetic.  It's not hard to add routines to turn that into arbitrary precision arithmetic, though.  And, yes, assembly is best for this, otherwise you have to use less-than-a-word digits to handle overflows. I just said that.  Echo?  Here's [a really simple implementation](https://gist.github.com/3221201) of part of a bigint library I wrote recently for /r/dailyprogrammer. It can do addition and multiplication on arbitrarily-sized unsigned integers. The numbers are stored as a series of 16-bit "digits" in memory. Very cool. Can you elaborate on `iterate()`? I'm not familiar with square-root algorithms.

Also, to compute 100000 base-10 digits, how did you determine the number of iterations?          </snippet></document><document><title>a variety of interesting articles</title><url>http://matt.might.net/articles/</url><snippet /></document><document><title>Baking Pi - Operating Systems Development</title><url>http://www.cl.cam.ac.uk/freshers/raspberrypi/tutorials/os/</url><snippet>   This was written by a friend of mine. At 19 years old, he's just finished his first year of Comp Sci with Maths at Cambridge. Alex Chadwick, who wrote this, is totally awesome. He's one of the students we've had interning over the summer at the lab working on Raspberry Pi stuff. Do check out the other projects too (I think there are some updates to the site still to go out, e.g. some of the downloads on Alex's course aren't working - plus not all the other projects are up yet).

EDIT to add: I should really highlight that Alex wrote his own USB driver with HID support. We're hoping lots of people will want to steal this relatively small piece of code (small compared to the Synopsys dwc_otg) and use it for basic input in minimal OS projects. Alex Chadwick, who wrote this, is totally awesome. He's one of the students we've had interning over the summer at the lab working on Raspberry Pi stuff. Do check out the other projects too (I think there are some updates to the site still to go out, e.g. some of the downloads on Alex's course aren't working - plus not all the other projects are up yet).

EDIT to add: I should really highlight that Alex wrote his own USB driver with HID support. We're hoping lots of people will want to steal this relatively small piece of code (small compared to the Synopsys dwc_otg) and use it for basic input in minimal OS projects.  I only skimmed through the lessons, but do they actually go into the core details about OS development? i.e. real mode vs protected, boot loaders, loading programs, multitasking and so forth. Those are the really interesting bits about OS development imo. Anyway, [here's](http://www.osdever.net/tutorials/) few OS dev lessons for anyone interested. I only skimmed through the lessons, but do they actually go into the core details about OS development? i.e. real mode vs protected, boot loaders, loading programs, multitasking and so forth. Those are the really interesting bits about OS development imo. Anyway, [here's](http://www.osdever.net/tutorials/) few OS dev lessons for anyone interested.    I wonder if I could follow along with an emulator?  

Edit: better emulator link: http://wiki.meego.com/SDK/Qemu I'm trying it right now with qemu.

I have no idea if it's working yet (especially since the first 5 chapters make LEDs blink, which you won't see on an emulator).

Had a heck of a time getting qemu to work correctly on OS X 10.8 correctly though.

---------------

**EDIT** Yeah, can't get it to display anything. Either I'm doing something horribly wrong (probable), or it just won't work with qemu. It won't work without lots of changes. The course is specifically targeted at the Raspberry Pi, and I don't think an emulator exists for it.  This is very cool, wish I had a Pi.   [deleted]  [deleted]</snippet></document><document><title>Quotient filter: A new (2007) probabilistic
data structure</title><url>http://en.wikipedia.org/wiki/Quotient_filter</url><snippet>  So, what exactly makes this different from a hash table?  That it only stores the 'hash' without keys or values?

I ask because if I saw this used somewhere or implemented something like this, I'd probably call it a half-a-hash-table. </snippet></document><document><title>An argument against call/cc</title><url>http://okmij.org/ftp/continuations/against-callcc.html</url><snippet /></document><document><title>Does anyone know of any internet radio stations that are focuses around computer programming? </title><url>http://www.reddit.com/r/compsci/comments/z2hqv/does_anyone_know_of_any_internet_radio_stations/</url><snippet>I would love to listen to news, topics and content about computer programming in a discussion style radio station while I do work. So my brain can think about programming without reading. I was wondering if anyone knows of audio books or internet radio or something like this that I might be able to use? I love to hear about most languages and concepts, so anything would be great.

Edit: I accidentally a word in title, sry.  You mean podcasts.

http://geekswithblogs.net/mbcrump/archive/2010/06/15/10-best-programming-podcast-2010-edition.aspx You mean podcasts.

http://geekswithblogs.net/mbcrump/archive/2010/06/15/10-best-programming-podcast-2010-edition.aspx  How about [Software Engineering Radio](http://www.se-radio.net/)? yeah, se-radio is decent. you might want to check http://teachmetocode.com/ as well.  A very good podcast not included in the list mentioned elsewhere in answers to this post:
http://itc.conversationsnetwork.org/  I don't think you'll find much (at least I haven't) in ways of radio broadcasts. However, you might want to just listen to programming talks.  You should check out [Build &amp;amp; Analyze](http://5by5.tv/buildanalyze), co-hosted by Marco Arment, the guy who made [Instapaper](http://instapaper.com). They talk about both really technical stuff, general self-employed&#8211;developer considerations, app development politics, their hatred for the patent system, and coffee makers and toasters.

You might also like [Hypercritical](http://5by5.tv/hypercritical), with by John Siracusa, where they, as the name suggests, complain about stuff. They touch on all sorts of stuff, both incredibly technical and layman accessible. And we all know there's enough out there to complain about.

In both cases, just jump in at the latest episode, and if you like it consider taking it from the top. And check out the other stuff on [5by5.tv](http://5by5.tv); you won't regret it. Especially [Back to Work](http://5by5.tv/b2w).  musicforprogramming.net   How bout http://coding.fm/ ? How bout http://coding.fm/ ? Doesn't seem to work for me.

Unless the joke is that you get the most done with no music? You're not listening closely enough. I suggest you try the angry dev stream, that may explain things.  Not working in chrome, but does work in firefox          So, people can work while listening people talk. Woaw. If I was a native english speaker, could I do that too? I can't, so... Maybe? Can you do that in your native language? (What is your native language?)</snippet></document><document><title>Place to know about coding challenges, competitions?</title><url>http://www.reddit.com/r/compsci/comments/z370g/place_to_know_about_coding_challenges_competitions/</url><snippet>CompSci enthusiast here who doesn't want to miss a learning opportunity. I already tried Google but would also like to know what people have to say/share.  Weekly | Bi-weekly competitions:

* [TopCoder](http://www.topcoder.com)

* [Codeforces](http://www.codeforces.com)


Monthly:

* [Codechef](http://www.codechef.com)


Annual:

* [Google Code Jam](http://code.google.com/codejam/)

* [Facebook Hacker Cup](http://www.facebook.com/hackercup)


[Programming Contest Calendar](http://www.codingdoor.com)

Will add more as I remember :)  [TopCoder](http://www.topcoder.com/) is the first thing that comes to mind re: coding challenges.    http://uva.onlinejudge.org/
http://ProjectEuler.net
http://PythonChallenge.com
/r/bacongamejam
 http://uva.onlinejudge.org/
http://ProjectEuler.net
http://PythonChallenge.com
/r/bacongamejam
     </snippet></document><document><title>Join us in reading "G&#246;del, Escher, Bach" this fall! [x-r/GEB]</title><url>http://www.reddit.com/r/compsci/comments/z04u3/join_us_in_reading_g&#246;del_escher_bach_this_fall/</url><snippet>Hello /r/compsci-entists!

If you're not familiar with it, *[G&#246;del, Escher, Bach: An Eternal Golden Braid](http://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach)* is a famous book by Douglas Hofstadter that playfully weaves together such fields as comp-sci, biology, cog-sci, psychology, art, music, literature, philosophy, mathematics, comp-sci, etc. in order to explore the nature of consciousness, meaning, and truth.

The plan is to read this over a period of 12-14 weeks beginning around September 7 and finishing in early December. We'll have discussions in the /r/GEB subreddit and regular meetings via IRC.

If you're interested, indicate so on [the signup thread here](http://www.reddit.com/r/GEB/comments/yzmvj/starting_fall_readthrough/). I hope you can join us!  I started reading it, got to about page 180...it's HARD to read, and you almost have to read it slowly to digest it. It's no doubt interesting but it's not for those wanting a casual experience. I started reading it, got to about page 180...it's HARD to read, and you almost have to read it slowly to digest it. It's no doubt interesting but it's not for those wanting a casual experience. Good book, but definitely not a page turner.  Good book, but definitely not a page turner.  I started reading it, got to about page 180...it's HARD to read, and you almost have to read it slowly to digest it. It's no doubt interesting but it's not for those wanting a casual experience. I started reading it, got to about page 180...it's HARD to read, and you almost have to read it slowly to digest it. It's no doubt interesting but it's not for those wanting a casual experience. I started reading it, got to about page 180...it's HARD to read, and you almost have to read it slowly to digest it. It's no doubt interesting but it's not for those wanting a casual experience.           12-14 weeks? How long is it? That seems a bit excessive.     Anyone who says it's a good book hasn't actually read it.  Imagine Morpheus saying...    
What if I told you    
I read it 20 years ago.
    
Then the knights who say nee follow with...    
Bring us a Crab Cannon.

 What if I told you that you're in the wrong subreddit? my undergrad is CS... And my humor is Monty Python... Eh, I was talking more about the use of memes.</snippet></document><document><title>Why are there no (few?) computer science crackpots? Other disciplines seem to have no shortage.</title><url>http://www.reddit.com/r/compsci/comments/yzw66/why_are_there_no_few_computer_science_crackpots/</url><snippet>I am sure physicsts and mathematicians can all tell you stories of getting letters containing plans for perpetual motion machines, proofs of fermat's last theorum, faster than light travel, etc. Tell me about comp sci crackpots!

I don't really mean "buy my vaporware console" but real science crackpot stuff like impossible algorithms etc  If you follow infosec, you will find no shortage of cooks who claim to eliminate 'all practical security vulnerabilities' what this tweak or that sandbox. Or people who find security vulnerabilities that can only be exploited if you have root to begin with. See Raymond Chen's "airtight hatchway" Sometimes you only have root access to certain commands and can't execute arbitrary code as root.  In those cases an exploit that gives you the ability to spawn a root shell is pretty bad, though admittedly this is probably a very uncommon situation and misconfiguring sudo is a more common issue. Oops. I didn't clarify - these are code injection issues and only boil down to stupidity once you strip away a lot of obfuscation. See [this link]( http://blogs.msdn.com/b/oldnewthing/archive/2006/05/08/592350.aspx) that I was too lazy to find earlier Not exactly convinced by his point, as it makes a lot of assumptions about the context in which a program might be executed. A few small changes in the context, and suddenly your "it's just a harmless bug" turns into "instant root access for everybody". It's much better to just assume every code injection is critical then to search for excuses why it might not be. Sometimes you only have root access to certain commands and can't execute arbitrary code as root.  In those cases an exploit that gives you the ability to spawn a root shell is pretty bad, though admittedly this is probably a very uncommon situation and misconfiguring sudo is a more common issue. If you follow infosec, you will find no shortage of cooks who claim to eliminate 'all practical security vulnerabilities' what this tweak or that sandbox. I can in fact eliminate all security vulnerabilities!  Just follow these simple instructions.

Step 1) Shut off your wifi and unplug your ethernet and modem cables, if any.

Step 2) Remove the power cord and battery from your machine.

These steps are adequate for any form of electronic attack, but there is still the concern of those pesky physical attacks.  If you would like to prevent them as well, continue with these simple instructions:

Step 3) Place machine in large ceramic pot.

Step 4) Acquire some thermite.

Step 5) Douse machine liberally with thermite, and ignite with a strip of magnesium.

Congratulations, your data is now secure.  And you have a new block of iron to use as a door stop!  Bonus!





 Ha! You fool! As you put your machine in the large ceramic pot, you didn't see the infra-red security camera I placed behind the plant. Using the reflected image of your hard drive from the mirror, I defragmented the data through a CAT-7 thermocouple and de-obfuscated the hash of your password. BINGO, your credit card data is mine! I got a below-minimum-wage job at a restaurant.

BINGO, your credit card data is mine! Ha! You fool! As you put your machine in the large ceramic pot, you didn't see the infra-red security camera I placed behind the plant. Using the reflected image of your hard drive from the mirror, I defragmented the data through a CAT-7 thermocouple and de-obfuscated the hash of your password. BINGO, your credit card data is mine! Ha! You fool! As you put your machine in the large ceramic pot, you didn't see the infra-red security camera I placed behind the plant. Using the reflected image of your hard drive from the mirror, I defragmented the data through a CAT-7 thermocouple and de-obfuscated the hash of your password. BINGO, your credit card data is mine! I can in fact eliminate all security vulnerabilities!  Just follow these simple instructions.

Step 1) Shut off your wifi and unplug your ethernet and modem cables, if any.

Step 2) Remove the power cord and battery from your machine.

These steps are adequate for any form of electronic attack, but there is still the concern of those pesky physical attacks.  If you would like to prevent them as well, continue with these simple instructions:

Step 3) Place machine in large ceramic pot.

Step 4) Acquire some thermite.

Step 5) Douse machine liberally with thermite, and ignite with a strip of magnesium.

Congratulations, your data is now secure.  And you have a new block of iron to use as a door stop!  Bonus!





 Either I didn't get the memo that thermite comes in liquid form, or you didn't get the memo that "douse" is a liberal liquid-specific covering. You can douse something with a powder. It still means to cover thoroughly.  Verb:	
Pour a liquid over; drench: "he doused the car with gasoline and set it on fire".
Extinguish (a fire or light): "stewards appeared and the fire was doused".



douse&#8194; &#8194;[dous]  Show IPA verb, doused, dous&#183;ing, noun
verb (used with object)
1.
to plunge into water or the like; drench: She doused the clothes in soapy water.
2.
to splash or throw water or other liquid on: The children doused each other with the hose.
3.
to extinguish: She quickly doused the candle's flame with her fingertips.
4.
Informal . to remove; doff.
5.
Nautical .
a.
to lower or take in (a sail, mast, or the like) suddenly.
b.
to slacken (a line) suddenly.
c.
to stow quickly.


 [Firemen Douse Fire With Sand](http://news.google.com/newspapers?nid=888&amp;amp;dat=19670824&amp;amp;id=RopQAAAAIBAJ&amp;amp;sjid=pFwDAAAAIBAJ&amp;amp;pg=3814,1657540) [Firemen Douse Fire With Sand](http://news.google.com/newspapers?nid=888&amp;amp;dat=19670824&amp;amp;id=RopQAAAAIBAJ&amp;amp;sjid=pFwDAAAAIBAJ&amp;amp;pg=3814,1657540) Either I didn't get the memo that thermite comes in liquid form, or you didn't get the memo that "douse" is a liberal liquid-specific covering. Either I didn't get the memo that thermite comes in liquid form, or you didn't get the memo that "douse" is a liberal liquid-specific covering. I think you can douse something with powder just fine. If you follow infosec, you will find no shortage of cooks who claim to eliminate 'all practical security vulnerabilities' what this tweak or that sandbox.  There are a few. Ones that spring to mind were a company that claimed to offer compression of up to 99% on *any* type of file (though code and algorithm were kept secret for industry competition reasons) and a guy that claimed to have single handedly created an os that put linux and windows to shame. Again not a single loc was produced... There are a few. Ones that spring to mind were a company that claimed to offer compression of up to 99% on *any* type of file (though code and algorithm were kept secret for industry competition reasons) and a guy that claimed to have single handedly created an os that put linux and windows to shame. Again not a single loc was produced... True fact: claiming 99% compression on any type of file is equivalent to claiming 1% compression on any type of file. Explain, please. If you could really compress *any* file, you could also compress the compressed file. And so on. So even if you could only compress *every* file by a tiny amount, you could just repeat the process as desired.

This should make it clear why the claim is absurd. Recursive compression. Where are my one bit games? 0 Great game, but the graphics sucked. 7/10. I feel that the sequel, 1 was a lot better. 0 All the works of human history, in all possible orders, with all possible word mistakes and interpretations, compressed a billion times to this - 0 All the works of human history, in all possible orders, with all possible word mistakes and interpretations, compressed a billion times to this - 0 I was excited until I realized the decompressor weighs in at 45PB. If you could really compress *any* file, you could also compress the compressed file. And so on. So even if you could only compress *every* file by a tiny amount, you could just repeat the process as desired.

This should make it clear why the claim is absurd. If you could really compress *any* file, you could also compress the compressed file. And so on. So even if you could only compress *every* file by a tiny amount, you could just repeat the process as desired.

This should make it clear why the claim is absurd. True fact: claiming 99% compression on any type of file is equivalent to claiming 1% compression on any type of file. There are a few. Ones that spring to mind were a company that claimed to offer compression of up to 99% on *any* type of file (though code and algorithm were kept secret for industry competition reasons) and a guy that claimed to have single handedly created an os that put linux and windows to shame. Again not a single loc was produced... Doues [softram](http://en.wikipedia.org/wiki/SoftRAM) count as a crank or just fraud?  That's a fraud. I suppose it would be possible to compress RAM, but not without sacrificing CPU power. Cache is so much faster than main memory that it's sometimes actually a good idea to compress RAM now. Just think of RAM as another tier of slower-but-larger storage like SSD or spinning disk. [Blosc](http://blosc.pytables.org/trac) is a package that does this. It's used in the [PyTables](http://www.pytables.org/moin) library if you want to try it out. There are a few. Ones that spring to mind were a company that claimed to offer compression of up to 99% on *any* type of file (though code and algorithm were kept secret for industry competition reasons) and a guy that claimed to have single handedly created an os that put linux and windows to shame. Again not a single loc was produced...  Probably the closest is the [P versus NP problem](http://en.wikipedia.org/wiki/P_versus_NP_problem). There have been a number of claimed solutions, but nobody has definitively solved it. Probably the closest is the [P versus NP problem](http://en.wikipedia.org/wiki/P_versus_NP_problem). There have been a number of claimed solutions, but nobody has definitively solved it. Probably the closest is the [P versus NP problem](http://en.wikipedia.org/wiki/P_versus_NP_problem). There have been a number of claimed solutions, but nobody has definitively solved it.  Will this become the "show us your CS crackpots" thread? [Here's one](http://www.cosmicfingerprints.com/read-prove-god-exists/) that comes to my mind. (tl;dr: All codes are man-made; DNA is a code; therefore intelligent design.)

Some extreme forms (and even mainstream forms, depending on who you ask) of singularity/futurism crowd could be considered crackpot. Math has crackpots who continue to dispute Cantor's diagonalization proof; presumably these people also dispute the undecidability of the halting problem, since it is the same thing. Rebuttal to his claim:

Not all codes are manmade. Some are generated using GAs or other training. The training was started by humans, but not all selection processes are human-initiated, and there's clear selection processes in nature. I think there's a deeper flaw to this argument.

In particular, it's isomorphic to something like this: all manmade codes are made by man, therefore codes in nature have to be designed intelligently. Essentially he's saying that any counter-example would have to also be manmade because everything that isn't a counterexample *is* manmade.The argument makes literally no sense even if you ignore particular details. Will this become the "show us your CS crackpots" thread? [Here's one](http://www.cosmicfingerprints.com/read-prove-god-exists/) that comes to my mind. (tl;dr: All codes are man-made; DNA is a code; therefore intelligent design.)

Some extreme forms (and even mainstream forms, depending on who you ask) of singularity/futurism crowd could be considered crackpot. Math has crackpots who continue to dispute Cantor's diagonalization proof; presumably these people also dispute the undecidability of the halting problem, since it is the same thing.  'Looking for technical co-founder'... 'Looking for technical co-founder'... 'Looking for technical co-founder'...  I think [this](http://www.youtube.com/watch?v=00gAbgBu8R4) qualifies Exactly right.

Although I don't think they made any specifically false claims, it was just very misleading as to how applicable it would be to gaming. (No dynamic animations, necessitates a massive compute farm that the rendering computer can query, etc.) Wait, this part?

&amp;gt; ...necessitates a massive compute farm that the rendering computer can query...

Where did that come from? I thought they had it demo'd running on laptops. The downside was that you could have your unlimited-ly detailed tree, but you likely wouldn't have many varieties of tree. Not speaking from experience, but I remember Notch chiming in when this demo first came up. Something about massive memory requirements necessitating some sort of client-server infrastructure for the rendering. The point, as I recall, was that real-time rendering of these environments required that for this demo, two very powerful machines were running a compute cluster to process the point-cloud occlusion or something along those lines, and the laptop was just rendering the result.

The upshot was that, as I remember it, it was not a practical setup. Not speaking from experience, but I remember Notch chiming in when this demo first came up. Something about massive memory requirements necessitating some sort of client-server infrastructure for the rendering. The point, as I recall, was that real-time rendering of these environments required that for this demo, two very powerful machines were running a compute cluster to process the point-cloud occlusion or something along those lines, and the laptop was just rendering the result.

The upshot was that, as I remember it, it was not a practical setup. That was just Notch's claim at the time, it doesn't seem to be true.  It would be true if they were doing ray tracing or something, but as I understand it the engine essentially works by searching tables for one point in the virtual space per pixel in the display.  (like google searching, only billions of points instead of billions of web pages) You avoid a lot of computing by only having to worry about those points at any one time.  

Animation is a genuinely big challenge, though. &amp;gt; but as I understand it the engine essentially works by searching tables for one point in the virtual space per pixel in the display

That's basically how ray-casting is done, a ray is traced through each pixel from the viewport to find the point in the scene it corresponds to.  The 'search' is generally done through a spatial-partitioning structure which filter out the areas of the scene you're not interested in.

The google searching analogy that they talk about is particularly shallow and vague. Not speaking from experience, but I remember Notch chiming in when this demo first came up. Something about massive memory requirements necessitating some sort of client-server infrastructure for the rendering. The point, as I recall, was that real-time rendering of these environments required that for this demo, two very powerful machines were running a compute cluster to process the point-cloud occlusion or something along those lines, and the laptop was just rendering the result.

The upshot was that, as I remember it, it was not a practical setup. http://notch.tumblr.com/post/8386977075/its-a-scam

I wish a voxel-polygon hybrid would be used in games today. Polygons don't make much sense for trees, especially if you want them destructible. Wait, this part?

&amp;gt; ...necessitates a massive compute farm that the rendering computer can query...

Where did that come from? I thought they had it demo'd running on laptops. The downside was that you could have your unlimited-ly detailed tree, but you likely wouldn't have many varieties of tree. [deleted] [deleted] Ah... I don't see that as a problem, actually. I mean, what's used to build pre-rendered cinematics these days? [deleted] Exactly right.

Although I don't think they made any specifically false claims, it was just very misleading as to how applicable it would be to gaming. (No dynamic animations, necessitates a massive compute farm that the rendering computer can query, etc.) Is there a long-form rebuttal of this? I saw it pop up and thought "soooo... voxels?" ...are they not voxels? [deleted] [deleted] That's a fairly good post but keep in mind Notch can't make a "voxel renderer" with approximately 1ft by 1ft voxels run at an acceptable speed on modern gaming hardware so take from that what you will. [deleted] Is there a long-form rebuttal of this? I saw it pop up and thought "soooo... voxels?" ...are they not voxels? I think [this](http://www.youtube.com/watch?v=00gAbgBu8R4) qualifies He kept saying "unlimited detail".  All I could think of was [this](http://www.strategicdc.com/wp-content/uploads/2009/12/inconceivable.jpg) I think [this](http://www.youtube.com/watch?v=00gAbgBu8R4) qualifies maybe offtopic, but does anyone else think the way the "narrator" talks is pretty annoying? I think [this](http://www.youtube.com/watch?v=00gAbgBu8R4) qualifies  I think it's because computer science still is kind of an obscure subject for most people. You can turn on your TV and hear a thousand things about physics, and if you don't know much, you can still grasp a few things like "light" and "velocity" and "space".

However, I find most of computer science is absolutely unknown for almost everyone, except for a few words "I make a living using Photoshop. I'm basically a hacker." "I make a living using Photoshop. I'm basically a hacker." it gets [worse](http://www.youtube.com/watch?feature=player_embedded&amp;amp;v=hkDD03yeLnU), much, much [worse](http://www.youtube.com/watch?feature=player_embedded&amp;amp;v=O2rGTXHvPCQ). And some people actually buy into this [stuff](http://www.youtube.com/watch?feature=player_embedded&amp;amp;v=u8qgehH3kEQ). 

Still, it's just catchwords for the TV writers to give an impression about the characters. Everything else about pretty much anything in computer science lies (for people like these writers) in the dark depths of some arcane, dark, body of knowledge that only a few can even grasp.

btw, I got the videos [here](http://www.cracked.com/article_19160_8-scenes-that-prove-hollywood-doesnt-get-technology.html). If you haven't seen it, it's hilarious. Those examples were all intentionally bad. There is an in-joke among TV writers to write the most insanely bad geek talk.   I think the difference is that computers give you very fast and accurate feedback about your competence. Does your code work? 

Other fields make it a lot easier to fool yourself. There is no "your 'proof' of the Riemann Hypothesis failed to compile/crashed".

There are plenty of wackos around the fringes of geekdom. Example

http://www.forbes.com/sites/tarabrown/2012/03/26/dear-fake-geek-girls-please-go-away/ I think the difference is that computers give you very fast and accurate feedback about your competence. Does your code work? 

Other fields make it a lot easier to fool yourself. There is no "your 'proof' of the Riemann Hypothesis failed to compile/crashed".

There are plenty of wackos around the fringes of geekdom. Example

http://www.forbes.com/sites/tarabrown/2012/03/26/dear-fake-geek-girls-please-go-away/ There was an interesting discussion around one guy's claim to have a graph isomorphism algorithm that ran in linear time.

He posted a very short (like 300 line) piece of c-code and refused to either a) explain his algorithm in pseudocode for the theorists or b) provide binaries for the experimentalists.

Sure, people tried to compile and run it  - but anyone that did and found a counterexample was told "Ah, wait, here is now the fixed version!". Sigh. I think the difference is that computers give you very fast and accurate feedback about your competence. Does your code work? 

Other fields make it a lot easier to fool yourself. There is no "your 'proof' of the Riemann Hypothesis failed to compile/crashed".

There are plenty of wackos around the fringes of geekdom. Example

http://www.forbes.com/sites/tarabrown/2012/03/26/dear-fake-geek-girls-please-go-away/ I think the difference is that computers give you very fast and accurate feedback about your competence. Does your code work? 

Other fields make it a lot easier to fool yourself. There is no "your 'proof' of the Riemann Hypothesis failed to compile/crashed".

There are plenty of wackos around the fringes of geekdom. Example

http://www.forbes.com/sites/tarabrown/2012/03/26/dear-fake-geek-girls-please-go-away/ Yea exactly. It's so easy to spot bullshit in comp sci so people are less inclined to try. I think the difference is that computers give you very fast and accurate feedback about your competence. Does your code work? 

Other fields make it a lot easier to fool yourself. There is no "your 'proof' of the Riemann Hypothesis failed to compile/crashed".

There are plenty of wackos around the fringes of geekdom. Example

http://www.forbes.com/sites/tarabrown/2012/03/26/dear-fake-geek-girls-please-go-away/    Comp Sci Crakpots:

http://subbot.org/

http://losethos.com/

http://www.goingware.com/tips/

All three mentally ill and claiming to have done impossible stuff and things others claim as hard to do. Can be labeled as crackpots in computer science. Oh my lord, the Lose Thos guy is the worse. Dude is on disability and honestly believes the government is funding his work or something. Dude wrote a whole non-toy OS from scratch though in assembly. Mentally ill, but pretty impressive. Oh my lord, the Lose Thos guy is the worse. Dude is on disability and honestly believes the government is funding his work or something. Oh my lord, the Lose Thos guy is the worse. Dude is on disability and honestly believes the government is funding his work or something. Oh my lord, the Lose Thos guy is the worse. Dude is on disability and honestly believes the government is funding his work or something. Oh my lord, the Lose Thos guy is the worse. Dude is on disability and honestly believes the government is funding his work or something. Comp Sci Crakpots:

http://subbot.org/

http://losethos.com/

http://www.goingware.com/tips/

All three mentally ill and claiming to have done impossible stuff and things others claim as hard to do. Can be labeled as crackpots in computer science. I hate to admit that, aside from Lose Thos, I don't actually see what's wrong with the other ones.

Subbot -- I see some AI research. What's the claim?

Goingware/tips -- I actually agree with a lot of what's said on, say, the "Study fundamentals, not APIs, Tools, or OSes" page. So again, where's the crackpot?

I don't mean that those two aren't crackpots, it's just not obvious from a casual skimming of the homepage. Check Subbot's essays. He has one attempting the refute the law of the excluded middle. I won't say it's necessarily wrong (I haven't picked it apart), but that's a hell of an axiom to take on. Check Subbot's essays. He has one attempting the refute the law of the excluded middle. I won't say it's necessarily wrong (I haven't picked it apart), but that's a hell of an axiom to take on. I took a peek in there, and somehow missed that.

Still doesn't jump out at me as "crackpot". See: [Paraconsistent Logic](http://en.wikipedia.org/wiki/Paraconsistent_logic). I don't see a law of excluded middle argument there, but that's not exactly off-the-deep-end.

Same with the treatment of "This statement is a lie" -- it seems less a claim that his program proves it's not a paradox, and more a claim that he has a program which does not crash when faced with the problem. He also makes no claim that it is *in general* undecidable.

Unconventional? Sure. Useless? I certainly think so. But this is no Timecube Guy.

Compare to the top post -- "Infinite Detail Engine." Infinite, really? I took a peek in there, and somehow missed that.

Still doesn't jump out at me as "crackpot". See: [Paraconsistent Logic](http://en.wikipedia.org/wiki/Paraconsistent_logic). I don't see a law of excluded middle argument there, but that's not exactly off-the-deep-end.

Same with the treatment of "This statement is a lie" -- it seems less a claim that his program proves it's not a paradox, and more a claim that he has a program which does not crash when faced with the problem. He also makes no claim that it is *in general* undecidable.

Unconventional? Sure. Useless? I certainly think so. But this is no Timecube Guy.

Compare to the top post -- "Infinite Detail Engine." Infinite, really? Check Subbot's essays. He has one attempting the refute the law of the excluded middle. I won't say it's necessarily wrong (I haven't picked it apart), but that's a hell of an axiom to take on. I hate to admit that, aside from Lose Thos, I don't actually see what's wrong with the other ones.

Subbot -- I see some AI research. What's the claim?

Goingware/tips -- I actually agree with a lot of what's said on, say, the "Study fundamentals, not APIs, Tools, or OSes" page. So again, where's the crackpot?

I don't mean that those two aren't crackpots, it's just not obvious from a casual skimming of the homepage. Here is Goingware's other page:
http://www.softwareproblem.net/

He has many of them but the domain names expire quickly. It is hard to read all of his works. But he was in the news:

http://www.oregonlive.com/portland/index.ssf/2012/04/startup_weekend_entrepreneuria.html

http://www.advogato.org/article/1060.html

http://startupweekend.org/2012/04/30/not-even-bmob-threats-could-deter-portlands-entrepreneurs-at-startup-weekend/

It is hard to see why he became a crackpot, he eventually lost his job, his wife, had his computer equipment in a storage area, got hooked on speed, and eventually started to foil Startup and Dotcom events and got into legal trouble messing with 911 operators and such. I feel bad as I would talk to him over on Kuro5hin and in email as we share the same mental illness, but at one point he just snapped and lost control of his senses.

The Subbot guy is a forever alone sort of person who creates chatbots using AI in Ruby to create his on reality. He finds reality not worth his time and wants to create his own reality to substitute it for the real one. He just rejects reality and seems to be creating some sort of cyberspace for him to live in and be happy. I'm still not seeing that from the subbot guy. Creating chatbots using AI in Ruby, I see. To escape reality and substitute his own, where are you getting that?

Upvote because TIL... Comp Sci Crakpots:

http://subbot.org/

http://losethos.com/

http://www.goingware.com/tips/

All three mentally ill and claiming to have done impossible stuff and things others claim as hard to do. Can be labeled as crackpots in computer science. LoseThos is a 64-bit, ring-0-only, single address map (identity-mapped), OS.

What's impossible about www.losethos.com, mother fucker?  

I did a compiler, unlike Linus.

 9 years, full time

About all it's good for is animated songs from God, as you can see.

----

God says...

torture forsakest nay allegorically hurtful keeps editing 
men judgements judgements over well_I_never vote incomprehensible 
lords Manichaeus commencement anticipating 

----

You go right ahead, genius, and worry about man.  I'll worry about God. Do you plan on ever adding networking support for LoseThos? Comp Sci Crakpots:

http://subbot.org/

http://losethos.com/

http://www.goingware.com/tips/

All three mentally ill and claiming to have done impossible stuff and things others claim as hard to do. Can be labeled as crackpots in computer science.  No, there's definitely crackpots, it's just that most people outside the field can't tell the difference except when their software doesn't work.  Most people in the field seem to not be scientists, but rather spaghetti code writers who come up with some of the most idiotic rube-Goldberg solutions that give me headaches all day long fixing them.

Probably the most common is roll-your-own-security coders.  Somehow, they determine that the algorithms that took years of research by the NSA are either too complicated or too insecure and write their own, which ends up being a trillion times less secure.  I've seen people "encrypt" credentials in JavaScript, which makes no sense as SSL will do it for you, and every week there's a new database of passwords stolen from some company who didn't hash their passwords or didn't salt their hashes, which I'm sure you've heard about.

Then there's the people who build rube-goldberg solutions.  One of my favorites was probably the time that my coworker was trying to troubleshoot an issue with a document upload timing out.  She spent a few hours just looking at it, and then calls me.  I look at it and immediately see the problem.  Someone built it so that it:

1.  Reads the document
2.  Opens the database table with documents
3.  Reads the entire table of documents into memory locking the entire table from other users.  Now there's several thousand documents at this point, and it's reading over the network.
4.  Adds a new record to the in memory table
5.  Stores the new document in the record
6.  Write the whole several hundred megabyte thing back to the database across the network.

She did not accept my recommendation to remove the "read entire 700MB table into memory" step.

Then there's the C# coder who would read absolutely everything into a "DataSet" object to pass between method calls.  UGh.

XML is probably one of the biggest crackpot computer science ideas ever.

There's people who do make debugging as hard as possible by doing try/catch everything blocks around every single function, essentially suppressing every single error.  Had one application, where the users didn't even know it wasn't working, they did data entry on this app for a year, and lost every bit of it because the developer used this pikachu (gotta catch em all!) exception handling, and always displayed a popup box saying that it saved successfully despite that it didn't.  Ended up ripping most of them out, and the users blamed me for breaking it, because errors would occur constantly, despite that I spend three weeks fixing everything I could find myself.  Of course, there where no unit tests.

Also: the guy my boss hired who couldn't build a SQL statement.  He sent me the existing statement, the field he was supposed to add and the condition, and said how do I put this together?  Uh, try Google; selects aren't that hard. No, there's definitely crackpots, it's just that most people outside the field can't tell the difference except when their software doesn't work.  Most people in the field seem to not be scientists, but rather spaghetti code writers who come up with some of the most idiotic rube-Goldberg solutions that give me headaches all day long fixing them.

Probably the most common is roll-your-own-security coders.  Somehow, they determine that the algorithms that took years of research by the NSA are either too complicated or too insecure and write their own, which ends up being a trillion times less secure.  I've seen people "encrypt" credentials in JavaScript, which makes no sense as SSL will do it for you, and every week there's a new database of passwords stolen from some company who didn't hash their passwords or didn't salt their hashes, which I'm sure you've heard about.

Then there's the people who build rube-goldberg solutions.  One of my favorites was probably the time that my coworker was trying to troubleshoot an issue with a document upload timing out.  She spent a few hours just looking at it, and then calls me.  I look at it and immediately see the problem.  Someone built it so that it:

1.  Reads the document
2.  Opens the database table with documents
3.  Reads the entire table of documents into memory locking the entire table from other users.  Now there's several thousand documents at this point, and it's reading over the network.
4.  Adds a new record to the in memory table
5.  Stores the new document in the record
6.  Write the whole several hundred megabyte thing back to the database across the network.

She did not accept my recommendation to remove the "read entire 700MB table into memory" step.

Then there's the C# coder who would read absolutely everything into a "DataSet" object to pass between method calls.  UGh.

XML is probably one of the biggest crackpot computer science ideas ever.

There's people who do make debugging as hard as possible by doing try/catch everything blocks around every single function, essentially suppressing every single error.  Had one application, where the users didn't even know it wasn't working, they did data entry on this app for a year, and lost every bit of it because the developer used this pikachu (gotta catch em all!) exception handling, and always displayed a popup box saying that it saved successfully despite that it didn't.  Ended up ripping most of them out, and the users blamed me for breaking it, because errors would occur constantly, despite that I spend three weeks fixing everything I could find myself.  Of course, there where no unit tests.

Also: the guy my boss hired who couldn't build a SQL statement.  He sent me the existing statement, the field he was supposed to add and the condition, and said how do I put this together?  Uh, try Google; selects aren't that hard. &amp;gt; XML is probably one of the biggest crackpot computer science ideas ever.

I suggested using JSON to get away from some of the XML overhead we have. My coworker thought about it and came back the next day with a solution where we would serialize the JSON into a CDATA block inside the XML. Or maybe the other way around. I tried to be polite. No, there's definitely crackpots, it's just that most people outside the field can't tell the difference except when their software doesn't work.  Most people in the field seem to not be scientists, but rather spaghetti code writers who come up with some of the most idiotic rube-Goldberg solutions that give me headaches all day long fixing them.

Probably the most common is roll-your-own-security coders.  Somehow, they determine that the algorithms that took years of research by the NSA are either too complicated or too insecure and write their own, which ends up being a trillion times less secure.  I've seen people "encrypt" credentials in JavaScript, which makes no sense as SSL will do it for you, and every week there's a new database of passwords stolen from some company who didn't hash their passwords or didn't salt their hashes, which I'm sure you've heard about.

Then there's the people who build rube-goldberg solutions.  One of my favorites was probably the time that my coworker was trying to troubleshoot an issue with a document upload timing out.  She spent a few hours just looking at it, and then calls me.  I look at it and immediately see the problem.  Someone built it so that it:

1.  Reads the document
2.  Opens the database table with documents
3.  Reads the entire table of documents into memory locking the entire table from other users.  Now there's several thousand documents at this point, and it's reading over the network.
4.  Adds a new record to the in memory table
5.  Stores the new document in the record
6.  Write the whole several hundred megabyte thing back to the database across the network.

She did not accept my recommendation to remove the "read entire 700MB table into memory" step.

Then there's the C# coder who would read absolutely everything into a "DataSet" object to pass between method calls.  UGh.

XML is probably one of the biggest crackpot computer science ideas ever.

There's people who do make debugging as hard as possible by doing try/catch everything blocks around every single function, essentially suppressing every single error.  Had one application, where the users didn't even know it wasn't working, they did data entry on this app for a year, and lost every bit of it because the developer used this pikachu (gotta catch em all!) exception handling, and always displayed a popup box saying that it saved successfully despite that it didn't.  Ended up ripping most of them out, and the users blamed me for breaking it, because errors would occur constantly, despite that I spend three weeks fixing everything I could find myself.  Of course, there where no unit tests.

Also: the guy my boss hired who couldn't build a SQL statement.  He sent me the existing statement, the field he was supposed to add and the condition, and said how do I put this together?  Uh, try Google; selects aren't that hard. No, there's definitely crackpots, it's just that most people outside the field can't tell the difference except when their software doesn't work.  Most people in the field seem to not be scientists, but rather spaghetti code writers who come up with some of the most idiotic rube-Goldberg solutions that give me headaches all day long fixing them.

Probably the most common is roll-your-own-security coders.  Somehow, they determine that the algorithms that took years of research by the NSA are either too complicated or too insecure and write their own, which ends up being a trillion times less secure.  I've seen people "encrypt" credentials in JavaScript, which makes no sense as SSL will do it for you, and every week there's a new database of passwords stolen from some company who didn't hash their passwords or didn't salt their hashes, which I'm sure you've heard about.

Then there's the people who build rube-goldberg solutions.  One of my favorites was probably the time that my coworker was trying to troubleshoot an issue with a document upload timing out.  She spent a few hours just looking at it, and then calls me.  I look at it and immediately see the problem.  Someone built it so that it:

1.  Reads the document
2.  Opens the database table with documents
3.  Reads the entire table of documents into memory locking the entire table from other users.  Now there's several thousand documents at this point, and it's reading over the network.
4.  Adds a new record to the in memory table
5.  Stores the new document in the record
6.  Write the whole several hundred megabyte thing back to the database across the network.

She did not accept my recommendation to remove the "read entire 700MB table into memory" step.

Then there's the C# coder who would read absolutely everything into a "DataSet" object to pass between method calls.  UGh.

XML is probably one of the biggest crackpot computer science ideas ever.

There's people who do make debugging as hard as possible by doing try/catch everything blocks around every single function, essentially suppressing every single error.  Had one application, where the users didn't even know it wasn't working, they did data entry on this app for a year, and lost every bit of it because the developer used this pikachu (gotta catch em all!) exception handling, and always displayed a popup box saying that it saved successfully despite that it didn't.  Ended up ripping most of them out, and the users blamed me for breaking it, because errors would occur constantly, despite that I spend three weeks fixing everything I could find myself.  Of course, there where no unit tests.

Also: the guy my boss hired who couldn't build a SQL statement.  He sent me the existing statement, the field he was supposed to add and the condition, and said how do I put this together?  Uh, try Google; selects aren't that hard.  [Perspex](http://scienceblogs.com/goodmath/2007/09/09/the-perspex-machine-superturin/), anyone?   "The singularity as near!" is pretty much the perpetual motion machine of CS.    This whole thread reminds me of the [Programming language inventor or serial killer quiz](http://www.malevole.com/mv/misc/killerquiz/) got 9/10 5/10 : (

Damn you, evil-looking scheme inventor! This whole thread reminds me of the [Programming language inventor or serial killer quiz](http://www.malevole.com/mv/misc/killerquiz/) Somewhat related, professor or hobo. http://individual.utoronto.ca/somody/quiz.html This whole thread reminds me of the [Programming language inventor or serial killer quiz](http://www.malevole.com/mv/misc/killerquiz/) [Why not both?](http://en.wikipedia.org/wiki/Hans_Reiser) The article said that he created a file system, and not a programming language.   There are plenty. I remember reading the rantings of a guy who thought he could create a universal compression tool. universal as gzip ?   People claiming to have created a compression algorithm that can create a loss less compression on any file and be able to decrease the size. This is hilarious since it's so incredibly easy to prove impossible (you can't map less bit combinations to more bit combinations), and yet there are people still trying to achieve it. &amp;gt; There are people who claim they created a compression algorithm that can compress any file regardless of its content. This includes already compressed files and files containing random data. Those files cannot be compressed because attempts to do so would make them larger.
&amp;gt;
&amp;gt; It is a hilarious claim because it's incredibly easy to prove impossible. If it were possible, one could compress a file and then recompress it again, and again, where the file grows smaller upon each round of compression without losing data integrity. You can't infinitely compress data because it would eventually lead to a file containing 1 bit of data, which cannot possibly represent the original file. Yet there are people still trying to achieve it.
  Marvin Minsky famously thought that all neural network research was full of crackpots.  (I conclude that Minsky himself was the crackpot.)

Stephen Wolfram's A New Kind of Science drew praise and scorn.  I think his ideas have some merit, but his presentation is *way* over the top.

Douglas Lenat, of the Cyc project, made some outrageous claims, and hasn't delivered on them.       Cause we're all crackpots to some degree.   Richard Stallman is brilliant and has some radical ideas. But I saw him speak in person once and I thought he was kind of terrifying. For example, http://www.youtube.com/watch?v=I25UeVXrEHQ The scariest thing about Stallman is that he might have been right all along. The man displays a frightening degree of prescience. Really, he reminds me a /lot/ of the old-testament style of prophet. Richard Stallman is brilliant and has some radical ideas. But I saw him speak in person once and I thought he was kind of terrifying. For example, http://www.youtube.com/watch?v=I25UeVXrEHQ Richard Stallman is brilliant and has some radical ideas. But I saw him speak in person once and I thought he was kind of terrifying. For example, http://www.youtube.com/watch?v=I25UeVXrEHQ  There are plenty, [check this out](http://www.members.tripod.com/cpu_bios/)!  Where do brogrammer blogs fit in on the scale of cs crackpots? Programming != CS     Because [we](http://en.wikipedia.org/wiki/Linus_Torvalds) [celebrate](http://en.wikipedia.org/wiki/Richard_Stallman) [ours](http://en.wikipedia.org/wiki/Eric_S._Raymond) Get out. Because [we](http://en.wikipedia.org/wiki/Linus_Torvalds) [celebrate](http://en.wikipedia.org/wiki/Richard_Stallman) [ours](http://en.wikipedia.org/wiki/Eric_S._Raymond)         I was recently recalling "Topmind" (top standing for table-oriented programming). He was quite out-spoken in OOP related Usenet forums.

http://c2.com/cgi/wiki?TopMind     I will just leave [this one](http://kryptochef.net/indexh2e.htm) here.    </snippet></document><document><title>Norvig vs. Chomsky and the Fight for the Future of AI</title><url>http://www.tor.com/blogs/2011/06/norvig-vs-chomsky-and-the-fight-for-the-future-of-ai</url><snippet>   Excellent article.  I'm not qualified to challenge Chomsky, obviously, but I especially like that his positions aren't accepted by default. 

The comparison to Bill O'Reilly is perhaps unfortunate, in that it borders on deadly insult.

The tangential article on [Colourless green ideas sleep furiously](http://en.wikipedia.org/wiki/Colorless_green_ideas_sleep_furiously) is also very thought provoking.   Norvig's approach seems to me more likely to be wrong, because it's too extreme: no structure, just statistics. Statistics will play a big role, but structure can play a big role in disambiguating. See for example this paper  http://arxiv.org/abs/1003.4394.  [deleted] Seems like some of the tension here is between 'black boxes' and 'white boxes.'  A statistical machine learning model can often make good predictions (but only as good as the representativeness of its learning set), but what comes out is an opaque black box, yielding little insight into the underlying processes.  The other approach, call it 'deductive' in contrast to the former 'inductive' one, attempts to create small models of small isolated (if that's even possible in linguistics) phenomena.  These may be composed into more complex systems.
Yeah, so the deductive approach yields usually poor predictive results, but in some sense it is more scientifically interesting, mirroring the usual approach of the sciences in composing parsimonious theories of natural phenomena.  The inductive approach can get you up and running quickly (with enough starting data) and make you some money, but it is often lacking in insight. On the other hand, however, Leo Breiman (in Statistical Modeling: The Two Cultures http://www.recognition.su/wiki/images/8/85/Breiman01stat-ml.pdf) correctly points out the kinds of things that can go wrong with a 'deductive' modeling approach.  He also has some good comments on the interpretability of algorithmic modeling (read: machine learning) results.  The jury is still out, and what's most important is that the two cultures *do* exist to debate and learn from each other.  The absolutist of any flavor is the real loser. Seems like some of the tension here is between 'black boxes' and 'white boxes.'  A statistical machine learning model can often make good predictions (but only as good as the representativeness of its learning set), but what comes out is an opaque black box, yielding little insight into the underlying processes.  The other approach, call it 'deductive' in contrast to the former 'inductive' one, attempts to create small models of small isolated (if that's even possible in linguistics) phenomena.  These may be composed into more complex systems.
Yeah, so the deductive approach yields usually poor predictive results, but in some sense it is more scientifically interesting, mirroring the usual approach of the sciences in composing parsimonious theories of natural phenomena.  The inductive approach can get you up and running quickly (with enough starting data) and make you some money, but it is often lacking in insight. [deleted] &amp;gt; insight in the statistical model comes from properly constructing a model that takes data and behaves in the same way a human would.

In what way is that insight?  Nobody has yet figured out how the human brain processes input data into behaviors, so it's a fair bet that any model that one created that does this would be similarly impossible to interpret.

&amp;gt; If you tried to reason about gases by figuring out the exact behaviour of every particle, you'd get nowhere.

This is true, but scientists do have a theory of the particle, along with its degrees of freedom and so forth, that a statistical ensemble is composed of (the statistics is intractable without it!).  The linguistic analogue is that it is much harder to figure out the meanings of text without having any conception of verbs, nouns, prepositions, etc.  

Thus, the statistically-minded researcher can gain a great deal from the work that the formalist has done in clearly defining these ideas.  And the formalist can learn much from what the empiricist observes about how phenomena actually play out in real human language communities, even extracting unforeseen linguistic categories.  The two camps have a lot to learn from each other. [deleted] [deleted] [deleted] Norvig's approach seems to me more likely to be wrong, because it's too extreme: no structure, just statistics. Statistics will play a big role, but structure can play a big role in disambiguating. See for example this paper  http://arxiv.org/abs/1003.4394.  Norvig's approach seems to me more likely to be wrong, because it's too extreme: no structure, just statistics. Statistics will play a big role, but structure can play a big role in disambiguating. See for example this paper  http://arxiv.org/abs/1003.4394.      I got God.  I don't need A.I.

God says...


13:23 Much food is in the tillage of the poor: but there is that is
destroyed for want of judgment.

13:24 He that spareth his rod hateth his son: but he that loveth him
chasteneth him betimes.

13:25 The righteous eateth to the satisfying of his soul: but the
belly of the wicked shall want.

14:1 Every wise woman buildeth her house: but the foolish plucketh it
down with her hands.

14:2 He that walketh in his uprightness feareth the LORD: but he that
is perverse in his ways despiseth him.

14:3 In the mouth of the foolish is a rod of pride: but the lips of
the wise shall preserve them.

----

(That was about my previous post.)

God?  Did You plan intelligence or did it happen?  ROFLMAO

God says...

down girl Hadst willing chastity recited redoubling virtuous 
thundering farther religious sudden fighting penalty orderest 
Eat acts etext00 revealedst Placed Julian gasped Sacrifice 
mockeries bend grammarians hell cracks_me_up ears </snippet></document><document><title>What work has been done in using cellular automata to model neural networks? (the thought process leading to this question is inside).</title><url>http://www.reddit.com/r/compsci/comments/z01es/what_work_has_been_done_in_using_cellular/</url><snippet>There was a post in /r/math on whether mathematics would be rediscovered in the same way in alternate histories. In composing my response, it occurred to me that one might be able to view the emergence of a system of logic as supervised learning in a neural network where the teacher is the success or failure of a physical experiment based on a mathematical model.

That led me to consider testing it, but the fact that there are 10^11 neurons led me to consider the memory necessary to model a human brain - even if one byte were used per neuron, that's 100 GB. The "one byte per" led me to consider the following: what if instead of modeling a single neuron as a system of differential equations modeling neuronal physiology and biochemistry as is done often in computational neuroscience nowadays, we used a single byte per neuron and tailored a system of cellular automaton rules so that they matched up with convergent excitatory/inhibitory inputs to a single neuron? Would a glider correspond to keeping a thought "on-line", like repeating a phone number to yourself to remember it? If so, this would greatly reduce the resources needed to model such a large system and might yield some insights.

So, does anyone know of any work being done along these lines? Maybe I'm missing something crucial, but, at first glance, it sounds feasible to me.  As you point out, the information would still have to be stored somewhere (e.g., in the form of a glider), so whatever memory requirements you have would still be there, wouldn't they?

You either have to put some of this information in the rules / gameboard (e.g., instead of cells being on or off, they have a number of bits associated with them, modelling their state), which wouldn't help you, or you have to "encode" this information within the rules of the game, which seems difficult, and would probably be less concise in terms of memory requirement. 

Finally, it is possible that there maybe some surprisingly small cellular automaton that somehow abstractly encodes the essential functions of the brain, maybe even using little memory, but finding this object, if it exists, is a harder question than modelling the brain in the first place.  As you point out, the information would still have to be stored somewhere (e.g., in the form of a glider), so whatever memory requirements you have would still be there, wouldn't they?

You either have to put some of this information in the rules / gameboard (e.g., instead of cells being on or off, they have a number of bits associated with them, modelling their state), which wouldn't help you, or you have to "encode" this information within the rules of the game, which seems difficult, and would probably be less concise in terms of memory requirement. 

Finally, it is possible that there maybe some surprisingly small cellular automaton that somehow abstractly encodes the essential functions of the brain, maybe even using little memory, but finding this object, if it exists, is a harder question than modelling the brain in the first place.  I've done some work in computational neuroscience that tried to model neurons based on biochemical processes and this entailed dozens of differential equations, not to mention the computing requirements. Using the byte-per representation would drastically reduce the resource requirements to a manageable level.

The difficulty of encoding the rules for integration of inputs is exactly what seems like the tough part and would be the first part to be investigated. Can we model three excitatory and two inhibitory inputs using simple rules? If not, could we construct a higher dimensional analog of Conway's Game of Life that makes it possible?

I have no idea, but I also don't see why it would necessarily be impossible. There's two different issues here:

1. How precise do we need to be to approximate the functioning of a neuron sufficiently well for large-scale brain simulation?

2. How can we encode the resulting approximation?

First, one can approximate the functioning of a neuron using a smaller number of bits and less computational resources than are used by current physical modelling techniques. Whether you use cellular automata or anything else to model resulting system is not the principal question. 

The question is instead: Is there a simple to compute approximation of a neuron's functioning that does not lose essential information. This is a difficult question that probably has less to do with computer science than with neuroscience. It's possible that neurons in the brain have some easy to compute functional abstraction, but its also possible that the brain uses weird ass analog stuff to compute things that would require you to do rich physical modelling.

Second, if you know that in the least precise sufficient approximation of a neuron, each neuron requires n bits of storage, then these n bits will pop up no matter how you encode things: They will be there if you model the system as an algorithm in C++, they will be there if you model it as an artificial neural net, and they will be there if you model it as a cellular automaton. 

The only way you can get around that is if there is some deeper functional abstraction, that captures brain behaviour, but has an easier representation than the state of a set of neurons.

 I think that you're underestimating the computation involved. An example ODE-modeled neuron might use several dozen equations and several dozen variables, not to mention the computation involved in, say, a standard foruth order Runge-Kutta ODE approximation. If we can create similar cellular automata rules so that each neuron uses one byte and its next step of activity is based on its surrounding neurons, the resources involved become drastically decreased. I know that in computer science we usually look at big O notation and ignore constants, but the difference between 100GB and 1TB is enough, at the moment, to make a project infeasible.

On top of that, if we can find cellular automata rules that roughly approximate the functioning of a brain, we can solidly demonstrate the feasibility of cognitive processes that don't rely on complex interplays of enzymes or even necessarily on the existence of a biological cell as we know it.

This is the "deeper functional abstraction that captures brain behavior but has an easier representation than the state of a set of neurons" (assuming, of course, the neural network -&amp;gt; cellular automata conversion is possible). </snippet></document><document><title>Is there a *nux/Mac-compatible version of Logic Friday?</title><url>http://www.reddit.com/r/compsci/comments/yzu6o/is_there_a_nuxmaccompatible_version_of_logic/</url><snippet>I use [Logic Friday](http://sontrak.com/) fairly frequently in my research, but I've only been able to find a version of it for Windows. The computing power and accessibility of my Windows machine is currently limited, so I'm wondering: is there a *nix/Mac-compatible version of Logic Friday out there?

**edit**: I found a *nix-compatible version  ftp://ftp.cs.man.ac.uk/pub/amulet/balsa/other-software/espresso-ab-1.0.tar.gz

Documentation is lacking, but this seems to be the right documentation: http://bear.ces.cwru.edu/eecs_cad/man_octtools_espresso_inputs.html

It's not as easy-to-use as Logic Friday, unfortunately.   What are you using it for? There may be other software that meets your needs, but is not exactly the same program. </snippet></document><document><title>Pascal's Apology, or why computer science is complicated.</title><url>http://carlos.bueno.org/2012/08/pascals-apology.html</url><snippet>  I have no idea if a child would get even half of this, but as a programmer myself, I applaud his effort. He does a great job of approaching problems in an intuitive manner without all that extra Jargon getting in the way. The delightful Alice in Wonderland-style story helps to keep you in that imaginative and curious mindset. Absolutely beautiful!  Seems like a cool book. Why the *wandering* salesman, and not travelling though? Changing names for the sake of it.

Anyway, my favorite laymans explanation for theoretical computer science is framing tough questions in terms of [quantitative theology](http://www.scottaaronson.com/blog/?p=56). That post is an excellent read and gets you down the rabbit hole. Also a good read are the first few pages of [this dissertation](http://people.cs.uchicago.edu/~fortnow/papers/thesis.pdf) by Lance Fortnow. 1.1: Victor and the Great Pulu.

These are by far my favorite examples of teaching computer science. They're just so damn beautiful. The problem as stated isn't the traveling salesman problem. The traveling salesman problem is, roughly, given a list of cities and the distances between them, work out the shortest path that visits each city exactly once. Oh shoot, I only read the first paragraph of the description and not the second haha. Whoops. That's just TSP with a nearest neighbor approximation, though.  &amp;gt; Blaise Pascal once famously ended a letter with an apology: I'm sorry that this was such a long letter, but I didn't have time to write you a short one.

I have seen this quote attributed to Pascal, Mark Twain, Abraham Lincoln and George Bernerd Shaw. Anyone happen to know the actual author?

Sorry this comment is off topic, but I'm curious. It's strange. There seems to be a whole ecosystem of quotes which are so good they get attached to a range of speakers. I can think of a couple more.

&amp;gt; Never explain; never apologise

which might be Disraeli or apparently a range of others.

&amp;gt;The definition of insanity is doing the same thing over and over and expecting different results.

Einstein (almost certainly not), or Twain (again).

And of course the old favourite:
&amp;gt; The trouble with quotes on the Internet is that you can never know if they are genuine.  (Abraham Lincoln)

 &amp;gt; Blaise Pascal once famously ended a letter with an apology: I'm sorry that this was such a long letter, but I didn't have time to write you a short one.

I have seen this quote attributed to Pascal, Mark Twain, Abraham Lincoln and George Bernerd Shaw. Anyone happen to know the actual author?

Sorry this comment is off topic, but I'm curious.  &amp;gt; Let me show you what I mean: A Von Neumann randomness extractor takes as input a Bernoulli sequence with p not necessarily equal to 1/2, and outputs a Bernoulli sequence with p equal to 1/2. More generally, this algorithm applies to any exchangeable sequence relying on the fact that for any pair, 01 and 10 are equally likely.

&amp;gt; And now you know as much as you did before.

I thought that was a pretty readable explanation :| Compare it to the alternative explanation. It isn't.  Are you the author of the book?

I just read the first 2 sample chapters and so far the book looks really good. The jargon speaking monsters are pretty original and the xor lizard idea is actually pretty nice. Yes. :) I'm glad you like it. I've been struggling with how to articulate what the book is about and why I wrote it, and this piece is the best answer I have so far.   The method of creating a character and having them explore complex topics in science in a novelized form is hardly new but sure is a good approach. The description of *Lauren Ipsum* reminded me a lot of [George Gamow's Mr. Tompkins books](http://en.wikipedia.org/wiki/Mr_Tompkins) which dealt largely with relativity and quantum physics. When you have a bumbling character learning particle behavior by playing quantum billiards (for example) you know you've got a captivating and accessible story.

Seeing this applied to computer science is very heartening. Might have to pick up this book.   I think the idea behind the book is great, and the first chapter was an entertaining read.  Throw in some ROUS's and being only mostly dead... There is more than one Princess Bride reference in the story. :) Both ROUSs and the Jargon in Ipsum are inspired by the chiguire.
https://www.google.com/search?q=chiguire  What he is talking about seems to be math, not science. Computer science *is* math. Is it science? &amp;gt;Computer science is a terrible name for this business.
First of all, it's not a science. It might be engineering or it might be art, but we'll actually see that computer so-called science actually has a lot in common with magic, and we'll see that in this course.

&amp;gt;So it's not a science. It's also not really very much about computers. And it's not about computers in the same sense that physics is not really about particle accelerators, and biology is not really about microscopes and petri dishes. And it's not about computers in the same sense that geometry is not really about using surveying instruments.

- Hal Abelson Source?  &amp;gt;Computer science is a terrible name for this business.
First of all, it's not a science. It might be engineering or it might be art, but we'll actually see that computer so-called science actually has a lot in common with magic, and we'll see that in this course.

&amp;gt;So it's not a science. It's also not really very much about computers. And it's not about computers in the same sense that physics is not really about particle accelerators, and biology is not really about microscopes and petri dishes. And it's not about computers in the same sense that geometry is not really about using surveying instruments.

- Hal Abelson &amp;gt;It's also not really very much about computers. And it's not about computers in the same sense that physics is not really about particle accelerators, and biology is not really about microscopes and petri dishes.

That's a tired, and plain wrong, description. Of course computer science is about computers. Software running on computers is the purpose, not a tool to reach the purpose (such as gaining knowledge of external phenomena, as in other fields). It's not even comparable to microscopes. 

It's actually the opposite: mathematics is the tool in CS. Algorithms are expressed in math simply because communication of abstract ideas happens using math.  Is it science? Doesn't seem like it to me.  CS has a lot of "given these axioms, this result happens" and not very much "how does this system work?  Let's poke it and find out", which makes it more similar to math. you are joking right ? 
&amp;gt;  not very much "how does this system work? Let's poke it and find out", 

Can you give an example of science that does this? or not? Fundamentally, a lot of great theorems are built by poking around existing systems and finding a newer more elegant solution. 

&amp;gt; which makes it more similar to math

so your point is math does not explain how a system works ? On the contrary, I think math is the only thing that explains how a system works. 
 Computer science *is* math. As I understand it, there are certain aspects that approach science &#8212; for example when you get into AI there are certain things you have to poke to find out, even when the foundations were all equations.</snippet></document><document><title>Predicting the Future: Randomness and Parsimony</title><url>http://nuit-blanche.blogspot.com/2012/08/predicting-future-randomness-and.html</url><snippet /></document><document><title>Codebreaker | Britain's Greatest Codebreaker | Alan Turing Documentary Film</title><url>http://www.turingfilm.com/</url><snippet> </snippet></document><document><title>Greg Wilson's brilliant slides about the separation between theory and practice in computer science</title><url>http://www.slideshare.net/gvwilson/two-solitudes</url><snippet>   tl;dr: the point is, you should read this book: http://www.aosabook.org/en/index.html Sadly, so much open source is in the "objectively bad, but it works anyways" camp. as opposed to closed source code? O_o As opposed to "I would trust these people to write my pacemaker software". You would trust closed source pacemaker software over open source software that's actually peer reviewable? Have you ever worked on a closed source code base? 99.9% of the time they are worse than an open source student project. There's hacks upon hacks upon hacks in there. Show me the peer reviewable pacemaker software. It doesn't exist. Open source isn't a panacea. It just expands the number of software engineers with the opportunity to perform due dilligence. I'm knees deep in open source software every day here at the Open Source Lab, and the quality of code is no better than the code base I worked on for proprietary software. &amp;gt; Show me the peer reviewable pacemaker software. It doesn't exist.

That wasn't the question or the point. The point was you were arguing that closed source pacemaker software would inherently be better. Pointing out that all pacemaker software is currently closed just makes your claim more ridiculous, because you don't have any open source pacemaker software to put down.

&amp;gt; the quality of code is no better than the code base I worked on for proprietary software.

But it's not worse? In which case your original argument is still bunk? In any case open source is not a guarantee of quality. But when there's enough interest and thus enough eyeballs on something, it tends to increase the quality. But the project has to attract enough talented devs, which isn't always the case.

Also it looks like Open Source Lab hosts 160 projects... generalizing much? &amp;gt; That wasn't the question or the point.

To bring this back to the OP, my point is that in my experience, open source doesn't "tend to increase the quality." Nor should we expect students to read source code read and learn from random open source projects as good examples of software architecture or quality. It's entire possible to not trust open source coders or medical device software devs to produce quality code. While at least there's some theoretical liability incentive to not fuck up, wheras the GPL explicitly disclaims all liability. So why should I believe the projects described in AOSP are worth studying?

For example, Moodle, featured in Volume 2, is a pretty popular, packaged in many popular distributions, and has 324 different contributors. It's a fairly rarefied success in the world of open source. And yet it has a cron.php that exits without doing anything with probability .75. "For performance". It uses SQL locks in places for no apparent reason, turning what should be an easily parallelized process into a serial one. I know this because the OSL runs Moodle in production for a handful of community colleges and a good deal of the K-12 districts in Oregon. This is just one example from my experience, and I can pull on others, but it's Friday and I have better things to do. &amp;gt; While at least there's some theoretical liability incentive to not fuck up, wheras the GPL explicitly disclaims all liability. So why should I believe the projects described in AOSP are worth studying?

How do you get a job at a place called the Open Source Lab and have such a misunderstanding of it? You are especially confused.

First off, the GPL is not the only open source license. Second, even if it were, you think that Fortune 100 companies sign deals with Redhat where Redhat has zero liability if their systems crash? The GPL explicitly disclaims liability because that's what *all* software agreements do (yes, go read the EULA that comes with any closed source software and you will see the same). That doesn't mean Redhat can't have a separate contract that makes them liable -- it just means the GPL by itself doesn't create liability.

I think you're mistakenly thinking of "open source" as "made by random kids on the internet" -- in reality most open source developers are *adults paid by companies*. Those companies either have a business that relies on the software but where it isn't their product, or they offer services relating to the software to make their profit. In the case of a pacemaker it's pretty obvious how they would make their money despite being open source -- by selling *the pacemaker*.

Finally, you seem to believe in an alternative universe where open source pace maker software would somehow not be subject to all the rules and regulations in the medical world regarding safety. Whatever standards apply regarding code quality would still be in effect, likely use of a very strictly defined subset of C and passing static analysis tests.

In other words, open source pace maker software would be produced exactly like the closed source pace maker software, except there would be the potential for people outside the company to spot bugs and suggest improvements. There is only an upside! And it could save someone's life. Honestly it's kind of ridiculous that life critical medical devices aren't required to be open source by law.  I would really like to see the talk for these slides. Does anyone have a link? I have attended the Software Carpentry Boot camp at University of Waterloo. Learned more in two days than at all of my schooling up to that point, and it was free. Greg is Awesome. Check the softwarecarpentry.org to see when it is near you This? http://software-carpentry.org/ This? http://software-carpentry.org/    So, you posted an author's pitch for us to buy his book?   [Double dipping OP](http://www.reddit.com/r/programming/comments/yw9ra/greg_wilsons_brilliant_slides_about_the/) [Double dipping OP](http://www.reddit.com/r/programming/comments/yw9ra/greg_wilsons_brilliant_slides_about_the/) [Double dipping OP](http://www.reddit.com/r/programming/comments/yw9ra/greg_wilsons_brilliant_slides_about_the/)</snippet></document><document><title>Foundations of Databases - a free book</title><url>http://webdam.inria.fr/Alice/</url><snippet /></document><document><title>Area with the Kinect?</title><url>http://thelinell.com/blog/kinecting-the-dot/</url><snippet>   If you have the field of view of the camera, the size if the sensor and you know how the depth values map to real-world measurements, then it's just trig. To get a good answer, you also need to fit planes to the depth values. You really just need to get a ruler and get busy! How though? I'm not bad at math, but apparently some things just go over my head until it clicks.  I'm confused by the formulation of the problem.

Are you looking for the surface area of one of the objects, or all of them added together? If the latter, how exactly is it supposed to tell they're actually grouped together and not just noise?  http://en.wikipedia.org/wiki/Convex_hull Can't quite say I understand how to use this in relation..may just be me not quite understanding it.  edit: I realize this is terribly long.  If you'd rather just chat add me on skype (sram1337). I'm off class for the rest of the day.


Why not just take an object of known surface area, place it a known distance away from the kinect and see how many data points it returns within a certain threshhold (From the looks of your data lets say 0.0 - 100.0).

  Say at 1 meter away, a 10x10cm object returns 3000 datapoints. Each datapoint can be though of as a tiny little square, with the entire veiw of the Kinect being the sum of all of those squares. Therefore, you could conclude that at 1m away, an object that returns 3000 datapoints, no matter how those datapoints are aranged, has a surface area of 100cm^2 (assuming the object is flat and perpendicular to the camera).  But what about at different distances?

Now for humans, percieved length scales linearly with distance:

	Percieved length = (Actual length) / (Distance from eye)


Since Area is length^2 :

Percieved length * Percieved length = Percieved Area

Percieved Area = (Actual Area) / (Distace from eye)^2


You can test this yourself if youd like.  
	Cut two squares or paper, one half as long and half as tall as the other.

	Hold the larger one twice as far as the smaller one from your eye.

	They will appear to be the exact same size.

I will assume this is the same for the kinect

So now you have the equation


	3000 datapoints = k(100cm^2 )/((1 meter)^2)

3000 being perceived area

k being a scalar

100 being actual area

1 meter being distance


I forgot to say you will want know what 1 meter is equal to in whatever units the kinect returns (Probably close to 100.0)

now you just say


	3000 datapoints = k(100cm^2 )/( (100.0)^2 )

Solve for k
	k = 30000

You now have the function

	P(A, d) = 30000(A)/(d^2 )

where P is the number or datapoints at that distance

      A is Actual area of the point its measuring

      d is distance of datapoint from camera

Algebra yeilds the function
	A(P,d) = (P*(d^2 ) )/300000

Then just plug each datapoint into the function using P=1 because youre only doing one at a time
and it should give you the amount of area in cm^2 that datapoint is representing


Hope this helps!

I spent a while typing this all up so please point out any mistakes I have made =]

</snippet></document><document><title>Which charity for the /r/compsci sidebar?</title><url>http://www.reddit.com/r/compsci/comments/yx7mc/which_charity_for_the_rcompsci_sidebar/</url><snippet>There's a [new reddit feature](https://www.redditdonate.com/) which lets us put a donation button on the sidebar. I know Computer Science has a way of getting detached and theoretical but we probably shouldn't forget that most of us are pretty lucky and that other people could use a little help. In that spirit, I'd like to add a donation link but I don't want to impose my favorite charity (www.donorschoose.org) without asking y'all. So, Computer Scientists of Reddit, which charity shall it be?   EFF? </snippet></document><document><title>Programming Language history book</title><url>http://www.reddit.com/r/compsci/comments/yvxzw/programming_language_history_book/</url><snippet>Hey what would be a good book for learning about the evolution and history of programming languages?  Like showing how LISP influenced the family of functional languages or Smalltalk's influence.  Also all those other languages I never learned in school.   There is not only one book about these, here is a small list:

* &#171; *Concepts of Programming Languages* &#187; by Robert W. Sebesta;
* &#171; *Essentials of Programming Languages* &#187; by Daniel P. Friedman and Mitchell Wand;
* &#171; *Design Concepts in Programming Languages* &#187; by Franklyn Turbak, David Gifford, Mark A. Sheldon;
* &#171; *Modern Programming Languages: A Practical Introduction* &#187; by Adam Brooks Webber;
* &#171; *Programming Languages: Principles and Practices* &#187; by Kenneth C. Louden and Lambert.  [deleted] [deleted]</snippet></document><document><title>Is there a difference between a "parameter" and an "argument"?</title><url>http://www.reddit.com/r/compsci/comments/yupva/is_there_a_difference_between_a_parameter_and_an/</url><snippet>  https://en.wikipedia.org/wiki/Parameter_%28computer_programming%29#Parameters_and_arguments

&amp;gt; These two terms are sometimes loosely used interchangeably; in particular, "argument" is sometimes used in place of "parameter". Nevertheless, there is a difference. Properly, parameters appear in procedure definitions; arguments appear in procedure calls. It's even more complicated than that. I use this distinction often enough without thinking, but there's also another one I use (and I'm not alone): a function maps arguments to a value, and parameters select the function to be applied from a family of similar functions.

In sort(xs, method='merge sort'), xs is an argument and method is a parameter. This distinction is especially used in numerical code, as it's also used in mathematics. For example, the density function of a normal distribution has one argument (x) and two parameters (&#956;, &#963;); it's a parametric family of univariate distributions. There's also argmax but no parammax. In a non-numerical context, one similarly talks about parametric polymorphism, but not argumentic(?) polymorphism (you have a parametric family of similar functions indexed by a type parameter).

You can have a global parameter (a "setting") of an algorithm, and you can tune the parameters of an algorithm (say, the 'C' parameter of a Support Vector Machine or the flags of an optimizing compiler), but you can't have global arguments and you can't tune arguments. So there is some linguistic differentiation.  I've always viewed parameters as the specification defined in the method's/function's signature and the arguments as the actual values passed into the parameters.  When I asked a professor the same question, he didn't have a clear answer.  I don't think there is any real consensus.  Differentiating between the two in the way I did can be helpful.  For example, in JavaScript, functions can accept any number of arguments.  However, they might have only two parameters defined.  If you were to try to convey the same concept in a world where argument and parameter mean the same thing, it would be a little harder to say. I've always viewed parameters as the specification defined in the method's/function's signature and the arguments as the actual values passed into the parameters.  When I asked a professor the same question, he didn't have a clear answer.  I don't think there is any real consensus.  Differentiating between the two in the way I did can be helpful.  For example, in JavaScript, functions can accept any number of arguments.  However, they might have only two parameters defined.  If you were to try to convey the same concept in a world where argument and parameter mean the same thing, it would be a little harder to say. I'm going to agree with this guy, although in practice I have always seen them used interchangeably. I've always viewed parameters as the specification defined in the method's/function's signature and the arguments as the actual values passed into the parameters.  When I asked a professor the same question, he didn't have a clear answer.  I don't think there is any real consensus.  Differentiating between the two in the way I did can be helpful.  For example, in JavaScript, functions can accept any number of arguments.  However, they might have only two parameters defined.  If you were to try to convey the same concept in a world where argument and parameter mean the same thing, it would be a little harder to say. Though I like your delineation of the two, I disagree with your JS argument because it's easy to describe with either word unilaterally:

&amp;gt; "The function has two arguments defined, but can actually be passed any number of arguments.

&amp;gt; "The function has two parameters defined, but can actually be passed any number of parameters. If you make argument and parameter interchangeable, the term "defined" becomes slightly ambiguous.  Does "defined" mean the argument has been assigned a value, or does "defined" mean the argument has been passed in?  In your example, the reader can determine what you're saying because you used the word, "but," to indicate that the second phrase is related to the first phrase.  However, in other contexts, it might be harder to distinguish.  For example, what if someone asked:

&amp;gt; How many arguments are defined for this function?

Is he asking how many of the arguments have values, or is he asking how many parameters does the function expect? I think that only in the context of your consideration of argument/parameter does the meaning of defined become ambiguous.

If you're referring to a function prototype (which you must be whenever referring to the number of inputs/outputs), then defined means "in existence". In no language that I know of can you specify inputs and literally not provide them. You can configure defaults, but you cannot leave the values undefined and still match the function prototype.

In some languages, you can over-specify arguments, but you cannot under-specify. In both instances, if another matching prototype is defined for the over-specification, it will be called instead of the de facto prototype.

In code:

* `void foobar(int foo, int bar, bool baz)` cannot be executed by calling `foobar(0)`.
* `void foobar(int foo, int bar, bool baz)` can be executed in some languages by calling `foobar(0, 1, true, c, d, e)`.
* `void foobar(int foo, int bar, bool baz)` cannot be executed by calling `foobar(0, 1, true, c, d, e)` if `void foobar(int foo, int bar, bool baz, var c, var d, var e)` is defined.

So, summarily, if someone asks "How many arguments are defined for this function?" with the intent of knowing how many inputs are *valid*, he is asking the wrong question. I've always viewed parameters as the specification defined in the method's/function's signature and the arguments as the actual values passed into the parameters.  When I asked a professor the same question, he didn't have a clear answer.  I don't think there is any real consensus.  Differentiating between the two in the way I did can be helpful.  For example, in JavaScript, functions can accept any number of arguments.  However, they might have only two parameters defined.  If you were to try to convey the same concept in a world where argument and parameter mean the same thing, it would be a little harder to say. I've always viewed parameters as the specification defined in the method's/function's signature and the arguments as the actual values passed into the parameters.  When I asked a professor the same question, he didn't have a clear answer.  I don't think there is any real consensus.  Differentiating between the two in the way I did can be helpful.  For example, in JavaScript, functions can accept any number of arguments.  However, they might have only two parameters defined.  If you were to try to convey the same concept in a world where argument and parameter mean the same thing, it would be a little harder to say. I've always viewed parameters as the specification defined in the method's/function's signature and the arguments as the actual values passed into the parameters.  When I asked a professor the same question, he didn't have a clear answer.  I don't think there is any real consensus.  Differentiating between the two in the way I did can be helpful.  For example, in JavaScript, functions can accept any number of arguments.  However, they might have only two parameters defined.  If you were to try to convey the same concept in a world where argument and parameter mean the same thing, it would be a little harder to say. I've always viewed parameters as the specification defined in the method's/function's signature and the arguments as the actual values passed into the parameters.  When I asked a professor the same question, he didn't have a clear answer.  I don't think there is any real consensus.  Differentiating between the two in the way I did can be helpful.  For example, in JavaScript, functions can accept any number of arguments.  However, they might have only two parameters defined.  If you were to try to convey the same concept in a world where argument and parameter mean the same thing, it would be a little harder to say. I've always viewed parameters as the specification defined in the method's/function's signature and the arguments as the actual values passed into the parameters.  When I asked a professor the same question, he didn't have a clear answer.  I don't think there is any real consensus.  Differentiating between the two in the way I did can be helpful.  For example, in JavaScript, functions can accept any number of arguments.  However, they might have only two parameters defined.  If you were to try to convey the same concept in a world where argument and parameter mean the same thing, it would be a little harder to say. I've always viewed parameters as the specification defined in the method's/function's signature and the arguments as the actual values passed into the parameters.  When I asked a professor the same question, he didn't have a clear answer.  I don't think there is any real consensus.  Differentiating between the two in the way I did can be helpful.  For example, in JavaScript, functions can accept any number of arguments.  However, they might have only two parameters defined.  If you were to try to convey the same concept in a world where argument and parameter mean the same thing, it would be a little harder to say.   From what I remember of an old textbook, parameter refers to what the specification says a method takes, and arguments refer to the values actually passed in.

So if you have a function:

&amp;gt; int sum(int x, int y);

you can say the sum function takes parameters x and y. However if you ever call it:

&amp;gt; int z = sum(2, 3);

you'd say z stores the result of the sum function called with arguments 2 and 3.

I have never seen the words used in a context that really seemed to care about this distinction though, so I doubt people really pay attention to it.     Klingon function calls do not have 'parameters' - they have 'arguments' -- and they ALWAYS WIN THEM. There is no Maybe or Either in Klingon Haskell!    Actual parameter: the argument passed to the function e.g: function(4) in this case actual param is 4.




Formal parameter: the argument the function takes as defined in the function 


e.g: void function(int n) in this case formal param is "n"


EDIT: 
No clue why this is downvoted. I am simply quoting:  

http://www.cambridge.org/gb/knowledge/isbn/item2704253/?site_locale=en_GB  You are simply too late to the game. Notice that the top-ranked post is an anecdote, and that one of its top-ranked responses makes the same point you do. This is the absurdity of pure democracy in full display. By the point I got to this thread, all the top rated posts and replies were all anecdotes. Link would be helpful. You are simply too late to the game. Notice that the top-ranked post is an anecdote, and that one of its top-ranked responses makes the same point you do. This is the absurdity of pure democracy in full display. While the top posts are correct one should never deviate from actual terminology. You do not call a Bypass operation a Heart-Fix-Upper. Thats why I thought I would give him the definitions from a great book written by my professor.      The way I think about it, say you have a function f(x) = ax^2 + bx + c.  Simple quadratic.  a, b, and c are the parameters, and x is the argument.  In a computer science context, you could imagine a procedure as such:

    function f = createQuadratic(1, 2, 3);
    var y = f(5);

f represents the function 1x^2 + 2x + 3 -- 1, 2, and 3 are the parameters -- and argument is 5.

As you can see, it's a conceptual difference, not a material one.  You could well have a function evaluateWeirdExpression() that takes four values a, b, c, and d and returns ad^2 + bd + c, a cubic polynomial in four variables, and it would be pretty much the same thing.

Another way to think of it is that parameters are part of the problem description.  In a model of temperature versus x-position and time, the step size and grid size are parameters of the problem.  You may have a function that performs some calculation on the time step and grid step to return, say, a characteristic velocity of some sort.  The time step and grid step would be arguments of that function, but that characteristic velocity might be considered a derived parameter.  The starting temperature profile might be a parameter depending on the goal of your model.  The current temperature at a particular grid point at a particular time, though, is not a parameter of the problem but a result. Would you say that parameters are arguments to the function that *defines* the function? That would explain why they're so often conflated.        Parameters are thread safe, arguments not necessarily</snippet></document><document><title>Chaitin's "Life as Evolving Software" completely misses the point</title><url>http://egtheory.wordpress.com/2012/06/29/proving-darwin/</url><snippet>  I don't understand the logic of trying to prove something about a system by using a model that is essentially different than the original.  

Evolution requires natural selection, which requires reproduction by a nontrivial population.  What possible reason would someone have to try to prove that model by using a different model based on one organism?  In evolution more than just DNA is relevant.  The DNA is the software, the cell structures are the hardware.  Without hardware context DNA is meaningless.  Does the RAM command the processor or does the CPU use the ram.  There are scientific examples of hereditary influence outside of the DNA sequence.  The thin bone vault has examples.  Things such as pathogens, culture, cell structures, are examples of one system effecting another outside of the DNA sequence in nontrivial ways.  Anytime a system effects another information is passed.  Of the thousands of interactions a few become consistent and there by become just as important as any other information exchange.   I think the blog is missing the point, I don't think Chaitin is aiming to directly prove Darwin correct at a mathematical level, he's aiming to prove/find a more general and simple framework that accomplishes similar things. Biological evolution, our current DNA based system, could then be viewed as merely an implementation by nature of these more general concepts.

Additionally, once you have such a ridiculously basic model that still provides results reminiscent of what we think of as evolution, concepts of individuals and heredity could be added on, one at a time, and evaluated independently. But I'm not Chaitin either, and I've only read one of his papers, so it's hard for me to say what his end goal is too.</snippet></document><document><title>Life as Evolving Software, Greg Chaitin at PPGC UFRGS [talk]</title><url>http://www.youtube.com/watch?v=RlYS_GiAnK</url><snippet>  typo in the OP's link; correct link is here http://www.youtube.com/watch?v=RlYS_GiAnK8  </snippet></document><document><title>What are the basic Programming Languages used/taught by Computer Science Students ?</title><url>http://www.reddit.com/r/compsci/comments/ys2z8/what_are_the_basic_programming_languages/</url><snippet>Hi /r/comsci , I just got accepted at a college and willing to study Computer Science. I searched a lot and found that at the first years of majoring in Computer Science , students learn many Programming languages , so my question is , what are the most common/popular/demanded languages I should start studying so I can have a good start in taking my major ? And what do you advise me to study in my spare time other than Programming languages ?

  Nowadays most universities start students off with [Java](http://www.oracle.com/technetwork/java/javase/downloads/index.html) or [Python](http://www.python.org/getit/). Those languages are both easy to learn while being versatile and they both see actual use in industry. However, it's more important that you understand the fundamentals of programming logic, control flow, data storage, etc. than any particular language.

If you want other things to study, look into discrete math and theory. Even something simple like knowing what a DFA is and their limitations would put you way ahead of the curve.

As an aside, this question isn't really suitable for this forum. /r/learnprogramming would probably be a better place to post this. &amp;gt;However, it's more important that you understand the fundamentals of programming logic, control flow, data storage, etc. than any particular language.

Second this. If you know how to program in one language, you pretty much know how to do it in every language or at least how to learn it quickly. That's true very much for imperative programming (and in most cases including object-oriented programming), which is by and large the most-often taught paradigm today.  I would recommend new learners should not settle down onto one paradigm, but also try to learn a functional paradigm, logic paradigm, etc., and learn the strengths and weaknesses of them.  By learning multiple paradigms in such a fashion, one can really learn any language.  I know there is a roadblock to many programmers when they try to learn Haskell when they hit functors and monads (much like myself). Nowadays most universities start students off with [Java](http://www.oracle.com/technetwork/java/javase/downloads/index.html) or [Python](http://www.python.org/getit/). Those languages are both easy to learn while being versatile and they both see actual use in industry. However, it's more important that you understand the fundamentals of programming logic, control flow, data storage, etc. than any particular language.

If you want other things to study, look into discrete math and theory. Even something simple like knowing what a DFA is and their limitations would put you way ahead of the curve.

As an aside, this question isn't really suitable for this forum. /r/learnprogramming would probably be a better place to post this. Thank you , I did register for a class in Java and Python and some C# , and I just posted it here because I wanted ask people in the Computer Science field , since they have more experience in what "Computer Science" needs when it comes to programming !  Nowadays most universities start students off with [Java](http://www.oracle.com/technetwork/java/javase/downloads/index.html) or [Python](http://www.python.org/getit/). Those languages are both easy to learn while being versatile and they both see actual use in industry. However, it's more important that you understand the fundamentals of programming logic, control flow, data storage, etc. than any particular language.

If you want other things to study, look into discrete math and theory. Even something simple like knowing what a DFA is and their limitations would put you way ahead of the curve.

As an aside, this question isn't really suitable for this forum. /r/learnprogramming would probably be a better place to post this. And as a second step, a functional language like Scheme is common to bridge the gap between imperative programming, which is often more approachable to beginners since it's basically commanding the computer to do something, and the more sophisticated approaches associated with functional programming.  (Although Scheme &amp;amp; co. may not be the most useful languages, the fundamental concepts/skills gained by programming in it are very useful when applied to Java, Python, or C# &amp;amp; co.)   My university teaches Scheme (lisp dialect) and C in first year, C++ in second year, and Java never. First year with Scheme? Where? Sounds awesome. First year with Scheme? Where? Sounds awesome. MIT used to do it. NorthEastern does (they have a phenomenal PLT group there). Adelphi does. Indiana might (I know Dybvig and Friedman are there). There's a bunch of schools that do. MIT used to do it. NorthEastern does (they have a phenomenal PLT group there). Adelphi does. Indiana might (I know Dybvig and Friedman are there). There's a bunch of schools that do. My university teaches Scheme (lisp dialect) and C in first year, C++ in second year, and Java never. What?!  Languages chosen for pedagogical reasons instead of fashion?  In 2012?  Where is this mythical place?  [deleted] So C++,Java ,Python !! I did read that LISP and Pascal are important too , is this right ? Are those two a priority , or just an extra thing to know more  about the history of programming , in other words , are they even used nowadays ? You likely won't be learning Pascal - don't bother looking further into it.  Lisp, however, is worth looking into when you get some CS classes under your belt. At Berkeley you learn Scheme, a dialect of Lisp, in the first programming class. Then Java and C. Berkeley just switched to Python last year

edit: year before last, 2010 academic year You likely won't be learning Pascal - don't bother looking further into it.  Lisp, however, is worth looking into when you get some CS classes under your belt. [deleted] [deleted] [deleted] So C++,Java ,Python !! I did read that LISP and Pascal are important too , is this right ? Are those two a priority , or just an extra thing to know more  about the history of programming , in other words , are they even used nowadays ? So C++,Java ,Python !! I did read that LISP and Pascal are important too , is this right ? Are those two a priority , or just an extra thing to know more  about the history of programming , in other words , are they even used nowadays ? [deleted] Pascal?  Which class would use Pascal? I used Pascal for Data Structure &amp;amp; Analysis of Algorithms, over 20 years ago. It's the  Latin of computer languages.
 [deleted] I'd say Pascal is dead, even academically.

It's a nice syntax, though.    Java and Python are definitely the big ones. I personally recommend getting familiar with javascript - if for no other reason than to develop a full appreciation for closures. If possible, I'd recommend understanding C++ object memory management since it makes the programmer much more conscientious of memory usage in managed languages.

Javascript is of course useful in practice, and also interesting as a study of a prototype language, but I feel like someone trying to 'learn' CS would just get increasingly confused by delving deeper into Javascript's internals (not only because prototype-based languages are kind of weird, but also because Javascript has so such a weird history of inconsistent implementations.) You can almost entirely safely ignore prototyping, though. &amp;gt; You can almost entirely safely ignore prototyping, though.

I think that's true if one is just going to use prefab libraries and implement recipes found online.  But, otherwise, 'safely' and 'ignore prototyping' are incompatible.  The simple act of namespacing touches on prototyping, while customizing jQuery or creating one's own library requires it. Java and Python are definitely the big ones. I personally recommend getting familiar with javascript - if for no other reason than to develop a full appreciation for closures. Lisp/Scheme will give you a full appreciation of closures.  You can do manly stuff like data structures that are closures all the way down.      Just look up Brainfuck. It's painfully easy. Easily painful you mean...      At our university we start out with [Standard ML](http://en.wikipedia.org/wiki/Standard_ML) and it's then used through out the first algorithmics course. It's a beautiful language :) Do you go to CMU?  I know they use it because they invented it!  :)  SML beautiful?  Maybe, but debatable.    Java for Com Sci at my uni, Python for Applied Computing/general IT. C++ for Comp Sci at mine and Java/VB for MIS...            Carnegie Mellon has been doing Ruby on Rails for their first class in their masters curriculum.   </snippet></document><document><title>Anyone out there ever go to school for BioInformatics?</title><url>http://www.reddit.com/r/compsci/comments/yrqrm/anyone_out_there_ever_go_to_school_for/</url><snippet>Next fall I want to transfer to Nova Southeastern as a CompSci major. Looking through the minors they offer, BioInformatics was one and it was very appealing to me. From what I learned, it blends math, biology and computer science together. Bioinformatics has helped us decode the human genome and helped us in AI and neuroscience. 

I want to work on ways to help people with disabilities play video games. I want to do this either through changes to UI, or custom made controllers. However, I think there are huge advances to be made for people with disabilities with things Google Glass, and huge strides to be made in helping completely disabled people function with computers just by linking them up to electrical impulses in the brain. 

I have a year before I transfer, but I would like to get more information on this so I don't end up rushing. I'm not sure if this is the appropriate subreddit, but any information would be appreciated. 

Edit:
From the Nova Southeastern Website:

Bioinformatics is a new scientific discipline that merges biology, computer science, mathematics, and other areas into a broad-based field that has profound impacts on all fields of biology. It is the comprehensive application of mathematics (e.g., probability and statistics), science (e.g., biochemistry), and a core set of problem-solving methods (e.g., computer algorithms) to the understanding of living systems. The bioinformatics minor provides foundational study in this emerging field of study.  I think that would be HCI ([human-computer interaction](http://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction)), rather than bioinformatics.

For your specific purpose, a lot of universities are doing research on [brain-computer interfaces](http://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface), so definitely check that out!  Yeah, that's not Bioinformatics.

I suppose that might be 'computational bio*mechanics*' (if such a field exists), but as clarle says it's probably more like HCI.

edit: No, it seems [computational biomechanics](http://www.compbiomech.com/) is something else again.   Maybe you're looking for [Biomedical Engineering](http://en.wikipedia.org/wiki/Biomedical_engineering). Which is actually a multidisciplinary engineering combining many different things together to help people.

Prosthetics is one sub-discipline, and Neural Engineering is another sub-discipline. Those can definitely help people with disabilities, and maybe even enhance people w/o disabilities. Win-win? :) Thanks! I had considered bioengineering, but I thought it dealt more with making things like dialysis pumps and heart monitors. 

One reason I was thinking about BioInformatics was I thought it had more to do with neural networking and connecting computers directly to people's brains. Working in prosthetics seems much more geared towards what I want to do though. </snippet></document><document><title>Surprisingly good video, made by the Navy, from 1962 about how a computer works.</title><url>http://gizmodo.com/5936933/how-the-navy-taught-its-engineers-about-them-new-fangled-computer-machines-back-in-1962</url><snippet>  [youtube link](http://www.youtube.com/watch?v=jDQROzrFD1o)  For what it's worth, most U.S. military-produced instructional videos are pretty damn good. At least some of you have seen the mechanical differential (ala automobile) one that's gone around, right? For what it's worth, most U.S. military-produced instructional videos are pretty damn good. At least some of you have seen the mechanical differential (ala automobile) one that's gone around, right? That one popped into my mind as I was watching this one. I love how they explain things with such simplicity and elegance.  If you like this, you need to watch this [3 part 1963 video](http://www.youtube.com/watch?v=FuKREmsiD9o) on the first GUI. It's very well made and incredibly fascinating. It's also great to see a computer occupying the whole room while the engineer fiddles with very physical buttons, knobs and switches, but also makes extensive use of a touch screen. I was amazed to find out real time 3D graphics were possible at such an early point in the development of digital computers. If you like this, you need to watch this [3 part 1963 video](http://www.youtube.com/watch?v=FuKREmsiD9o) on the first GUI. It's very well made and incredibly fascinating. It's also great to see a computer occupying the whole room while the engineer fiddles with very physical buttons, knobs and switches, but also makes extensive use of a touch screen. I was amazed to find out real time 3D graphics were possible at such an early point in the development of digital computers.    </snippet></document><document><title>Google Scholar allows you to create alerts for when a paper is cited</title><url>http://googlescholar.blogspot.com/2010/06/google-scholar-alerts.html</url><snippet>  Yep. I refer to this as my "vanity alert". </snippet></document><document><title>Sources for self teaching of Comp sci</title><url>http://www.reddit.com/r/compsci/comments/ypf1u/sources_for_self_teaching_of_comp_sci/</url><snippet>Hi. I am currently a freshmen at UNiversity of Iowa and due to certain aspects I can only be a part time student my first year. My only class is calculus 2. However I have already declared my major as Computer Science because I have taken Ap Java and a couple other smaller classes. Over this year I would like to dive into the world of Computer Science a little more and was wondering if anyone knew any websites or articles that would be easy to find so that I can sort of self teach myself some aspects of computer science and create a better foundation to learn on. Thank you for any help in advance

edit: Thank you everyone. This has been perfect and I will make full use of everything that you guys have mentioned  There are a few places online where you can access courses in **theoretical computer science**. [Coursera](https://www.coursera.org/) has a few courses. This site is great because it behaves like an online university. There are assignments, projects, and you get grades. One downside of this is that you can't access courses at any time, only when their "semester" (for lack of a better term) is running.

There are also some CS courses on [this website](http://aduni.org/courses/). This site is really just a collection of course notes and assignments so you can just look through it whenever you like. Seeing as you dont seem to have any experience in the mathematical and theory-heavy side of CS, BUT you have experience in Calculus I'd recommend skipping the "Mathematics for CS" class, and checking out courses like like Discrete Math, Algorithms, and Theory of Computation.

I'm not sure where your interests lie, or what the CS program is like at U-Iowa; but my university puts a heavy emphasis on theoretical knowledge. Seeing as that stuff is never even touched upon by high school courses, you might want to check some of the above out just because it will give you a head start above everyone else.

As for some material on **programming**, I know of the following:

- [Code Kata](http://codekata.pragprog.com/2007/01/code_kata_one_s.html) - This site consists of a bunch of small projects that you can complete in any language. The nice thing about this site is that it asks you to solve each problem in multiple ways (i.e. using certain data structures, certain algorithms, etc..)
- [Project Euler](http://projecteuler.net/problems) Provides a list of pretty difficult math exercises that can generally only be solved by writing an algorithm.
- [The Python Challenge](http://www.pythonchallenge.com/) This right here might be my favorite programming-related website on the internets. The site is one big riddle. Your job is to come up with inventive ways of using python to solve each riddle. They are very, very difficult (for me at least).

The first two are language agnostic and IMO really helpful in building the skills that will turn you into a better programmer. Code Kata is better for teaching you practical experience, while Project Euler will help you be able to think through complex math problems. Both of which are very important skills for any computer scientist. 

 There are a few places online where you can access courses in **theoretical computer science**. [Coursera](https://www.coursera.org/) has a few courses. This site is great because it behaves like an online university. There are assignments, projects, and you get grades. One downside of this is that you can't access courses at any time, only when their "semester" (for lack of a better term) is running.

There are also some CS courses on [this website](http://aduni.org/courses/). This site is really just a collection of course notes and assignments so you can just look through it whenever you like. Seeing as you dont seem to have any experience in the mathematical and theory-heavy side of CS, BUT you have experience in Calculus I'd recommend skipping the "Mathematics for CS" class, and checking out courses like like Discrete Math, Algorithms, and Theory of Computation.

I'm not sure where your interests lie, or what the CS program is like at U-Iowa; but my university puts a heavy emphasis on theoretical knowledge. Seeing as that stuff is never even touched upon by high school courses, you might want to check some of the above out just because it will give you a head start above everyone else.

As for some material on **programming**, I know of the following:

- [Code Kata](http://codekata.pragprog.com/2007/01/code_kata_one_s.html) - This site consists of a bunch of small projects that you can complete in any language. The nice thing about this site is that it asks you to solve each problem in multiple ways (i.e. using certain data structures, certain algorithms, etc..)
- [Project Euler](http://projecteuler.net/problems) Provides a list of pretty difficult math exercises that can generally only be solved by writing an algorithm.
- [The Python Challenge](http://www.pythonchallenge.com/) This right here might be my favorite programming-related website on the internets. The site is one big riddle. Your job is to come up with inventive ways of using python to solve each riddle. They are very, very difficult (for me at least).

The first two are language agnostic and IMO really helpful in building the skills that will turn you into a better programmer. Code Kata is better for teaching you practical experience, while Project Euler will help you be able to think through complex math problems. Both of which are very important skills for any computer scientist. 

    Since it hasn't been mentioned yet: The Khan Academy Why's he being downvoted? It's a valid resource that addresses the topic. 
(by the way, next time use a [link](http://www.khanacademy.org/science/computer-science)) Since it hasn't been mentioned yet: The Khan Academy   [Arsdigita University](http://aduni.org/)   I always laugh when I see the answers to questions like this. I swear people seem to have forgotten that books exist.

Go on Stanford or MIT's course websites. Find course lectures. Acquire the books they use. For a book, I recommend [Michael Sipser's *Introduction to the Theory of Computation*](http://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/113318779X/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1345775849&amp;amp;sr=1-1&amp;amp;keywords=sipser). It's incredibly expensive as well.

For more affordable resources :

* [Introduction to Algorithms](http://www.amazon.com/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844), aka CLRS, is excellent (I have one). It's the one used at MIT for their eponymous course AFAIK.

* [The Algorithm Design Manual](http://www.amazon.com/Algorithm-Design-Manual-Steven-Skiena/dp/1849967202/ref=pd_sim_b_3) has good reputation; I don't have it but people say it's as good as CLRS.
 It's incredibly expensive as well.

For more affordable resources :

* [Introduction to Algorithms](http://www.amazon.com/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844), aka CLRS, is excellent (I have one). It's the one used at MIT for their eponymous course AFAIK.

* [The Algorithm Design Manual](http://www.amazon.com/Algorithm-Design-Manual-Steven-Skiena/dp/1849967202/ref=pd_sim_b_3) has good reputation; I don't have it but people say it's as good as CLRS.
 For a book, I recommend [Michael Sipser's *Introduction to the Theory of Computation*](http://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/113318779X/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1345775849&amp;amp;sr=1-1&amp;amp;keywords=sipser).    </snippet></document><document><title>What article/paper/book best explains G&#246;del's Incompleteness Theorem and Turing's Undecidability Proof and why they're important?</title><url>http://www.reddit.com/r/compsci/comments/ypsb3/what_articlepaperbook_best_explains_g&#246;dels/</url><snippet>This is a topic I'm very interested in and the idea of whether or not math and computing has its limits is something that always comes up in my conversations with others who are interested in the Philosophy of Math and Science. However I feel that I don't know enough about CS-theory to get anything out of G&#246;del's and Turing's proofs.

Does anyone have a good text that explains these (and other related concepts) really well? It doesn't need to be completely layman, as I'm a CS undergrad, but I'd like something a bit more entertaining than a proof.  I get teased by people that I am one of only 3 people in the world to have actually finished this book, one of those being the author and the other being the person who recommended to me, but [Godel, Escher, Bach: An Eternal Golden Braid](http://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1345763522&amp;amp;sr=1-1&amp;amp;keywords=godel+escher+bach) was pretty interesting.  It covers the profoundness of the topic and is interspersed with Alice in wonderland style dialog that comes at the topic from another angle.  Deep but captivating overall. 

On a tangent, Goedel's theorem and Turing's incompleteness theorem, along with some other mathematicians who have gazed out over the edge of logic and gone somewhat mad are covered in the BBC documentary [Dangerous Knowledge](http://www.youtube.com/results?search_type=search_playlists&amp;amp;search_query=dangerous+knowledge&amp;amp;uni=1). I get teased by people that I am one of only 3 people in the world to have actually finished this book, one of those being the author and the other being the person who recommended to me, but [Godel, Escher, Bach: An Eternal Golden Braid](http://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1345763522&amp;amp;sr=1-1&amp;amp;keywords=godel+escher+bach) was pretty interesting.  It covers the profoundness of the topic and is interspersed with Alice in wonderland style dialog that comes at the topic from another angle.  Deep but captivating overall. 

On a tangent, Goedel's theorem and Turing's incompleteness theorem, along with some other mathematicians who have gazed out over the edge of logic and gone somewhat mad are covered in the BBC documentary [Dangerous Knowledge](http://www.youtube.com/results?search_type=search_playlists&amp;amp;search_query=dangerous+knowledge&amp;amp;uni=1).  I am currently working my way through [_Godel's Proof_ by Nagel and Newman](http://www.amazon.com/Godels-Proof-Ernest-Nagel/dp/0814758169). So far, it
has been rather interesting. I would highly recommend it if you are
interested. Some of the formulas on the Kindle edition don't look the greatest, but it is still legible. Also, it is surprisingly affordable.
 I was going to suggest this. Nice read. The proof here is concise and easy to read in a day, if you have the time. Also, Amazon  has this book for free shipping and dirt cheap (I think I paid like $3 for it -- used), I would get this.

The GEB is a more philosophical book that explains and talks about way more than you need to know (if your goal is to simply understand Godels argument).      These are very basic things, you can find it in usual textbooks such as:

*  [Introduction to the theory of Computation](http://essv.labri.fr/2011/03/introduction-to-the-theory-of-computation/)
* [Computational Complexity](http://essv.labri.fr/2011/03/computational-complexity/)
* [Computers and Intractability; A Guide to the Theory of NP-Completeness](http://essv.labri.fr/2011/03/computers-and-intractability-a-guide-to-the-theory-of-np-completeness/)
* [Introduction to Automata Theory, Languages, and Computation](http://essv.labri.fr/2011/03/introduction-to-automata-theory-languages-and-computation/)
* and many others... [deleted]</snippet></document><document><title>How did Google create their geocoding database?</title><url>http://www.reddit.com/r/compsci/comments/yobz7/how_did_google_create_their_geocoding_database/</url><snippet>... or anyone for that matter? How does an organization or person go about creating that database? I'm not interested in doing it myself, I'm just fascinated by the scope of such a project. Did people have to comb over countless paper maps that include house number information? Is there a much easier way I'm not thinking of? Did a large database already exist before Google got into the game? Any one know?   The most popular source of geocoding data in the US is from the Census.  The [TIGER dataset](http://www.census.gov/geo/www/tiger/) is distributed free from the government and includes the locations of counties, streets, cities and even interpolation points for house numbers (nobody really knows where specific houses are from a geocoding standpoint as it's a nightmare to keep track of.  So instead, they have one or more general ranges of house numbers a street can have and each end of the range has a geo-position attached and you interpolation between the ranges to figure out approximately where each house is).  The census of course spends all its time collating data like this.

There's also other large sources of geocoding data, but they're mostly commercial and almost always use TIGER as a base.  These are companies that are highly vested in mapping (e.g. [Navteq](http://www.navteq.com/)) and spend a large part of their time getting geolocation data however they can.  I don't know to what degree Google does their geospatial operations themselves but I know they do or did license some of it from other companies.  

There is definitely a huge scope, geocoding is an utter pain in the ass.  

Also, check [this out](http://wiki.openstreetmap.org/wiki/Potential_Datasources) The most popular source of geocoding data in the US is from the Census.  The [TIGER dataset](http://www.census.gov/geo/www/tiger/) is distributed free from the government and includes the locations of counties, streets, cities and even interpolation points for house numbers (nobody really knows where specific houses are from a geocoding standpoint as it's a nightmare to keep track of.  So instead, they have one or more general ranges of house numbers a street can have and each end of the range has a geo-position attached and you interpolation between the ranges to figure out approximately where each house is).  The census of course spends all its time collating data like this.

There's also other large sources of geocoding data, but they're mostly commercial and almost always use TIGER as a base.  These are companies that are highly vested in mapping (e.g. [Navteq](http://www.navteq.com/)) and spend a large part of their time getting geolocation data however they can.  I don't know to what degree Google does their geospatial operations themselves but I know they do or did license some of it from other companies.  

There is definitely a huge scope, geocoding is an utter pain in the ass.  

Also, check [this out](http://wiki.openstreetmap.org/wiki/Potential_Datasources) Ah! I hadn't thought of the census. So are there some Census takers who walk around with gps devices? Ah! I hadn't thought of the census. So are there some Census takers who walk around with gps devices? The problem with TIGER data is that for privacy reasons they don't give you the GPS coordinates of any street addresses. They'll give you the start and endpoints of the block, and you have to guess (usually using linear interpolation) where the actual addresses are.

Nowadays the commercial sources (I think Google originally used NavTaq before collecting it themselves) all have databases with each individual address's GPS coordinates. This makes them significantly more accurate than census data, especially in rural areas where a 'block' may be miles long with uneven address distribution.     Interns.  Interns.  You know who's downvoting you? Probably angry interns. Interns.  They do much more interesting stuff than that, trust me.      </snippet></document><document><title>How would Alan Turing develop biology?</title><url>http://egtheory.wordpress.com/2012/06/23/turing-biolog/</url><snippet /></document><document><title>Which book for first compilers course ?</title><url>http://www.reddit.com/r/compsci/comments/ykprq/which_book_for_first_compilers_course/</url><snippet>I am CS undergrad in my last year, and should have my first compilers class beginning next month. I had a simple compilers-related class in second year of uni, but it was more about coding in different languages than actual compilers.

Problem is, when I searched I found a lot of online courses, with a lot of them choosing different books, which have mixed reviews.

What you think is a good,modern and simple enough book ? We probably won't dive deep into the subject, so no need for really advanced reference books I am sure, but I don't want something thats outdated or misses some important parts, or is full of only theoretical/practical stuff.  Dragon Book.  Dragon Book.  [LFTL](http://www.amazon.com/Compilers-Principles-Techniques-Tools-Edition/dp/0321486811). link for the lazy? Dragon Book.  By far the most well-known and authoritative book covering everything from peephole optimizations to LALR parsers. Dragon Book.  I have a similar request but I do not care about compiling. All I need is the part that take a string and turn it into an abstract syntax tree. Is dragon book still the best? I have a similar request but I do not care about compiling. All I need is the part that take a string and turn it into an abstract syntax tree. Is dragon book still the best? According to your siblings, it focuses on parsers, so yes! However, it might be a bit heavy if what you really want is to be able to get a parser up and running. Although very dated, [Crenshaw's series](http://compilers.iecc.com/crenshaw/) on compilers will offer you a practical guide to writing a recursive descent parser. The code emission parts are *very* simple and *very* outdated, mind you. Read as much of it until you're able to write an arithmetic parser, by then the concept should have sunk in.

*edited* Do you have any recommendations for texts that delve into code generation/emission? Dragon Book.  Dragon Book.   The [Appel books](http://www.cs.princeton.edu/~appel/modern/) are very good, up to date and accessible. Less heavy going than the (excellent) Dragon book.  In my experience, Dragon Book is rather advanced on compilation techniques for imperative languages (C, basically), but rather old-fashioned in the techniques it presents.

I have a personal preference for Andrew Appel "Modern Compiler Implementation with ML". This is probably one of the best book on compilation to have around. It exists in some other programming languages (notably Java) but those versions are not that good, because less idiomatic. Also, it's probably a bit too advanced for the need you describe -- but may still be useful to someone else coming to this thread with slightly different needs.

Have you had a look at the material you can find on the web? A search for "compiler lecture notes" returns lot of results that would probably be just as fitting, for a not-in-depth introduction, than buying a book. Thanks a lot of your reply :)

I usually found it better for "advanced" courses to have a book, because they tend to have a clear division for the subject, and the introduction parts show what is important and whats not.

Most lecture notes I find turn out to be copied from books or are just slides which come for the instructors with the books, with some\a lot of details removed, that was true for several courses like AI, web design, and programming courses in general, with some are really outdated.

It looks like most people so far are recommending the dragon book &amp;amp; Appel's book. I'll try to get both and see which is more fitting (Java version for me, as it would be closer to the course I believe than ML. I generally never liked using functional languages for anything "big"). Compilers are a domain problem where functional languages with pattern matching *excel*. If you know some SML or OCaml (I don't think it's a good idea if you don't know them at all), have a look at both versions. Which is why you implement a little ML/Scheme pattern matcher first, if you can. amiright?  I think I don't understand your question. Can you elaborate?

If you are thinking of adding pattern matching to the implementation language of your compiler, it's not necessarily easy to do as a library. In a OO language, a visitor pattern is an approximation that can be tolerated but won't completely satisfying (eg. deep pattern matching); the [Tom](http://tom.loria.fr/wiki/index.php5/Main_Page) project has worked on bringing elaborate pattern matching to mainstream languages.

If you're thinking of adding pattern matching to the language you're trying to compile, I'm not sure how that's relevant, and that's also not easy.to do efficiently. See [neelk's blog post](http://semantic-domain.blogspot.fr/2012/08/an-ml-implementation-of-match.html) for a first approach and eg. [this article](http://moscova.inria.fr/~maranget/papers/ml05e-maranget.pdf) for a full-fledged implementation. Thanks a lot of your reply :)

I usually found it better for "advanced" courses to have a book, because they tend to have a clear division for the subject, and the introduction parts show what is important and whats not.

Most lecture notes I find turn out to be copied from books or are just slides which come for the instructors with the books, with some\a lot of details removed, that was true for several courses like AI, web design, and programming courses in general, with some are really outdated.

It looks like most people so far are recommending the dragon book &amp;amp; Appel's book. I'll try to get both and see which is more fitting (Java version for me, as it would be closer to the course I believe than ML. I generally never liked using functional languages for anything "big").     Cooper's [Engineering a Compiler](http://www.amazon.com/Engineering-Compiler-Second-Edition-Cooper/dp/012088478X/ref=pd_sim_b_1) is quite good.  I'll also second the recommendation for Appel's books.
  Dragon book is what my prof has us get. He also prefers the older Red dragon book. 

You should be aware that there are videos of compiler lectures on coursera.com  Dragon book is what my prof has us get. He also prefers the older Red dragon book. 

You should be aware that there are videos of compiler lectures on coursera.com        I didn't know there was any book besides Dragon Book.</snippet></document><document><title>Anyone interested in Software Engineering related subreddit?</title><url>http://www.reddit.com/r/compsci/comments/ylx0r/anyone_interested_in_software_engineering_related/</url><snippet>I created /r/swe if so    r/swe?  As in the Society of Women Engineers? SwE is the moniker we use at my University.   </snippet></document><document><title>Finding unknown algorithm using only input/output pairs and Z3 SMT solver</title><url>http://blog.yurichev.com/node/71</url><snippet>  </snippet></document><document><title>Optimal solution to Google's Egg Game</title><url>http://www.cs.umd.edu/~gordon/ysp/egg.pdf</url><snippet>  &amp;gt;(6) Sixth Drop: d6 = 61 + 8 = 69, use 69th floor. If does not break, break-floor is in [70; 100].
&amp;gt;
(7) Seventh Drop: d7 = 70 + 7 = 77, use 77th floor. If does not break, break-floor is in [78; 100].
&amp;gt;
(8) Eighth Drop: d8 = 70 + 7 = 77, use 77th floor. If does not break, break-floor is in [78; 100].
&amp;gt;
(9) Ninth Drop: d9 = 78 + 6 = 84, use 84th floor. If does not break, break-floor is in [85; 100].


Why would they repeat a drop from 77?  That makes no sense whatsoever. I noticed the same thing and was very puzzled... glad it's just most likely a typo rather than my own utter failure to understand basic math :) &amp;gt;(6) Sixth Drop: d6 = 61 + 8 = 69, use 69th floor. If does not break, break-floor is in [70; 100].
&amp;gt;
(7) Seventh Drop: d7 = 70 + 7 = 77, use 77th floor. If does not break, break-floor is in [78; 100].
&amp;gt;
(8) Eighth Drop: d8 = 70 + 7 = 77, use 77th floor. If does not break, break-floor is in [78; 100].
&amp;gt;
(9) Ninth Drop: d9 = 78 + 6 = 84, use 84th floor. If does not break, break-floor is in [85; 100].


Why would they repeat a drop from 77?  That makes no sense whatsoever.  I remember reading a really good article about the same problem a few months back. It was even generalized to allow N floors and K eggs. Unfortunately, all that I can remember is that the website had a black background and useful diagrams. Damn!

Edit: Found it! http://www.datagenetics.com/blog/july22012/index.html  I worked this out a couple of years ago so was hoping they would prove how to minimise the average number of drops. Minimising the worst case doesn't seem that hard. 

My suspicion is that the same solution minimises the average case but I didn't work out how to prove this. A general question from a young CS student: how do you define the average case when evaluating an algorithm? Do you assume uniform distribution or something else? A general question from a young CS student: how do you define the average case when evaluating an algorithm? Do you assume uniform distribution or something else? A general question from a young CS student: how do you define the average case when evaluating an algorithm? Do you assume uniform distribution or something else? I worked this out a couple of years ago so was hoping they would prove how to minimise the average number of drops. Minimising the worst case doesn't seem that hard. 

My suspicion is that the same solution minimises the average case but I didn't work out how to prove this. did you work out the solution when you have k eggs? I worked this out a couple of years ago so was hoping they would prove how to minimise the average number of drops. Minimising the worst case doesn't seem that hard. 

My suspicion is that the same solution minimises the average case but I didn't work out how to prove this. &amp;gt; My suspicion is that the same solution minimises the average case but I didn't work out how to prove this.

I've put some thought into this problem too and since I didn't find better solutions among a couple of candidates with different structures using pen and paper, I finally wrote a quick follow-the-gradient style optimization script for the expected number of drops.

The "official" solution is very close to being optimal for the average, but it's not. It scores 9.97 and I've found a set of solutions that score 9.95. They may well be optimal as I always ended up with them no matter what random initial vector I had started my optimization from.

So described by floor increments, one of the seemingly optimal guys:

  13 13 12 11 10 9 8 7 5 4 3 2 2 1

...and it has some room for pushing floors around a bit, like you can start with 14-12-12 or have 8-6-6 instead of 8-7-5, but it's nothing really interesting. However, if the number of total floors are 1+2+3+4+5+...+x for whatever x, the avearage optimizing and worst-case optimizing solutions are identical, and not too surprisingly x, x-1, ... 3, 2, 1.

Now back to work. Dammit!  "Google's"? Maybe it was a Google interview question. Could you imagine if you were hit with that question during an interview and they expect you come up with an answer better than ~~51~~ 50? o.O My brother and I share puzzles when we get together. He shared this with me. I solved it with him on the spot. It took a bit of time -- maybe 30-35 minutes.

You can do it, too. Maybe it was a Google interview question. Could you imagine if you were hit with that question during an interview and they expect you come up with an answer better than ~~51~~ 50? o.O "Google's"? [deleted]    Back when I solved this problem, presented to me as a riddle, after I came up with a solution I used the observed self-similarity which allows for a divide and conquer strategy to generate this difference recurrence formula for E eggs and T tries:

MaximumPossibleFloors(E,T) gives the maximum possible floors for which the breaking-floor can be established using E eggs and T tries.

MaximumPossibleFloors(E,T) = MaximumPossibleFloors(E-1,T-1) + 1 +MaximumPossibleFloors(E, T-1).

This formula is simply explained by noting that:

* when an egg crashes I must be able to check all the previous floors with one egg and one try less(=MaximumPossibleFloors(E-1, T-1)). 

* when it doesn't crash, from that point on I disregard all the lower floors (=MaximumPossibleFloors(E-1, T-1)), and the floor I tried (=1). It's as if those floors never existed. How many floors can I check with the same number of eggs, and one try less (=MaximumPossibleFloors(E,T-1)).


This solves the question easily in the case of two eggs. One can easily see, and if need be, prove that MaximumPossibleFloors(1,T)&#8801;T.
Plugging this into the differential recurrence relation:
MaximumPossibleFloors(2, T)= (T-1)+1+MaximumPossibleFloors(2, T-1).

Solving this, either with Fourier Transform* or simply [wolframAlpha](http://www.wolframalpha.com/input/?i=g%28t%29+%3D+%28t-1%29+%2B+1+%2B+g%28t-1%29%2C+g%281%29%3D1), adding the easy observation that the base case with one try is 1:
MaximumPossibleFloors(2,T) = (T^2 +T)/2.

Solving (T^2 +T)/2&amp;gt;100 for *natural* T gives T &amp;gt; 14.

The equation for E=2 might be familiar as it is [the sequence of triangular numbers](http://oeis.org/A000217).


EDIT: Fourier Transform approach:

&amp;gt;z{g(T)=(T-1)+1+g(T-1)}

&amp;gt;&amp;lt;=&amp;gt;Z{g(T)}= Z{T}+Z{g(T-1)}

&amp;gt;&amp;lt;=&amp;gt;Z{g(T)}= z/(z-1)^2 + Z{g(T)}/z

&amp;gt;&amp;lt;=&amp;gt;z*Z{g(T)}-Z{g(T)} = z^2 /(z-1)^2

&amp;gt;&amp;lt;=&amp;gt;Z{g(T)} = z^2 /(z-1)^3

&amp;gt;&amp;lt;=&amp;gt;g(T) = Z^-1 {z^2 /(z-1)^3 } </snippet></document><document><title>Programming Languages Have Social Mores Not Idioms</title><url>http://learncodethehardway.org/blog/AUG_19_2012.html</url><snippet> </snippet></document><document><title>RACTER: Artistic AI from 1982</title><url>http://www.futilitycloset.com/2012/08/15/racter/</url><snippet>  [Here](http://ubu.artmob.ca/text/racter/racter_policemansbeard.pdf)'s a PDF of The Policeman's Beard is Half Constructed, if you're interested.</snippet></document><document><title>which companies/ universities do research in AI?</title><url>http://www.reddit.com/r/compsci/comments/ygr0f/which_companies_universities_do_research_in_ai/</url><snippet>Specifically: reasoning in AI. I'm not sure how this overlaps with other subfields (machine learning, etc). 
     [The University of Edinburgh](http://www.inf.ed.ac.uk) Yup, I did my undergrad there in AI and Comp Sci.  The AI department is actually older than the CS department!  They have a focus on symbolic reasoning (at least they did 1995-1999 when I was there). I have a similar undergrad degree. If you don't mind, what do you do now? I'm a software developer (mainly user interface). It's a bit complicated :-)  

You can read my bio [on my blog](http://blog.locut.us/about/).  I'm happy to answer any questions. Thanks for freenet.  :)  Any time :-) It's a bit complicated :-)  

You can read my bio [on my blog](http://blog.locut.us/about/).  I'm happy to answer any questions. [The University of Edinburgh](http://www.inf.ed.ac.uk)  Here are the top 20 U.S. Universities in AI, according to [US News &amp;amp; World Report](http://grad-schools.usnews.rankingsandreviews.com/best-graduate-schools/top-science-schools/artificial-intelligence-rankings):

#1	Massachusetts Institute of Technology, Cambridge, MA
#2	Carnegie Mellon University, Pittsburgh, PA
#3	Stanford University, Stanford, CA
#4	University of California - Berkeley, Berkeley, CA
#5	University of Texas - Austin, Austin, TX
#6	University of Washington, Seattle, WA
#7	Georgia Institute of Technology, Atlanta, GA
#8	University of Illinois - Urbana-Champaign, Urbana, IL
#8	University of Massachusetts - Amherst, Amherst, MA
#10	University of Pennsylvania, Philadelphia, PA
#11	University of Michigan - Ann Arbor, Ann Arbor, MI
#12	Cornell University, Ithaca, NY
#13	University of Maryland - College Park, College Park, MD
#14	University of California - Los Angeles, Los Angeles, CA
#15	University of Southern California, Los Angeles, CA
#16	Columbia University, New York, NY
#17	Brown University, Providence, RI
#17	California Institute of Technology, Pasadena, CA
 Good for MIT!  The Georgia Tech of the north!!! Don't you mean 'The University of Texas' of the north? Here are the top 20 U.S. Universities in AI, according to [US News &amp;amp; World Report](http://grad-schools.usnews.rankingsandreviews.com/best-graduate-schools/top-science-schools/artificial-intelligence-rankings):

#1	Massachusetts Institute of Technology, Cambridge, MA
#2	Carnegie Mellon University, Pittsburgh, PA
#3	Stanford University, Stanford, CA
#4	University of California - Berkeley, Berkeley, CA
#5	University of Texas - Austin, Austin, TX
#6	University of Washington, Seattle, WA
#7	Georgia Institute of Technology, Atlanta, GA
#8	University of Illinois - Urbana-Champaign, Urbana, IL
#8	University of Massachusetts - Amherst, Amherst, MA
#10	University of Pennsylvania, Philadelphia, PA
#11	University of Michigan - Ann Arbor, Ann Arbor, MI
#12	Cornell University, Ithaca, NY
#13	University of Maryland - College Park, College Park, MD
#14	University of California - Los Angeles, Los Angeles, CA
#15	University of Southern California, Los Angeles, CA
#16	Columbia University, New York, NY
#17	Brown University, Providence, RI
#17	California Institute of Technology, Pasadena, CA
  [Numenta](https://grok.numenta.com/) does some interesting stuff. Hawkins' book *On Intelligence* is a great read, btw. [Numenta](https://grok.numenta.com/) does some interesting stuff. numenta's main engineer started vicarious systems. But both of them are not really doing good work (probabilistic networks). Better stuff is being done at companies that do neural networks - there's a lot of them.   If you are interested in AI and poker, check out:
http://poker.cs.ualberta.ca/

Edit: OP is redditor for 50 minutes.... ? If you are interested in AI and poker, check out:
http://poker.cs.ualberta.ca/

Edit: OP is redditor for 50 minutes.... ?            AI is a fad.</snippet></document><document><title>Writing an interpreter, CESK-style</title><url>http://matt.might.net/articles/cesk-machines/</url><snippet> </snippet></document><document><title>Five Deep Questions in Computing </title><url>http://www.cs.cmu.edu/afs/cs/usr/wing/www/publications/Wing08.pdf</url><snippet>   </snippet></document><document><title>Quantum Computer used by Harvard to solve Protein folding problems</title><url>http://nextbigfuture.com/2012/08/dwave-adiabatic-quantum-computer-used.html</url><snippet>  &amp;gt;According to the researchers, 10,000 measurements using an 81 qubit version of the experiment gave the correct answer just 13 times.

Aaaaaand there's the part I was looking for. Okay... Why?

13 / 10,000 is *brilliant*. This is *extremely good*. What's your problem, really? I'm actually puzzled by this. That's really good if the 13 right answers were the same and the 9987 other answers were all uniformly distributed throughout the search space. Just select the largest group after enough tries and you're good.

If they're all off by one errors in 81 groups (#qubits), the machine is useless. 

EDIT: Oh, no it won't. That would make it 100% reliable after analysis. Worst example ever. 

Just, equally sized groups then. 770 groups, all with 13 elements. Then it's useless! &amp;gt; That's really good if the 13 right answers were the same and the 9987 other answers were all uniformly distributed throughout the search space. Just select the largest group after enough tries and you're good.

You're missing the point.

I'm not entirely sure what computational problem they've been solving, but it's something we can verify in polynomial time -- that is to say, it is in NP (and, quite obviously now, in P, which is a subset of NP). So now, we can go through all 10,000 answers, and put each of them into the polynomial-time verification algorithm. It can then take these 13 correct answers, and *have 13 correct answers* in polynomial time. This is a polynomial time algorithm for solving this problem.

Now, I'm sure this problem wasn't NP-complete. But it might be something that was exponential on non-quantum computers. Which is incredibleness.

The thing I keep hearing is that Quantum Computers may be able to factor in polynomial time. This would mean that Quantum Computers could generate RSA secret keys from public keys, spoof secure email, and find your facebook account password, all in reasonable time. I'm not sure how verification can help. Let's transform the problem to TSP. I can check an answer to see whether it's a valid combination (one of the n!). is that verification?

I can't, however, check whether a given combination is minimal, and that's the point. I could filter out invalid combinations and take the shortest path, but there's no guarantee whether it's included in the set. Instead of viewing it as an optimization problem, view it as a decision problem. So, take coloring. You could say, "What is the optimal coloring," and yes, you'd be hard-pressed verify if a given coloring was indeed optimal. But another version of the problem -- still NP-complete -- is to ask whether or not we can achieve a given K-coloring. Can we do 2? If not, can we do 3? Go on. 

If we ask an algorithm to give us an 2-coloring of the graph, and ask it 10,000 times, and get 0 right answers, and then move on to a 3-coloring, and get 0 right answers, and move on to a 4-coloring, and get 14 right answers, and check back at the 3-coloring another 50,000 times and still get jack shit, we have our answer... Well, kind of. I guess that's still an approximation algorithm, huh? Well, I don't know whether it's probability at hand, or what -- I have no idea how the 13 "right" answers were found out of the 10,000...

I feel like there's bound to be a decision problem we could solve if we had a 13/10k oracle.

Keep in mind that being right 13/10k of the time might as well be 9999/10k, in terms of the polynomial/exponential distinction here.

----------

The more immediate impact, though, I'd say, is in crypto. In crypto, if 13/10k guesses at the key will get us the right key, then we can, in polynomial time, break the crypto. We can verify the answer by trying it. It's not an optimization problem, it's a decision problem -- you can't just check if the answer is valid, but if it's right.

----------

Can't believe this is an afterthought... But SAT. If you can find a solution to SAT, you've solved the problem. There's no *optimality* there to be concerned with at all. If you have an algorithm that *sometimes* provides a valid solution to SAT, you have an algorithm that solves SAT, and P=NP, right? &amp;gt;  But SAT. 

Oh yeah! Thanks for that thought. 

&amp;gt;  and P=NP, right?

You have to be careful with that, I don't think that applies to quantum computers.  &amp;gt; You have to be careful with that, I don't think that applies to quantum computers.

I'd say does. Quantum computers are still deterministic turing machines, with the same power -- that is to say, they solve the same set of problems.

There is a separate class of problems called QP, and technically, no, I suppose QP is not necessarily equal to P, but... I'm not concerned with whether or not a non-quantum computer can solve a given problem so much as I am concerned with whether mankind can solve a given problem.

We still don't have Nondeterministic TMs. We have Quantum TMs. QP is easy, NP might be hard. But if NP = QP, then NP is easy... Right?

I suppose it would be nice if we had Quantum Servers out there, and all, say, the Facebook devs had to do to solve TSP was use PHP instead of Javascript, but that sort of thing might only be a matter of time. There's no QP class (well, if there were, it would trivially equal either P or RP depending on your definition). It's BQP you're thinking of.  
  
BQP does not equal NP. Sorry, it simply does not. If NP = BQP, then NP = Co-NP which is "almost as false" as P = NP. Instead of viewing it as an optimization problem, view it as a decision problem. So, take coloring. You could say, "What is the optimal coloring," and yes, you'd be hard-pressed verify if a given coloring was indeed optimal. But another version of the problem -- still NP-complete -- is to ask whether or not we can achieve a given K-coloring. Can we do 2? If not, can we do 3? Go on. 

If we ask an algorithm to give us an 2-coloring of the graph, and ask it 10,000 times, and get 0 right answers, and then move on to a 3-coloring, and get 0 right answers, and move on to a 4-coloring, and get 14 right answers, and check back at the 3-coloring another 50,000 times and still get jack shit, we have our answer... Well, kind of. I guess that's still an approximation algorithm, huh? Well, I don't know whether it's probability at hand, or what -- I have no idea how the 13 "right" answers were found out of the 10,000...

I feel like there's bound to be a decision problem we could solve if we had a 13/10k oracle.

Keep in mind that being right 13/10k of the time might as well be 9999/10k, in terms of the polynomial/exponential distinction here.

----------

The more immediate impact, though, I'd say, is in crypto. In crypto, if 13/10k guesses at the key will get us the right key, then we can, in polynomial time, break the crypto. We can verify the answer by trying it. It's not an optimization problem, it's a decision problem -- you can't just check if the answer is valid, but if it's right.

----------

Can't believe this is an afterthought... But SAT. If you can find a solution to SAT, you've solved the problem. There's no *optimality* there to be concerned with at all. If you have an algorithm that *sometimes* provides a valid solution to SAT, you have an algorithm that solves SAT, and P=NP, right? No. I can write a program to randomly guess an assignment with probability 1/2 and return False with probability 1/2. It'll be correct at least a 1/2^(n+1) fraction of the time, but it doesn't actually do anything close to resolving P vs. NP. You'd have to go through exponentially many rounds before you get to a correct solution. &amp;gt;According to the researchers, 10,000 measurements using an 81 qubit version of the experiment gave the correct answer just 13 times.

Aaaaaand there's the part I was looking for. Well, 2^-81 is about 0.000 000 000 000 000 000 000 000 4

whereas 13/10000 is 0.000 13.

Am I missing something here? Well, 2^-81 is about 0.000 000 000 000 000 000 000 000 4

whereas 13/10000 is 0.000 13.

Am I missing something here? 13/10000 is useless, a majority of the answers you get are wrong.

A useful algorithm would have some constant probability (greater than 1/2) of giving the correct answer, then you could take the majority. Even this relatively uninformative article shows why what you said isn't the case. If you lower the search space significantly for a problem whos solutions are easy to check, the algorithm is useful.

You're putting up constraints that are unnecessary. TL;DR: Quantum Computer used by Harvard to *significantly reduce the search space of* Protein folding problems 13/10000 is useless, a majority of the answers you get are wrong.

A useful algorithm would have some constant probability (greater than 1/2) of giving the correct answer, then you could take the majority.  Usually I'm rather comfortable  reading such articles, but this doesn't tell me anything.

Is it fast? How much faster than regular computers? Is the big O complexity different? Was it reliable enough?

The only answer that I found was that it was 80.3% reliable at 20 mK. But that was a local maximum because of how it was designed, I think? (Meaning that cooling it down would make it become less reliable (to 55%) before going back up to 100% at 0 K.)

I'm sure of one thing, there's going to be a lot of badly investigated sensationalist articles about this. Usually I'm rather comfortable  reading such articles, but this doesn't tell me anything.

Is it fast? How much faster than regular computers? Is the big O complexity different? Was it reliable enough?

The only answer that I found was that it was 80.3% reliable at 20 mK. But that was a local maximum because of how it was designed, I think? (Meaning that cooling it down would make it become less reliable (to 55%) before going back up to 100% at 0 K.)

I'm sure of one thing, there's going to be a lot of badly investigated sensationalist articles about this. The Nature blog post did not answer your complexity question. it did not answer the faster than regular computers question. 

I [Brian Wang, who writes Nextibigfuture] will note that I did not repeat and resummarize the key parts of 80 articles related to Dwave for each new article. I write about ten articles per day. I usually cover tech and science news before it gets to Reddit. I provide the links to the research papers and information released from the company or research institution. For whatever reason, a lot of my stuff gets rejected from Reddit Science or sometimes ends up being less popular than a "more legitimate source".

I also covered, that Dwave got more funding to cover the next two years, so they should reach profitably and be able to IPO
http://nextbigfuture.com/2012/08/dwave-systems-landed-228-million-in.html

I had an interview with DWave CTO Geordie Rose in Dec, 2011
http://nextbigfuture.com/2011/12/dwave-systems-tour-and-interview-with.html

The amount of speedup depends upon the different quantum algorithms that are being run.

If the (cancer radiation) treatment optimization problem were indicative of the speedup of different algorithms, then one might expect (I am extrapolating the 512/128 qubit example)

512 qubits 1000 times faster than 128 qubits
2048 qubits 1000 times faster than 512 qubits

Dwave sold a system to Lockheed for $10 million back in late 2010
http://nextbigfuture.com/2011/05/d-wave-systems-sells-its-first-quantum.html

I made a public bet in 2006 that Dwave would get to over 100 qubits for a commercial system.
http://longbets.org/266/

&#8220;There will be a quantum computer with over 100 qubits of processing capability sold either as a hardware system or whose use is made available as a commercial service by Dec 31, 2010&#8221; 

== Obviously I nailed this prediction with the Dwave sale to Lockheed. Nostrodamus was cryptic poetry writing pansy in terms of predictions

In Jan 2010, IEEE Spectrum picked Dwave systems as a loser.
http://spectrum.ieee.org/computing/hardware/loser-dwave-does-not-quantum-compute

In july 2011, IEEE Spectrum recants and admits they were wrong
http://spectrum.ieee.org/podcast/computing/hardware/big-win-for-the-losers-at-dwave

IEEE spectrum could not predict as well as I did when I made a prediction in 2006 when they were predicting nearly 4 years later.


Did Nature blog make any accurate predictions ? Did Nature blog cover the speedup issue ? 

I have written over 80 articles on Dwave and adiabatic quantum computers
http://nextbigfuture.com/search/label/dwave

Adiabatic computers do outperform all classical computers for some problems
http://dwave.wordpress.com/2010/06/05/adiabatic-quantum-computing-the-greatest-idea-in-the-history-of-humanity/


Also about Scott Aaronson at Shetyl Optimized 
http://nextbigfuture.com/2012/04/scott-aaronson-visited-offices-of.html

Scott talking about the evidence of quantumness in Nature
http://www.scottaaronson.com/blog/?p=639

http://www.nature.com/nature/journal/v473/n7346/full/nature10012.html

&amp;gt;I [Scott Aaronson] don&#8217;t have any regrets about pouring cold water on D-Wave&#8217;s previous announcements, &amp;gt;because as far as I can tell, I was right!  For years, D-Wave trumpeted &#8220;quantum computing demonstrations&#8221; &amp;gt;that didn&#8217;t demonstrate anything of the kind; tried the research community&#8217;s patience with hype and irrelevant &amp;gt;side claims; and persistently dodged the central question of how it knew it was doing quantum computing &amp;gt;rather than classical simulated annealing. 

&amp;gt;I [Scott Aaronson] hereby announce my retirement as Chief D-Wave Skeptic, a job that I never wanted in the &amp;gt;first place.  New applicants for this rewarding position are urged to apply in the comments section; background &amp;gt;in experimental physics a must.

&amp;gt;I [Scott Aaronson] hereby retire my notorious comment from 2007, about the 16-bit machine that D-Wave &amp;gt;used for its Sudoku &amp;gt;demonstration being no more computationally-useful than a roast-beef sandwich. D-Wave &amp;gt;does have something today that&#8217;s more computationally-useful than a roast-beef sandwich; the question is &amp;gt;&#8220;merely&#8221; whether it&#8217;s ever more useful than your laptop. Geordie presented graphs that showed D-Wave&#8217;s &amp;gt;quantum annealer solving its Ising spin problem &#8220;faster&#8221; than classical simulated annealing and tabu search &amp;gt;(where &#8220;faster&#8221; means ignoring the time for cooling the annealer down, which seemed fair to me). &amp;gt;Unfortunately, the data didn&#8217;t go up to large input sizes, while the data that did go up to large input sizes only &amp;gt;compared against complete classical algorithms rather than heuristic ones.

&amp;gt;In summary, while the observed speedup is certainly interesting, it remains unclear exactly what to make of it, &amp;gt;and especially, whether or not quantum coherence is playing a role.

So Scott ended up being wrong, but slowly and partially backed away from his claims. But now he claims that he made the claims because Dwave did not spend time publishing proof experiments instead of building a business. 

I [Brian Wang, Nextbigfuture] was right all along and covered it in detail and described the algorithms and the applications as they developed.  But go ahead look to Scott Aaronson who was wrong and look to IEEE Spectrum who were wrong or a Nature blog (which has less coverage and fewer answers). Except Scott Aaronson isn't backing away from his claims. He was completely correct -we should not be expected to believe extraordinary claims that have *absolutely no proof* behind them.

Also, HOLY SHIT, stop writing, go learn some stuff, and change your profession to one more productive than blogspam. Your post is a small essay in which you claim you shit ice cream while criticizing one of the top scientific journals in the world and a well-regarded expert in the theory side of quantum computing. You, by contrast, clearly have no idea what you're talking about.

The fact that your self-aggrandizing, fact-less rant has ANY upvotes makes me wonder if I'm not imagining reddit existing from a padded room somewhere. The claims have no proof ? They published evidence in Nature. Aaronson did back away. He just did not back all the way away...yet.  I criticized the Nature blog, not the Nature journal. Aaronson does know more quantum computer theory but his predictions were wrong. He predicted that Dwave would have no financial success. Lockheed paid Dwave $10 million for services and a system. Google works with Dwave regularly on image recognition and machine learning training. My prediction was recorded at Longbets. Every word and justification of it was right. Those are facts. So yes it was a rant, but a fact filled rant. Too bad you cannot deal with the facts that go against your beliefs. IEEE Spectrum was wrong. It is a fact and I provided the links. They had to recant. I am confident more proof will continue to come and better results from the 512 qubit chip that already exists and larger chips that will follow and further proof of the quantum science will be published as well. The trends in publications  by Dwave and collaborators (Harvard, Google etc...) is obvious. The increase in qubits and system power is obvious. But I understand that you will want to wait. No problem. It has been 6 years since I first attended the debut of the 16 qubit system in Mountain View at the Computer History Museum. I know it will be about another 2-6 years for more to be done in the science and the commercial development. 2-3 years for published work from the 512 qubit systems and for commercial work with its faster capabilities. 4-6 years for 1024 or 2048 qubit systems.

6 years ago the argument and mockery was about 16 qubits and Sudoku problems. Now it is multiple research papers in Nature and other journals, multi-million dollar sale to Lockheed, protein folding, machine learning training with Google. 128 qubit chip with 81 used for 2 years old work to get written and published. 512 qubit chip already used for 10 months.

Aaronson 6 years said Dwave was more useless computationally than a sandwich. Last year he admitted Dwave was doing useful work but that quantum experimentalists were needed to critique what Dwave was doing.

I am just saying what happened as someone who tracked it closely and who was right about what did happen. I am telling you what will happen with the 512 qubit and larger systems. There is the possibility that Dwave could stay with refining 512 qubit systems for a while if they have a lot of commercial success with those and do not have the free resources to run the larger system development in parallel. The Nature blog post did not answer your complexity question. it did not answer the faster than regular computers question. 

I [Brian Wang, who writes Nextibigfuture] will note that I did not repeat and resummarize the key parts of 80 articles related to Dwave for each new article. I write about ten articles per day. I usually cover tech and science news before it gets to Reddit. I provide the links to the research papers and information released from the company or research institution. For whatever reason, a lot of my stuff gets rejected from Reddit Science or sometimes ends up being less popular than a "more legitimate source".

I also covered, that Dwave got more funding to cover the next two years, so they should reach profitably and be able to IPO
http://nextbigfuture.com/2012/08/dwave-systems-landed-228-million-in.html

I had an interview with DWave CTO Geordie Rose in Dec, 2011
http://nextbigfuture.com/2011/12/dwave-systems-tour-and-interview-with.html

The amount of speedup depends upon the different quantum algorithms that are being run.

If the (cancer radiation) treatment optimization problem were indicative of the speedup of different algorithms, then one might expect (I am extrapolating the 512/128 qubit example)

512 qubits 1000 times faster than 128 qubits
2048 qubits 1000 times faster than 512 qubits

Dwave sold a system to Lockheed for $10 million back in late 2010
http://nextbigfuture.com/2011/05/d-wave-systems-sells-its-first-quantum.html

I made a public bet in 2006 that Dwave would get to over 100 qubits for a commercial system.
http://longbets.org/266/

&#8220;There will be a quantum computer with over 100 qubits of processing capability sold either as a hardware system or whose use is made available as a commercial service by Dec 31, 2010&#8221; 

== Obviously I nailed this prediction with the Dwave sale to Lockheed. Nostrodamus was cryptic poetry writing pansy in terms of predictions

In Jan 2010, IEEE Spectrum picked Dwave systems as a loser.
http://spectrum.ieee.org/computing/hardware/loser-dwave-does-not-quantum-compute

In july 2011, IEEE Spectrum recants and admits they were wrong
http://spectrum.ieee.org/podcast/computing/hardware/big-win-for-the-losers-at-dwave

IEEE spectrum could not predict as well as I did when I made a prediction in 2006 when they were predicting nearly 4 years later.


Did Nature blog make any accurate predictions ? Did Nature blog cover the speedup issue ? 

I have written over 80 articles on Dwave and adiabatic quantum computers
http://nextbigfuture.com/search/label/dwave

Adiabatic computers do outperform all classical computers for some problems
http://dwave.wordpress.com/2010/06/05/adiabatic-quantum-computing-the-greatest-idea-in-the-history-of-humanity/


Also about Scott Aaronson at Shetyl Optimized 
http://nextbigfuture.com/2012/04/scott-aaronson-visited-offices-of.html

Scott talking about the evidence of quantumness in Nature
http://www.scottaaronson.com/blog/?p=639

http://www.nature.com/nature/journal/v473/n7346/full/nature10012.html

&amp;gt;I [Scott Aaronson] don&#8217;t have any regrets about pouring cold water on D-Wave&#8217;s previous announcements, &amp;gt;because as far as I can tell, I was right!  For years, D-Wave trumpeted &#8220;quantum computing demonstrations&#8221; &amp;gt;that didn&#8217;t demonstrate anything of the kind; tried the research community&#8217;s patience with hype and irrelevant &amp;gt;side claims; and persistently dodged the central question of how it knew it was doing quantum computing &amp;gt;rather than classical simulated annealing. 

&amp;gt;I [Scott Aaronson] hereby announce my retirement as Chief D-Wave Skeptic, a job that I never wanted in the &amp;gt;first place.  New applicants for this rewarding position are urged to apply in the comments section; background &amp;gt;in experimental physics a must.

&amp;gt;I [Scott Aaronson] hereby retire my notorious comment from 2007, about the 16-bit machine that D-Wave &amp;gt;used for its Sudoku &amp;gt;demonstration being no more computationally-useful than a roast-beef sandwich. D-Wave &amp;gt;does have something today that&#8217;s more computationally-useful than a roast-beef sandwich; the question is &amp;gt;&#8220;merely&#8221; whether it&#8217;s ever more useful than your laptop. Geordie presented graphs that showed D-Wave&#8217;s &amp;gt;quantum annealer solving its Ising spin problem &#8220;faster&#8221; than classical simulated annealing and tabu search &amp;gt;(where &#8220;faster&#8221; means ignoring the time for cooling the annealer down, which seemed fair to me). &amp;gt;Unfortunately, the data didn&#8217;t go up to large input sizes, while the data that did go up to large input sizes only &amp;gt;compared against complete classical algorithms rather than heuristic ones.

&amp;gt;In summary, while the observed speedup is certainly interesting, it remains unclear exactly what to make of it, &amp;gt;and especially, whether or not quantum coherence is playing a role.

So Scott ended up being wrong, but slowly and partially backed away from his claims. But now he claims that he made the claims because Dwave did not spend time publishing proof experiments instead of building a business. 

I [Brian Wang, Nextbigfuture] was right all along and covered it in detail and described the algorithms and the applications as they developed.  But go ahead look to Scott Aaronson who was wrong and look to IEEE Spectrum who were wrong or a Nature blog (which has less coverage and fewer answers). [deleted] Usually I'm rather comfortable  reading such articles, but this doesn't tell me anything.

Is it fast? How much faster than regular computers? Is the big O complexity different? Was it reliable enough?

The only answer that I found was that it was 80.3% reliable at 20 mK. But that was a local maximum because of how it was designed, I think? (Meaning that cooling it down would make it become less reliable (to 55%) before going back up to 100% at 0 K.)

I'm sure of one thing, there's going to be a lot of badly investigated sensationalist articles about this.  From a legitimate source:

[http://blogs.nature.com/news/2012/08/d-wave-quantum-computer-solves-protein-folding-problem.html](http://blogs.nature.com/news/2012/08/d-wave-quantum-computer-solves-protein-folding-problem.html) Legitimate and intelligible, thanks.  I don't see in what way the blogl.nature.com article was better.

Don't just attack smaller content providers for no reason. AFAICT it's someone trying to provide better information, and they do seem to put in work for it.   </snippet></document></searchresult>
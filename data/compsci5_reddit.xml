<searchresult><compsci /><document><title>Question about Simulated Annealing for Discrete Constraint Satisfaction Problems</title><url>http://www.reddit.com/r/compsci/comments/1376z1/question_about_simulated_annealing_for_discrete/</url><snippet>Hello,

I'm investigating combinatorial test design algorithms, and one of the [primary resources](http://www.public.asu.edu/~ccolbou/src/tabby/catable.html) I've been using has a list solutions to a set covering array problem parameters, many of which are listed as having been found via simulated annealing.

I also know that other discrete constraint satisfaction problems (e.g., n-queens) can be solved with simulated annealing as well. I'm wondering what exactly the strategy would be for using a sub-optimal approximation algorithm when attempting to find solutions.

For example, in combinatorial testing, there are many solutions for finding t-wise testing on a set of variables, and we don't care about finding the best; we just want one that's not terrible. But there are even more answers that don't meet the criteria of the constraints (i.e., not all t-wise tuples are represented in the solution).

So, when building a simulated annealing algorithm to find a solution to n-queens or t-wise testing, what is the strategy to avoid returning a solution that is not only suboptimal solution, but also doesn't even meet the criteria of the constraints? Similarly, how would you know if a given set of inputs even has a solution (e.g., t-wise testing + variable combination constraints could yield no valid covering array).</snippet></document><document><title>Surprised by a research opportunity, no idea what to research... </title><url>http://www.reddit.com/r/compsci/comments/134z0i/surprised_by_a_research_opportunity_no_idea_what/</url><snippet>I'm getting to the end of my first semester of junior year as a CS undergrad student. I was approached by the department's head of research, and offered a two semester research sequence of independent studies to take the place of a few other CS classes, but I have a very small window to submit my research proposal. 

These types of opportunities are really only offered to the top of our major, and I had no idea I was doing well enough to even be considered, so I have put no effort into considering possible research opportunities. I have spent the last few days running around the Computer Science Publications I could get my hands on, and I just haven't found anything that clicked as an interesting, beneficial research opportunity. 

I decided to now turn to reddit, and ask you all, "What are some up and coming areas that warrant a full 2 semester research program that I should look into?"

Thanks in advance for all your help!!   The correct answer is to find a professor that you like, working on things that you like, and talk to them. That will give you someone with whom you can meet and discuss possible directions to follow that they can personally help you with.

Without knowing anything in advance about you, your qualifications, or your interests, this is really the only possible advice, because the answer to your question is basically "everything". Haha, okay, well that's where this get's tricky. I'm currently thousands of mile away from my college, participating in a semester abroad, and the research proposal is due before I'm set to return. 

Like I said, I didn't know I would even have a reasonable chance for this, so I never spent much time speaking with our professors on this topic, and in addition to this, the professors at my college aren't actively involved in research while teaching.

My qualifications are exceptionally limited, some self taught web stuff (PHP, JavaScript), a bunch of classwork which has almost all been done in Java, and self-taught Python skills. My interests are still pretty undeveloped, so far in my search a lot of new computer forensics information is what has most interested me, but I have no idea where I would start in there for research.

I know this seems like a lost cause, but I'm afraid that if I allow this opportunity to pass me by I will regret it for the rest of my career, so I'm kind of flailing here hoping to catch some person who will be my hero for the rest of my academic career for giving me my research idea haha.  If you're flailing hard enough that you think it might slip from your grasp, email _any_ professor and ask for advice. Worst case is they say no. Second worst case is they point you to someone else to talk to -- and that's not a bad case.

If that doesn't/can't work, talk to the head of research. Saying "Given my position, I don't know how to begin" is better than simply not doing anything. Haha, okay, well that's where this get's tricky. I'm currently thousands of mile away from my college, participating in a semester abroad, and the research proposal is due before I'm set to return. 

Like I said, I didn't know I would even have a reasonable chance for this, so I never spent much time speaking with our professors on this topic, and in addition to this, the professors at my college aren't actively involved in research while teaching.

My qualifications are exceptionally limited, some self taught web stuff (PHP, JavaScript), a bunch of classwork which has almost all been done in Java, and self-taught Python skills. My interests are still pretty undeveloped, so far in my search a lot of new computer forensics information is what has most interested me, but I have no idea where I would start in there for research.

I know this seems like a lost cause, but I'm afraid that if I allow this opportunity to pass me by I will regret it for the rest of my career, so I'm kind of flailing here hoping to catch some person who will be my hero for the rest of my academic career for giving me my research idea haha.  Just curious... which country are you in now... Haha, okay, well that's where this get's tricky. I'm currently thousands of mile away from my college, participating in a semester abroad, and the research proposal is due before I'm set to return. 

Like I said, I didn't know I would even have a reasonable chance for this, so I never spent much time speaking with our professors on this topic, and in addition to this, the professors at my college aren't actively involved in research while teaching.

My qualifications are exceptionally limited, some self taught web stuff (PHP, JavaScript), a bunch of classwork which has almost all been done in Java, and self-taught Python skills. My interests are still pretty undeveloped, so far in my search a lot of new computer forensics information is what has most interested me, but I have no idea where I would start in there for research.

I know this seems like a lost cause, but I'm afraid that if I allow this opportunity to pass me by I will regret it for the rest of my career, so I'm kind of flailing here hoping to catch some person who will be my hero for the rest of my academic career for giving me my research idea haha.  Haha, okay, well that's where this get's tricky. I'm currently thousands of mile away from my college, participating in a semester abroad, and the research proposal is due before I'm set to return. 

Like I said, I didn't know I would even have a reasonable chance for this, so I never spent much time speaking with our professors on this topic, and in addition to this, the professors at my college aren't actively involved in research while teaching.

My qualifications are exceptionally limited, some self taught web stuff (PHP, JavaScript), a bunch of classwork which has almost all been done in Java, and self-taught Python skills. My interests are still pretty undeveloped, so far in my search a lot of new computer forensics information is what has most interested me, but I have no idea where I would start in there for research.

I know this seems like a lost cause, but I'm afraid that if I allow this opportunity to pass me by I will regret it for the rest of my career, so I'm kind of flailing here hoping to catch some person who will be my hero for the rest of my academic career for giving me my research idea haha.  lol_fps_newbie is extremely correct.  Email a professor.  Explain the situation, your interests, and ask if they might have any ideas for projects. It doesn't matter if they don't know you, though it would help if you've taken a class from them.

One thing I learned when I started grad school is that you don't start with your own research ideas.  You glom on to a professor who does interesting stuff, research the hell out of a particular little idea they have, and at that point you'll be in a better place to start your own research. If you independently come up with your own idea, without any background, there's a 95% chance that either (1) it was done in the 1970's and is old hat or (2) nobody thinks it's interesting or (3) both. Haha, okay, well that's where this get's tricky. I'm currently thousands of mile away from my college, participating in a semester abroad, and the research proposal is due before I'm set to return. 

Like I said, I didn't know I would even have a reasonable chance for this, so I never spent much time speaking with our professors on this topic, and in addition to this, the professors at my college aren't actively involved in research while teaching.

My qualifications are exceptionally limited, some self taught web stuff (PHP, JavaScript), a bunch of classwork which has almost all been done in Java, and self-taught Python skills. My interests are still pretty undeveloped, so far in my search a lot of new computer forensics information is what has most interested me, but I have no idea where I would start in there for research.

I know this seems like a lost cause, but I'm afraid that if I allow this opportunity to pass me by I will regret it for the rest of my career, so I'm kind of flailing here hoping to catch some person who will be my hero for the rest of my academic career for giving me my research idea haha.  Find a security/forensics guy in your department. Professors usually lists their research interests on their websites/profile pages. Email him/her and explain your situation. Ask for advice.

Honestly, no one expects you to come up with a research project out of the blue. Some students do, but most need help getting started. No one is going to judge you if you say "I would love to do this, but I don't know where to start". They were in the same boat when they were your age. They will understand. The correct answer is to find a professor that you like, working on things that you like, and talk to them. That will give you someone with whom you can meet and discuss possible directions to follow that they can personally help you with.

Without knowing anything in advance about you, your qualifications, or your interests, this is really the only possible advice, because the answer to your question is basically "everything".  CS prof here. You have almost no chance of writing a proposal that makes sense to a researcher without some help*. Get in touch with a prof you respect in the department, explain the situation and ask for advice or a referral to someone else who can help. 

*This is by no means disrespecting your intelligence; it's just that it's extremely difficult to know what needs to done without quite a lot of experience. We call the introductory training on this  "grad school".

edit: I should have added: if you want to work with someone, you _must_ read at least a bit of their stuff before contacting them. It's very tedious to field requests about working on topic A when I'm a topic B researcher, and I might well just ignore the request because someone is asking for my time without investing their own.  Thanks for you input sir, I'm right now in need of a topic more than a fullblown written proposal. It will be edited and flushed out, but I need to have a solid foundation, even before I find a finalized adviser, etc.

I appreciate your time and input though. The topic is the hardest part, but you're welcome. That should be "fleshed out" by the way, unless you mean something quite different :)
 I meant the full research proposal request, the topic, is mostly mine to flush out, which is how I ended up here, asking the wonderful people of reddit for topic ideas...

If you happen to know about anything cool in the world of computer forensics happening currently I'm kind of leaning towards finding out a project in that general area. Lesson 1 in research: listen carefully to professors.

/u/freak_t is trying to tell you that "flushed out" is the wrong phrase and makes him/her think you're going to put your proposal in the toilet. You mean "fleshed out".  Lesson 1 in research: listen carefully to professors.

/u/freak_t is trying to tell you that "flushed out" is the wrong phrase and makes him/her think you're going to put your proposal in the toilet. You mean "fleshed out".  Lesson 1 in research: listen carefully to professors.

/u/freak_t is trying to tell you that "flushed out" is the wrong phrase and makes him/her think you're going to put your proposal in the toilet. You mean "fleshed out".  Of course, right now, you ARE attempting to flush out a research topic.  For a start, I'd pick to work with the professor you like the most or are closest to, in whatever they tell you to work on.

The reality is that from your current position you simply have no clue what research is like, what different areas of computer science are like, or how to pick between them. I know because I was in the same position not too long ago. The only time you are informed enough to decide is long after the decision has been made.

So don't worry about making an informed decision based on what a good project would be or something like that. It will change a dozen times or more before you're through anyway. But if you have a good mentor, those changes will all be for the good and you'll be happy with where they lead, hopefully.

As for the writeup, my undergrad experience with such things was that you turn it in and everybody forgets it, then you pick your real project and do that instead. So I'd worry more about finding something plausible in the short term than that this writeup will actually lock you in/set in stone your research path. But yeah, ask professor(s) for advice on that too.

.......

To try to answer the original question, here are some random areas:

 * programming languages, compilers, functional programming, provable correctness of programs
 * systems, large scale design and analysis
 * theory: algorithms, randomness, graphs, complexity and learning theory, electronic commerce
 * artificial intelligence (this doesn't really mean what you think it means): proposing/solving problems, search, multi-agent systems, much more
 * computer vision
 * machine learning: interpreting large amounts of data, classifying, predicting, natural language processing
 * human-computer interaction: user experience, crowdsourcing/human computation

Hmm, come to think of it, the [wikipedia page for CS](http://en.wikipedia.org/wiki/Computer_science) is a good start too.

If you mention one you're curious about, I'm sure someone here will have something to say about it. Wow, thanks for the tips, you hit the nail on the head with almost all your information up there. I'm not sure our program is quite as flexible as the average research, but I have 0 experience, so I'm just pondering based off my other CS experiences so far. 

I have an email out to my favorite professor, but his specialty area was proofs, and proof carrying code, which i have no knowledge, skill, or much interest in, so he has suggested I try someone else before committing to him. 

What is computer vision in the context you mentioned it, and where would I start reading up on it?

Thanks!! I guess the wiki page would be a start!

http://en.wikipedia.org/wiki/Computer_vision

I don't know much about it, but I think the basic idea is: Here's a picture/video, what's in it? A really tough problem to solve.  I work in an undergraduate/graduate research lab at my school and have been doing so for the last 2 years (I am a senior right now). While we focus on government funded research I get some time to work on my own research (getting ready to start a thesis next Fall).

The most interesting work I've done so far has been in Machine Learning (ML) / Artificial Intelligence (AI). It involves a lot of math, so if you hate doing lots of math (typically calculus and statistics) this might be a bad area. There's lots of active research being done right now in Machine Learning and so you can really pick any area and go from there. Most universities also have a professor or two who specialize in ML and are typically the most active research professors. ([Some ML Research Options](http://www.csml.ucl.ac.uk/courses/msc_ml/?q=node/37))

The other big area of research right now is Computer Security. Typically a lot less math but a little less to do in this field since there is a lot of money being put into it right now. At our school the biggest area of security research is in Hardware/Firmware Security. Cloud-based security is a big topic right now as well. If you want to investigate this field check out some of the most recent conferences ([Defcon](https://www.defcon.org/html/links/dc-archives.html), [IronGeek](http://www.irongeek.com/), [Security Tube](http://www.securitytube.net/)) The math doesn't scare me off, so I have been looking through the ML options you gave, but I guess my trouble is seeing what I can do in this that others have not already been doing, which is obviously the whole point of research, it's just that I feel I need more background. I think tonight will be a very long reading session in machine learning for me. 

Computer security, in both hardware and software form, are done to death here, and by students who already have a leg up on me by competing on our cyber warfare competition team, etc. So I'm kind of shying away from that, but it's still obviously a possibility.  If you interested in ML then there is [Kaggle](https://www.kaggle.com/) which turns research projects into competitions. Might find something your interested there. As others have said, finding a PhD professor who is willing to work with you will help a lot and give you the leg up you need to produce something worthwhile.  Figure out if P=NP or not. Figure out if P=NP or not. I think this can be solved, and I think the person who eventually solves it is as likely to be a random undergrad as anyone else.

That said, nobody should work on this when they need to produce actual research results. (I'm sure you agree :P ) There's a million bucks at stake if you can solve the problem. I'm sure you agree that's a lot of moneh! I suspect having a P algorithm to solve NP problems would allow you to win some of the other million-dollar prizes out there, too.  How strong is your knowledge of forma logics?
Go into description logics and ontology-related algorithms for semantic web.  Big data wrangling... it's crazy how tossed together and stressed the systems currently being used for this are... it's something that has recently fascinated me, because of the combined implications of distributed computing, distributed filesystems, programming, and other issues that are we are starting to realize are going to be big issues in the future. SciDB is interesting for example.

It has tons of practical implications as well, for scientists (think: professors who might actually help you), for business, for government, gaming etc. I found [this](https://465cd5ad-a-62cb3a1a-s-sites.googlegroups.com/site/romansimakov/files/scidb-vldb09.pdf?attachauth=ANoY7creo2vb6MdGc3HqDV-eXwtmbotbP-Dbe-2mSh_A-YsrK8TpZFf9R28bm0Mw0LVD8j-exvGMS6haNa9PfoofpNdka3TBE7BhjdK3MKEs7OY9EkipP0A9Kb0hVJNCKe6S3yJeABDBvD_VEnp8mVSAG1R9KBg-om_6dEn6j5ynnA2yJ-bRlYSfikNECV3vLhS35uo_M63xHVyY0HPJfRLFimZwLNtQa6psttHKVmHckOAQx3r3tnU%3D&amp;amp;attredirects=0), is it what I'm looking for to get a start on learning more about this? What parts of Big Data Wrangling would need more work? Would it be working within Hadoop optimization algorithms or am I barking up the wrong tree? I think that paper is a bit old, but it does seem like a good start. There have been lots of changes to SciDB and similar big DBlike solutions since then. (2009)

As far as I can tell, all of it. Right now RDBM are bottlenecking, and are making up for the bottleneck with specialized solutions such as hardware, db's in memory, and stuff like that. To me, none of this seems like a good long term solution. There are fundamental limitations on the way tradition RDB's work. I already told you the parts that I think are the most interesting. Distributed file systems (BTRFS, HAMMER, Hadoop), distributed hardware (less specialized, more broad = cheaper, easier adoption), but primarily, the right tools created for the right purposes (erlang is nasty, but python is sweet.. etc)

I also think that focusing on non Hadoop is the way to go, personally. Again, I'm just an IT guy with a recent fascination and limited knowledge of this issue, but to me it seems an awful lot of people are betting on Hadoop without really understanding it's origins and it's purpose. 

Bottom line is this, I think it's less about algorithms than everyone wants to think. (still lots about algorithms, but I would say less software specific algorithms) It's about clean lines from the ground up (and everything in between) that make truly scalable and distributed computing and big data capable of taking us into the future we potentially see before us. Nobody and nothing is delivering this yet. Whoever does will own the data of the future.  General purpose GPU programming is a relatively active topic.

Just for example you can try to see what problems research groups in your college are working on and suggest to implement their algorithm for a GPU in an optimized fashion. 

Perhaps they have some stuff like computer vision algorithms written in Matlab that they want to optimize.

Your paper would describe the implementation, design decisions, complications of GPU programming and benchmarks of various data distribution and executions you tried.

Some interesting projects in the field I've recently ran across:

http://www.par4all.org/

http://wiki.postgresql.org/wiki/PGStrom

http://graphics.im.ntu.edu.tw/~bossliaw/GPGPU_final/report.pdf





 Okay, that's pretty interesting, as I did my research I got the feeling that GPU programming was pretty much wrapped up and done, but reading those links you've passed it seems like there is definitely still room for another research process. 

We don't have research groups here, so I might have to find a different way to break into it, but I'll do some more reading, and see if improved GPU programming algorithms are for me.

Thanks for your help and the great links!! Wrapped eh? Very much far from it.

Speaking from experience writing efficient parallel programs is hard.
Efficient parallel programming is the future of high powered computation, we have pretty much hit a wall in terms of single core frequency and performance. Going forward hundreds and maybe even thousands of smallish cores are on the way with different memory models (local memory, shared memory).
It's going to be a real brainfuck coding such machines in an efficient manner and translating existing algorithms and processes to work efficiently.

Here are a few links to get you started if interested:

http://www.extremetech.com/computing/116561-the-death-of-cpu-scaling-from-one-core-to-many-and-why-were-still-stuck

http://en.wikipedia.org/wiki/Intel_MIC

http://en.wikipedia.org/wiki/Tilera

http://www.1024cores.net/

http://http.developer.nvidia.com/GPUGems2/gpugems2_part01.html







  Bonus points for the use of the word, "Brainfuck" haha

I didn't mean to suggest it was completely problem free, but that the issues nowadays were simple matters of optimization, not so much requiring new leaps to work, but I'm going to delve into this these next few days, and see if I can whittle it down to a reasonable amount of research. Disregarding the Air Force "Aim High" motto, I seriously doubt you will break any major scientific ground in your two semester research.

I don't mean to discourage but I fear there will be no Fields medals or Turing awards granted for your work.

This analogy works best:
http://matt.might.net/articles/phd-school-in-pictures/

In this analogy your research paper will be like coming to the boundary and giving it a kick. The boundary might give and a pimple will arise but most likely it will not.

  What do you like? Programming problems in general, I've enjoyed all my programming classes far more than my theory classes. Also reading through the academic releases I found some computer forensics stuff to be very interesting, but don't know where I would start with that. Programming problems in general, I've enjoyed all my programming classes far more than my theory classes. Also reading through the academic releases I found some computer forensics stuff to be very interesting, but don't know where I would start with that. In addition to discussing with your professors, if you like forensics you might want to get your feet wet by researching Android malware or post your question over at computerforensicsworld.com or forensicfocus.com. They can surely nudge you on where to start if you want to research that area. Just curious, what's the current state of android malware? What parts of it would a research program take a look at?  &amp;gt; I was approached by the department's head of research, and offered a two semester research sequence of independent studies to take the place of a few other CS classes.

I assume there will already be a subset of professors who would take your independent studies with. Go to their home pages and check the kind of work they do. Narrow down on something which sounds "nice" to you.

To be quite fair, expecting a first year CS under graduate student to submit a research project proposal is a little bit harsh and pre-mature. The best way to gel into research is to start slowly - help some professor in a mundane task and explore with him. Once it seems interesting enough you can pick up pace and then get involved in greater detail.

Questions out of curiosity (answer with your own discretion) - 

1] Are you offered for an independent study in your own CS department?

2] Which university do you study at? Yea, I'm still an undergrad, but our school doesn't have a graduate program so any research that comes out is undergrad with PhD holding faculty advising and assistance. 

Two answer your questions, 
1. Independent studies are rare, outside of specific exceptions like this research sequence.
2. I'm at USAFA, the United States Air Force Academy &amp;gt; Yea, I'm still an undergrad, but our school doesn't have a graduate program so any research that comes out is undergrad with PhD holding faculty advising and assistance.

If this is the case; wouldn't it be fairly easy to deduce the faculty who have PhDs under them? If so, you can make a list down the topics of research. You can then proceed to make your research proposal. Another completely different direction, is to approach it from the opposite side. You can think of an idea yourself (like storing large images in compact formats i.e. image compression) and then work backwards towards various research problems. The former is a traditional way to get things done while the latter is more the modern hacker culture mentality. Take your pick! Almost all of our faculty have PhD's and come from varied backgrounds and educations, so finding the adviser isn't really the hardest part, but doing this planning from this far away over only email is a challenge. Can you suggest a "No Research Proposal"? This is crazy - not even in the best of places is a first year student suppose to write a research proposal. Your initial interest in research should be all about exploration.  Unfortunately that's not an option for me, the independent study exceptions are only approved with detailed requests about the intended research and final turn ins and eventual goals of the work, etc. Do you have any specific interesting in building something? If yes, why don't you pen that down as a research proposal. OR Did you learn something in class which you found interesting? If so, can you think about something which can apply that knowledge? These are the best two ways to proceed. All others would be a crass painting. I'm more into the programming and software side, the hardware is interesting but I have no background, if I could find an awesome building project I would be interested in trying it, but who knows. I meant building software (not hardware). Would you like to build something? For example, lets say you want to build something like Instagram. Cool idea and there are inherent research problems.

[1] Scaling - How can you store large images for millions of users?
[2] Performance - How do you handle millions of users logging into your application simultaneously?

Great. Now, you go and read material for Databases which talk about storing images. You can also look into indexing techniques which can help retrieve these images when a user logs in. 

You see what I am getting at? You have a product in mind and you work backwards and carve out a research problem which makes your research proposal. You can use the product in your mind as a motivation and deduction as your research problem. Compadre?  Bit offbeat to some of the suggestions mentioned and maybe something a bit more applied. Have you considered something in medical image analysis or computer vision?

Increasingly the use of machine learning techniques in medical imaging is seen as the way forward. It's a neat field applying a lot of computational concepts into applied practice and given the area and modality you can find yourself in you can end up looking many really interesting research problems that have clinical relevance... Wow, thanks for the tip, that's actually pretty interesting to me, because I ended up in CS after finding out pre-med wasn't for me, but I still have a bit of interest in medical fields... 
 If this sort of thing is of interest, PM me or something. I've just come out of a PhD in the UK doing medical image analysis and would only be too happy to give any advice. We have a wealth of imaging data specifically looking at obstetrical ultrasound that a good student could produce some cool applications or methods. If this sort of thing is your interest I'd point you to VTK, ITK, OpenCV and other imaging libraries. Python x,y once installed gives you access to all these great libraries complete with example code to play with...  This is what you do - you say: "I'm interested but I don't know where to begin." Usually the way these things work is that you pick a faculty mentor who will help and direct you.

So, check out the webpages of the professors you like and find out what their research interests are. If you find someone you think you would get along with, and who has interesting things listed under their research, email them and ask for advice.

Most profs always have five or six "sideline" publications ideas - things they don't have time or budget to pursue, but that lies squarely in their sphere of interests. So they are usually happy to mentor a student to do their grunt work.

The deal usually works like this:

They will hook you up with a project, and give you a good idea where to start. You investigate the methods, write the code, do the tests and compile results - all the time consuming, dirty work. You get credits and a lot of experience, and maybe even a thesis or a publication in a scientific journal out of it. Your mentor then can take all that research, code and keep expand upon it to get more publications.   Research a solution for the Halting Problem. It has already been solved. [The solution was given by Alan Turing in 1936](http://en.wikipedia.org/wiki/Halting_problem). I thought that his solution to the problem was that it was impossible for there to be a solution. Exactly, so why suggest that he researches any further something that has been known in its entirety for almost 100 years?</snippet></document><document><title>Can someone help me understand how a virtual address space is converted to physical memory?</title><url>http://www.reddit.com/r/compsci/comments/1350oi/can_someone_help_me_understand_how_a_virtual/</url><snippet>I understand that any process running on a computer runs in a virtual address space. But how is that converted to actual RAM space? How is a virtual address converted to a physical address for different simultaneous processes?  There is dedicated hardware in your computer to support virtual address spaces, called a [memory management unit](http://en.wikipedia.org/wiki/Memory_management_unit). A [page table](http://en.wikipedia.org/wiki/Page_table) stores the actual relation between virtual memory addresses and physical memory addresses. There is a [translation lookaside buffer](http://en.wikipedia.org/wiki/Translation_lookaside_buffer) that acts as a cache of this information, which allows lookups to happen instantaneously instead of hitting the (slower) page table.

Does that help? This is exactly what I needed, thank you! I am still a bit confused about the actual address translation from the TLB, but you have opened up a starting point. I honestly didn't know where to start. Thank you! Think of the TLB as a cache of some recent page translations. All the translation system (MMU) does is say that virtual address A maps to physical address B. The TLB keeps around some info: "I recently saw that A1 maps to B1, and A2 maps to B2; for anything else, I need to go look it up in the page table". In hardware, there are a number of slots to hold these recent translations, and when the processor gets a request for address A, it checks if A == A1, A == A2, ... A == An, all in parallel. If no match, go access the page table. Pretty simple :-)  Instead of thinking about this like a computer would do it, think of this metaphor.

You're writing a research paper at your desk! You're plucking along at your research paper, but from time to time you need to refer to books. There are three places you can place your books: your desk, a cart next to your desk, and a bookshelf across the room. 

Now, you've been working on this paper for a while, so your desk is full of books... but you need to go read a book that isn't on your desk right now (i.e. currently on the bookshelf). In order to solve this dilemma, you pick a book from your desk **that you used least recently** and **move it to the cart next to your desk** (removing the least recently used thing from the cart and moving it to the bookshelf if it is also full). If you need to use this book again soon (which makes sense, since you already used it to write your paper and you might have to refer to it again), you know exactly where it is and can get it very quickly. Then, you walk over to the bookshelf (which takes time) and **move the target book from the bookshelf to your desk**. Now, for every subsequent time that you need a book that is currently not on your desk, you first **remove the least recently used book** from your desk, **check the cart next to your desk** to see if the book is in that cart, and if it isn't, **go and get the book from the bookshelf**.

This metaphor isn't perfect at all, but it's pretty close to how virtual paging works. 

* The books are **memory pages**, memory blocks of fixed size.
* The book cart is a **Translation Lookaside Buffer** (TLB).
* The bookshelf is **slow memory** (the actual hardware implementation varies from system to system).
* The desk space is RAM space.
* Failing to find a book on your desk that you want is called a **page fault** (or **hard fault**).
* Looking for a book on the book cart and failing to find it is called a **TLB miss**.
* The location of a book on the desk is the **physical address**.
* The location of a book in the bookshelf is called the **virtual address**.

Now, for simultaneous processes, consider now multiple people working on research papers in a library. They can all get books from the bookshelf and take them to their own desks (their own process memory). However, if you try to take somebody else's book, they might come over to your desk and punch you (access violation error, or segmentation fault). 

This isn't close to perfect at all but I hope it helps out. Please correct me if I'm wrong r/compsci!

 Instead of thinking about this like a computer would do it, think of this metaphor.

You're writing a research paper at your desk! You're plucking along at your research paper, but from time to time you need to refer to books. There are three places you can place your books: your desk, a cart next to your desk, and a bookshelf across the room. 

Now, you've been working on this paper for a while, so your desk is full of books... but you need to go read a book that isn't on your desk right now (i.e. currently on the bookshelf). In order to solve this dilemma, you pick a book from your desk **that you used least recently** and **move it to the cart next to your desk** (removing the least recently used thing from the cart and moving it to the bookshelf if it is also full). If you need to use this book again soon (which makes sense, since you already used it to write your paper and you might have to refer to it again), you know exactly where it is and can get it very quickly. Then, you walk over to the bookshelf (which takes time) and **move the target book from the bookshelf to your desk**. Now, for every subsequent time that you need a book that is currently not on your desk, you first **remove the least recently used book** from your desk, **check the cart next to your desk** to see if the book is in that cart, and if it isn't, **go and get the book from the bookshelf**.

This metaphor isn't perfect at all, but it's pretty close to how virtual paging works. 

* The books are **memory pages**, memory blocks of fixed size.
* The book cart is a **Translation Lookaside Buffer** (TLB).
* The bookshelf is **slow memory** (the actual hardware implementation varies from system to system).
* The desk space is RAM space.
* Failing to find a book on your desk that you want is called a **page fault** (or **hard fault**).
* Looking for a book on the book cart and failing to find it is called a **TLB miss**.
* The location of a book on the desk is the **physical address**.
* The location of a book in the bookshelf is called the **virtual address**.

Now, for simultaneous processes, consider now multiple people working on research papers in a library. They can all get books from the bookshelf and take them to their own desks (their own process memory). However, if you try to take somebody else's book, they might come over to your desk and punch you (access violation error, or segmentation fault). 

This isn't close to perfect at all but I hope it helps out. Please correct me if I'm wrong r/compsci!

  &amp;gt; for different simultaneous processes?

Every time the operating system performs a context switch, it has to switch out the page tables and flush the TLB.  Shocking right?  It's pretty amazing that multitasking systems can have such good performance.  The key is that the time slice quantum is usually on the order of a few ms, and rarely less than 1 ms.  At the level of a processor, that's an eternity.  On server systems where throughput is more valuable than latency, the time slice tends to be quite high to minimize these context switch overheads, but that's less acceptable on a desktop where latency and UI responsiveness is more important.  With multi-core systems now being commonplace, it's less of an issue.
 </snippet></document><document><title>How a Cloud Computing Platform Could Be Exploited to Steal Encryption Data</title><url>http://www.technologyreview.com/news/506976/how-to-steal-data-from-your-neighbor-in-the-cloud/</url><snippet>  They could clear the cache when they switch between different virtual machines on the same processor, or use cache partitioning, or a combination of both.

But I guess the point is attacks are possible when resources are shared and each VM can interfere with other VMs. This could also be considered bad for business, since the performance that you're buying depends on what else is running at the time, so companies have a non-security incentive to minimize the effects. Exactly -- performance isolation solves multiple problems: it removes side-channels and it gives some level of guarantee that you'll get what you pay for.

There seems to be a fundamental tradeoff between isolation and utilization, though. As soon as we co-locate applications/VMs on one machine to get higher utilization, we introduce potential side-channels. Unconditional prioritization (e.g. VM A's cache blocks always evict VM B's cache blocks, VM A's disk requests always come before VM B's disk requests) might be a way to at least give one-way isolation, though...</snippet></document><document><title>How could I go about learning how computer hardware/software work from the ground up?</title><url>http://www.reddit.com/r/compsci/comments/131rf2/how_could_i_go_about_learning_how_computer/</url><snippet>I'm a software engineering major, so I already have a decent understanding of high-level programming languages.

But, I still don't understand how a computer works as a whole. I want to work my way up from the most basic parts, like learning how a NOT gate works, to CPU and GPU design, machine code, BIOS, and everything else.

Before I jump in to some random books, I was wondering if anyone knows of a good resource that already has all of this information laid out. Maybe something that shows a good order to learn everything in, or a book that gives a brief overview of all of it.

Thanks.

  Yes, there is (though I haven't done it yet myself).
http://www.nand2tetris.org/ Doing this now. Got as far as the CPU implementation. Its hard, but I recommend it. Yes, there is (though I haven't done it yet myself).
http://www.nand2tetris.org/ This looks really cool! 
Does anyone know if something like this exists for "The internet from the ground up" ? The internet is a lot simpler. It's basically just a set of protocols that exist at different layers, and the networking hardware that communicates using those protocols.

[See the OSI model.](http://en.wikipedia.org/wiki/OSI_model)

[And here's the hardware involved.](http://en.wikipedia.org/wiki/Networking_hardware) It even tells you what OSI layers they correspond to.  How the hardware and software interact for the internet might be simpler but the actual backbone of the internet is fairly complex in how it keeps track of its neighbors and whether or not they are still "alive". 'Tis true. Once you start talking about the backbone you get into magic and sorcery beyond my qualifications. That backbone sounds really cool!
How would I go about learning more about it? Can you please give me some words to google? How the hardware and software interact for the internet might be simpler but the actual backbone of the internet is fairly complex in how it keeps track of its neighbors and whether or not they are still "alive". This looks really cool! 
Does anyone know if something like this exists for "The internet from the ground up" ? Yes, there is (though I haven't done it yet myself).
http://www.nand2tetris.org/  Read this book. http://dump.udderweb.com/CODE.pdf it is exactly what you want. 100% agree, I was in the same spot as you. Code is really well written and enjoyable to read. But I would get it here: 

http://www.amazon.com/gp/aw/d/0735611319 Read this book. http://dump.udderweb.com/CODE.pdf it is exactly what you want. Read this book. http://dump.udderweb.com/CODE.pdf it is exactly what you want. Read this book. http://dump.udderweb.com/CODE.pdf it is exactly what you want.  [The Elements of Computing systems](http://www.amazon.com/The-Elements-Computing-Systems-Principles/dp/0262640686/ref=sr_1_1?ie=UTF8&amp;amp;qid=1352733912&amp;amp;sr=8-1&amp;amp;keywords=the+elements+of+computing+systems) covers much of this territory, and is well-suited to independent study.  Check out [Computer Systems: A Programmer's Perspective by Byrant and O'Hallaron](http://csapp.cs.cmu.edu/).  I had it recommended to me as a book that goes from zero to everything. Looking at the table of contents, it looks great! It covers gates, memory hierarchies, information representation, and so on.   If you still have access to your school, I would recommend  taking your school's Logic and Design course, sometimes called Machine Architecture.            </snippet></document><document><title>Traveling Salesman: The Most Misunderstood Problem</title><url>http://www.nomachetejuggling.com/2012/09/14/traveling-salesman-the-most-misunderstood-problem/</url><snippet>   Am I missing something, or is the entire point of the article to highlight that NP is a class of **decision** problems, and as such, "Given a weighted graph G, find the cost of its lowest cost Hamiltonian cycle, or infinity if it does not exist" is not a decision problem, and so it can't be in NP?

In short, is this anything more than pedantry?

I see little use to the distinction, the problems are bound to be similar in any respectable notion of hardness. If you solve the optimization one, you solve the decision one in at most the same time, if you solve the decision one, you solve the optimization one in at most O(log(n)) times more operations.

I'd think the same of an article saying "Aha! You speak of problems, but NP is actually defined for LANGUAGES! YOU ARE ALL WRONG! Muahahaha!"  Isn't this usually referred to TSP now? Travelling sales-person. Isn't this usually referred to TSP now? Travelling sales-person. Yes, all but one of my profs have referred to it as TSP. I have heard it called the Messenger Problem once too, which is a reference to an earlier description of the problem, I believe. 

On a related note, I took Algorithms a few years ago, and I was the only female in a class of 40. When the professor brought up this topic, someone asked if it was still the "Traveling Salesman" or if it was the "Traveling Salesperson" and the professor went into a 20 minute long rant about how women were changing everything, and "not for the better". He said the fields of Computer Science and Maths have gone downhill steadily since women entered it. It was a miserable semester with him, but it's over now, no big deal. I just thought it was kind of a wtf kinda relevant story that my fellow CS Redditors might find interesting/weird. People like that exist? What the hell? 

Not that "nomenclature" arguments aren't common as hell in computer science departments. At my school, if you want to derail the student led group "Open Source Club" at my school, just ask them whether or not it should be titled the "free software club" and the resulting arguments over the correct terminology will eat up all the time that was going to be spent talking about VIM.  Yeah, they exist, unfortunately. I could understand debating whether to use the name "Traveling Salesman" or "Traveling Salesperson" or "the Messenger Problem" or whatever, there are all sorts of compelling arguments for any of those names, but at the end of the day it doesn't matter much. It's just the strangely bitter and exclusionary nature of his wrath that was so memorable, sadly.  Man that's weird. I've been in my schools CS major for the last year, and I haven't actually heard anyone make reference to the fact that genders exist. More happily even is that no one makes references to memes or any of the other nerd junk that often comes along with the kinds of people who enter the major.   This article is so pointless it hurts. :/ Maybe pointless to r/programming or software engineering but it's not pointless to the discipline of computer science.  The point is that the complexity classes (and especially NPC) is defined for the decision problems. So talking about TSP-optimize and how it is "only" NP-hard or anything is just pointless :P

Plus, you can easily create an optimization algorithm from a decision one, just do binary search on path lengths. I disagree with you, this article is not pointless. It provides a good opportunity to clarify the difference between NP-hard and NP-complete. By the way, your parent comment is a bit confusing:

&amp;gt; The point is that the complexity classes (and especially NPC) is defined for the decision problems.

If one wants to be rigorous, this sentence is wrong -- some complexity classes explicitly include optimization problems. For example, an optimization problem belongs to the NP-hard class if it has an NP-complete decision version ([source, CTRL+F optimization](http://en.wikipedia.org/wiki/NP-hard)).

&amp;gt; So talking about TSP-optimize and how it is "only" NP-hard or anything is just pointless 

Certainly not pointless. Stating the NP-hardness of TSP-optimize establishes that TSP-optimize is a more difficult problem than TSP-decision (because even verifying a solution is difficult).

EDIT: added some remarks
EDIT2: removed the last part because I understood your point (altough I do not agree, see my comment further down) NP-hard consists of many problems, but NP has only decision problems. So the author's reason that TSP-optimize is not NP is entirely wrong, it isn't in NP because it isn't a decision problem. If a problem isn't a decision problem, then talking about a 'yes' instance verifier is meaningless. The author goes on to try and find a verifier that works on all instances of TSP-optimize, which is **not** necessary for a problem to be in NP-complete - as far as we know, no NP-complete problem has a verifier for all instances.

&amp;gt; Stating the NP-hardness of TSP-optimize establishes that TSP-optimize is a more difficult problem than TSP-decision (because even verifying a solution is difficult).

But TSP-optimize isn't any more difficult than TSP-decision. If you can solve one in polynomial time, you can solve the other in polynomial time. If you can verify one in polynomial time, you can verify the other in polynomial time. The author's conclusion that verifying TSP-optimize is more difficult is incorrect, because he was looking for a TSP-optimize-verifier that worked on all instances, but was satisfied with a TSP-decision-verifier that only verified 'yes' instances. It's only more difficult in the sense that he's demanding more from it.

If you were able to verify all TSP-decision instances, then you could verify all TSP-optimize instances in the following way: find the minimum cycle of length n. Then run TSP-decision looking for a cycle of length less than n, which will be false, and get the certificate that proves it is false. Then the TSP-optimize-verifier can consist of checking the cycle, and running the TSP-decision-verifier on the certificate. &amp;gt; If you can verify one in polynomial time, you can verify the other in polynomial time. 

Assuming what you said is true, since you can verify TSP-decision in poly time, you could verify TSP-optimize in poly time too, then TSP-optimize would be NP-complete, which is a contradiction. &amp;gt; since you can verify TSP-decision in poly time

You can only verify half of the TSP-decision problems in poly time. See [NP](https://en.wikipedia.org/wiki/NP_%28complexity%29) vs. [co-NP](https://en.wikipedia.org/wiki/Co-NP). You're right, I got misled by incomplete wording.

EDIT: I should retract my criticism, what you said appears to be spot-on: it makes no sense to ask for a 'yes'/'no'-certificate in the TSP-optimize problem. 

Still.. TSP-decision can at least be polynomially verified for 'yes' instances, whereas TSP-optimize can never be verified polynomially at all. Wouldn't that count as "harder"? Yea, I think the article mislead a lot of people due to the incorrect definition of NP (including the author himself).

I'll leave it up to you to decide if you want to call one harder, but it isn't harder in any rigorous sense of the word. Here are a few things to consider:

* you can solve TSP-decision if you can solve TSP-optimize
* you can solve TSP-optimize if you can solve TSP-decision
* TSP-decision is fully verifiable if TSP-optimize is fully verifiable
* TSP-optimize is fully verifiable if TSP-decision is fully verifiable
* you can verify some instances of TSP-decision (the 'yes' ones)
* you can verify some instances of TSP-optimize (the [XY](http://www.i-programmer.info/news/181-algorithms/4796-the-xy-travelling-salesman-problem-solved.html) ones, and other subsets that have nice properties)
* you can verify *half of all instances* of TSP-decision, but you can *half-verify all instances* of TSP-optimize (make sure the answer isn't too small, but you can't make sure the answer isn't too big) The point is that the complexity classes (and especially NPC) is defined for the decision problems. So talking about TSP-optimize and how it is "only" NP-hard or anything is just pointless :P

Plus, you can easily create an optimization algorithm from a decision one, just do binary search on path lengths. &amp;gt; Plus, you can easily create an optimization algorithm from a decision one, just do binary search on path lengths.

An NP-hard problem X can be solved by an NP-complete problem Y using a binary search technique and you say that this implies that X is NP-complete ? 

Your pointlessness, it hurts my brain I'm just saying that if you can solve one, you can solve the other, and the distinction OP is making is just for the sake of sounding smart, at least that's my impression from the article.

Is there a path of length K? Run optimization alg, compare result.

Give me the best path. Run decision alg for k=1,2,4,8...  I still don't think that would work. Have you considered that k may not be integral? In other words, the distances between the cities have no reason to be integers, unless it's a more narrow TSP(-decide/-opt) formulation. Well, ok. Theoretically, *real* real numbers are a problem, but practically, you can think of reals as if they were integers (they actually ARE in the computer!).

Turing machines can't operate on *real* real numbers. Agreed, in an actual computer, all distances in a TSP instance can be scaled to integers via a constant multiplicative factor. ~~But if we keep going that way.. all paths lengths are upper-bounded by a constant (say MAX_INT..), thus you could enumerate all of them in constant time (as an exponential of a constant, will be quite a huge number). Kind of defeats the purpose of complexity analysis..~~

EDIT: The last part of this comment was pointless. It makes sense to consider that any TSP instance only has rational distances. Even without the limit. The amount of real numbers you can actually compute is far smaller than all the reals (so called computable reals). No turing machine can work with uncomputable reals. Basically, manipulation of uncomputable reals = hyper-turing computation. That's why the complexity analysis mostly only deals with integers (computable reals are countable) This article is so pointless it hurts. :/ It's not great, I admit, but it's an okay explanation of P vs NP and decision vs optimization problems. I've read better explanations for both but this is what it is: average. I mean sure this does an ok job at explaining p vs np and np hard vs np complete,  but it does do through a completely roundabout way. The author talks as though he's found some revelation about TSPs but the thing is,  this dichotomy between decision problems and optimization problems exists for almost every np problem. Not only that,  most compatability courses explain away the difference on the first week since it is trivial to realize that an optimization problem can be reduced to a decision problem and vice versa in polynomial time. 

The article is incorrect and also preaches about a meaningless distinction between two functionally identical types of problems. sure it does an ok job of explaining p vs np but the bulk of it is very misleading 


 &amp;gt; The author talks as though he's found some revelation about TSPs

He absolutely does not. All he does is point out what he believes to be a simple misunderstanding about a popular C.S. problem then give a quick explanation about it and a couple of definitions to aid that explanation. That's all. He's not trying to share some deep revelation or exemplify a mastery of all things computationally scientific.
 Yeah, people are acting like this was a journal submission or something.  It's just a blog post by a guy who learned that something he thought was true for years wasn't.

Source: I'm the author The more theory-oriented subreddits (like /r/compsci and /r/math ) just tend to be that way. It's silly. They assume everything is supposed to be written like a textbook and cover all potentially relevant material. And posting common knowledge is borderline offensive to some. The thought that an author wasn't aspiring to that level simply doesn't cross anyone's mind, it seems.    Ugh, this just doesn't follow from the preceding discussion of what it means to be in NP, NP-Hard, and NP-Complete.

&amp;gt; Therefore, the decision version of the problem, TSP-DECIDE, is in NP. And since it&#8217;s also in NP-Hard, TSP-DECIDE is NP-Complete.

It should be, TSP-DECIDE, is in NP, and since it's also NP-Complete, then it is in NP-Hard.
 &amp;gt; It should be, TSP-DECIDE, is in NP, and since it's also NP-Complete, then it is in NP-Hard.

Nope, because you start by proving that it is NP-hard before establishing NP-completeness: [A decision problem L is NP-complete if it is in the set of NP problems and also in the set of NP-hard problems.](http://en.wikipedia.org/wiki/NP-complete) TSP-DECIDE is in NP-Hard is a corollary of it being in NP-Complete.

&amp;gt; A problem H is NP-hard if and only if there is an NP-complete problem L that is polynomial time Turing-reducible to H. In other words, L can be solved in polynomial time by an oracle machine with an oracle for H.^[[1]](http://en.wikipedia.org/wiki/NP-hard)

Let L=TSP-DECIDE. First we determine that it's in NP-Complete, which requires reducing a known NP-Complete problem to TSP-DECIDE. Thus we have established that L is in NP-Complete. Now by the definition of NP-Hard, let H=L. L is NP-Complete and is polynomial-time reducible to H (this is trivial). Therefore H is NP-Hard.

The corollary is that any L in NP-Complete problem is NP-Hard, by virtue of being polynomial-reducible to itself. It's not even the corollary that frustrates me, and the statement in the article is factually correct, but it's an unpleasant statement simply because TSP-DECIDE is NP-Complete, and by virtue of it being NP-Complete it's NP-Hard.

It's possible, but unusual to show that a problem is in NP and NP-Hard to show it's NP-Complete. First we show TSP-DECIDE is NP and then it's poly-time reducible to TSP-OPTIMIZE. Therefore it is NP-Complete, but the preceding discussion doesn't make this claim. It's a leap to make that statement, whereas the one I presented also makes a leap, a smaller leap, which is well understood, that is TSP-DECIDE is NP-Complete. 

[1]: [Wikipedia: NP-Hard](http://en.wikipedia.org/wiki/NP-hard)

Edit: Corrected, only one reduction is needed after showing the candidate problem is in NP.
 TSP-DECIDE is in NP-Hard is a corollary of it being in NP-Complete.

&amp;gt; A problem H is NP-hard if and only if there is an NP-complete problem L that is polynomial time Turing-reducible to H. In other words, L can be solved in polynomial time by an oracle machine with an oracle for H.^[[1]](http://en.wikipedia.org/wiki/NP-hard)

Let L=TSP-DECIDE. First we determine that it's in NP-Complete, which requires reducing a known NP-Complete problem to TSP-DECIDE. Thus we have established that L is in NP-Complete. Now by the definition of NP-Hard, let H=L. L is NP-Complete and is polynomial-time reducible to H (this is trivial). Therefore H is NP-Hard.

The corollary is that any L in NP-Complete problem is NP-Hard, by virtue of being polynomial-reducible to itself. It's not even the corollary that frustrates me, and the statement in the article is factually correct, but it's an unpleasant statement simply because TSP-DECIDE is NP-Complete, and by virtue of it being NP-Complete it's NP-Hard.

It's possible, but unusual to show that a problem is in NP and NP-Hard to show it's NP-Complete. First we show TSP-DECIDE is NP and then it's poly-time reducible to TSP-OPTIMIZE. Therefore it is NP-Complete, but the preceding discussion doesn't make this claim. It's a leap to make that statement, whereas the one I presented also makes a leap, a smaller leap, which is well understood, that is TSP-DECIDE is NP-Complete. 

[1]: [Wikipedia: NP-Hard](http://en.wikipedia.org/wiki/NP-hard)

Edit: Corrected, only one reduction is needed after showing the candidate problem is in NP.
 Ugh, this just doesn't follow from the preceding discussion of what it means to be in NP, NP-Hard, and NP-Complete.

&amp;gt; Therefore, the decision version of the problem, TSP-DECIDE, is in NP. And since it&#8217;s also in NP-Hard, TSP-DECIDE is NP-Complete.

It should be, TSP-DECIDE, is in NP, and since it's also NP-Complete, then it is in NP-Hard.
</snippet></document><document><title>Computer can recognize hand drawn sketches from 250 categories. Is this the Draw-Something partner of the future?</title><url>http://news.brown.edu/pressreleases/2012/09/sketches</url><snippet>  This isn't a very good article.

Press releases devoid of any meat should stay in /r/technology  </snippet></document><document><title>Hi r/compsci , I need your help with Neural Networks (Specifically, applying a back propagation algorithm!)</title><url>http://www.reddit.com/r/compsci/comments/131fk7/hi_rcompsci_i_need_your_help_with_neural_networks/</url><snippet>Sad to say, this is something I should be learning in university, but the lecturer is, well, less than ideal and often rambles off topic. Having programmed a software model, I now find that my understanding of things is off by a fair factor...

For what it's worth, I understand how to do the forward pass just fine, it's just that I seem to have the wrong equations (or am applying them wrongly) for finding the error of the network, along with modifying the weights to reduce that error.

Help? If this is the wrong subreddit, any other suggestions as to where to post, or any resources which explain it really really well?

Thanks in advance!  You should ask in r/machinelearning. It would also be worth your time to check out Geoff Hinton's Coursera course on neural networks, he covers back propagation really well.  Neural networks are awesome, and as such, there are a lot of resources online.

Sklargblar pointed out the Coursera course, but simply Googling "back propagation" gives you hundreds of relevant results, many of which are written in a teaching format. I've read most of the first page of google sadly, but the way the lecturer has explained them, and actual mathematical formula are so different i've just ended up confused.  </snippet></document><document><title>vintage u.s. navy mechanical computation porn [video][sfw]</title><url>https://www.youtube.com/watch?v=s1i-dnAH9Y4</url><snippet>  whoever made this knows how to illustrate a concept. damn. Exactly what I was thinking! Videos like this uncover how bad some of my university professors are :( 

Learned so much in 20 minutes (watched only part 1 for now) the pacing, illustrations, and voice over were all spot on.     Wow, I feel like I honestly just learned a lot about geometry and mechanics from this. I had no idea how those old physical computers worked. Wow, I feel like I honestly just learned a lot about geometry and mechanics from this. I had no idea how those old physical computers worked.   All these beautiful pieces can be replace with a simple op amp now... all in 60 years. Its like we had all the basic concepts in the 40's and 50's we just didn't have any better way.      </snippet></document><document><title>For anyone thinking of doing a PhD in CS: a reputable UK university is offering two places paying a phenomenal 51000 euro (~70000USD) / year, doing Natural Language processing.</title><url>http://mailman.uib.no/public/corpora/2012-November/016556.html</url><snippet>  Yeah I've noticed that NLP is becoming huge about now. I think there was a video on the front page the other day where microsoft was demonstrating their technology. Recently there were some job posting at IBM at their Watson research facility looking for PhDs in NLP. The hard part is saying if this field is still going to be popular 5 years down the road (you know, if you were just starting a PhD now). Are you kidding? It's been in progress for ~70 years, and with the EU alone spending billions on translators each year, MT (machine translation) is a long-standing business that's just becoming exciting.



Only.. my NLP post-doc pays less than this. :( I feel like it's in vogue in now, and industry has a tendency to change on a dime. While they're interested in it now, there is no guarantee they will find it useful 5 years down the line.   &#163;40k and a PHD? think i might apply....oh wait, nope:

' In particular, candidates cannot have lived in the UK for more than 12 months in the 3 years immediately prior to their appointment. '

 Don't forget the &#8364;16k travel budget ..  I suddenly have this image of Ali G, saying 'is it 'cause I'm English'?  Is that a lot of money for a UK job? Seems low considering the cost of living there is higher than the US For a PhD position, yes. The going rate is ~&#163;13k for a studentship, compared to roughly ~&#163;0 in the US. The cost of living in Sheffield is around &#163;10k (&#8364;12.5k)/year according to the uni's website.

Compare also http://www.eui.eu/ProgrammesAndFellowships/AcademicCareersObservatory/AcademicCareersbyCountry/UnitedKingdom.aspx#GrossSalaries (slightly old data) it's $18-25k in the US actually. Ah, OK. From phdcomics I had gathered that the wages earned from tutoriing and working on projects worked out as just a few $k greater than the fees, but that's all the evidence I had. I'm at a state school, so I'd expect to be getting screwed more than most at private schools, and my fees end up being &amp;lt;$1000 You have private universities with PhD programs?  Is that a lot of money for a UK job? Seems low considering the cost of living there is higher than the US uh, 70k usd for a PhD stipend is like triple or quadruple the standard here in the states. You totally misunderstood what he said. What might seem high to US students could actually be low in the UK, due to the cost of living there.

Edit: Downvotes? Lol, I guess whoever did that should study natural language processing. Start from basic English reading and work your way up... Is that a lot of money for a UK job? Seems low considering the cost of living there is higher than the US Is that a lot of money for a UK job? Seems low considering the cost of living there is higher than the US Yes, it's a lot. For comparison, a graduate level software development job with IBM pays [&#163;30k](http://www-05.ibm.com/employment/uk/graduate-programmes/technology/index.shtml) (on the high end for this kind of role), and this is offering &amp;gt;&#163;10k more *for a PhD!* Interesting.  I dropped out of college and I make more than what these students are getting.  You'd think someone would be worth more money if they're smart enough to be getting a doctorate. You're getting paid to be student, not an employee. The student fees are waived, /and/ you get this much to live off while you study.</snippet></document><document><title>Hi r/compsci. I was wondering, what research have you done?</title><url>http://www.reddit.com/r/compsci/comments/12x7dj/hi_rcompsci_i_was_wondering_what_research_have/</url><snippet>I'm in the process of picking a topic for my own first research project, and I thought I might try to learn from your collective experience and advice. What research have you done? What was the subject and how did you come to find it? Did you enjoy it? Tell me if you hated it. What advice (not necessarily a suggestion for a topic) would you have for someone choosing a research topic now?  I've done research into encoding type hierarchies in object oriented languages. Basically, looking to improve things like `instanceof` in java and similar.


**example:**

You really want to quickly be able to tell if an object is of a particular type, or a subclass of that type (is-a relationship). It's an operation which happens super frequently, so it needs to be quick.

An easy way to do this is with a bitfield: Assign `MyClass` a bitmask of `0000 0100` and if  an object has a type encoding of `0110 0100` you know that it "is a" `MyClass`. 

A simple AND operation; very quick! However, if you have 1,000 classes in your application, this would require type encodings with a length of 1 KiB. That's a bummer!

 -

So my research was to find an implementation which was both quick and space-efficient. I was able to match the leading implementation, but not beat it. Ah well, that's the way research goes sometimes! I've heard that comparing class types was a no-no. I've had times where I've had to do it, but my peers always say not to. Although I'm not sure why. In general any code that looks like:

    void function(object foo) {
        if(isType&amp;lt;bar&amp;gt;(foo)) {
            bar newFoo = (bar)foo;
            newFoo.DoStuff();
        }
    }

can be coded in a more object oriented way with inheritance, i.e.

    foo.function();

This is why we have virtual functions in C++, so that in cases like this, we can know at runtime what the proper type of foo is and what function we need to call for it. But it requires that you own the invoked class. Not really. If you don't own the classes it's still preferable to implement this via the type system:
 
    void function(bar foo) {
        foo.DoStuff();
    }

    void function(baz foo) {
        foo.DoOtherStuff();
    }

    ...etc...    I have done research on Drug-Protein relationship mapping, and Computational Turbulence.       You're probably familiar with DFA minimization. I found a way to make DFAs even smaller when you're willing to fudge the language a little bit. cool! I'm loving computability right now. I don't even know why. You're probably familiar with DFA minimization. I found a way to make DFAs even smaller when you're willing to fudge the language a little bit. Sounds interesting! Can you link to some of your work?  "Generating a software solution using a DSL written with a Meta Programming System" - Was a senior thesis so I didn't get as heavily into it as I would have liked. Basically I used Jetbrains MPS to examine the possibility of using meta-programming to generate a language for a single project. The domain would become that of your specific problem (as well as areas where the software might grow over time). It took me a long time to form a single direction for my research. My original topic statement ended up not really taking shape. With the direction of a new psuedo-advisor I ended up realizing a lot more of what I wanted to pursue and managed to get a healthy amount done on it. I would say make sure you have an advisor who is flexible and who is interested in every aspect of your research. My original advisor was very language-oriented but did not really have much interest in the software engineering side of the equation and this let to a lot of floundering. I eventually found another professor who realized what I was doing and introduced me to the existing software engineering knowledge. In the end I got valuable advice from both advisers, but the initial direction might have been less foggy if I had had someone who could be interested in my entire idea.  Wireless real time systems, particularly those dealing with rail.

I really appreciated a IEEE membership to get articles on-line. Expensive at $35/month but it helped me with the Ph.D.   Pick a topic that makes you excited, you will get stuck and you need the energy to make progress.  Seek out mentors/advisors that you connect with and who are interested in the same general area.  It's fine to modify your project to take advantage of any mentors' strengths, but do not switch topics in order to make someone else happy.

As you figure out what you're interested in, lay out a research path for yourself.  Of what problem do you want to be the expert?  The problem should be specific enough that you are one of hundreds, if not tens, of experts that people approach.  Once you've answered that, come up with a path to get from here to there.  You can modify the details as you go, but have a detailed plan. 

Good luck!           **NERDDDSSS!!!!!!!!!!!!!!!!!!!!!!**^**NERDDDSSS!!!!!!!!!!!!!!!!!!!!!!**^**NERDDDSSS!!!!!!!!!!!!!!!!!!!!!!**^**NERDDDSSS!!!!!!!!!!!!!!!!!!!!!!**^**NERDDDSSS!!!!!!!!!!!!!!!!!!!!!!**^**NERDDDSSS!!!!!!!!!!!!!!!!!!!!!!** This is /r/compsci... **NERDDDSSS!!!!!!!!!!!!!!!!!!!!!!**^**NERDDDSSS!!!!!!!!!!!!!!!!!!!!!!**^**NERDDDSSS!!!!!!!!!!!!!!!!!!!!!!**^**NERDDDSSS!!!!!!!!!!!!!!!!!!!!!!**^**NERDDDSSS!!!!!!!!!!!!!!!!!!!!!!**^**NERDDDSSS!!!!!!!!!!!!!!!!!!!!!!**</snippet></document><document><title>[Question] Where will I find definitions of performance of computer systems and related topics?</title><url>http://www.reddit.com/r/compsci/comments/12ylf4/question_where_will_i_find_definitions_of/</url><snippet>Is there any good source for core definitions or taxonomy of performance related topics (effectiveness, utilization, efficiency etc). I know Meyer's work on performability where there is a nice formal definition of effectiveness, I know state of art book by Ferrari. I am searching for a reference similar to Laprie's work on taxonomy of dependable and secure systems. </snippet></document><document><title>Early Prediction of Movie Box Office Success based on Wikipedia Activity Big Data</title><url>http://arxiv.org/abs/1211.0970</url><snippet>  First thought:  What would this predict for Snakes on a Plane?  SoaP had tremendous online traffic for a very long time... and flopped utterly.  Predicting the success of movies (and other forms of popular media) has a long history of terrible results.  The book "A Drunkard's Walk" examines many different criteria, all of which prove to be no more useful than random chance at predicting the success of a movie.  It also goes on to show how people working in the movie industry form myths and superstitions about 'what the public wants', and ignores repeated proofs that their superstitions are pointless.  Even the people whose job it is to call winners and losers (publishers, movie production companies, etc) perform no better than random chance, yet they are permitted to manipulate our culture based on their superstitions.  

They say there were 535 movies released in the time period they looked at (2010), but "We could track the corresponding page in Wikipedia for 312 of them."  No mention of why they could not track the 223 others?  That's a pretty significant group of films to just throw out... enough that if the algorithm could not rate those films, then it would completely destroy all significance of this algorithm.  The success of the algorithm, if it claim to be able to rate ALL movies, has to be considered across ALL the movies.  Being able to predict the success of movies that have very active Wikipedia pages, or that have Wikipedia pages at all, is not very useful if it ignores such a large portion of the film market.

It is also arguable that if you can only predict the financial success of a film after the number of theaters it will be shown in is already determined, your prediction is not worth much.  Theaters and movie publishers are among those most interested in what films will be successful, and they would need a prediction before the film is finished production, far before theaters have already paid to lease the print.

I'd be interested to see how the algorithm performs if they were to assume a success rate of 0 for the films that they "could not" track via Wikipedia. Disclaimer - I am not the author of the paper

&amp;gt; They say there were 535 movies released in the time period they looked at (2010), but "We could track the corresponding page in Wikipedia for 312 of them." No mention of why they could not track the 223 others?


I think they have mentioned about this in Page 5. They did not manually collect the movies - they searched for movie titles on Wikipedia and came up with the pages. There can be many plausible explanations on why they could not collect the 223 other pages ranging from data not existing to movie names having disambiguous names.  Some more information could be nice but this is not a research method flaw per se as it looks like they have thought about it.

&amp;gt;What would this predict for Snakes on a Plane? SoaP had tremendous online traffic for a very long time... and flopped utterly. 

Please go through the paper once again. You are making an implicit assumption that the authors are indicating there is a correlation between online traffic and box office success. This is hardly the case - there are multiple indicator variables including number of edits, collaborative rigor etc. which contribute to the predictor variable (box office success).

&amp;gt; It is also arguable that if you can only predict the financial success of a film after the number of theaters it will be shown in is already determined, your prediction is not worth much.

Once again you make an implicit assumption. Number of theatres are NOT the only indicator variables. It is just one of them. You are just dismissing the study, it is wise to hold on to your horses. This is the first version of the draft and in time things will get more clear on the effect of different variables on movie box office success.

&amp;gt; I'd be interested to see how the algorithm performs if they were to assume a success rate of 0 for the films that they "could not" track via Wikipedia.

This is a fundamental research question and the answer to this is that there is "no silver bullet". These guys have provided a fresh research perspective which says "Wikipedia CAN be used to predict movie box office success". It does NOT say "Wikipedia IS THE BEST to predict movie box office success". If you are building a movie success predictor, you MAY want to consider Wikipedia as one of the data sources. You DO NOT have to. It is your choice but results show that Wikipedia can definitely be used to improve some prediction. It is an important data source and worthy of consideration is the message of the paper. If you want a silver bullet, a research paper is not the correct place to look for.


 &amp;gt;Some more information could be nice but this is not a research method flaw per se as it looks like they have thought about it.

It doesn't matter if they thought about it.  If they didn't actually do something to compensate for the massive sample bias that this introduces, it does invalidate their research.  They simply implicitly assume that the films which they were unable to track via Wikipedia were randomly selected (or at least they would be required to make this assumption to justify any claim that this does not introduce significant sampling bias).  I think that is highly unlikely.  Most likely the films which they were unable to track via Wikipedia were the films which were getting very little public attention.  Verifying if this is the case would be very easy to do.  Instead of throwing out those films, you account for the fact that their method provides no prediction for them.  Then you observe what the actual success rate of those films is.  The success of each of those films directly reduces the effectiveness of their overall technique.  They did not claim 'our technique can predict the success of films listed on Wikipedia', they claimed it predicts the success of films.  Those are two different conclusions, and they only supported one of them.

&amp;gt;You are making an implicit assumption that the authors are indicating there is a correlation between online traffic and box office success.

No, I'm not, actually.  I asked about Snakes on a Plane because I believe it would be a counterexample as it was an unusual case.  This is true for all of the indicators they chose, I imagine.  Verifying a technique by running it against interesting representative historical cases is very useful for illuminating possible limitations.  Other techniques to predict the success of movies often get tripped up by things like this.  They predict that, for instance, subtitled films won't succeed in the US.  And then The Passion of the Christ hits theaters and shows that the "support" shown by an absence of subtitled films in the pool of successful films is just a self-fulfilling prophecy with no substance.  I asked the question about Snakes on a Plane because I am honestly curious how their technique would deal with it.  I'm not assuming that it will fail to account for it.  Don't read between the lines, I don't write my posts there.

&amp;gt;Once again you make an implicit assumption. Number of theatres are NOT the only indicator variables.

I am not assuming that it is the ONLY variable.  They made clear, however, that is IS a *necessary* one.  And so I pointed out that if the algorithm requires that indicator, it significantly detracts from its usefulness.  I did not mention the other indicators because they don't matter at all to this point.

&amp;gt;This is a fundamental research question and the answer to this is that there is "no silver bullet".

What has a "silver bullet" got to do with a potentially significant methodological error?  The only way that you can throw out nearly half of your population and still make statistically valid conclusions that apply to the entire population (which is what they claim) is if you can show that the half you have left is randomly selected from the population.  I find it highly unlikely that this is the case, and their failure to check this (which would be easy to do) is a flaw.  It's a flaw that can be corrected.  It might require them to edit their conclusions, and instead of calling their method 'a way to predict film success' they might need to call it 'a way to predict film success of films that are popular and/or receive online buzz prior to release'.  That's still a significant technique!

It perplexes me why you discuss hypothetical motives or desires on my part.  I read a piece of research, and I expressed concerns and asked some questions.  Scientific rigor is not something that requires personal investment.  </snippet></document><document><title>Do you work in Computer Vision? What do you do?</title><url>http://www.reddit.com/r/compsci/comments/12widk/do_you_work_in_computer_vision_what_do_you_do/</url><snippet>As my Vision course started, they went through all the practical applications Vision can be applicable to. I was wondering what you do in your field and is it what you expected?

EDIT: Thanks for the responses! I'm going to start browsing /r/computervision.   I work on various open source projects and run a Computer Vision company, Lambda Labs. We maintain an open source project as well as a commercial Computer Vision API. You'll probably quickly learn that there are a few different layers to the CV stack.


1) Low-level - filters, features, edge detection - Image filters, image processing, etc.

2) Mid-level - segmentation, clustering

3) High-level - detection, object recognition, scene understanding - Augmented Reality, facial recognition, security, movie analysis, (also Robotics)


There are a wide variety of applications at each of these levels. It just depends on what interests you the most.

The higher you climb up the stack, the more you'll find yourself reading Artificial Intelligence and Machine Learning papers. I typically prefer high-level vision problems--which is where Lambda Labs has focused most of it's research and development.

Is there anything else that you're specifically interested in? That's awesome. I believe that in my course we are going over segmentation soon.  I'm definitely more interested in the high level augmented reality problems.    For my graduation-final-work I did a Vision-Based Navigation system, studying it as a substitute for GPS. The user would take a photo with a smartphone, then the photo is compared to a lot of reference-photos, when a close match is found it means the user is in that region/spot. For my graduation-final-work I did a Vision-Based Navigation system, studying it as a substitute for GPS. The user would take a photo with a smartphone, then the photo is compared to a lot of reference-photos, when a close match is found it means the user is in that region/spot.   </snippet></document><document><title>Maximum Flow in O(nm) Time</title><url>http://blog.computationalcomplexity.org/2012/11/andrew-goldberg-guest-blog-on-new-max.html?utm_source=twitterfeed&amp;amp;utm_medium=twitter</url><snippet>  The author's page on it.

http://jorlin.scripts.mit.edu/Max_flows_in_O(nm)_time.html    Serendipitously, my Algorithms class only just wrapped up our discussion on the max-flow problem yesterday.

Can someone more knowledgeable give us a tl;dr of the algorithm? I tried to glance through the paper and video and from what I can tell, the algorithm is an iterative-improvement algorithm and the flow networks are "contracted" and simplified to make things quicker. It would be awesome if someone could expand on this. If that's possible&#8212;I realize it's a really complicated algorithm. Serendipitously, my Algorithms class only just wrapped up our discussion on the max-flow problem yesterday.

Can someone more knowledgeable give us a tl;dr of the algorithm? I tried to glance through the paper and video and from what I can tell, the algorithm is an iterative-improvement algorithm and the flow networks are "contracted" and simplified to make things quicker. It would be awesome if someone could expand on this. If that's possible&#8212;I realize it's a really complicated algorithm. Same here, I've got a test on it in 9 hours.  Same here, I've got a test on it in 9 hours.  CS 3510 at GTECH? I just took that exam.. Thats whats up. Shit might have sucked. Dude I fucked up. That will be my dropped test for sure. It was alright, I had learned the ford fulkerson this morning off a youtube video. Number 1 killed me though, I had no idea how to do it using A. Turns out it was incredibly simple :/ Holy shit! Me too! Did you use this video? https://www.youtube.com/watch?v=MPTjwiR_lCw

Also, I did not realize the second question was about Matrix multiplication. Fuck my life.  Yes! It was perfect, I wrote down all that junk on my sheet.

As for the second, I've been printing the practice exam on the back of my crib sheet. Noticed that after staring at the question for 15 minutes. Holy shit! Me too! Did you use this video? https://www.youtube.com/watch?v=MPTjwiR_lCw

Also, I did not realize the second question was about Matrix multiplication. Fuck my life.    Is this restricted to integer weights? Integer weights is equivalent to rational weights (they can be converted in negligible runtime), and rational weights work for all implementations of floats/doubles (generic systems don't actually store real numbers) This isn't /r/programming.  We want to know the theoretical bound.  In practice since everything is bounded by a 32-bit number, everything can be done in O(1) time, right? &amp;gt; since everything is bounded by a 32-bit number, everything can be done in O(1) time, right?

[arbitrary-precision arithmetic](http://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic), used much more frequently than symbolic computation.

ebix is just pointing out that in this particular problem, solving for integers is equivalent to solving for rationals. Which is important, as there are many applications of network flows that require rationals or arbitrarily high precision floating point numbers, but probably not many that require solving for maximum flow with symbolic numbers. In fact, many previous algorithms for this problem don't work with real numbers.

Perhaps not as cool as solving for real numbers, but this problem has many real world applications. I didn't read the paper that closely, but is it true that solving for integers is equivalent to solving for rationals?  Yes, you can convert between the two, but if I recall correctly there are some max flow algorithms that work better on integers since there is a discrete number of integers between any two integers.  This is not the case with rationals, even if you can convert them into integers. The maximum flow problem has the nice property that if you take a problem instance and a solution, and multiply all numbers by a constant, the new solution will be valid for the new instance.

So, given a flow problem for rationals, you can find the lcm of all the denominators. Multiply all capacities by that number, and now all capacities are integers. Once you solve the problem, just take all the integer flows and divide by that lcm, and you have your solution.

[Here](http://i.imgur.com/Sfd98.png)'s an excerpt from [Algorithm Design](http://www.amazon.com/Algorithm-Design-Jon-Kleinberg/dp/0321295358) (section 7.2) that talks about real numbers.
 Is this restricted to integer weights? From my short watching of the video it doesn't seem so.</snippet></document><document><title>That's fast: Cosmic simulation scores more than 10 petaflops sustained performance. Crossposted to /r/Computing, /r/Physics</title><url>http://ascr-discovery.science.doe.gov/bigiron/petaverse1.shtml</url><snippet> </snippet></document><document><title>Homework help - Cache Hits/Misses</title><url>http://www.reddit.com/r/compsci/comments/12v2ts/homework_help_cache_hitsmisses/</url><snippet>Hey everyone. I need some homework help. My professor LOVES to rip off problems from other schools. She recently posted this gem:

http://www.cs.princeton.edu/courses/archive/fall04/cos471/finals/final3.pdf

Scroll down to question IV: Caches. I have no idea how they are getting this solution. If someone could explain this like I am 5, it would greatly help. Please note I am studying this and I do know what a cache hit/miss signifies. I am having trouble with this specific problem. Thanks.

Edit: Solved and understood. Thank you guys. I was able to follow all problems under IV: Caches because of you guys. Your help is much appreciated. Upvotes for all!   Basics, you know a cache hit is when an address and its value is within the cache, and cache miss is when the address is not within the cache. How to determine if address is in cache
(2^n ), the index for that address will be the last n bits

i.e. 8 bit cache (2^(n=3)) gives you the last 3 bits as the cache index. The remaining portion of the address is called the Tag.

address 10110^2  means cache index 6 is where the address will reside. The tag for this address is then 10^2

This means that to determine a cache hit/miss, you check the index and the tag. 

    If input_address == (tag, cache_index),
       then cache hit.
    Else
        cache miss

In the case of the  HW, there is also a valid flag, signifying if the cache index is in use. 0 for no, 1 for yes. this changes the pseudo to become 

    If cache[cache_index].v ==1 and input_address == (tag, cache_index)
       cache hit
    Else
      cache miss

No there is different kinds of caches to worry about. In the case of your homework, direct and associative are the main ones to worry about.
Direct mapped means that within your cache, one row(index) correlates to one cache address.

n-Associative means that within your cache, one row will correlate to n addresses
Therefore when you can have hit for addresses with the same index
i.e. if a = 11011^2  is accessed and b = 10011^2 is then accessed, b doesn't cause a to become evicted from the cache. So if a is referenced again, it will still reside within the cache.

a problem with n-associative, however, is the rules you use with you DO need to evict. One of the more common rules is Least Recently Used, i.e. keep a count of use for each cache entry. So when you evict an entry, you do so based on the least most used entry at the time.

So given the same 8-byte cache cache from the Direct Mapped, you can make an equivalent 2-way associative or 4-way associative cache.

Calculating Efficiency of the cache:
Standard metric is Hit Rate/Miss Rate. This is done by literally checking the hits/address for a given sequence of address accesses.

      i.e. hits = 2
           num_address = 7
           hit_rate = hits/num_address //  2/7

Same is way applies to miss_rate

Q5 is standard run time speeds. If cache hit, use normal CPI, if miss, use CPI + miss penalty
  Is there something specific about this problem that you are having issues with or just the whole problem in general?</snippet></document><document><title>Is this a thing? Efficient discovery of (hard to phrase, this is an example) which light switches go to which rooms in a skyscraper you're looking at from the outside?</title><url>http://www.reddit.com/r/compsci/comments/12u9io/is_this_a_thing_efficient_discovery_of_hard_to/</url><snippet>I was walking home tonight and looked up at a skyscraper. Some rooms were lit and some weren't. I started wondering... if I had a lot of switches, each wired to the lights in one and only one outward-facing room in the skyscraper, could I map switches to which room each controlled the lights of... in better than linear time (linear with respect to the number of rooms)?

Obviously I could turn each one on and off sequentially and observe which room lit up, but I'm hoping there's a great application of dynamic programming or divide and conquer here. I could also see there being a not-difficult proof that this task takes at least linear time.

Thanks!  Imagine 8 rooms, 8 switches (i.e., 8 bits):

Turn on 4 switches simultaneously, selected at random .  Capture which 4 rooms light up.

Turn off 2 of those 4, and on 2 of the remaining 4.  Capture which 4 lights turn on.

Turn off 1 of the 2 remaining on from step 1, plus 1 of the 2 turned off from step one, plus one of the 2 on from those left off in step 1 but turned on in step 2, plus one of the two that has been left off the entire time.  Capture which lights turn on.

You've just walked all states of 7 of the lights, in 3 steps.  The 8th light remains on through all steps.  Assuming that the time to analyze is zero, the problem reduces from O(N) to O(LogN).

If there's any pattern to the lighting, (i.e., switches sequence horizontally or vertically), patterns would reduce solution steps even more. From a practical point of view, you've toggled a switch for 12 times (if I counted correctly). Just switching them on sequentially and writing down which light turns on needs only 7 toggles (you can save yourself the last one, obviously).

As always, it depends what you consider as your atomic operation. Is it switch toggling, or setting a configuration and go looking. I believe OP meant switch toggling. Imagine 8 rooms, 8 switches (i.e., 8 bits):

Turn on 4 switches simultaneously, selected at random .  Capture which 4 rooms light up.

Turn off 2 of those 4, and on 2 of the remaining 4.  Capture which 4 lights turn on.

Turn off 1 of the 2 remaining on from step 1, plus 1 of the 2 turned off from step one, plus one of the 2 on from those left off in step 1 but turned on in step 2, plus one of the two that has been left off the entire time.  Capture which lights turn on.

You've just walked all states of 7 of the lights, in 3 steps.  The 8th light remains on through all steps.  Assuming that the time to analyze is zero, the problem reduces from O(N) to O(LogN).

If there's any pattern to the lighting, (i.e., switches sequence horizontally or vertically), patterns would reduce solution steps even more.  It would take at least linear time. Essentially, the skyscraper is a map (the switches are the keys and the lights which they turn on are the values). You're asking for an algorithm that can copy this map without iterating through all of its mappings, which doesn't exist. There's an implicit assumption here that the measure of cost is how many switches have to be flipped in total, and not how many trips inside and outside you need to make (which seems like it would be the real dominator of time).

An efficient O(logN) divide-and-conquer algorithm exists for the version using this latter measure; coincidentally, this exact solution was required for a problem posed in the recent Topcoder Open 2012 under the name [SwitchesAndLamps](http://apps.topcoder.com/wiki/display/tc/TCO+2012+Round+2A).

The idea is to keep track of which subsets of switches could correspond to which subsets of lights; within each group we'll flip half of these switches, step outside, and record which changed - this reduces the cardinality of each 'ambiguous subset' by roughly half and can be repeated until the number of possible matches for each switch is exactly 1. It would take at least linear time. Essentially, the skyscraper is a map (the switches are the keys and the lights which they turn on are the values). You're asking for an algorithm that can copy this map without iterating through all of its mappings, which doesn't exist. I think you're right that it would take at least linear time, but I don't agree with your reasoning. You don't need to iterate through all the switches because the last one can be determined through the process of elimination. (Assuming an equal number of switches and rooms, that is.)  </snippet></document><document><title>Fun with Bioinformatics</title><url>http://www.reddit.com/r/compsci/comments/12tw6j/fun_with_bioinformatics/</url><snippet>Hey folks,

My first post here in compsci...

I'm currently an undergrad and in an algorithms course which I am thoroughly enjoying. I am considering graduate school and perhaps doing research in bioinformatics, anyone have any fun or interesting practice problems? I really enjoy problems that require dynamic programming, or extensive recursive calls. I'm really trying to gage how much I may or may not enjoy the field and before I go into any research I thought it would be a good idea to get some common bioinformatics problems that extend beyond problems like lcs and edit distance.  Check out [Rosalind](http://rosalind.info/problems/as-table/), it's like Project Euler for bioinformatics problems.

It goes from easiest to hardest in terms of difficulty, but tries to get you to implement actual algorithms in bioinformatics. Wow, looks great, this is exactly what I was hoping for. Do you do work in bioinformatics? Yeah, I'm doing some machine learning research with support vector machines and protein hotspots at the moment.  It's pretty neat stuff. :) Are you doing your research in an Australian institution by any chance? Nope, I'm at McGill University, in the glorious country of Canada. Check out [Rosalind](http://rosalind.info/problems/as-table/), it's like Project Euler for bioinformatics problems.

It goes from easiest to hardest in terms of difficulty, but tries to get you to implement actual algorithms in bioinformatics. Check out [Rosalind](http://rosalind.info/problems/as-table/), it's like Project Euler for bioinformatics problems.

It goes from easiest to hardest in terms of difficulty, but tries to get you to implement actual algorithms in bioinformatics. Check out [Rosalind](http://rosalind.info/problems/as-table/), it's like Project Euler for bioinformatics problems.

It goes from easiest to hardest in terms of difficulty, but tries to get you to implement actual algorithms in bioinformatics.  &amp;gt; I really enjoy problems that require dynamic programming, or extensive recursive calls.

Usually very deep recursion is avoided, since high performance generally means using languages without tail call optimization (e.g. C, C++, and pretty much everything vaguely like them) so on big data you will blow the stack.

Many things are conceptually recursive, of course, but in a high performance context you'd usually write them in an iterative style. Understood, I like the concept of them, not necessarily applying them, that is what dynamic programming is for, I should have been a little more specific about what I meant exactly. Use a functional language like Haskell - the challenge makes programming fun again, and the performance loss isn't too great (though if you're really worried learn to use bytestrings and when to use strict evaluation). The problem with using Haskell in bioinformatics research is that on the CS side of things, not only do we create algorithms, but we need to release properly tested tools. Haskell is all fine-and-dandy until you release your tool for biologists to download and use.

For that reason alone, its safer and easier to stay with Python and/or Java/C++ The problem with using Haskell in bioinformatics research is that on the CS side of things, not only do we create algorithms, but we need to release properly tested tools. Haskell is all fine-and-dandy until you release your tool for biologists to download and use.

For that reason alone, its safer and easier to stay with Python and/or Java/C++ What exactly is your contention? Haskell can't be "properly tested"? Or it can't be "released"? Or..?  </snippet></document><document><title>
A new algorithm predicts which Twitter topics will trend hours in advance</title><url>http://web.mit.edu/snikolov/Public/trend.pdf</url><snippet>  This sounds about as useful as my sarcasm prediction algorithm. This sounds about as useful as my sarcasm prediction algorithm.  Is it Nate Silver's? From now on I only believe in his predictions. Is it Nate Silver's? From now on I only believe in his predictions. how did he do btw, ive been meaning to check 50/50. Perfect predictions. 9/9 is more like it. The dustbunny under my couch could have called the other 41. Still impressive, though.  9/9 is more like it. The dustbunny under my couch could have called the other 41. Still impressive, though.   Now we just need to figure out a way to predict the stock market based on Twitter, and we'll be in business! There is already some work done on this front [0]. Ruiz et al basically concluded the stock price of a company is directly proportional to the number of discussion topics of that company viz. if there for a company X there are fewer topics under discussion the stock prices would be poor.

[0] http://www.cs.ucr.edu/~vagelis/publications/wsdm2012-microblog-financial.pdf &amp;gt; Ruiz et al basically concluded the stock price of a company is directly proportional to the number of discussion topics of that company viz.

Did you actually read the paper you linked? That's not the conclusion at all. The correlation is with volume and price movement, not the price itself. The authors even say the correlation is small, there's no suggestion that there's a direct proportion.   [deleted]</snippet></document><document><title>Grokking relativization</title><url>http://www.reddit.com/r/compsci/comments/12uexx/grokking_relativization/</url><snippet>I've been jumping around in Sipser's "Introduction to the Theory of Computation", reading proofs that interest me.

I'm trying to grok Theorem 9.20 (part 1), which says that an oracle A exists whereby P^A != NP^A.

The proof contains the following line, which is tripping me up:

"We design A as follows. Let M-1, M-2, ... be a list of all polynomial time oracle TMs. We may assume for simplicity that M-i runs in time n^i."

Is the collection of all polynomial time oracle TMs enumerable? If so, how can you do it? If not, then is A actually decidable? If A is not decidable...

Sipser argues that a proof of P = NP could not be a simple simulation proof, because such a proof would also demonstrate P^A = NP^A for all A. But if A is not decidable, then theorem 9.20 is (imho) a rather meaningless statement.  Eh, nevermind. I looked at Arora and Barak's [book](http://www.cs.princeton.edu/theory/index.php/Compbook/Draft#diag), page 70, and they do it slightly differently. Instead of M-i being the i-th polynomial time oracle TM, M-i is just the TM encoded by the number i, and they terminate the simulation if it runs for too many steps.

Still, am I right in thinking that the list of all polynomial time oracle TMs is not recursively enumerable? You could always have a machine which spins in a tarpit for a long (but constant) time before doing the work which is polynomial. Eh, nevermind. I looked at Arora and Barak's [book](http://www.cs.princeton.edu/theory/index.php/Compbook/Draft#diag), page 70, and they do it slightly differently. Instead of M-i being the i-th polynomial time oracle TM, M-i is just the TM encoded by the number i, and they terminate the simulation if it runs for too many steps.

Still, am I right in thinking that the list of all polynomial time oracle TMs is not recursively enumerable? You could always have a machine which spins in a tarpit for a long (but constant) time before doing the work which is polynomial.</snippet></document><document><title>How to get better at solving algorithms?</title><url>http://www.reddit.com/r/compsci/comments/12reo5/how_to_get_better_at_solving_algorithms/</url><snippet>I've started doing interviews for internships, as I'm aspiring to get one this summer. However, I've done horrible on them.

I got A+ in my classes. I could implement and use data structures efficiently. I understood the algorithms presented to me. But I feel this wasn't/isn't an accurate measurement of my problem solving skills.


When presented with *new* algorithms or problems I have never encountered before, my brain stops working. I can rarely figure out the solutions all by myself. I always have to consult online references or other people's code, and I feel this doesn't make me better. I'm simply memorizing how other people got to it.


I've even bought books (Cracking the coding interview). I can't solve many of the problems they present. I have to read the solutions, then I get it (who wouldn't /eyeroll). But this is not helping me become better.



I've also worked on several projects related to web programming (creating user systems, forums) and game programming (simple 2D games with networking capabilities). I've learned a lot from these projects. But most of what I've learned concerns APIs and technologies (DirectX, MySQL, Winsock) and how they work. I rarely ran into having to implement or solve a puzzle type algorithms (maybe it's because I never got into doing advanced AI?) It's all been about understanding how a specific technology works. And if I had to use something complicated, I just used libraries which already implementing what I had to do.

**tl;dr - got good grades and understood all concepts in comp.sci. courses. Have had plenty of non-academic programming/project experience. Have read books on algorithms. BUT I still suck at them if presented with one I haven't previously seen the solution to.**

So does anyone have any tips on improving my algorithm skills that doesn't include trying to solve them and looking up a solution when I fail? 

*****
Thanks for all the advice. I really appreciate it. I'll start working on "easy" problems and go at them until I get at least a brute force solution. From there I'll try and clean up my code/solution. I'll try not to resort to looking at solutions in the same 48 hours of having started a problem at least.

Again, thanks for the advice. And for those of you who also struggle, it's good to know I'm not alone ;)
  No.

There is no way to improve your algorithm writing/problem solving skills without practice. Try to solve them, but don't look at the solution until you have given it your absolute best (this may take hours or days depending on the complexity of the problem).

Perhaps projecteuler.net will be of interest to you.

In the end, there is no easy mechanical way to develop algorithms. If it were easy, we would have already written a program to write programs. No.

There is no way to improve your algorithm writing/problem solving skills without practice. Try to solve them, but don't look at the solution until you have given it your absolute best (this may take hours or days depending on the complexity of the problem).

Perhaps projecteuler.net will be of interest to you.

In the end, there is no easy mechanical way to develop algorithms. If it were easy, we would have already written a program to write programs. &amp;gt; this may take hours or days depending on the complexity of the problem

I want to stress that this is not an exaggeration. In the algorithms courses at my university ([example](http://www.win.tue.nl/~speckman/2IL05.html)) we would get a 1-2 page exercise set every week, and it would usually take me 1-2 days to complete it. I'm one of the better students in my class. They don't teach us how to implement an algorithm, but to analyze it and adapt it.

When we couldn't figure out a problem in algorithms classes, we could go to the teacher and he would ask the right questions such that we would discover what we should change in our current solution.

There were no answer sheets at all.

If you are practicing algorithms and you need some hints on how to continue, I think you can ask at /r/algorithms. In that post, explain what you thought about / tried and why it doesn't work. (This also helps clear up your mind, and sometimes helps you find a solution.) If you want you can send me the link to your post and if I have time I'll give you a hint. Does this time taken per problem get better with more practice? I am largely in the same boat as OP. Yes. I was the same way when I took my first algorithms class a little less than a year ago. I practiced a lot to get an A in that class, and since then I've taken more algorithms classes. I used to fear algorithms tests because of how I would blankly stare at the paper... but now, it's a lot better, and it'll get better for you too. 

I'm interviewing for jobs now, and I got asked an algorithm question in my last interview. I wrestled with it for about 5 minutes before I found the optimal solution. It'll get better with practice! :)  Does this time taken per problem get better with more practice? I am largely in the same boat as OP. Yes. When you have practiced a technique a number of times, for instance dynamic programming, it becomes easier to see how it applies to new problems.

Then when you master that technique, you can move on and practice other techniques. Your toolbox keeps growing, and you'll be able to solve more and more problems. No.

There is no way to improve your algorithm writing/problem solving skills without practice. Try to solve them, but don't look at the solution until you have given it your absolute best (this may take hours or days depending on the complexity of the problem).

Perhaps projecteuler.net will be of interest to you.

In the end, there is no easy mechanical way to develop algorithms. If it were easy, we would have already written a program to write programs. No.

There is no way to improve your algorithm writing/problem solving skills without practice. Try to solve them, but don't look at the solution until you have given it your absolute best (this may take hours or days depending on the complexity of the problem).

Perhaps projecteuler.net will be of interest to you.

In the end, there is no easy mechanical way to develop algorithms. If it were easy, we would have already written a program to write programs.     Try programming competition problems. Most online judges will categorize the problems, or at least let you see how many people have solved them; this will often be a good indicator of how approachable the problem is. Try doing those, as they're often relatively short to code (i.e. less than 100 lines) and require much more thinking than actual coding. Solving those will train you in the process of thinking possible solutions to problems, and improving them to fit tight time/efficiency bounds.

Two sites I can recommend are [spoj.pl](http://www.spoj.pl/) and [TopCoder](http://www.topcoder.com/). If you would like to train more of your mathematical side, perhaps also give [Project Euler](http://projecteuler.net/) a shot. Its aim is not to pass test cases, but to think one particular instance of a problem through. Note that these will involve more mathematics/optimized algorithms than data structures, on average. Try programming competition problems. Most online judges will categorize the problems, or at least let you see how many people have solved them; this will often be a good indicator of how approachable the problem is. Try doing those, as they're often relatively short to code (i.e. less than 100 lines) and require much more thinking than actual coding. Solving those will train you in the process of thinking possible solutions to problems, and improving them to fit tight time/efficiency bounds.

Two sites I can recommend are [spoj.pl](http://www.spoj.pl/) and [TopCoder](http://www.topcoder.com/). If you would like to train more of your mathematical side, perhaps also give [Project Euler](http://projecteuler.net/) a shot. Its aim is not to pass test cases, but to think one particular instance of a problem through. Note that these will involve more mathematics/optimized algorithms than data structures, on average. For those of us without a nice CS degree, do you know a site that has interactive tutorials for programming related math?    It's is normal that finding solutions by yourself takes much effort and time than just trying to understand the solutions other came up with.

Come to terms with the fact that it does indeed take time, but get satisfaction when you indeed find the solution in the fact that you basically reinvented the solution, because, this time, you really were a creator instead of a learner.    [deleted] &amp;gt; If the solution has equal to or worse complexity than O(n^2 ), then it's wrong. Keep trying. 

That's absolutely not true. For example, many DP problems have O( n^2 )solutions. I qualified with educational/interview problems. I also defined them as "rules of thumb". I'm not going to debate this. Alright, I only wanted to express my disagreement. In my experience, this isn't even a good rule of thumb. I see.. as opposed to the answers of ["No" (useless) and "Practice" (obvious).](http://www.reddit.com/r/compsci/comments/12reo5/how_to_get_better_at_solving_algorithms/c6xn7qf)

Practical advice always trumps useless advice.

Obvious advice was already known so it's redundant.     Assuming you're solving an educational/interview type of problem that's moderately difficult or hard, I have a few rules of thumb I follow.

1. If you found the solution within 5-10 minutes, then it's wrong. It's an obvious solution and likely very inefficient. Keep trying. If you found the solution within 10-60 minutes, then it's probably a decent one but not an optimal solution. Keep trying.

1. If the solution has equal to or worse complexity than O(n^2 ), then it's wrong. Keep trying. Educational problems usually have several solutions with complexities ranging between O(log n) and O(n log n).

1. Put the problem down for a while. Keep rumminating on it. Sometimes it can take a few days to come up with an Ah-ha! moment and realize there's an entirely different strategy available. Try to reframe the problem as a slightly different problem. Try to represent the problem in a different way.


Some things to watch out for:

1. Some problems are deceptively simple. On the surface they look super easy, but they're actually very difficult. Run through several concrete examples and flesh out various scenarios. If you came up with an answer very quickly, be suspicious. It could be one of these.

1. Some problems are deceptively difficult.  Try to simplify it  with the simplest cases. If it's 3D, then look at it in 2D. If it's 1000 items, try it with 5.

1. Some [problems](http://en.wikipedia.org/wiki/Dividing_a_circle_into_areas) produce answers matching a pattern for the first several iterations (1, 2, 4, 8, 16...) so you think you've arrived at the correct solution, but you actually haven't (1, 2, 4, 8, 16, 31, 57...) Be thorough, try concrete examples with extreme values.

1. Some problems are unsolvable or have no known solution. The question is asked to see how you handle impossible scenarios. Look for concessions or try to limit the problem domain. Sometimes being able to alter the problem will make an unsolvable problem solvable. Sometimes it is sufficient to produce an approximate solution.   </snippet></document><document><title>Counting votes, in the precinct and on the Web | How computational scientists are rethinking U.S. elections&#8212;and making e-commerce smarter</title><url>http://www.seas.harvard.edu/news-events/press-releases/counting-votes</url><snippet>  &amp;gt; As Xia puts it, "You want people to be happy. But in these low-stakes, online applications, you really want to figure out what is the truth."

I wonder how these themes about computational election theory, and preference aggregation theory in general, would apply to things like crowdsourcing and Amazon Mechanical Turk. Anyone know? There's definitely related research! I'm wondering if you have a specific idea in mind? Because it's still a pretty broad question with lots of research going on.

Some examples are

 * asking crowdsourcing questions to multiple people, combining answers with a voting rule
 * how do people make mistakes in answering questions (related to voting rules and probability)
 * how do we reward people in crowdsourcing so that they want to answer truthfully

From one perspective, you might say that crowdsourcing and voting are actually polar opposites: In voting, people care very much about the outcome of the election, and we absolutely forbid monetary transfers. In crowdsourcing, people (usually) don't care what answer the designer ends up choosing, and we do allow paying people to report.

But, often approaches/results in voting can be applied in crowdsourcing. One example is the *maximum-likelihood* approach to voting. The idea is that there is some "true" "correct" ranking of candidates, but nobody knows for sure what it is. Instead, everybody randomly observes some noisy ranking with some probability. E.g., if the true ranking is A &amp;gt; B &amp;gt; C, maybe you observe B &amp;gt; A &amp;gt; C with a certain probability, and C &amp;gt; A &amp;gt;B with another, and so on. Then our voting rule selects the maximum-likelihood "true" ranking given all the votes that people cast. So we might pick B &amp;gt; A &amp;gt; C because it has the highest chance, out of any other ranking, of noisily generating the votes people cast.

You can imagine that we might like to apply this approach in crowdsourcing! Because we think there is actually one "true" answer to our question, but people noisily observe answers that they think are right.

I can provide some citations if you're interested as well! Thanks, I can see how those principles would apply well to crowdsourcing--great explanation. But now you've raised another question: Do computational scientists (or, maybe I should ask, do YOU) believe there is an objective truth to be found in a political election? Is there really a "correct" ranking of candidates? I'm not sure I understand that assumption in the maximum likelihood method. </snippet></document><document><title>The set partition problem and drawing the Presidential election</title><url>http://liveatthewitchtrials.blogspot.ie/2012/11/drawing-presidential-election.html</url><snippet>  I don't exactly understand the question. You just want to get the number of ways a candidate can receive exactly 269 electoral votes? This can be solved very quickly using dynamic programming. Here's a python script that runs instantaneously:

    from collections import defaultdict
    evotes = [9,3,11,6,55,9,7,3,3,29,16,4,4,20,11,6,6,8,8,4,10,11,16,10,6,10,3,5,6,
        4,14,5,29,15,3,18,7,7,20,4,9,3,11,38,6,3,13,12,5,10,3]
    totals = defaultdict(int)
    totals[0] = 1
    for v in evotes:
        for t, n in totals.items():
            totals[t+v] += n
    print totals[269]

The result is 16976480564070. This assumes every state is winner-take-all, but it can be easily modified to handle Maine and Nebraska. I don't exactly understand the question. You just want to get the number of ways a candidate can receive exactly 269 electoral votes? This can be solved very quickly using dynamic programming. Here's a python script that runs instantaneously:

    from collections import defaultdict
    evotes = [9,3,11,6,55,9,7,3,3,29,16,4,4,20,11,6,6,8,8,4,10,11,16,10,6,10,3,5,6,
        4,14,5,29,15,3,18,7,7,20,4,9,3,11,38,6,3,13,12,5,10,3]
    totals = defaultdict(int)
    totals[0] = 1
    for v in evotes:
        for t, n in totals.items():
            totals[t+v] += n
    print totals[269]

The result is 16976480564070. This assumes every state is winner-take-all, but it can be easily modified to handle Maine and Nebraska. anyone know how to do this in Python 3k? apparently items() is acting like  iteritems() or something and giving me this


    RuntimeError: dictionary changed size during iteration Yeah that's true, `items` returns a view in Python 3, not a list. Make it `list(totals.items())`. I don't exactly understand the question. You just want to get the number of ways a candidate can receive exactly 269 electoral votes? This can be solved very quickly using dynamic programming. Here's a python script that runs instantaneously:

    from collections import defaultdict
    evotes = [9,3,11,6,55,9,7,3,3,29,16,4,4,20,11,6,6,8,8,4,10,11,16,10,6,10,3,5,6,
        4,14,5,29,15,3,18,7,7,20,4,9,3,11,38,6,3,13,12,5,10,3]
    totals = defaultdict(int)
    totals[0] = 1
    for v in evotes:
        for t, n in totals.items():
            totals[t+v] += n
    print totals[269]

The result is 16976480564070. This assumes every state is winner-take-all, but it can be easily modified to handle Maine and Nebraska.   This seems like an interesting problem, but I'm not familiar enough with the language to read all of your solution. Do you have it written in something more common? Maybe something C-like? Or R possibly? I have added GLPK code to the post that is closer to C in case that helps  Yup, it's basically the sum-subset problem. There probably isn't any way to do it efficiently.  Yup, it's basically the sum-subset problem. There probably isn't any way to do it efficiently.  I think its pretty funny that no one knows theoretically given the way the electoral college is distributed how likely an election is to be a draw Well, that's without reducing possibilities. For example, there is no chance that Obama will lose NY or CA, which decreases the complexity by several orders of magnitude. Since there are really only 10 or so real swing states, it's possible to solve the problem in a reasonable time.   I get

     $ fz number_draws.mzn 
     Error: syntax error, unexpected FZ_INT_LIT, expecting FZ_INT in line no. 30

Which appears to be

    $ sed -n '30p' number_draws.mzn 
    set of 1..num_states: STATES = 1..num_states;

:-(

    $ fz -help
    Gecode FlatZinc interpreter
     - Supported FlatZinc version: 1.5
    
    Gecode configuration information:
     - Version: 3.7.3
     - Variable types: BoolVar IntVar SetVar
     - Thread support: enabled (8 processing units)
     - Gist support: enabled

</snippet></document><document><title>What is a good online source to learn formal languages and automata theory?</title><url>http://www.reddit.com/r/compsci/comments/12nz85/what_is_a_good_online_source_to_learn_formal/</url><snippet>EDIT: Thanks for all your suggestions! That course by Ullman looks really great!  Coursera has a course by Ullman on Automata. Its not in session right now but you could still go through the videos/exercises. Probably the best online resource you're going to find. Goes through the Chomsky hierarchy (regex -&amp;gt; cfg -&amp;gt; r.e. languages) as well as the basics of complexity theory. Unless I'm missing something, Ullman's course not being in session means that you CANNOT view his video lecutres on Coursera, there's no archive mode I'm aware of.

However, some students have uploaded a lot of (but not all) his lectures from that course [here](http://www.youtube.com/playlist?list=PL82C4B8475CAC3F95). You can just click [preview](https://class.coursera.org/automata/lecture/preview/index) to see all the videos.  There is a nice set of video lectures at [ArsDigita university] (http://archive.org/details/arsdigita_08_theory_of_computation).  I'm currently taking Automata Theory at The University of Texas/Austin with Prof. Elaine Rich, who wrote [this book](http://www.amazon.com/Automata-Computability-Complexity-Theory-Applications/dp/0132288060), which I've found to be very good. It is very thorough and has plenty of examples, with nice appendices and a great index.

Khan Academy has a (fairly) new computer science series, and there is a [single turing machine lesson](http://www.khanacademy.org/cs/turing-machine/938201372), although I've not looked into it. I'm jealous.  I love Elaine Rich's book, as well as her AI book, both of which are my favorite books on their respective subjects.  Here's a [free online textbook](http://valis.cs.uiuc.edu/~sariel/teach/notes/373/book.pdf) (large pdf) from Illinois. Sipser's book is the most commonly used textbook in the subject, but I also like Kozen's *Automata &amp;amp; Computability* book a lot.   Currently a Comp Sci student at North Carolina State University and we use a book by Sipser as additional material. It's a nice book, good explanations and pictures. I'll post the name of it once I can remember it.   </snippet></document><document><title>Help understanding the difference between C4.5 and CART</title><url>http://www.reddit.com/r/compsci/comments/12n6z8/help_understanding_the_difference_between_c45_and/</url><snippet>I'm taking a machine learning class (which is awesome!), and I'm having trouble understanding the difference between the two classification tree methods C4.5 and CART. I am reading the C4.5 article and the CART: Classification and Regression Trees book.

The differences I can see:

1. C4.5 is an algorithm, and CART is a generalized methodology
2. C4.5 used entropy and information gain, CART used the Gini coefficient

Am I way off? What are the other differences? My professor taught us ID3 and C4.5. He doesn't know much about CART, which is why I'm asking here.  Have a look at the paper linked here: [http://www.reddit.com/r/MachineLearning/comments/12mxy7/top_10_algorithms_in_data_mining_outdated_but_a/](http://www.reddit.com/r/MachineLearning/comments/12mxy7/top_10_algorithms_in_data_mining_outdated_but_a/), section 1.2 explains major differences between C4.5 and CART.  From my knowledge of C4.5 and ID3, you are correct.

I have no experience with CART, but from a quick Google search it seems to be, just as you say, a generalized methodology. C4.5 and ID3 fall under the [TDIDT](http://de.wikipedia.org/wiki/Top-Down_Induction_of_Decision_Trees) family. I am willing to say that ID3 and C4.5 could be considered a part of CART as well, but don't quote me on that.</snippet></document><document><title>I spent way too long writing this, but CS people may enjoy it.</title><url>http://cs.gsu.edu/~nmancuso1/losingedge.html</url><snippet>  /r/cringe is perhaps the more relevant sub for this. I'd rather its silliness elicit a cringe than nothing at all!  ... Trust me, you do not want to end up on /r/cringe. 

They're a pack of locusts. Probably. Again, the lyrics aren't really meant to be taken seriously. Like all ideas, it started at the bar over a couple of beers.   I read it in his voice.     </snippet></document><document><title>CSTheorists as connectors: from Poincar&#233; to mathematical medicine &#171; Theory, Evolution, and Games Group</title><url>http://egtheory.wordpress.com/2012/11/04/connectors/</url><snippet /></document><document><title>Learning how the basic logic blocks of a simple computer are designed?</title><url>http://www.reddit.com/r/compsci/comments/12jy72/learning_how_the_basic_logic_blocks_of_a_simple/</url><snippet>Currently in my 2nd year and I want to transfer into computer science from the faculty I'm currently in. Next semester I need to take a course which topic covers:

basic Von Neumann computer architecture; an introduction to assembly language programming; combinational logic design; and sequential logic design. 


Anyone know a really good textbook where I can study this on my own time during winter? 


edit: I guess I should've asked for COMPUTER ARCHITECTURE books! 

Psylock524 for the website: http://www.nand2tetris.org/course.php,  
com2kid for the book http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319,  




  Those other guys are all wrong! Read [Code](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319) by Petzold. End of story. 

You will literally have a leg up on everyone else who is entering the program, that book is *amazing*. Literally? Literally? Those other guys are all wrong! Read [Code](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319) by Petzold. End of story. 

You will literally have a leg up on everyone else who is entering the program, that book is *amazing*. I second this. This is the single greatest introductory book on computer science. There is absolutely no competition. Better than SICP? Those other guys are all wrong! Read [Code](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319) by Petzold. End of story. 

You will literally have a leg up on everyone else who is entering the program, that book is *amazing*. seems like a lot of people agree with you! What is your opinion on the book The Elements of Computing Systems:?

http://www.amazon.com/Elements-Computing-Systems-Building-Principles/dp/0262640686 Also known as [NAND2Tetris](http://www.nand2tetris.org/). oh wow... exactly what i was looking for. thanks! seems like a lot of people agree with you! What is your opinion on the book The Elements of Computing Systems:?

http://www.amazon.com/Elements-Computing-Systems-Building-Principles/dp/0262640686 Those other guys are all wrong! Read [Code](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319) by Petzold. End of story. 

You will literally have a leg up on everyone else who is entering the program, that book is *amazing*. I have a BS in Comp Sci, but I'm not a low level god of computers. Will this book be too simple and boring?  I have a BS in Comp Sci, but I'm not a low level god of computers. Will this book be too simple and boring?  Those other guys are all wrong! Read [Code](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319) by Petzold. End of story. 

You will literally have a leg up on everyone else who is entering the program, that book is *amazing*. Those other guys are all wrong! Read [Code](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319) by Petzold. End of story. 

You will literally have a leg up on everyone else who is entering the program, that book is *amazing*. Those other guys are all wrong! Read [Code](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319) by Petzold. End of story. 

You will literally have a leg up on everyone else who is entering the program, that book is *amazing*. seems like a good book to get the feel of computer science, I"ll be sure to read it. I'm trying to find a more practical book though, something like Logic and Computer Design Fundamentals, by M. Mano and C. R. Kime.

Or do you think CODE is better? thanks. CODE will teach you everything from the basics of discreet math to how the basic building blocks of a computer work. 

I don't know if I'd say it is *practical* but you will most certainly have a better grasp of how computers function after you have read it.

Knowing how an ALU works isn't really what I'd call a practical skill, but it is something I believe software engineers should know about! :)  seems like a good book to get the feel of computer science, I"ll be sure to read it. I'm trying to find a more practical book though, something like Logic and Computer Design Fundamentals, by M. Mano and C. R. Kime.

Or do you think CODE is better? thanks.  [Code](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319) (mentioned by com2kid below) *is* fantastic, but it's more of a regular-old-book book.  For something more tangible, hands-on, and "textbooky" I'd recommend [The Elements of Computing Systems](http://www.amazon.com/Elements-Computing-Systems-Building-Principles/dp/0262640686) which I made my way through one winter a few years ago before I was even really a CS student.  You start off with NAND gates and end with a complete computing platform including hardware in HDL, a virtual machine, compiler, standard library, the works (All of which \*you wrote!\*).  It's maybe my favorite textbook of all time. TECS is neat. I did the first couple of chapters with VHDL because I dislike using toy systems for education, but never finished the whole CPU as I began to design a "better" ISA for it, which complicated the decoder a bit. Everything is admittedly very "toy", but I'd say it's just the right level to insure that people can progress through the whole thing and \*actually build something\* without having any kind of a "tool barrier" (save knowing a programming language well enough to do the software stuff).  

I'd be interested in seeing what a "non-toy" version of TECS would look like, and if people could get through it in the same way... Everything is admittedly very "toy", but I'd say it's just the right level to insure that people can progress through the whole thing and \*actually build something\* without having any kind of a "tool barrier" (save knowing a programming language well enough to do the software stuff).  

I'd be interested in seeing what a "non-toy" version of TECS would look like, and if people could get through it in the same way... TECS is neat. I did the first couple of chapters with VHDL because I dislike using toy systems for education, but never finished the whole CPU as I began to design a "better" ISA for it, which complicated the decoder a bit. [Code](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319) (mentioned by com2kid below) *is* fantastic, but it's more of a regular-old-book book.  For something more tangible, hands-on, and "textbooky" I'd recommend [The Elements of Computing Systems](http://www.amazon.com/Elements-Computing-Systems-Building-Principles/dp/0262640686) which I made my way through one winter a few years ago before I was even really a CS student.  You start off with NAND gates and end with a complete computing platform including hardware in HDL, a virtual machine, compiler, standard library, the works (All of which \*you wrote!\*).  It's maybe my favorite textbook of all time. [Code](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319) (mentioned by com2kid below) *is* fantastic, but it's more of a regular-old-book book.  For something more tangible, hands-on, and "textbooky" I'd recommend [The Elements of Computing Systems](http://www.amazon.com/Elements-Computing-Systems-Building-Principles/dp/0262640686) which I made my way through one winter a few years ago before I was even really a CS student.  You start off with NAND gates and end with a complete computing platform including hardware in HDL, a virtual machine, compiler, standard library, the works (All of which \*you wrote!\*).  It's maybe my favorite textbook of all time. I'm a high school student interested in computer science with some knowledge of Python. Both of these books sound interesting, but is there a point to reading each one? Or do they contain more or less the same information? If they're both worth reading, which one would you recommend first? [Code](http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319) (mentioned by com2kid below) *is* fantastic, but it's more of a regular-old-book book.  For something more tangible, hands-on, and "textbooky" I'd recommend [The Elements of Computing Systems](http://www.amazon.com/Elements-Computing-Systems-Building-Principles/dp/0262640686) which I made my way through one winter a few years ago before I was even really a CS student.  You start off with NAND gates and end with a complete computing platform including hardware in HDL, a virtual machine, compiler, standard library, the works (All of which \*you wrote!\*).  It's maybe my favorite textbook of all time. I believe the entire TECS text is online here: http://www1.idc.ac.il/tecs/plan.html    Be careful.  What you're describing here is not computer science.  Computer science actually has nothing to do with comput*ers*, it is about comput*ing*.  If you're interested in computers then electronics/electrical engineering might be more up your street.  If you really want to get how software works with hardware, I think you should check out The Elements of Computing along with it's [companion site](http://www1.idc.ac.il/tecs/plan.html) that includes everything you need (software, lessons, the book in pdf).   

That book is what inspired [this guy](http://www.youtube.com/watch?v=LGkkyKZVzug) to build a 16 bit alu in minecraft and what led to a plethora of other architectures in the game.

It's practical, understandable, you can actually make something from it, and you'll understand how code you write is implemented on the hardware side.

*edit: internetftw  actually mentioned this first.  I didn't look through all of the comments before posting. Yup.  That's actually 3 comments now suggesting TECS.

btw: I'm also the "this guy" you speak of.        If you ever need to simulate logic gates (this comes in useful if you're reading TECS) I recommend Logism: http://ozark.hendrix.edu/~burch/logisim/index.html  CS major here, haven't completed a BS yet - Feynman's Lectures on Computation.  He glosses over some of the introductory parts, but that is a damned good read and stepped me through the abstract theory of the elements of computing theory, reading voltage levels to assign arbitrary values of 1 and 0 for the basic building blocks of CS theory, walking through logic gates to step up through general computing systems, to progress into the thermodynamic limits of physical computing.  All in all, even with some dated sections, a solid 8-9/10 book to learn about the nuts and bolts of CS theory and computer design.  Challenging, yet problems sets, and Feynman's style for making the subject material comprehensible for even laymen make this book one of my favorite supplements to other textbooks.  This book acts more as a companion guide to other text books.  I highly recommend it 
</snippet></document><document><title>Categorize non standardised entries in big datasets</title><url>http://www.reddit.com/r/compsci/comments/12k4e9/categorize_non_standardised_entries_in_big/</url><snippet>Hi,
I'm working on the National Vulnerability Database (NVD). I want to categorise the vulnerable software by category. I already have the categories and a good training set to feed into a machine learning algorithm.

The original idea was to use the description of the vulnerability in NVD to categorise the software, but this won't obviously work (because it doesn't describe the software). 

Then we thought to download the first paragraph of the Wikipedia entry for that software. This works only 10% of the time, as many entries do not match any page. [This is an example of a page that cannot load](http://en.wikipedia.org/wiki/Asl_ux_4800) Further manual google queries seem to identify that software as a VOIP server. In some other cases, e.g. for the software Swift, [the returned page is definitely not related to the software](http://en.wikipedia.org/wiki/Swift), and in the [disambiguation page](http://en.wikipedia.org/wiki/Swift_(disambiguation)#Software_and_information_technology) it is not even clear which entry should be the one of interest.

Do you have suggestions to mitigate this problem? More reliable software-related databases other than wikipedia? Better ways to query the dataset instead of feeding the bare software name provided by NVD (e.g. up-ux_v, vendor:Nec)? Ways to include the vendor in the query, so to make the results more reliable?

Ever faced a problem like that?

Thanks! </snippet></document><document><title>The LCS Algorithm</title><url>http://www.reddit.com/r/compsci/comments/12ippp/the_lcs_algorithm/</url><snippet>I've recently learned an efficient solution to the longest common subsequence problem via dynamic programming. (referencing Data Structures and Alorithms in Java, 5th, Goodrich &amp;amp; Tamassia)

The method is to use recursion :
Briefly, 

LCS(X,Y)
for i = -1 to (n-1) {

    for j = -1 to (n-1) {

        if (x[i] == y[j])

            LCS(i, j) = LCS(i -1, j - 1) + 1

        else 

            LCS(i, j) = max{ LCS(i - 1, j), LCS(i, j-1) }
    }
}

Further explained is that this will return a number that is the length of the subsequence and that in order to return the subsequence itself, an additional postprocessing is necessary... 

I have 2 questions regarding this problem.
1) Why is recursion necessary?
It seems to me that simply looping through will do the trick: 

int[] array = new int[x.length];

for (int i = x.length-1; i &amp;gt; 0; ) {

	for (int j = y.length-1; j &amp;gt; 0; ) {

		if (x[i] == y[j]) {

			array[i] = 1;

			i--;

			j--;

			}

			else {

				if (i &amp;gt; j || i == j)

					i--;

				else if (j &amp;gt; i)

					j--;

				}


2) why would a postprocessing step be necessary?
One could simply save the array "array" to store chars and save each char at position i of the array

Am I missing something and is the runtime of this looping and saving chars not on par with the recursive method (that being around O(mn))?  1) Recursion is never necessary. Anything you can do with recursion you can do with iteration. It just makes it more intuitive to read and understand.

2) I don't understand what you mean about post processing. Can you explain further? 1) cool. that makes sense

2) the text gives a good example of how they find the sequence...nearly identical to this: http://wordaligned.org/articles/longest-common-subsequence

where a matrix is constructed with sequence1 on the x axis and sequence2 on the y. Comparisons are made top-down by numbering common elements. Then one can follow the numbers bottom-up and the diagonal jumps (marked with the orange arrows in the picture) indicate common elements

There's a lot of detail on how to extract an LCS from the grid...i follow what's going on (implementation would be another story) but it seems like so much . . .

I dont understand why the array itself, through first iteration, cant just store each char that they have in common. Seems like a much simpler implementation to me that wont cost extra time.  1) cool. that makes sense

2) the text gives a good example of how they find the sequence...nearly identical to this: http://wordaligned.org/articles/longest-common-subsequence

where a matrix is constructed with sequence1 on the x axis and sequence2 on the y. Comparisons are made top-down by numbering common elements. Then one can follow the numbers bottom-up and the diagonal jumps (marked with the orange arrows in the picture) indicate common elements

There's a lot of detail on how to extract an LCS from the grid...i follow what's going on (implementation would be another story) but it seems like so much . . .

I dont understand why the array itself, through first iteration, cant just store each char that they have in common. Seems like a much simpler implementation to me that wont cost extra time.  1) Recursion is never necessary. Anything you can do with recursion you can do with iteration. It just makes it more intuitive to read and understand.

2) I don't understand what you mean about post processing. Can you explain further? 1) Recursion is never necessary. Anything you can do with recursion you can do with iteration. It just makes it more intuitive to read and understand.

2) I don't understand what you mean about post processing. Can you explain further? If your function is not primitive recursive, doing it with loops will be quite difficult. 1) Recursion is never necessary. Anything you can do with recursion you can do with iteration. It just makes it more intuitive to read and understand.

2) I don't understand what you mean about post processing. Can you explain further?  Recursive versions of algorithm sometimes are easier to read, or make more sense for certain tasks. In particular LCS is often explained as a [dynamic programming](http://en.wikipedia.org/wiki/Dynamic_programming) algorithm - which really means that you can solve the whole problem by solving several sub-problems. Furthermore, it's possible to sub-divide the smaller problems (etc) until you get to trivial ones.

This is clearer in the recursive version, which really says something like "the LCS of two sequences [i, j] is the maximum of the LCS of sub-sequences [i - 1, j] and [i, j - 1]. The reason for the post-processing step is that the recursive algorithm is just calculating the lengths of all sub-sequences ([see this part of the wiki](http://en.wikipedia.org/wiki/Longest_common_subsequence_problem#Computing_the_length_of_the_LCS) for details).

Remember that two sequences can have more than one common sub-sequence,  and it is even possible to have two sub-sequences of equal length. Therefore it may be necessary to list them all.  I'm in your class at GT. :)   Like themandotcom said, recursion is never necessary.

If you are interested, you can read the paper on LCS I cobbled together.
http://scottjulian.net/lcs/  1) There is no need for recursion.  Read LCS(i,j) = LCS(i-1, j-1) + 1 as an assignment to the 2-dimensional array LCS.

I don't think your version (with the 1-dimensional array) works, though.  In particular, it's not clear to me where you intend to store the answer.  Every index in array is either uninitialized or 1.

2) You can avoid the postprocessing step, but you need to keep track of the longest common subsequence of x[0...i] and y[0..j] for each i,j.  If you can do (non-destructive) sequence append &amp;amp; length in constant time and space, then that's fine.  Otherwise (e.g., if you implement sequences with arrays), you've turned an O(n^2) algorithm into an O(n^3) algorithm.    Maybe a noob question, but what is this algorithm used for? Does it have a specific application? It seems that a pretty good application of this is in bioinformatics. For example trying to find the similarity between two DNA sequences maybe taken from different species. And then perhaps one is trying to find a common ancestor or something like that. 

On another level, maybe one was interested in targeting a certain gene for whatever reason (tagging it, blocking it, something like that) and maybe you could use something like this to find variants that were in the ballpark of around 90% sequence similarity. Well, it might make the chemistry easier and more cost efficient to synthesize a strand given that 10% error.

I think that this is also the rudiments of finding text in a document (although that's more of a pattern match) or really reaching out there, search engines... although I'm sure that they use much more advanced algorithms along with data structures in place. Idk, I'm kinda making stuff up at this point. Maybe a noob question, but what is this algorithm used for? Does it have a specific application? It's one of the "classic" algorithms - probably too simplistic to be of any real world use nowadays. However, it is one of the simple examples of the "Dynamic Programming" approach to Algorithm Design. Understanding how LCS works is a good way for noobs to  try to wrap their heads around DP.</snippet></document><document><title>Finding all partitions of a given size with a given sum.</title><url>http://www.reddit.com/r/compsci/comments/12jbbt/finding_all_partitions_of_a_given_size_with_a/</url><snippet>If I have a set of integers from 1 to m\*n, how would I iterate through all ways of dividing them up into m sets of n? Each set must sum to the same number: n(m\*n + 1)/2. Order within the set doesn't matter, nor does the order of the sets. So for example, m = 3, n = 3 should give me {{1, 5, 9}, {2, 6, 7}, {3, 4, 8}} and {{1, 6, 8}, {2, 4, 9}, {3, 5, 7}}.

I've tried recursively iterating through n-sized subsets of the 'leftover' numbers, but I end up with many duplicates, where the same group with a different order of sets is counted many times. I also end up wasting time on groups where the very first set doesn't pass the sum test, but I descend into the recursive part anyways. What's a more efficient way of doing this?   So, I forgot that this is compsci and not /r/badcode, so [I wrote you a solution in python](http://ideone.com/3TGARU).

(It's only one statement, so it must be fast! /s)

(More [readable](http://ideone.com/YbtvCo) version.  Same bad runtime!)
 oh christ. this is wonderful, I'd love to see an /r/codegolf some time in the future for code like this  Another algorithm in Python http://ideone.com/ktsmlj

First all n-size subsets with desired total (n(m*n + 1)/2) are generated. This is done by starting from empty set and successively adding new numbers to it. There is only a simple pruning based on subset size and total. 

Once list of subsets is generated, the problem is reduced to finding all exact covers of integers from 1 to m*n. This is done by selecting some subset with index i which contains only numbers which are not already covered, and then trying to solve cover problem for remaining numbers, but using subsets with index higher than i (this solves problem of duplicated solutions). 

I wonder, for how big n and m are you trying to solve this problem? Up to 6, for both of them, so storing all of them would be difficult. I'm testing each cover for other properties (thankfully I know how to do that part), and only saving them if it passes that test. It seems like I'd have to generate all of the subsets with that way, and then weed out the working ones. I don't know the technical term (i think it's "stateless"?), but is there a way to go from one cover to the "next" one? Like, if I were just doing one set and no sum constraint, a method that advances {1, 3, 6} to {1, 3, 7}. I think I'd have to define some sort of ordering on the covers? There are 1947792 six-elements subsets of numbers from 1 to 36. Assuming that above program is working correctly, 32134 of them sum to desired number. And it took roughly 16 second to generate all subsets, so in fact it is quite feasile to store all of them in memory.

Though, after running this for n=m=6, one thing is clear, there are large number of solutions. You could look at some existing algorithms for exact cover - for example http://en.wikipedia.org/wiki/Algorithm_X   This is called the sum subset problem, which is NP Complete. You get $1Million if you find an efficient solution.  But I know how large these subsets must be, plus I'm drawing from a very non-generic pool. Doesn't that change the problem significantly? But I know how large these subsets must be, plus I'm drawing from a very non-generic pool. Doesn't that change the problem significantly? This is called the sum subset problem, which is NP Complete. You get $1Million if you find an efficient solution.  Yes, but there *are* listing algorithms for such problems, even if they are not efficient.</snippet></document><document><title>Zero Robotics</title><url>http://www.reddit.com/r/compsci/comments/12jbas/zero_robotics/</url><snippet>I'm curious - is anybody here a part of NASA/MIT's Zero Robotics? Alliance stage is starting soon (really soon) and I'm wondering what our competition or soon-to-be allies may be.

-crockrovers

www.zerorobotics.org if anyone is wondering what I'm talking about. It's a programming competition for who can build the best AI in a virtual environment, simply put, for only high schools. Registration is already closed.</snippet></document><document><title>Connectionless vs Connection-Oriented Communications</title><url>http://www.reddit.com/r/compsci/comments/12irag/connectionless_vs_connectionoriented/</url><snippet>So, I can *tell* you the difference between connectionless and connection-oriented communications, but I can't visualize the difference, and I couldn't design a system using one or the other.

So far, I have it as such:

Connectionless communications involve sending packets **toward** a destination rather than **directly to** a destination. For instance, a router P has a packet for router Q, and knows to send a packet through port 8001 to Q, but doesn't know or care about anything beyond that.

Whereas in connection-oriented communications there's a definite connection established between P and Q. P wants to send the packet to Q and simply does so. Q receives the data, does some error checking and acknowledges the reception of the packet to P.

That's basically where I'm at, but I can't *see* the difference between the two. Google's helped me get this far, but there's only so much I can do with static explanations. Can somebody here help me understand the difference?

EDIT: I realized that a better question is "How does connection-oriented communication work?"  &amp;gt;Connectionless communications involve sending packets toward a destination rather than directly to a destination. 

I think this is a mischaracterization of what is actually going on. Both types of communication are sending packets directly to a destination. The difference lies in the fact that the sender waits for the recipient to acknowledge the start of the communication before sending data.

Connection-oriented protocols work like this: when a sender wants to begin a conversation with a recipient, the sender first sends a "hello" packet, called a SYN, to the recipient. The recipient acknowledges receipt of the SYN by sending a "hello" response packet back, called a SYN-ACK (standing for SYNchronize ACKnowledge). Then, the sender, upon receipt of the SYN-ACK, sends an ACK, acknowledge packet to the recipient, establishing the beginning of the conversation.

Connection-oriented protocols specify certain measures for things like error correction, packet ordering, and flow control, which connectionless protocols do not. Think of it this way, in a connection oriented communication, the sender is actually listening to the recipient, making sure that the recipient acknowledges receipt of data, and tells the recipient the proper way to order data packets. Connectionless protocols basically constitute a sender just throwing data at a recipient - there's no checking to make sure the recipient is receiving data in the proper order, uncorrupted, or if it's even receiving the data packets at all.

Visualize it like two people on the phone. Sam (the sender) is making a call to his coworker, Reese (the recipient). In a connectionless conversation, the call would go a little like this.


**Sam**: "Reese, I need you to come to my office ASAP." * click * (hangs up the phone without a word of acknowledgement from Reese)

On the other hand, a connection-oriented conversation would be more like:

**Sam**: Hi, Reese? (SYN)

**Reese**: Yes this is Reese, hi Sam. (SYN-ACK)

**Sam**: Hi, glad I got you on the phone (ACK). I need you to come to my office ASAP.

There's also a similar protocol for ending the conversation in a connection-oriented protocol. But really, I think that basically explains it.  Check out [this](http://en.wikipedia.org/wiki/Transmission_Control_Protocol#Connection_establishment) for a better, more thorough, explanation. So basically the difference between the two is the presence of acknowledgements and such between the end points. I asked my Computer Networks professor the same question earlier today and that's how he described it too. Correct. In a connectionless communication, the sender has no way of knowing that the recipient actually received the message.  It might seem stupid to have connectionless protocols when we can use connection-oriented protocols, like TCP, which can enable resending of dropped packets and other services, however, there are clear benefits of connectionless protocols like UDP.  UDP and other connectionless protocols, since they don't have to take time and resources to make sure the data got there, can just focus on sending the data quickly, which is why UDP and similar protocols are well suited to streaming data, like video and music.  It doesn't matter if a few packets are dropped when streaming video or audio - a couple pixels might be wrong, or maybe a part of the waveform is messed up, but it will still most likely be basically right. If you use a connection-oriented protocol, it's difficult to stream content because it has to make sure that everything is in the proper order, nothing's missing ,etc. So basically the difference between the two is the presence of acknowledgements and such between the end points. I asked my Computer Networks professor the same question earlier today and that's how he described it too.  Imagine a room full of people and you are just next to person A.

Now imagine you want to pass your pen to person B who is at the other extreme of the room.

So you say to person A, look I give you my pen, I want it to reach person B.

So person A does similarly with someone, let's call this other person C, who is nearby but closer to B than you are. He passes along with your pen an indication to C that you want your pen to reach B so when C decides what to do he choses a D person who is closer to B than you or him are.

And so on... I think this is an example of connectionless...

I did not come up with this but saw it some time ago exemplified in an internet video with a real room full or real people... Well that's definitely connectionless, yeah.

I think my real question should be: "How does connection-oriented communication work?", rather than "what is the difference?", since connectionless is pretty easy to visualize. I'm mostly confused about the low-level "how" of connection-based communications. Imagine a room full of people and you are just next to person A.

Now imagine you want to pass your pen to person B who is at the other extreme of the room.

So you say to person A, look I give you my pen, I want it to reach person B.

So person A does similarly with someone, let's call this other person C, who is nearby but closer to B than you are. He passes along with your pen an indication to C that you want your pen to reach B so when C decides what to do he choses a D person who is closer to B than you or him are.

And so on... I think this is an example of connectionless...

I did not come up with this but saw it some time ago exemplified in an internet video with a real room full or real people...</snippet></document><document><title>50 Places You Can Learn to Code (for Free) Online - Online College Courses</title><url>http://www.onlinecollegecourses.com/2012/08/06/50-places-you-can-learn-to-code-for-free-online/</url><snippet>  /r/CarlHProgramming  
http://www.computerscienceforeveryone.com/  
  
Made by a Redditor.  &amp;gt; Stack Overflow - Like Stack Exchange, Stack Overflow is a Q&amp;amp;A site, but this one is all about language-independent programming questions.

No, that's false.  Stack Overflow is a Stack Exchange site.  It is *not* all about language-independent programming questions.  The vast majority of questions there are tagged with a specific programming language.

 Also, wasn't Stack Overflow the first of the Stack Exchange sites?     I would add edX to that list, check it out :)     </snippet></document><document><title>Finding the probability that a Erd&#337;s&#8211;R&#233;nyi graph G contains a certain number 'x' images of graph H.</title><url>http://www.reddit.com/r/compsci/comments/12i82u/finding_the_probability_that_a_erd&#337;sr&#233;nyi_graph_g/</url><snippet>I'm trying to find an algorithm or formula to calculate for a certain graph H, the probability that is has a certain number of images x in an Erd&#337;s&#8211;R&#233;nyi graph of size n.

An [Erd&#337;s&#8211;R&#233;nyi](http://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model) graph of size n is a graph where the probability that there is an edge between 2 nodes is p.

I'm trying to do this empirically and hoping to find a general algorithm or formule.
I found a general formula for the simple problem if H is a graph with just one edge:

    P(Image(H,G) = 2*x) = C((e,x) * p^x * (1-p)^(e-x)

with e = ((n(n-1))/2) the number of edges,

and C(n,k) the [binomial coefficient](http://en.wikipedia.org/wiki/Binomial_coefficient).

I'm trying to extend this to the case where H is a triangle, and to the case where H is a path of 2 edges, but I can't seem to find a pattern.

Does anyone know of such a formula or algorithm? Or can push me in the right way? Or knows some papers that might be relevant? All help would be welcome.  Let me elaborate a bit:
Define a random variable X_uv for every possible edge (u,v) in the random graph. X_uv is one with probability p and zero with probablitiy (1-p). We then define a random variable T denoting the number of triangles in the graph.
We can express T as follows via the edge variables:

T = \sum_{1&amp;lt;=u&amp;lt;v&amp;lt;w&amp;lt;=n} X_uv X_vw X_wu

In an Erd&#337;s&#8211;R&#233;nyi graph, the edges are chosen independently. Therefore, the expected value is simply

E[T] = \sum{1&amp;lt;=u&amp;lt;v&amp;lt;w&amp;lt;=n} E[X_uv X_vw X_wu] =    
           \sum{1&amp;lt;=u&amp;lt;v&amp;lt;w&amp;lt;=n} E[X_uv] E[X_vw] E[X_wu] 
        = (n choose 3) p3

Now, calculate T2, E[T2] and then the variance Var[T] = E[T2] - E[T]2 (sorry, too lazy).

Afterwards, apply Chebychev's inequality (or something more tight, Chernoff bounds may be applicable, my probablitiy theory is rather rusty) to estimate the probability for more than E[T] many triangles. Thanks, but my knowledge of probability theory is apparently not enough to solve this problem :(

Can you elaborate a bit more please?

What do you mean by "\sum_{1&amp;lt;=u&amp;lt;v&amp;lt;w&amp;lt;=n}"?

What does variable T2 represent?

 Hey, the sum just goes over all triples (u,v,w) in the graph with u != v != w (not repeating itself, thus the u &amp;lt; v &amp;lt; w). 

T2 should actually be T^2, the formatting is botched. You simply have to square the above expression (the sum) and simplify it to something usable. So with sum you select the number of possible ways a triangle can be formed in the graph?
En because in an erdos-renyi graph, each vertex can be connected to every other vertex. This equals to the number of times you can select 3 separate vertices from the total=n vertices?

Thanks!  Am I misunderstanding, or do you want an actual implementation of a subgraph matching algorithm? There is a lot of research in this area, particularly that called "frequent subgraph mining" and similar. I'm not searching for a subgraph matching algorithm that works for every graph.
I'm trying to predict what the chance is to have a certain number of subgraphs of H in G, with G a random Erd&#337;s&#8211;R&#233;nyi graph of size n.  How about calculating the expected number of occurences and then using something like chernoff bounds to estimate the probability? </snippet></document><document><title>Big Data: crude oil of our time | COSMOS magazine</title><url>http://www.cosmosmagazine.com/blog/6132/information-overload#.UJNkB5_o15Q.reddit</url><snippet> </snippet></document><document><title>Noam Chomsky on Where Artificial Intelligence Went Wrong</title><url>http://www.theatlantic.com/technology/archive/2012/11/noam-chomsky-on-where-artificial-intelligence-went-wrong/261637/</url><snippet>   Noam has this amazing ability to say a lot of different things without making a general point or giving any decent suggestions. 

As a biologist, I am keenly aware of the limitations of AI algorithms. At the same time compsci is the only way to sift through so much of the data being generated. He mentions developmental pattern in humans and nematodes. This process is very complex, but large throughput data and some intelligent algorithms are able to tease out some interesting relationships.  &amp;gt;This process is very complex, but large throughput data and some intelligent algorithms are able to tease out some interesting relationships.

He's not saying that  they won't, what he's saying is that the current approaches are unlikely to result in actual AI. You seem to be thinking in terms of usefulness of these algorithms for data analysis, which they indeed are good at. He is talking about understanding what human style cognition is and coming up with a theory of the mind. Which is not what modern AI is about (at least according to "AI: A modern approach"). He seems to be talking about making machines "think humanly", while Russell and Norvig focus on "acting rationally". The "thinking humanly" aspect seems to be the domain of cognitive science, right? It's a tautology that people who think AI is simply stats would define it as such, no? What Chomsky is talking about is having a model for human cognition and building systems using that model. 

It's not about thinking rationally or humanely, it's about computing the way the brain computes. We know that for certain tasks the brain model is far more effective than the von Nemuman architecture. To build machines which we would consider intelligent we'd have to understand how this model works. What Norvig and Russell are focusing on is short term pragmatic engineering. I personally take the opposite view.  AI is for people with great ambition who get nothing done.  

Machine learning, on the other hand, its for people who are only somewhat ambitious and want to solve a single problem at a time, but it provides an effective way to solve real problems.

If you ever meet someone who loves lisp and talks about agents, run away.  But if you find someone who loves data and talks about held out error rates, you are in business.

  </snippet></document><document><title>Noam Chomsky on forgotten methodologies in artificial intelligence.</title><url>http://www.youtube.com/watch?v=yyTx6a7VBjg</url><snippet /></document><document><title>Is the log-sum-exp trick a valid substitute over scaling when trying to avoid numerical underflow in HMM calculations?</title><url>http://www.reddit.com/r/compsci/comments/12hfrn/is_the_logsumexp_trick_a_valid_substitute_over/</url><snippet>[Rabiner](http://www.cs.cornell.edu/Courses/cs4758/2012sp/materials/hmm_paper_rabiner.pdf) describes a way to use dynamic scaling to avoid underflow when finding the forward/backward likelihoods in a HMM, but I'd rather use the simpler log-sum-exp trick -- if it's as accurate.

Is this the case? I'm implementing    hidden semi-Markov training, and I'd rather avoid deriving a different scaling method for that.  Well, I'd think it depends on where you probabilities fall, but I imagine in most cases you're unlikely to lose much precision using the logs.  I'd try to run through a relevant example and see the range of the logL.  If it's extremely far from 0, you might lose some precision.

You'd really want to get a computational stats/math opinion, though. Unfortunately I don't have any relevant examples, so verifying things is a little tricky. I wouldn't worry too much about loss of precision in log-representation. Not sure if I wanna just assume that though :)

If scaling is significantly better, then it'd be preferable.   You should post this on the ML subreddit.   Not sure if this is relevant, but there is a scaling for HMMs that maintains precision.  I don't remember the details, but you can find it in Bishop's Pattern Recognition. That's the same scaling I'm talking about actually.</snippet></document><document><title>Question about Iterative Depth First Search</title><url>http://www.reddit.com/r/compsci/comments/12fvjp/question_about_iterative_depth_first_search/</url><snippet>I was playing around with an iterative DFS and while I thought I understood the basics, I came across a test case I can't seem to make sense of.

Here's my code: http://pastie.org/private/eoopyh7wyocwg8l40rj3vq
and the test case: http://pastie.org/private/kjve0zzglqf3lvmokaug2a
and an image of the graph: http://imgur.com/rIWUf

As of now, when the test case is inputted into the program, it ouputs "1 2 4 6 3 5". This seemed odd to me as it doesn't seem like it's going deep into the graph, or performing a DFS at all.

After playing around a bit I was able to get the output "1 2 3 5 4 6" which makes more sense. I did this by adding the neighbors of a node onto the stack in reverse order (line 41) and printing the node after popping it from the stack rather then where I currently print it.

My questions: why does the order in which I push neighbors on to the stack matter? I don't think I've ever seen any mention of this in any textbooks.

When is the right time to print the node (to show the traversal order)? I figured it should be right when I mark the node as visited, but that doesn't seem to be the case.

thanks  You silly head :) you're outputting nodes inside your edge relaxation loop. In other words, the nodes you're outputting do not correspond to the nodes being processed by the DFS in the traditional definition of 'process'.

What was happening is that first, you printed 1. Then you explore 1's neighbours in the order, 2, 4, 6, and outputted them in that order (note that this is DISTINCT from 'processing' 2,4,6 -- this is merely 'touching' them). When you explored 2, you pushed 2's children (3,5) onto the stack, and then you printed those. This accounts for the order '1 2 4 6 3 5'.

Remove both existing cout statements and put in a single one just after line 39 which says cout &amp;lt;&amp;lt; v &amp;lt;&amp;lt; " "; This should fix it. I see; I've changed it to only output on line 39 (http://pastie.org/private/obgd68xtv1n4blm88u6hpq), but now it produces the order "1 6 4 2 5 3", which still doesn't seem right to me. It seems like it should go from 6 to 2, rather than 6 to 4, regardless of the order that I visit the neighboring nodes.

I understand why this is happening (order I'm pushing and popping off stack), but doesn't this mean that my for loop is incorrect?

Also, should I printing the node on line 39 to mark it as part of my DFS path, or should it technically be after the for loop? (after I've pushed all its neighbors onto the stack). I understand the code would produce the exact same output, I'm just wondering when exactly you can say a node has been visited (processed?). </snippet></document><document><title>Formalising Flow Typing with Union, Intersection and Negation Types</title><url>http://whiley.org/2012/10/31/formalising-flow-typing-with-union-intersection-and-negation-types/</url><snippet /></document><document><title>The affinity between compsci, analytic philosophy and logic</title><url>http://www.reddit.com/r/compsci/comments/12ci5y/the_affinity_between_compsci_analytic_philosophy/</url><snippet>Hello all,

I'm a student of philosophy specialising in analytic philosophy - especially logic and ontology. Recently, I've been looking into the overlaps between philosophy (of the analytic kind, concerned with logic and math) and computer science, which I have always found to be interesting but daunting. I love logic, and have an affinity for math as well. I have worked with, among other stuff, developing my own systems of modal logic for different concepts. So I guess my basic questions are: Is hardcore theoretical logic of use to computer science, and what are some ways to get to use my skills in logic within computer science? In other words - where do I start? As I said, I'm more interested in the theoretical aspects than in programming. Is it worth looking into computer science from such a viewpoint at all?

Thanks for reading.  Since every logic gives rise to a type system (i.e. [Curry-Howard isomorphism](http://en.wikipedia.org/wiki/Curry-Howard_isomorphism)) I'd suggest type theory. [Type Theory and Functional Programming](http://www.cs.kent.ac.uk/people/staff/sjt/TTFP/)  although the link appears to be broken at the moment.  There is http://www.cafepress.com/haskell_books.11895795 if you have $20    I work in Natural Language Processing, specifically on Computational Semantics, that is, how to make machines understand natural language. In order to a machine to do that, you have to come up with a formal representation of knowledge and how it is expressed in language. Knowledge Representation has been a great deal among the Artificial Intelligence community for the past 50 years, and it turns out (computational) ontologies are a great tool in that respect.

I think your passion and skill with logic (which I personally share :) ) can be of good use at least on two levels, in Computer Science. On one hand, the art of computer programming is based on formal languages, which are an application of logic (a lot of theorem proving is going on in theoretical CS). On the other hand, all the effort in the field of AI, which is applied CS, is based on the work of logicians and employ formal methods on a daily basis.

Did I mention I'm a computer scientist that works in an Arts faculty? As ilikecomputahs correctly noted, we use Prolog extensively, but there are also a lot of hacking going on. What can I say, it's a lot of fun :)

PM me for more pointers, I know of a couple of Summer Schools you might be interested in. Not the OP but what you do sounds very interesting. Like the OP I have a strong interest in analytic philosophy, although I'm more interested in semantics than things like ontology. I'm currently pursuing a BS in Cognitive Science with a minor in Computer Science, but also have completed significant coursework in philosophy and linguistics. I have no idea where I want to take this, but your post made me really curious in what you do. Could you possibly elaborate a bit more about what you do and/or send me those names of summer schools to look up?  Sure thing. We built a corpus of English texts aligned with their corresponding semantic representations. These representations (called Discourse Representation Structures) are formal structures which have direct translations into first order logic. This means you can possibly draw inferences, which in turn is key to all the study in Artificial Intelligence.

Of course there is a whole array of problems and issues when you try to infer such structures from real world texts (in our case they are mostly newspaper articles), and things are far to be perfect. But that's it, so far this is the state of the art when it comes to large collections of semantically annotated texts. You can find the resource (and a lot of additional information) here: [Groningen Meaning Bank](http://gmb.let.rug.nl/).

We had a BS student visiting the past semester and it was great (for us, I hope for him as well), so feel free to keep in touch.

The two summer schools I was mentioning are 1) the LOT summer school, mostly a Dutch thing. The next edition doesn't have a website yet, but you can find informations about the previous editions [here](http://www.lotschool.nl/index.php?p=10); 2) [European Summer School in Language, Logic and Information](http://esslli2013.de/), which in my experience is a lot of fun and a lot of connections and high level education. Thank you so much! I'm located stateside so both those programs might be a bit too expensive for me, but there definitely seems to be a lot of good information here. Thanks again for the reply, your work sounds really interesting. Sure thing. We built a corpus of English texts aligned with their corresponding semantic representations. These representations (called Discourse Representation Structures) are formal structures which have direct translations into first order logic. This means you can possibly draw inferences, which in turn is key to all the study in Artificial Intelligence.

Of course there is a whole array of problems and issues when you try to infer such structures from real world texts (in our case they are mostly newspaper articles), and things are far to be perfect. But that's it, so far this is the state of the art when it comes to large collections of semantically annotated texts. You can find the resource (and a lot of additional information) here: [Groningen Meaning Bank](http://gmb.let.rug.nl/).

We had a BS student visiting the past semester and it was great (for us, I hope for him as well), so feel free to keep in touch.

The two summer schools I was mentioning are 1) the LOT summer school, mostly a Dutch thing. The next edition doesn't have a website yet, but you can find informations about the previous editions [here](http://www.lotschool.nl/index.php?p=10); 2) [European Summer School in Language, Logic and Information](http://esslli2013.de/), which in my experience is a lot of fun and a lot of connections and high level education.    Wow, for the first time in the history of Reddit I feel kind of useful. 
I have a PhD in logic programming, more specifically in Inductive Logic Programming.
The short answer is hardcore theoretical logic is extremely useful in computer science, although in my opinion, the interaction between the two fields is still in its early stages.

If you like I can elaborate more when I'm off work. &amp;gt;although in my opinion, the interaction between the two fields is still in its early stages.

I disagree with this statement. Turing's original work in Computability theory laid the foundation for modern day computing. The foundations of computer science are intrinsically tied to Mathematical Logic.       You might be interested in a language like [Prolog](http://en.wikipedia.org/wiki/Prolog) Prolog is almost entirely uninteresting from a logician's perspective.  However, he may indeed be interested in [&#955;Prolog](http://en.wikipedia.org/wiki/%CE%9BProlog) or [Coq](http://en.wikipedia.org/wiki/Coq) or [Twelf](http://en.wikipedia.org/wiki/Twelf). You might be interested in a language like [Prolog](http://en.wikipedia.org/wiki/Prolog) You might be interested in a language like [Prolog](http://en.wikipedia.org/wiki/Prolog)</snippet></document><document><title>Salty subject: Computer modeling points to pores in graphene for saltwater conversion</title><url>http://ascr-discovery.science.doe.gov/universities/grossman1.shtml</url><snippet>  </snippet></document><document><title>Computational Complexity: Song of the Complexity Classes</title><url>http://blog.computationalcomplexity.org/2012/10/song-of-complexity-classes.html</url><snippet /></document><document><title>Killing the computer in order to save it.</title><url>http://www.nytimes.com/2012/10/30/science/rethinking-the-computer-at-80.html</url><snippet>   &amp;gt;&#8220;Nature abhors monocultures, and that&#8217;s exactly what we have in the computer world today,&#8221; said Dr. Shrobe. &#8220;Eighty percent are running the same operating system.&#8221; 

I'm not sure I see the point of this statement.  Seems like it's advocating for the world we had before robust standards -- trying to interface 30 different systems types running 150 different protocols seems like a nightmare I'd just as soon avoid.

Any SOP will generate something that is, trivially, a monoculture, but the problems with monocultures in nature stem from a lack of short-term flexibility; an organism with a particular genetic flaw that is being exploited cannot quickly and easily change its genetic code.  While computers and systems don't necessarily have that problem -- especially if they are adaptive, as Dr. Neumann envisions.  In other words, a robust and adaptive monoculture is the _only_ way an Internet can work. </snippet></document><document><title>The Physically Unclonable Functions Found in standard PC Components Project, or PUFFIN, discovered that every graphics processing unit has a unique and defining set of characteristics. That means it might be useful for security authentication.</title><url>http://h30565.www3.hp.com/t5/Feature-Articles/Your-GPU-s-Fingerprint-Could-Lead-to-New-Security-Methods/ba-p/8418</url><snippet>  If this is actually based on the physical differences from manufacturing, I would also expect it to be temperature dependent. It would be interesting to see if, for example, some people stop being able to log in on hot days. The uniqueness comes from the default state of the GPU's memory. Apparently subtle differences in the flip-flops cause every ram cell to have a default value that they settle on before they are initialized and used for normal computations. It's SRAM so this isn't the frame buffer memory, but it's definitely enough memory to have some very beefy keys. The uniqueness comes from the default state of the GPU's memory. Apparently subtle differences in the flip-flops cause every ram cell to have a default value that they settle on before they are initialized and used for normal computations. It's SRAM so this isn't the frame buffer memory, but it's definitely enough memory to have some very beefy keys. Expect GPU manifacturer to add zero-out-on-boot circuitry/software to thwart this kind of stuff.
 The had to join some kind of secret society to even get access to this part of a GPU. Also why would the GPU makers try and stop this? they wont stop it, just disable it on non quadro and similar variants The had to join some kind of secret society to even get access to this part of a GPU. Also why would the GPU makers try and stop this? The GPU manufacturers are not in the business of providing security, especially when that security hinges on some hardware minutiae that they *by definition* don't have control over.  What's going to happen when they switch manufacturing processes and it skews the default state distribution?  They have to preserve this unintentional hardware feature for a very long time for no real benefit to themselves and a large detriment so why on Earth would they do it?

And the "secret society" is probably just running at kernel-level instead of userland.  I doubt the manufacturers actually helped lest they encourage people to rely on unspecified behavior that's subject to change without notice or warning.  They totally fail to explain how that would solve the problem that if the device breaks, *"poof! goes your game access."* As we all know, of course, GPUs never break, especially not when properly educated personell is handling them withhin the specified parameters of operations, like gamers typically would.

The whole thing seems pretty dumb to me in the first place, TBH, depending on device-specific undefined behaviour which might vary with temperature, age and possibly other factors -- you might as well use one or more of these more sensible options to the same effect:

* Use a unique ID stored somewhere in the hardwares flash/eprom/eeprom/other type of ROM. If I can steal your EEPROM chip, I can steal your graphics card.
* Use a private-key read from the harddisk (because if the game can access this data from your GPU, so can other programs, and if the game can access it, a hypervisor can also fake it, so why bother doing it in hardware in the first place -- just read it from a file.) I think you're focusing too much on the GPU aspect/gaming aspect.

The significance of this discovery is of more general cryptographic applicability than just GPUs.  Using a unique ID stored in ROM is trivial to duplicate and is not cryptographically secure.  Same problem with the private-key from a hard disk.

The idea isn't that accessing this function is supposed to be difficult, on the contrary the function on the GPU should be as easily and freely accessible as possible.  The problem is that the function can not be duplicated, basically there would be no practical way to 'dump' the function from the physical hardware onto a hard drive.  The domain and range of the function is so unimaginably vast that it would be futile to try and make a lookup table mapping inputs to outputs.  Your comment that a hypervisor can fake it is not only false, it's the entire distinguishing feature of an uncloneable function, that you can not fake it.

Wikipedia provides a good overview of the significance of physically unclonable functions.  Their applicability should not be thought of as restricted to game access or even GPUs.  It's just an interesting and fairly relevant discovery that such an uncloneable function exists already in all of our computers without the need to explicitly introduce or even manufacture another piece of hardware for this purpose. I think you're focusing too much on the GPU aspect/gaming aspect.

The significance of this discovery is of more general cryptographic applicability than just GPUs.  Using a unique ID stored in ROM is trivial to duplicate and is not cryptographically secure.  Same problem with the private-key from a hard disk.

The idea isn't that accessing this function is supposed to be difficult, on the contrary the function on the GPU should be as easily and freely accessible as possible.  The problem is that the function can not be duplicated, basically there would be no practical way to 'dump' the function from the physical hardware onto a hard drive.  The domain and range of the function is so unimaginably vast that it would be futile to try and make a lookup table mapping inputs to outputs.  Your comment that a hypervisor can fake it is not only false, it's the entire distinguishing feature of an uncloneable function, that you can not fake it.

Wikipedia provides a good overview of the significance of physically unclonable functions.  Their applicability should not be thought of as restricted to game access or even GPUs.  It's just an interesting and fairly relevant discovery that such an uncloneable function exists already in all of our computers without the need to explicitly introduce or even manufacture another piece of hardware for this purpose. They totally fail to explain how that would solve the problem that if the device breaks, *"poof! goes your game access."* As we all know, of course, GPUs never break, especially not when properly educated personell is handling them withhin the specified parameters of operations, like gamers typically would.

The whole thing seems pretty dumb to me in the first place, TBH, depending on device-specific undefined behaviour which might vary with temperature, age and possibly other factors -- you might as well use one or more of these more sensible options to the same effect:

* Use a unique ID stored somewhere in the hardwares flash/eprom/eeprom/other type of ROM. If I can steal your EEPROM chip, I can steal your graphics card.
* Use a private-key read from the harddisk (because if the game can access this data from your GPU, so can other programs, and if the game can access it, a hypervisor can also fake it, so why bother doing it in hardware in the first place -- just read it from a file.)  I don't understand this "PUF" concept. Even if I can't literally clone somebody's hardware, assuming I know enough about the device, can't I just rig my computer to simulate it? I don't understand this "PUF" concept. Even if I can't literally clone somebody's hardware, assuming I know enough about the device, can't I just rig my computer to simulate it? How would you do that?  Imagine that the input to the function is some 2 kilobyte stream of data that returns a 2 kilobyte output.

The PUF is basically a function that exploits some physically random and intractable property of a device meaning there is nothing to really understand about it.  Are you going to store every combination of input output?  Remember the output is random so it's not like you can compress this.

With 2^2048 inputs mapped onto random outputs, that's literally more possible inputs than there are atoms in the universe.  Where would you store such a table? How random are we talking though? I'd be dubious enough about a function that had been carefully constructed by cryptographers, so to rely on a completely unknown function seems like not such a great idea... I'd expect there to be very exploitable correlations between different hardware, and between the values returned by one unit. How would you do that?  Imagine that the input to the function is some 2 kilobyte stream of data that returns a 2 kilobyte output.

The PUF is basically a function that exploits some physically random and intractable property of a device meaning there is nothing to really understand about it.  Are you going to store every combination of input output?  Remember the output is random so it's not like you can compress this.

With 2^2048 inputs mapped onto random outputs, that's literally more possible inputs than there are atoms in the universe.  Where would you store such a table? &amp;gt; The physical layout of SRAM cells is such that each of them falls to a 0 or 1 when unpowered, Dr. Lange explained. The choice depends on tiny manufacturing differences. When the SRAM is powered on, these values stay until drivers overwrite them with data.  
"Like fingerprints, the behavior of falling to 0 or to 1 is not perfectly deterministic, but we know how to deal with noisy data. It was known already that in general SRAM can be used to build PUFs," she said.  
What this means is the 0s and 1s of SRAM have a unique arrangement to each GPU &#8211; which enables your GPU to become your authenticator.

This gives me no reason to believe that copying the default state of a GPU's memory (1-2 GB) isn't enough to simulate having the physical card. How would you do that?  Imagine that the input to the function is some 2 kilobyte stream of data that returns a 2 kilobyte output.

The PUF is basically a function that exploits some physically random and intractable property of a device meaning there is nothing to really understand about it.  Are you going to store every combination of input output?  Remember the output is random so it's not like you can compress this.

With 2^2048 inputs mapped onto random outputs, that's literally more possible inputs than there are atoms in the universe.  Where would you store such a table?  Not sure I get this. It seems like it's pretty easily circumventable.

Malicious program gets default state of GPU memory.
Compute whatever is needed by the authenticator.
Run target program in a debugger, when asked for whatever, put what was precalculated in the result.
Profit.  </snippet></document><document><title>I found these free resources...</title><url>http://www.freetechbooks.com/</url><snippet>   I don't understand how someone can downvote this. Good job OP. Thanks! It's possible most of the downvotes are just reddits non disclosed vote balancing algorithm. </snippet></document><document><title>Reactive Demand Programming</title><url>https://github.com/dmbarbour/Sirea/blob/master/README.md</url><snippet>  Summary please? </snippet></document><document><title>Books/resources with code samples for learning about AI?</title><url>http://www.reddit.com/r/compsci/comments/1283y4/booksresources_with_code_samples_for_learning/</url><snippet>Hi
I've got AI- a modern approach but hate that it only has pseudo code. Was also surprised that khan academy doesnt have anything about AI.  I'm looking for a book that will take me through a bunch of topics with code samples ( c, lisp, python I'm fine with anything). 

Thanks!!   What's wrong with pseudo code? I actually like it much more when books don't use any one language.

Anyhow, if you want to read about lisp, Norvig also wrote: Paradigms of AI Programming: Case Studies in Common Lisp  http://www.freetechbooks.com/artificial-intelligence-f55.html The root website has a whole host of books on CS and math, in the CS category there were something like 22 books on AI alone.  

Check out a later post in this subreddit that has a link to the root. http://www.freetechbooks.com/artificial-intelligence-f55.html      </snippet></document></searchresult>